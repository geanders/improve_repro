--- 
title: "Improving the Reproducibility of Experimental Data Recording and Pre-Processing"
subtitle: "Training modules for laboratory-based researchers"
author: "Brooke Anderson, Michael Lyons, Mercedes Gonzalez-Juarrero, Marcela Henao-Tamayo, and Gregory Robertson"
site: bookdown::bookdown_site
output:
  bookdown::tufte_html_book:
    toc: yes
    css: toc.css
    tufte_variant: "envisioned"
    toc_depth: 1
  bookdown::tufte_book2:
    toc: yes
    includes:
      in_header: preamble.tex
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: no
github-repo: rstudio/bookdown-demo
description: "Online book with modules for improving the reproducibility of experimental data recording and preprocessing."
---

```{r include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tufte)
library(knitr)
```

# Rigor and reproducibility in computation

`r newthought("Science advances by building on")` previous results, an idea that
Isaac Newton captured well when he wrote, "If I have seen further it is by
standing on the shoulders of Giants." However, this base must be secure if
results from an experiment is to serve as a good foundation for future advances
[@garraway2017remember]. It is therefore critical that scientists work to ensure
that their studies are rigorous.

> "Robust findings become established over time as multiple lines of evidence
emerge. Achieving robustness takes rigour and reproducibility, plus patience
and judicious attention to the big picture." [@garraway2017remember]

Further, for scientists to build on previous work, they must be able to fully
understand that work, and even to convince themselves of the results by
reproducing key experiments before building on those results. Many of the 
best scientists---those that make the most groundbreaking discoveries---question
everything they are building on and even insist on trying to repeat some of 
the key experiments that gave the basis for their current work. They embrace
a spirit of, "Don't trust, try".

> "It should not need to be stated, but here goes. Reproducibility is the key 
underlying principle of science." [@gibb2014reproducibility]

> "We have learnt that to understand how life works, describing how the research
was done is as important as describing what was observed." [@lithgow2017long]

This is a spirit with deep roots in the scientific community. For example, 
a scientist who worked in the Enlightenment period might expect to share
a key finding not through a peer-reviewed paper, but rather by demonstrating
the experiment to other scientists in a scientific meeting. The other 
scientists would not be satisfied with only a report of the results of 
an experiment---they needed to see it with their own eyes, and in enough 
detail that they could go back and repeat it in their own laboratories. 

> "Ushering in the Enlightenment era in the late seventeenth century, chemist
Robert Boyle put forth his controversial idea of a vacuum and tasked himself
with providing descriptions of his work sufficient 'that the person I addressed
them to might, without mistake, and with as little trouble as possible, be able
to repeat such unusual experiments'. Much modern scientific communication falls
short of this standard. Most papers fail to report many aspects of the
experiment and analysis that we may not with advantage omit---things that are
crucial to understanding the result and its limitations, and to repeating the
work. We have no common language to describe this shortcoming. I’ve been in
conferences where scientists argued about whether work was reproducible,
replicable, repeatable, generalizable and other '-bles', and clearly meant quite
different things by identical terms. Contradictory meanings across disciplines
are deeply entrenched." [@stark2018before]

[Robert Koch? Paul Ehrlich?]

Scientists benefit from this level of skepticism, as it through trying to 
reproduce an experiment, the scientific community provides some checks on 
whether the initial result was rigorous and stands up under repetition. 

> "Results that generalize to all universes (or perhaps do not even require a 
universe) are part of mathematics. Results that generalize to our Universe belong
to physics. Results that generalize to all life on Earth underpin molecular 
biology. Results that generalize to all mice are murine biology. And results that
hold only for a particular mouse in a particular lab in a particular experiment
are arguably not science." [@stark2018before]

However, some of this emphasis on reproducing prior results---as well as the 
skepticism ---has become lower priority in scientific practice. One author notes:

> "The scientific community has lost the connection with the original culture of 
skepticism which existed in the 17th century with the scientists of the 
Royal Society who pioneered the scientific method as captured in their motto
*nullius in verba* ('take nobody's word'). They regarded the ability to replicate
results in independent studies as a fundamental criterion for the establishment 
of a scientific fact. Modern scientific practice presents single experiments
as proofs. When work is published, it is typically presented without self-criticism." [@neff2021past]

[Example of low level of reproducibility]

> "Additionally, the entire field of NGS analysis is in constant flux, and there
is little agreement on what is considered to be the 'best practice'. In this
situation, it is especially important to be able to reuse and to adopt various
analytical approaches reported in the literature. Unfortunately, this is often
difficult owing to the lack of necessary details. Let us look at the first and
most straightforward of the analyses: read mapping. To repeat a map- ping
experiment, it is necessary to have access to primary data and to know the
software and its version, parameter settings and name of the reference genome
build. From the 19 papers listed in BOX 1 and in Supplementary information S1
(table), only six satisfy all of these criteria. To investigate this further, we
surveyed 50 papers (BOX 2) that use the Burrows–Wheeler Aligner (BWA)15 for map-
ping (the BWA is one of the most popular mappers for Illumina data). More than
half do not provide primary data and list neither the version nor the parameters
used and neither do they list the exact version of the genomic reference
sequence. If these numbers are representative, then most results reported in
today’s publications using NGS data cannot be accurately verified, reproduced,
adopted or used to educate others, creating an alarming reproducibility crisis."
[@nekrutenko2012next]

[Need to bring this back, and how]

There are a number of different definitions of "reproducibility" that are used
across different disciplines  [@stark2018before]. In biological sciences, when
an experiment is described as being reproducible, it often means that the
experiment could be done from scratch in a different laboratory and that it
would reach the same conclusions  [@stark2018before], although there might be
some variability in the exact numerical values, stemming from natural
variability that comes from things like using different animals.

For computational research, the term "reproducible" typically means that another
researcher could get the exact results of the original study starting from the
original data collected for the experiment [@stark2018before]. In other words,
this type of reproducibility insists on a higher level of precision in matching
results, but starts at a point of replication (once the data are collected) that
ensures that this level of precise replication should be possible, as all the
steps of analysis at that point are based on computation and removes any
variability that comes from running the experiment and collecting the data.
Computational reproducibility, then, requires two things: the original data,
and very thorough instructions that describe how those data were processed and
analyzed [@nekrutenko2012next].

> "Replication of computational experiments requires access to input data sets,
source code or binaries of exact versions of software used to carry out the
initial analysis (this includes all helper scripts that are used to convert
formats, groom data, and so on) and knowing all parameter settings exactly as
they were used. In our experience, ... publications rarely provide such a level
of detail, making biomedical computational analyses almost irreproducible."
[@nekrutenko2012next]

When you make your research reproducible, you also improve the change
that it will be high impact, serving as a building block for more research, 
and receiving more citations as a result. 

> "Many classical publications in life sciences have become influential because
they provide complete information on how to repeat reported analyses so others
can adopt these approaches in their own research, such as for chain termination
sequencing technology that was developed by Sanger and colleagues and for PCR.
Today's publications that include computational analyses are very different.
Next-generation sequencing (NGS) technologies are undoubtedly as transformative
as DNA sequencing and PCR were more than 30 years abgo. As more and more
researchers use high-throughput sequencing in their research, they consult other
publications for examples of how to carry out computational analyses.
Unfortunately, they often find that the extensive informatics component that is
required to analyse NGS data makes it much more difficult to repeat studies
published today. Note that the lax standards of computational reproducibility
are not unique to life sciences; the importance of being able to repeat
computational experiments was first brought up in geosciences and became
relevant in life sciences following the establishment of microarray technology
and high-throughput sequencing." [@nekrutenko2012next]

[How this helps with rigor, computational rigor]

## Overview of these modules

The recent NIH-Wide Strategic Plan [@nih2016strategic]
describes an integrative view of biology and human health that includes
translational medicine, team science, and the importance of capitalizing on an
exponentially growing and increasingly complex data ecosystem [@nih2018data].
Underlying this view is the need to use, share, and re-use biomedical data
generated from widely varying experimental systems and researchers. Basic
sources of biomedical data range from relatively small sets of measurements,
such as animal body weights and bacterial cell counts that may be recorded by
hand, to thousands or millions of instrument-generated data points from various
imaging, -omic, and flow cytometry experiments. In either case, there is a
generally common workflow that proceeds from measurement to data recording,
pre-processing, analysis, and interpretation.  However, in practice the distinct
actions of data recording, data pre-processing, and data analysis are often
merged or combined as a single entity by the researcher using commercial or open
source spreadsheets, or as part of an often proprietary experimental measurement
system / software combination (Figure \@ref(fig:workflow)), resulting in key
failure points for reproducibility at the stages of data recording and
pre-processing. 

```{r workflow, out.width = "\\textwidth", fig.cap = "Two scenarios where 'black boxes' of non-transparent, non-reproducible data handling exist in research data workflows at the stages of data recording and pre-processing. These create potential points of failure for reproducible research. Red arrows indicate where data is passed to other research team members, including statisticians / data analysts, often within complex or unstructured spreadsheet files."}
knitr::include_graphics("figures/existing_blackboxes.jpg")
```

It is widely known and discussed among data scientists, mathematical modelers,
and statisticians [@broman2018data; @krishnan2016towards] that there is
frequently a need to discard, transform, and reformat various elements of the
data shared with them by laboratory-based researchers, and that data is often
shared in an unstructured format, increasing the risks of introducing errors
through reformatting before applying more advanced computational methods.
Instead, a critical need for reproducibility is for the transparent and clear
sharing across research teams of: (1) raw data, directly from hand-recording or
directly output from experimental equipment; (2) data that has been
pre-processed as necessary (e.g., gating for flow cytometry data, feature
identification for metabolomics data), saved in a consistent, structured format,
and (3) a clear and repeatable description of how the pre-processed data was
generated from the raw data [@broman2018data; @ellis2018share].

To enhance data reproducibility, it is critical to create a clear separation
among data recording, data pre-processing, and data analysis---breaking up
commonly existing ``black boxes" in data handling across the research process.
Such a rigorous demarcation requires some change in the conventional
understanding and use of spreadsheets and a recognition by biomedical
researchers that recent advances in computer programming languages, especially
the R programming language, provide user-friendly and accessible tools and
concepts that can be used to extend a transparent and reproducible data workflow
to the steps of data recording and pre-processing. Among our team, we have found
that there are many common existing practices---including use of spreadsheets
with embedded formulas that concurrently record and analyze experimental data,
problematic management of project files, and reliance on proprietary,
vendor-supplied point-and-click software for data pre-processing---that can
interfere with the transparency, reproducibility, and efficiency of
laboratory-based biomedical research projects, problems that have also been
identified by others as key barriers to research reproducibility
[@broman2018data; @bryan2018excuse; @ellis2018share; @marwick2018packaging]. In
these training modules,  we have choosen topics that tackle barriers to
reproducibility that have straightforward, easy-to-teach solutions, but which
are still very common in biomedical laboratory-based research programs.



## License

This book is licensed under the [Creative Commons
Attribution-NonCommercial-ShareAlike 4.0 International
License](https://creativecommons.org/licenses/by-nc-sa/4.0/), while all code in
the book is under the [MIT license](https://opensource.org/licenses/MIT).

Click on the **Next** button (or navigate using the
links at the top of the page) to continue.

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown', 'ggplot2', 'readr',
  'forcats', 'magrittr', 'dplyr', 'sf', 'htmlwidgets', 'DT', 'plotly', 
  'leaflet', 'flexdashboard'
), 'packages.bib')
```

