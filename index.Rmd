--- 
title: "Improving the Reproducibility of Experimental Data Recording and Pre-Processing"
subtitle: "Training modules for laboratory-based researchers"
author: "Brooke Anderson, Michael Lyons, Mercedes Gonzalez-Juarrero, Marcela Henao-Tamayo, and Gregory Robertson"
site: bookdown::bookdown_site
output:
  bookdown::tufte_html_book:
    toc: yes
    css: toc.css
    tufte_variant: "envisioned"
    toc_depth: 1
  bookdown::tufte_book2:
    toc: yes
    includes:
      in_header: preamble.tex
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: no
github-repo: rstudio/bookdown-demo
description: "Online book with modules for improving the reproducibility of experimental data recording and preprocessing."
---

```{r include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tufte)
library(knitr)
```

# Rigor and reproducibility in computation

`r newthought("Science advances by building on")` previous results, an idea that
Isaac Newton captured well when he wrote, "If I have seen further it is by
standing on the shoulders of Giants." However, this base must be secure if
results from an experiment is to serve as a good foundation for future advances
[@garraway2017remember]. It is therefore critical that scientists work to ensure
that their studies are rigorous.

> "Robust findings become established over time as multiple lines of evidence
emerge. Achieving robustness takes rigour and reproducibility, plus patience
and judicious attention to the big picture." [@garraway2017remember]

Further, for scientists to build on previous work, they must be able to fully
understand that work, and even to convince themselves of the results by
reproducing key experiments before building on those results. Many of the 
best scientists---those that make the most groundbreaking discoveries---question
everything they are building on and even insist on trying to repeat some of 
the key experiments that gave the basis for their current work. They embrace
a spirit of, "Don't trust, try".

> "It should not need to be stated, but here goes. Reproducibility is the key 
underlying principle of science." [@gibb2014reproducibility]

> "We have learnt that to understand how life works, describing how the research
was done is as important as describing what was observed." [@lithgow2017long]

This is a spirit with deep roots in the scientific community. For example, 
a scientist who worked in the Enlightenment period might expect to share
a key finding not through a peer-reviewed paper, but rather by demonstrating
the experiment to other scientists in a scientific meeting. The other 
scientists would not be satisfied with only a report of the results of 
an experiment---they needed to see it with their own eyes, and in enough 
detail that they could go back and repeat it in their own laboratories. 

> "Ushering in the Enlightenment era in the late seventeenth century, chemist
Robert Boyle put forth his controversial idea of a vacuum and tasked himself
with providing descriptions of his work sufficient 'that the person I addressed
them to might, without mistake, and with as little trouble as possible, be able
to repeat such unusual experiments'. Much modern scientific communication falls
short of this standard. Most papers fail to report many aspects of the
experiment and analysis that we may not with advantage omit---things that are
crucial to understanding the result and its limitations, and to repeating the
work. We have no common language to describe this shortcoming. I’ve been in
conferences where scientists argued about whether work was reproducible,
replicable, repeatable, generalizable and other '-bles', and clearly meant quite
different things by identical terms. Contradictory meanings across disciplines
are deeply entrenched." [@stark2018before]

[Robert Koch? Paul Ehrlich?]

Scientists benefit from this level of skepticism, as it through trying to 
reproduce an experiment, the scientific community provides some checks on 
whether the initial result was rigorous and stands up under repetition. 

> "Results that generalize to all universes (or perhaps do not even require a 
universe) are part of mathematics. Results that generalize to our Universe belong
to physics. Results that generalize to all life on Earth underpin molecular 
biology. Results that generalize to all mice are murine biology. And results that
hold only for a particular mouse in a particular lab in a particular experiment
are arguably not science." [@stark2018before]

However, some of this emphasis on reproducing prior results---as well as the 
skepticism ---has become lower priority in scientific practice. One author notes:

> "The scientific community has lost the connection with the original culture of 
skepticism which existed in the 17th century with the scientists of the 
Royal Society who pioneered the scientific method as captured in their motto
*nullius in verba* ('take nobody's word'). They regarded the ability to replicate
results in independent studies as a fundamental criterion for the establishment 
of a scientific fact. Modern scientific practice presents single experiments
as proofs. When work is published, it is typically presented without self-criticism." [@neff2021past]

[Example of low level of reproducibility]

> "Additionally, the entire field of NGS analysis is in constant flux, and there
is little agreement on what is considered to be the 'best practice'. In this
situation, it is especially important to be able to reuse and to adopt various
analytical approaches reported in the literature. Unfortunately, this is often
difficult owing to the lack of necessary details. Let us look at the first and
most straightforward of the analyses: read mapping. To repeat a map- ping
experiment, it is necessary to have access to primary data and to know the
software and its version, parameter settings and name of the reference genome
build. From the 19 papers listed in BOX 1 and in Supplementary information S1
(table), only six satisfy all of these criteria. To investigate this further, we
surveyed 50 papers (BOX 2) that use the Burrows–Wheeler Aligner (BWA)15 for map-
ping (the BWA is one of the most popular mappers for Illumina data). More than
half do not provide primary data and list neither the version nor the parameters
used and neither do they list the exact version of the genomic reference
sequence. If these numbers are representative, then most results reported in
today’s publications using NGS data cannot be accurately verified, reproduced,
adopted or used to educate others, creating an alarming reproducibility crisis."
[@nekrutenko2012next]

[Need to bring this back, and how]

There are a number of different definitions of "reproducibility" that are used
across different disciplines  [@stark2018before]. In biological sciences, when
an experiment is described as being reproducible, it often means that the
experiment could be done from scratch in a different laboratory and that it
would reach the same conclusions  [@stark2018before], although there might be
some variability in the exact numerical values, stemming from natural
variability that comes from things like using different animals.

For computational research, the term "reproducible" typically means that another
researcher could get the exact results of the original study starting from the
original data collected for the experiment [@stark2018before]. In other words,
this type of reproducibility insists on a higher level of precision in matching
results, but starts at a point of replication (once the data are collected) that
ensures that this level of precise replication should be possible, as all the
steps of analysis at that point are based on computation and removes any
variability that comes from running the experiment and collecting the data.
Computational reproducibility, then, requires two things: the original data,
and very thorough instructions that describe how those data were processed and
analyzed [@nekrutenko2012next].

> "Replication of computational experiments requires access to input data sets,
source code or binaries of exact versions of software used to carry out the
initial analysis (this includes all helper scripts that are used to convert
formats, groom data, and so on) and knowing all parameter settings exactly as
they were used. In our experience, ... publications rarely provide such a level
of detail, making biomedical computational analyses almost irreproducible."
[@nekrutenko2012next]

When you make your research reproducible, you also improve the change
that it will be high impact, serving as a building block for more research, 
and receiving more citations as a result. 

> "Many classical publications in life sciences have become influential because
they provide complete information on how to repeat reported analyses so others
can adopt these approaches in their own research, such as for chain termination
sequencing technology that was developed by Sanger and colleagues and for PCR.
Today's publications that include computational analyses are very different.
Next-generation sequencing (NGS) technologies are undoubtedly as transformative
as DNA sequencing and PCR were more than 30 years abgo. As more and more
researchers use high-throughput sequencing in their research, they consult other
publications for examples of how to carry out computational analyses.
Unfortunately, they often find that the extensive informatics component that is
required to analyse NGS data makes it much more difficult to repeat studies
published today. Note that the lax standards of computational reproducibility
are not unique to life sciences; the importance of being able to repeat
computational experiments was first brought up in geosciences and became
relevant in life sciences following the establishment of microarray technology
and high-throughput sequencing." [@nekrutenko2012next]

[How this helps with rigor, computational rigor]

## Overview of these modules

The recent NIH-Wide Strategic Plan [@nih2016strategic]
describes an integrative view of biology and human health that includes
translational medicine, team science, and the importance of capitalizing on an
exponentially growing and increasingly complex data ecosystem [@nih2018data].
Underlying this view is the need to use, share, and re-use biomedical data
generated from widely varying experimental systems and researchers. Basic
sources of biomedical data range from relatively small sets of measurements,
such as animal body weights and bacterial cell counts that may be recorded by
hand, to thousands or millions of instrument-generated data points from various
imaging, -omic, and flow cytometry experiments. In either case, there is a
generally common workflow that proceeds from measurement to data recording,
pre-processing, analysis, and interpretation.  However, in practice the distinct
actions of data recording, data pre-processing, and data analysis are often
merged or combined as a single entity by the researcher using commercial or open
source spreadsheets, or as part of an often proprietary experimental measurement
system / software combination (Figure \@ref(fig:workflow)), resulting in key
failure points for reproducibility at the stages of data recording and
pre-processing. 

```{r workflow, out.width = "\\textwidth", fig.cap = "Two scenarios where 'black boxes' of non-transparent, non-reproducible data handling exist in research data workflows at the stages of data recording and pre-processing. These create potential points of failure for reproducible research. Red arrows indicate where data is passed to other research team members, including statisticians / data analysts, often within complex or unstructured spreadsheet files."}
knitr::include_graphics("figures/existing_blackboxes.jpg")
```

It is widely known and discussed among data scientists, mathematical modelers,
and statisticians [@broman2018data; @krishnan2016towards] that there is
frequently a need to discard, transform, and reformat various elements of the
data shared with them by laboratory-based researchers, and that data is often
shared in an unstructured format, increasing the risks of introducing errors
through reformatting before applying more advanced computational methods.
Instead, a critical need for reproducibility is for the transparent and clear
sharing across research teams of: (1) raw data, directly from hand-recording or
directly output from experimental equipment; (2) data that has been
pre-processed as necessary (e.g., gating for flow cytometry data, feature
identification for metabolomics data), saved in a consistent, structured format,
and (3) a clear and repeatable description of how the pre-processed data was
generated from the raw data [@broman2018data; @ellis2018share].

To enhance data reproducibility, it is critical to create a clear separation
among data recording, data pre-processing, and data analysis---breaking up
commonly existing ``black boxes" in data handling across the research process.
Such a rigorous demarcation requires some change in the conventional
understanding and use of spreadsheets and a recognition by biomedical
researchers that recent advances in computer programming languages, especially
the R programming language, provide user-friendly and accessible tools and
concepts that can be used to extend a transparent and reproducible data workflow
to the steps of data recording and pre-processing. Among our team, we have found
that there are many common existing practices---including use of spreadsheets
with embedded formulas that concurrently record and analyze experimental data,
problematic management of project files, and reliance on proprietary,
vendor-supplied point-and-click software for data pre-processing---that can
interfere with the transparency, reproducibility, and efficiency of
laboratory-based biomedical research projects, problems that have also been
identified by others as key barriers to research reproducibility
[@broman2018data; @bryan2018excuse; @ellis2018share; @marwick2018packaging]. In
these training modules,  we have choosen topics that tackle barriers to
reproducibility that have straightforward, easy-to-teach solutions, but which
are still very common in biomedical laboratory-based research programs.

----------------------------------------------------------------------------

> "Today, one often hears that life sciences are faced with the 'big data problem.'
However, data are just a small facet of a much bigger challenge. The true 
difficulty is that most biomedical researchers have no capacity to carry out
analyses of modern data sets using appropriate tools and computational infrastructure
in a way that can be fully understood and reused by others. This struggle began with 
the introduction of microarray technology, which, for the first time, introduced
life sciences to truly large amounts of data and the need for quantitative training. 
What is new, however, is that next-generation sequencing (NGS) has made this problem
vastly more challenging. Today's sequencing-based experiments generate substantially 
more data and are more broadly applicable than microarray technology, allowing for
various novel functional assays, including quantification of protein--DNA binding or
histone modifications (using chromatin immunoprecipitation followed by high-throughput
sequencing (ChIP-seq)), transcript levels (using RNA sequencing (RNA-seq)), spatial
interactions (using Hi-C) and others. These individual applications can be combined
into larger studies, such as the recently published genomic profiling of a human
individual whose genome was sequenced and gene expression tracked over an extended
period in a series of RNA-seq experiments. As a result, meaningful interpretation 
of sequencing data has become particularly important. Yet such interpretation 
relies heavily on complex computation---a new and unfamiliar domain to many of our
biomedical colleagues---which, unlike data generation, is not univerally 
accessible to everyone." [@nekrutenko2012next]

> "We note that this discussion thus far has dealt only with technical reproducibility
challenges or with the ability to repeat published analyses using the original data 
to verify the results. Most biomedical researchers are much more acquainted with 
biological reproducibility, in which conceptual results are verified by an 
alternative analysis of different samples. However, we argue that the computational 
nature of modern biology blurs the distinction between technical and biological 
reproducibility." [@nekrutenko2012next]

> "The overwhelming majority of currently published papers using NGS technologies include
analyses that are not detailed ... Moreover, the computational approaches used in these
publications cannot be readily reused by others." [@nekrutenko2012next]

> "Many classical publications in life sciences have become influential because they 
provide complete information on how to repeat reported analyses so others can adopt
these approaches in their own research, such as for chain termination sequencing 
technology that was developed by Sanger and colleagues and for PCR. Today's publications
that include computational analyses are very different. Next-generation sequencing (NGS) 
technologies are undoubtedly as transformative as DNA sequencing and PCR were more than
30 years abgo. As more and more researchers use high-throughput sequencing in their 
research, they consult other publications for examples of how to carry out computational
analyses. Unfortunately, they often find that the extensive informatics component that 
is required to analyse NGS data makes it much more difficult to repeat studies published
today. Note that the lax standards of computational reproducibility are not unique to 
life sciences; the importance of being able to repeat computational experiments was
first brought up in geosciences and became relevant in life sciences following the
establishment of microarray technology and high-throughput sequencing. Replication 
of computational experiments requires access to input data sets, source code or binaries 
of exact versions of software used to carry out the initial analysis (this includes
all helper scripts that are used to convert formats, groom data, and so on) and 
knowing all parameter settings exactly as they were used. In our experience, ...
publications rarely provide such a level of detail, making biomedical computational 
analyses almost irreproducible." [@nekrutenko2012next]

> "The biological sciences have depended on other, less-reliable techniques 
for reproducbility. The most long-standing is the assumption that reproducibility
studies will occur organically as different researchers work on related problems.
In the past five years or so, funding agencies and journals have implemented 
more-stringent experimental-reporting and data-availablility requirements for 
grant proposals and submitted manuscripts. A handful of initiatives have attempted
to replicate published studies. The peer-reviewed 'Journal of Visualized Experiments'
creates videos to disseminate details that are hard to convey in conventional 
methods sections. Yet pitfalls persist. Scientists might waste resources trying
to build on unproven techniques. And real discoveries can be labelled irreproducible
because too few resources are available to conduct a validation." [@raphael2020controlled]

> "The scientific community has lost the connection with the original culture of 
skepticism which existed in the 17th century with the scientists of the 
Royal Society who pioneered the scientific method as captured in their motton
nullius in verba ('take nobody's word'). They regarded the ability to replicate
results in independent studies as a fundamental criterion for the establishment 
of a scientific fact. Modern scientific practice presents single experiments
as proofs. When work is published, it is typically presented without self-criticism."
[@neff2021past]

> "Transparency and quality management are key to improving scientific rigor and 
reproducibility. It is, therefore, good to see that the Open Science movement
is gaining traction, and that research institutions increasingly view poor
science as a reputation risk." [@neff2021past]

> "We scientists search tenaciously for information about how nature works through
reason and experimentation. Who can deny the magnitude of knowledge we have 
gleaned, its acceleration over time, and its expanding positive impact on society?
Of course, some data and models are fragile, and our understanding remains 
punctuated by false premises. Holding fast to the three Rs [rigor, reproducibility, 
and robustness] ensures that the path---although tortuous and treacherous at times---remains
well lit." [@garraway2017remember]

> "We have learnt that to understand how life works, describing how the research was
done is as important as describing what was observed." [@lithgow2017long]

> "In computational science, 'reproducible' often means that enough information 
is provided to allow a dedicated reader to repeat the calculations in the paper for
herself. In biomedical disciplines, 'reproducible' often means that a different lab, 
starting the experiment from scratch, would get roughly the same experimental 
result." [@stark2018before]

> "Results that generalize to all universes (or perhaps do not even require a 
universe) are part of mathematics. Results that generalize to our Universe belong
to physics. Results that generalize to all life on Earth underpin molecular 
biology. Results that generalize to all mice are murine biology. And results that
hold only for a particular mouse in a particular lab in a particular experiment
are arguably not science." [@stark2018before]

> "It should not need to be stated, but here goes. Reproducibility is the key 
underlying principle of science." [@gibb2014reproducibility]


> "Popularized by statistician John W. Tukey, EDA is an approach that emphasizes 
understanding data (and its limitations) through interactive investigation
rather than explicit statitical modeling. In his 1977 book *Exploratory Data
Analysis*, Tukey described EDA as 'detective work' involved in 'finding and 
revealing the clues' in data. As Tukey's quote emphasizes, EDA is much more an approach
to exploring data than using specific statistical methods. In the face of rapidly 
changing sequencing technologies, bioinformatics software, and statistical methods, 
EDA skills are not only widely applicable and comparatively stable---they're also 
essential to making sure that our analyses are robust to these new data and methods."
[@buffalo2015bioinformatics]

----------------------------------------------------------------------------


## License

This book is licensed under the [Creative Commons
Attribution-NonCommercial-ShareAlike 4.0 International
License](https://creativecommons.org/licenses/by-nc-sa/4.0/), while all code in
the book is under the [MIT license](https://opensource.org/licenses/MIT).

Click on the **Next** button (or navigate using the
links at the top of the page) to continue.

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown', 'ggplot2', 'readr',
  'forcats', 'magrittr', 'dplyr', 'sf', 'htmlwidgets', 'DT', 'plotly', 
  'leaflet', 'flexdashboard'
), 'packages.bib')
```

