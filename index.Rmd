--- 
title: "Improving the Reproducibility of Experimental Data Recording and Pre-Processing"
subtitle: "Training modules for laboratory-based researchers"
author: "Brooke Anderson, Michael Lyons, Mercedes Gonzalez-Juarrero, Marcela Henao-Tamayo, and Gregory Robertson"
site: bookdown::bookdown_site
output:
  bookdown::tufte_html_book:
    toc: yes
    css: toc.css
    tufte_variant: "envisioned"
  bookdown::tufte_book2:
    toc: yes
    includes:
      in_header: preamble.tex
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: no
github-repo: rstudio/bookdown-demo
description: "Online book with modules for improving the reproducibility of experimental data recording and preprocessing."
---

```{r include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tufte)
library(knitr)
```

# Overview

`r newthought("The recent NIH-Wide Strategic Plan")` [@nih2016strategic]
describes an integrative view of biology and human health that includes
translational medicine, team science, and the importance of capitalizing on an
exponentially growing and increasingly complex data ecosystem [@nih2018data].
Underlying this view is the need to use, share, and re-use biomedical data
generated from widely varying experimental systems and researchers. Basic
sources of biomedical data range from relatively small sets of measurements,
such as animal body weights and bacterial cell counts that may be recorded by
hand, to thousands or millions of instrument-generated data points from various
imaging, -omic, and flow cytometry experiments. In either case, there is a
generally common workflow that proceeds from measurement to data recording,
pre-processing, analysis, and interpretation.  However, in practice the distinct
actions of data recording, data pre-processing, and data analysis are often
merged or combined as a single entity by the researcher using commercial or open
source spreadsheets, or as part of an often proprietary experimental measurement
system / software combination (Figure \@ref(fig:workflow)), resulting in key
failure points for reproducibility at the stages of data recording and
pre-processing. 

```{r workflow, out.width = "\\textwidth", fig.cap = "Two scenarios where 'black boxes' of non-transparent, non-reproducible data handling exist in research data workflows at the stages of data recording and pre-processing. These create potential points of failure for reproducible research. Red arrows indicate where data is passed to other research team members, including statisticians / data analysts, often within complex or unstructured spreadsheet files."}
knitr::include_graphics("figures/existing_blackboxes.jpg")
```

It is widely known and discussed among data scientists, mathematical modelers,
and statisticians [@broman2018data; @krishnan2016towards] that there is
frequently a need to discard, transform, and reformat various elements of the
data shared with them by laboratory-based researchers, and that data is often
shared in an unstructured format, increasing the risks of introducing errors
through reformatting before applying more advanced computational methods.
Instead, a critical need for reproducibility is for the transparent and clear
sharing across research teams of: (1) raw data, directly from hand-recording or
directly output from experimental equipment; (2) data that has been
pre-processed as necessary (e.g., gating for flow cytometry data, feature
identification for metabolomics data), saved in a consistent, structured format,
and (3) a clear and repeatable description of how the pre-processed data was
generated from the raw data [@broman2018data; @ellis2018share].

To enhance data reproducibility, it is critical to create a clear separation
among data recording, data pre-processing, and data analysis---breaking up
commonly existing ``black boxes" in data handling across the research process.
Such a rigorous demarcation requires some change in the conventional
understanding and use of spreadsheets and a recognition by biomedical
researchers that recent advances in computer programming languages, especially
the R programming language, provide user-friendly and accessible tools and
concepts that can be used to extend a transparent and reproducible data workflow
to the steps of data recording and pre-processing. Among our team, we have found
that there are many common existing practices---including use of spreadsheets
with embedded formulas that concurrently record and analyze experimental data,
problematic management of project files, and reliance on proprietary,
vendor-supplied point-and-click software for data pre-processing---that can
interfere with the transparency, reproducibility, and efficiency of
laboratory-based biomedical research projects, problems that have also been
identified by others as key barriers to research reproducibility
[@broman2018data; @bryan2018excuse; @ellis2018share; @marwick2018packaging]. In
these training modules,  we have choosen topics that tackle barriers to
reproducibility that have straightforward, easy-to-teach solutions, but which
are still very common in biomedical laboratory-based research programs.

## License

This book is licensed under the [Creative Commons
Attribution-NonCommercial-ShareAlike 4.0 International
License](https://creativecommons.org/licenses/by-nc-sa/4.0/), while all code in
the book is under the [MIT license](https://opensource.org/licenses/MIT).

Click on the **Next** button (or navigate using the
links at the top of the page) to continue.

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown', 'ggplot2', 'readr',
  'forcats', 'magrittr', 'dplyr', 'sf', 'htmlwidgets', 'DT', 'plotly', 
  'leaflet', 'flexdashboard'
), 'packages.bib')
```

<!---

### Extra quotes for future revisions

> "Many journals provide mechanisms to make reproducibility possible, including
*PLoS*, *Nature*, and *Science*. This entails ensuring access to the computer 
code and datasets used to produce the results of a study. In contrast, replication of
scientific findings involves research conducted by independent researchers using 
their own methods, data, and equipment that validate or confirm the findings of
earlier studies. Replication is not always possible, however, so reproducibility 
is a minimum and necessaray standard for confirming scientific findings." [@keller2017evolution]

> "Reproducibility goes well beyond validating statistical results and includes empirical, 
computational, ethical, and statistical analyses. For example, empirical reproducibility 
ensures that the same results are obtained from the data and code used in the original 
study, and statistical reproducibility focuses on statistical design and analysis to 
ensure replication of an experiment. There are also definitions of ethical reproducibility, 
such as documenting the methods used in biomedical research or in social and behavioral 
science research so others can reproduce algorithms used in analysis." [@keller2017evolution]

> "Many studies have been undertaken to understand the reproducibility of scientific findings
and have come to different conclusions about the findings. For example, one scientist argues
that half of all scientific discoveries are false (Ioannidis 2005), others find that a large
portion of the reproduced findings produce weaker evidence compared with the original findings
(Nosek et al. 2015), and others find that 4/5 of the results are true positives (Jager and Leek 2013).
... Despite this controversy, the premise underlying reproducibility is data quality in the form of 
good experimental design and execution, documentation, and making scientific inputs available
for reproducing the scientific work." [@keller2017evolution]

Some aspects of data quality and analysis innovations have arisen from science, but some come 
instead from engineering, computer science, and business. Leveraging what has been found in 
all these areas can help improve the quality and reproducibility of laboratory-based research 
projects. 

> "In the computer science, engineering, and business worlds, data quality management focuses 
largely on administrative data and is driven by the need to have accurate, reliable data for daily 
operations. The kinds of data traditionally discussed in this data quality literature are
fundamental to the functioning of an organization---if the data are bad, ifrms will lose money or
defective products will be manufactured. The advent of data quality in the engineering and 
business worlds traces back to the 1940s and 1950s with Edward Deming and Joseph Juran.
Japanese companies embraced these methods and transformed their business practices using them.
Deming's approach used statistical process control that focused on measuring inputs and 
processes and thus minimized product inspections after a product was build." [@keller2017evolution]

> [In business, ] "Data quality is further defined from the perspective of the ease of use of the
data with respect to the integrity, accuracy, interpretability, and value assessed by the data
user and other attributes that make the data valuable." [@keller2017evolution]

--->
