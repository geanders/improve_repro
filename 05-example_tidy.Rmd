## Example: Creating a template for "tidy" data collection {#module5}

We will walk through an example of creating a template to collect data in a
"tidy" format for a laboratory-based research project, based on a research
project on drug efficacy in murine tuberculosis models. We will show the initial
"untidy" format for data recording and show how we converted it to a "tidy"
format. Finally, we will show how the data can then easily be analyzed and
visualized using reproducible tools.

**Objectives.** After this module, the trainee will be able to:

- Understand how the principles of "tidy" data can be applied for a real, complex research project;
- List advantages of the "tidy" data format for the example project 

In the last module, we covered three principles for designing tidy templates for 
data collection in a biomedical laboratory, motivated by an example dataset from 
a real experiment. In this module, we'll show you how to apply those principles
to create a tidier template for the example dataset from the last module.
As a reminder, those three principles are:

1. Limit the template to the collection of data.
2. Make sensible choices when dividing data collection into rows and columns.
3. Avoid characters or formatting that will make it hard for a computer program to process the data.

It is important to note that there's no reason that you can't continue to use a
spreadsheet program like Excel or Google Sheets to collect data. The spreadsheet
program itself can easily be used to create a simple template to use as you
collect data. In fact, we'll continue using a spreadsheet format in the rest of
this module and in the next one as we show how to redesign the data collection
for this example experiment. It is important, however, to think through how you
will arrange that template spreadsheet to make it most useful in the larger
context of reproducible research.

### Example data---Data on rate of bacterial growth 

Here, we'll walk through an example using real data collected in a laboratory
experiment. We described these data in detail in the previous module. As a
reminder, they were collected to measure the growth rate of *Mycobacteria
tuberculosis* under two conditions---high oxygen and low oxygen. They were
collected from five test tubes that were measured regularly over one week for
bacteria growth using a measure of optical density. Figure
\@ref(fig:growthexcel2) shows the original template that the research group used
to record these data.

```{r growthexcel2, fig.fullwidth = TRUE, echo = FALSE, out.width = "\\textwidth", fig.cap = "Example of an Excel spreadsheet used to record and analyze data for a laboratory experiment. Annotations highlight where data is entered by hand, where calculations are done by hand, and where embedded Excel formulas are used. The figures are created automatically using values in a specified column."}
knitr::include_graphics("figures/growth_curve_example.png")
```

In the previous module, we described features that make this template "untidy"
and potentially problematic to include in a larger pipeline of reproducible
research. In the next few sections of this module, we'll walk step-by-step
through changes that you could make to make this template tidier. We'll finish
the module by showing how you could then easily design a further step of the
analysis pipeline to visualize and analyze the collected data, so that the
advantages of real-time plotting from the more complex spreadsheet are not
missed when moving to a tidier template.

### Limiting the template to the collection of data

The example template (Figure \@ref(fig:growthexcel2)) includes a number of
"extra" elements beyond simple data collection---all the elements outside rows
1--15 of columns A--I. Outside this area of the original spread, there are a
number of extra elements, including plots that visualize the data, summaries
generated based on the data (rows 16--18, for example), notes about the data,
and even a macro (top right) that wasn't involved in data collection but instead
was used by the researcher to calculate the initial volume of inoculum to
include in each test tube. None of these "extras" can be easily read into a 
statistical program like R or Python---at best, they will be ignored by the program.
They can even complicate reading in the cells with measurements (rows
1--15 of columns A--I), as most statistical programs will try to read in all the 
non-empty cells of a spreadsheet unless directed otherwise. 

A good starting point, then, would be to start designing a tidy data collection
template for this experiment by extracting only the content from the box in
Figure \@ref(fig:extractraw). This would result in a template that looks like
Figure \@ref(fig:step1).

```{r step1, fig.fullwidth = TRUE, echo = FALSE, out.width = "\\textwidth", fig.cap = "First step in designing a tidy data collection template for the example project. A template has been created that focuses only on the raw data, removing all extra elements like plots, notes, macros, and summaries."}
knitr::include_graphics("figures/growth_curve_step1.png")
```

Notice that we've also removed any of the color formatting from the spreadsheet. It is fine to 
keep color in the spreadsheet if it will help the research to find the right spot to record data 
while working in the laboratory, but you should make sure that you're not using it to encode 
information about the data---all color formatting will be ignored when the data are read by a 
statistical program like R.

While the template shown in Figure \@ref(fig:step1) has removed a lot of the calculated values from the
original template, it has not removed all of them. Two of the columns are still values that were 
determined by calculation after the original data were collected. Column B and column D both provide
measures of the length of time since the start of the experiment, and both are calculated by 
comparing a measurement time to the time at the start of the experiment. 

The time since the start of the experiment can easily be calculated later in the analysis pipeline, 
once you read the data into a statistical program like R. By delaying this step, you can both 
simplify the data collection template (requiring fewer columns for the research in the laboratory 
to fill out) and also avoid the chance for mistakes, which could occur both in the hand calculations
of these values and in data entry, when the researcher enters the results of the calculations in the
spreadsheet cell. Figure \@ref(fig:step2) shows a new version of the template, where these calculated
columns have been removed. This template is now restricted to only data points originally collected 
in the course of the experiment, and has removed all elements that are based on calculations or other 
derivatives of those original, raw data points.

```{r step2, fig.fullwidth = FALSE, echo = FALSE, out.width = "\\textwidth", fig.cap = "Second step in designing a tidy data collection template for the example project. This template started from the previous one, but removed columns that were hand-calculated and then entered by the researcher in the previous template. This version has removed all calculated values on the template, limiting it to only the original recorded values required for the experiment."}
knitr::include_graphics("figures/growth_curve_step2.png")
```

### Making sensible choices about rows and columns

The second principle is to **make sensible choices when dividing data collection
into rows and columns**. There are many different ways that you could spread the
data collection into rows and columns, and in this step, you can consider which
method would meet a reasonable balance between making the template easy for the
researcher in the laboratory to use to record data and also making the resulting
data file easy to incorporate in a reproducible data analysis pipeline. 

For the example experiment, Figure \@ref(fig:extractraw) shows three examples 
that we can consider for how to arrange data collection across rows and columns.
All three build on the changes we made in the earlier step of "tidying" the template, 
which resulted in the template shown in Figure \@ref(fig:step2).

```{r columnoptions, fig.fullwidth = TRUE, echo = FALSE, out.width = "\\textwidth", fig.cap = "Examples of ways that data collection could be divided into rows and columns in the example template. Panel A shows an example where date and time are recorded in different columns. Panel B is similar to Panel A, but in this case, date and time are recorded in a single column. Panel C shows a classically 'tidy' data format, where each measurement date-time is repeated for each of the five test tubes, and columns give the test tube ID and absorbance measurement at that time for that tube (only part of the data is shown for this format, while remaining rows are off the page). While Panel C provides the 'tidiest' format, it may have some practical constraints when used in a laboratory setting. For example, it would require more data entry during data collection (since date-time is entered five times at each measurement time), and its long format prevent it all from being seen at once without scrolling on a computer screen."}
knitr::include_graphics("figures/growth_curve_column_options.png")
```

Panel A (an exact repeat of the template shown in Figure \@ref(fig:step2)) shows
an example where date and time are recorded in different columns. Panel B is
similar to Panel A, but in this case, date and time are recorded in a single
column. Panel C shows a classically "tidy" data format, where each measurement's
date-time is repeated for each of the five test tubes, and columns give the test
tube ID and absorbance measurement at that time for that tube (only part of the
data is shown for this format, while remaining rows are off the page). 

In this example, the template that may be the most reasonable is the one shown
in Panel B. While Panel C provides the "tidiest" format, it has some practical
constraints when used in a laboratory setting. For example, it would require
more data entry during data collection (since date-time is entered five times at
each measurement time), and its long format prevent it all from being seen at
once without scrolling on a computer screen. When comparing Panels A and B, the
template in Panel B has an advantage. The information on date and time are
useful together, but not individually. For example, to calculate the time since
the start of the experiment, you cannot just calculate the difference in dates
or just the difference in times, but instead must consider both the date and
time of the measurement in comparison to the date and time of the start of the
experiment. As a result, at some point in the data analysis pipeline, you'll
need to combine information about the date and the time to make use of the two
elements. While this combination of two columns can be easily done within a
statistical program like R, it can also be directly designed into the original
template for collecting the data. Therefore, unless there is a practical reason
why it would be easier for the researcher to enter date and time separately, the
template shown in Panel B is preferable to that shown in Panel A in terms of
allowing for the "tidy" collection of research data into a file that is easy to
include in a reproducible pipeline. Figure \@ref(fig:step3) shows the template
design at this stage in the process of tidying it, highlighting the column that
combines date and time elements in a single column. In this version of the
template, we've also been careful about how date and time are recorded, a
consideration that we'll discuss more in the next section.

```{r step3, fig.fullwidth = TRUE, echo = FALSE, out.width = "\\textwidth", fig.cap = "Third step in designing a tidy data collection template for the example project. This template started from the previous one, but combined collection of the date and time of the measurement into a single column and revised the format to include all date elements and to prevent automatic conversion by the spreadsheet program."}
knitr::include_graphics("figures/growth_curve_step3.png")
```

### Avoiding problematic characters or formatting

The third principle is to **avoid characters or formatting that will make it
hard for a computer program to process the data**. There are a number of special 
characters and formatting conventions that can be hard for a statistical program to 
handle. In the example template shown in Figure \@ref(fig:step3), for example, 
the column names include spaces (for example, in "Date and time"), as well as 
parentheses (for example, in "VA 001 (A1)"). While most statistical programs have 
tools that allow you to handle and convert these characters once the data are 
read in, it's even simpler to use simpler column names in the original data collection
template, and this will save some extra coding further along in the analysis pipeline.
Two general rules for creating easy-to-use column names in a data collection template
are: (1) start each column name with a letter and (2) for the rest of the column 
name, use only letters, numbers, or the underscore character ("_"). For example, 
"aerated1" would work well, but "1--aerated" would not.

Within the cell values below the column names, there is more flexibility. For example, 
if you have a column that gives the IDs of different samples, it would be fine to include
spaces and other characters in those IDs. 

There are a few exceptions, however. A big one 
is with values that record dates or date-time combinations. First, it is important to include
all elements of the date (or date and time, if both are recorded). For example, the year 
should be included in the recorded date, even if the experiment only took a few days. 
This is because statistical programs have excellent functions for working with data that
are dates or date-times, but to take advantage of these, the data must be converted into 
a special class in the program, and conversion to that class requires specific elements
(for example, a date must include the year, month, and day of month). 

Second, it is 
useful to avoid recording dates and date-times in a way that results in a spreadsheet 
program automatically converting them. Surrounding the information about a date in 
quotation marks when entering it (as shown in Figure \@ref(fig:step3)) can avoid this.

Finally, consider using a format to record the date that is unambiguous and so less likely
to have recording errors. Dates, for example, are sometimes recorded using only numbers---for
example, the first date of "July 9, 2019" in the example data could be recorded as 
"7/9/2019" or "7/9/19", to be even more concise. However, this format has some ambiguity.
It can be unclear if this refers to July 9 or to September 7, both of which could be 
written as "7/9". For the version that uses two digits for the year, it can be unclear 
if the date is for 2019 or 1919 (or any other century). Using the format "July 9, 2019", 
as done in the latest version of the sample template, avoids this potential ambiguity.

Figure \@ref(fig:growthsimple2) shows the template for the example experiment after the 
column names have been revised to avoid any problematic characters. This template is now in 
a very useful format for a reproducible research pipeline---the data collected using this 
template can be very easily read into and processed using further statistical programs like 
R or Python.

```{r growthsimple2, echo = FALSE, out.width = "\\textwidth", fig.cap = "Example of an simpler format that can be used to record and analyze data for the same laboratory experiment as the previous figure. Annotations highlight where data is entered by hand. No calculations are conducted or figures created---these are all done later, using a code script."}
knitr::include_graphics("figures/growth_curve_simple.png")
```

### Moving further data analysis to later in the pipeline

Once you have created a "tidy" template for collecting your data in the laboratory, 
you can create a report template that will input that data and then provide summaries
and visualizations. This allows you to separate the steps (and files) for collecting data
from those for analyzing data. Figure \@ref(fig:growthreport2) shows an example of 
a report template that could be created to pair with the data collection template shown
in Figure \@ref(fig:growthsimple2). 

```{r growthreport2, fig.fullwidth = TRUE, echo = FALSE, out.width = "\\textwidth", fig.cap = "Examples of an automated report that can be created to quickly generate summaries and estimates of the data collected in the simplified data collection template for the example experiment."}
knitr::include_graphics("figures/growth_curve_report.png")
```

To create a report template like this, you can use tools for reproducible
reports from statistical programs like R and Python. In this section, we will
give an overview of how you could create the report template shown in Figure
\@ref(fig:growthreport2).

We will be using the programming language R, including RMarkdown, which is R's
main tool for creating reproducible reports. If you have never used R or
RMarkdown before, you'll find it useful to learn some more about their use to
help in creating these types of reproducible reports from collected data.
Numerous excellent (and free) resources exist to help learn R. One of the best
is the book "R for Data Science" by Hadley Wickham and Garrett Grolemund. It is
available in print, as well as free online at https://r4ds.had.co.nz/.

...

This report is written using a framework called RMarkdown, which allows you to 
include executable code inside a nicely-formatted document, resulting in a 
document in Word, PDF, or HTML that is easy for humans to read while also 
generating results based on R code. We will cover this format in details in 
modules 3.7--3.9. In the rest of this section, we'll walk through some of the
R code that is "powering" the analysis in this document, but if you'd like 
to learn how to combine it into an RMarkdown document to create the report 
shown in Figure \@ref(fig:growthreport2), you can learn much more about 
RMarkdown in those later modules. 

The code used to generate the results in Figure \@ref(fig:growthreport2)
is all in the programming language R. A programming language can seem, at
first glance, much more difficult to learn and use than using a spreadsheet 
program like Excel to set up formulae and macros. However, languages like 
R have evolved substantially in recent years to allow for much more straightforward
coding than you may have seen in the past, and the barrier to learning to 
use them for straightforward data management and analysis is not much higher
than the effort required to become proficient in using a spreadsheet program. 
To demonstrate this, let's look through a few of the tasks required to 
generate the results shown in Figure \@ref(fig:growthreport2). We won't cover
all the code, just highlight some of the key steps. If you'd like to look 
in details over the code and the output document, you can download those
files and explore them: the [Rmarkdown file] and the [output PDF] are 
available here. If you'd like to try out the code in the Rmarkdown file, 
you'll also need the example data, which you can download [here]. [Add refs
to the last sentence.]

One key step is to read the collected data into R. When you use a spreadsheet
for both data collection and analysis, you don't need to read the data to start
working with them, since everything is saved in the same file. Once you separate
the steps of data collection and data analysis, however, you do need to take an
extra step to read the data file into another program for analysis. Fortunately,
this is very simple in R. The data in this example are recorded using an Excel
spreadsheet, and there is a simple function in R that lets you read data in from
this type of spreadsheet. After this step of code, you will have an object in R
called `growth_data`, which contains the data in a two-dimensional form very
similar to how it is recorded in the spreadsheet (this type of object in R is
called a dataframe).

[Figure---read in data from spreadsheet]

Another key step is to calculate, for each observation, the time since the start
of the experiment. In the original data collection template shown in Figure
\@ref(fig:growthexcel2), this calculation was done by hand by the researcher and
entered into the spreadsheet. When we converted the spreadsheet to a tidier
version, we took out all steps that involved calculations with the data, and
instead limited the data collection to only raw, observed values. This helps us
avoid errors and typos---instead of having the researcher calculate the
difference in time as they are running the experiment, they can just record the
time, and we can write code in the analysis document that handles the further
calculations, using well-designed and well-tested tools to do this calculation.

Figure [x] shows code that can be used for this calculation. At the start of this 
code, the data are stored in an object named `growth_data`. The `mutate` function
adds a column to the data, named `sampling_delta_time`, that will give the 
difference between the time of an observation and the start of the experiment.
Within the `mutate` call, a special function named `difftime` calculates the 
difference in two time points. This function lets us specify the time units 
we'd like to use, and here we can pick `"hours"` for the units. The `first` 
function lets us pull out the first value in the data for a recorded time---in
other words, the time when the experiment started. This lets us compare each
observation time to the time of the start of the experiment. The result 
of this code is a new version of the `growth_data` dataframe, with a new
column giving time since the start of the experiment:

[Figure---calculating the difference in time]

Another key step is to plot results from the data. In R, there is a package 
called `ggplot2` that provides tools for visualization. The tools in this 
package work by building a plot using "layers", adding on small elements
line by line through simple functions that each do one simple thing. 
While the resulting code can be long, each step is simple, and so it 
becomes simple to learn these different "layers" and learn how to combine
them to create complex plots. 

Figure [x] walks through the code for one of the visualizations in the report.
At this point in the report code, the data have been reformatted into an object
called `growth_data_tidy`, which has columns for each observation on the time
since the start of the experiment (`sampling_delta_time`), the measured optical
density (`optical_density`), whether the tube was aerated or low oxygen
(`growth_conditions`), and a short ID for the test tube (`short_tube_id`). The
code starts by creating a plot object, specifying that in this plot the color
will show the growth conditions, the position on the x-axis will show the time
since the start of the experiment, and the y-axis will show the optical density.
Layers are then added to this plot object that add points and lines to the plot
based on these mappings, and for the lines, it's further specified that the type
of line should show the test tube ID (for example, one tube will be shown with a
dotted line, another with a dashed line). Further layers are added to customize
the scale labels with `labs`, including the labels for the x-axis and y-axis and
the legends of the color and linetype scales. Another layer is used to customize
the appearance of the plot---things like the background color and the font
used---and another layer is added to use a log-10 scale for the x-axis.

[Figure---plotting some of the results]

While this looks like a lot of code, the process isn't any longer than it would
be to customize elements of a plot in a spreadsheet program. The advantages of
the coded approach are that you maintain a full record of all the steps you took
to customize the plot. This is something that you can use to reproduce your plot
later, or even to use as a starting point for creating a similar plot with new
data.

The next key step that we'd like to point out is how you can write and use small
functions to do customized tasks for the experimental data. As one example, for
the data in this example, we want to estimate doubling times based on the
observed data. The principal investigator has decided that we should do this
based on comparing bacteria levels at two times points---the measured time that
is closest to 65 hours after the start of the experiment, and the time that is
closest to 24 hours after the start of the experiment.

In the original data collection template---where the data were both recorded
and analyzed in a spreadsheet---this step was done by hand by the researcher, 
looking through the data and selecting the cell closest to each of these
times, and then connecting that cell to a spreadsheet formula calculation
to calculate the doubling time. We can make this process more rigorous and 
less prone to error by writing a small function that does the same thing, 
then using that function to automate the process of identifying the 
relevant observations to use in calculating the doubling rate.

Figure [x] shows how you can write and then use a small function in R. This
function will input your `growth_data` dataset, as well as a time that you are
aiming for, and will output the sampling time in the data that is closest
to---while not larger than---that time. It does that in a few steps within the
body of the function. First, the code in the function filters to only
observations earlier than the target time. Then it measures the difference
between each of the times for these observations and the target time, and uses
this to identify the observation with the closest time to the target. It pulls
out the time of this observation and returns it.

[Figure---writing and using a small function]

Small functions like this can easily be reused in other code for your research 
group. By writing the logic of the step out as a function---rather than redoing
the steps by hand or step-by-step each time you need to do it---you can save 
time later, and in return, you have extra time that you can spend in writing 
the original function and carefully checking to make sure that it works 
correctly. 

Finally, many of these steps require extensions to base R. When you download R,
you are getting a base set of tools. Many people have developed helpful
extensions that build on this base. These are stored and shared in what are
called **R packages**. You can install these extra packages for free, and you
use the `library` function in R to load a package you've installed, giving you
access to the extra functions that it provides. Figure [x] shows the spot in the
Rmarkdown code where we loaded packages we needed for this report. These include
packages with functions to read data in R from Excel (the `readxl`) package, as
well as a suite of packages with tools for cleaning and visualizing data (the
`tidyverse` package). In later modules, we'll talk some more about R coding
tools that you might find useful for working with biomedical data, including the
tools in the powerful and popular `tidyverse` suite of packages.

[Figure---loading packages]

Overall, you can see that the code in this document provides a step-by-step
recipe that documents all the calculations and cleaning that we do with the
data, as well as how we create the plots. This code runs every time we create
the report shown in Figure \@ref(fig:growthreport2), and it gives us a good
starting point if we run additional experiments that generate similar data. 

### Applied exercise

The Rmarkdown document includes a number of other steps, and you might find
it interesting to download the document and the example data and walk through
them to get a feel for the process. All the steps are documented in the 
Rmarkdown document with extensive code comments, to explain what's happening 
along the way. 


<!-- --------------------------------------------------------------------------- -->

<!-- **Older text** -->

<!-- ### Example---Data on rate of bacterial growth  -->

<!-- The first set of data are from a study on the growth of *Mycobacterium -->
<!-- tuberculosis*. The goal of this study was to compare growth yield and doubling -->
<!-- time of *Mycobacterium tuberculosis* grown in rich medium under two assay -->
<!-- conditions. One set of cultures were grown in tubes with a low culture volume -->
<!-- relative to a large air head space to allow free oxygen exchange. A second set -->
<!-- of cultures were grown in tubes filled to near capacity, resulting in limited -->
<!-- air head space which has been shown elsewhere to limit oxygen availability over -->
<!-- time. The caps on both sets of cultures were sealed to restrict air exchange -->
<!-- during the study. -->

<!-- Some background information is helpful in understanding these example data, -->
<!-- especially if you have not conducted this type of experiment. The increase in -->
<!-- the cell size and cell mass during the development of an organism is termed -->
<!-- growth. It is the unique characteristics of all organisms. The organism must -->
<!-- require certain basic parameters for their energy generation and cellular -->
<!-- biosynthesis. The growth of the organism is affected by both physical and -->
<!-- nutritional factors. There are multiple methods by which growth can be measured, -->
<!-- but the use of closed tissue culture tubes and a spectrophotometer to track -->
<!-- increases in optical density (absorbance at 600 nm) over time offers several -->
<!-- advantages: 1) it is less subject to technical error and contamination, 2) read -->
<!-- out is fast and simple, 3) growth as measure by increased absorbance (turbidity) -->
<!-- is directly proportional to increases in cell mass. There are four distinct -->
<!-- phases of bacterial growth. Lag phase, log (exponential phase), stationary -->
<!-- phase, death phase. From these data, bacterial generation times (doubling time) -->
<!-- during the exponential growth phase can be calculated. -->

<!-- $$ -->
<!-- \mbox{Doubling time} = \frac{log(2)(t_1 - t_2)}{log(OD_{t_1} - log(OD_{t_2}))} -->
<!-- $$ -->
<!-- where $t_1$ and $t_2$ are two time points and $OD_{t_1}$ and $OD_{t_2}$ are the  -->
<!-- optical densities at the two time points (all $log$s are natural in this case). -->

<!-- An excel-based workbook (Figure \@ref(fig:growthexcel2)) was created to allow the -->
<!-- student performing the work to (1) calculate the amount of initial inoculum -->
<!-- (cell culture) to add to each tube to begin the study, (2) record the raw data -->
<!-- absorbance measurements, (3) graph the data on both a log and linear scale, and -->
<!-- (4) calculate doubling time in two phases of growth using the equation listed -->
<!-- above. Columns were added to allow the student to track the time (column A), the -->
<!-- difference in time (hours) between each time point in which data were collected -->
<!-- (column B), the date on which data were gathered (column C), and the time in -->
<!-- hours for each data point from the start of the study for graphing purposes -->
<!-- (column D). Absorbance data for each sampling timepoint were listed in Columns -->
<!-- E-F (high oxygen conditions; VA001 A1, A3) or columns G-I (limited oxygen -->
<!-- conditions; VA001 L1, L2, L3). -->



<!-- What the researchers found appealing about the format of this Excel sheet was -->
<!-- the ease with which the student could accomplish the study goals. They also -->
<!-- cited transparency of the raw data and ease with which additional sampling data -->
<!-- points could be added. The data being graphed in real time and the inclusion of -->
<!-- a simple macro to calculate doubling time, allowed the student to see tangible -->
<!-- differences between the two assay conditions. This was also somewhat problematic -->
<!-- as the equation to calculate doubling time was based on anchored time points -->
<!-- built into the original spreadsheet resulting in two different results that were -->
<!-- not properly linked to the correct data time points. -->

<!-- Data that are saved in a format like that shown in Figure \@ref(fig:growthexcel2),  -->
<!-- however, are hard to read in for a statistical program like R, Perl, or Python. -->
<!-- In this format, the raw data (the time points each observation was collected and -->
<!-- the optical density for the sample at that time point) form only part of the  -->
<!-- spreadsheet. The spreadsheet also includes notes, automated figures, and  -->
<!-- cells where an embedded formula runs calculations behind the scenes. -->

<!-- Instead of this format, we can design a simpler format to collect the data. We'll -->
<!-- remove all figures and calculations, and instead save those to perform in a  -->
<!-- code script. Figure \@ref(fig:growthsimple) shows an example of a simpler  -->
<!-- format for collecting the same data. In this case, all the "extras" have been  -->
<!-- stripped out---this only has spaces for recording times points and the observed -->
<!-- optical density at those time points. In later chapters, we'll show how a code -->
<!-- script can be used to input these data into R and then perform calculations -->
<!-- and create figures. By separating out the steps of data recording from  -->
<!-- data analysis, you can ensure that all steps of analysis are clearly spelled  -->
<!-- out (and can be easily reproduced with other similar data) through a code  -->
<!-- script. Note that you can still collect the data in this simpler format using -->
<!-- a spreadsheet program, if you'd like---Figure \@ref(fig:growthsimple2) shows  -->
<!-- the data collection set up to be recorded in a spreadsheet program, for example. -->
<!-- Within the spreadsheet, you can choose to save the data in a plain text format -->
<!-- (a csv [comma-separated value] file, for example). -->


<!-- In this new data collection format, the data are not completely "tidy". This is because -->
<!-- there is still some information included in the column names that we might want to use  -->
<!-- for analysis and plotting---namely, the different experimental group names (e.g.,  -->
<!-- "aerated1", "low_oxygen1"). However, there is a balance in creating data collection  -->
<!-- spreadsheets. They should be in a format that is easy to read into an interactive  -->
<!-- programming environment like R, as well as in a format that will be easy to convert -->
<!-- to a truly "tidy" format once they are read in. However, it's okay to balance these -->
<!-- needs with aims to make the data collection spreadsheet easy for a researcher to use. -->

<!-- The example shown in Figure \@ref(fig:growthsimple2) is designed to be easy to use  -->
<!-- when collecting data. All data points for a single collection time are grouped together -->
<!-- on a single row. When a researchers collects data for one time point, he or she can  -->
<!-- easy confirm visually that all the experimental groups have been measured for that  -->
<!-- time point. This format still makes it easy to read the data into an interactive  -->
<!-- programming environment, however, since they are in a clear two-dimensional format,  -->
<!-- with column names in the first row and values in the remaining rows. The removal of -->
<!-- extraneous elements---like embedded formulas, the results of hand calculations or  -->
<!-- automated calculations, and annotations through notes or colored highlighting---remove -->
<!-- barriers when reading the data into more sophisticated software. Once the data are -->
<!-- read into R, there can be converted into a truly tidy data format with just a few -->
<!-- command calls.  -->

<!-- The following code shows an example of how easy it is to read data into R in the simplified -->
<!-- format shown in Figure \@ref(fig:growthsimple2). It also shows how a few lines of code -->
<!-- can then be used to convert the data into a truly "tidy" format, and how easily  -->
<!-- sophisticated plots can then be made with the data. -->

<!-- ```{r echo = TRUE, message = FALSE, warning = FALSE} -->
<!-- library("tidyverse") -->
<!-- library("readxl") -->

<!-- # Read data into R from the simplified data collection template -->
<!-- growth_curve <- read_excel("data/growth_curve_data_in_excel (1)/growth curve data_GR.xls",  -->
<!--                            sheet = "simplified_template") -->

<!-- # Example of data -->
<!-- growth_curve -->

<!-- # Convert to a fully tidy format -->
<!-- growth_curve <- growth_curve %>%  -->
<!--   pivot_longer(-sampling_date_time,  -->
<!--                names_to = "experimental_group",  -->
<!--                values_to = "optical_density") -->

<!-- # How the data look after this transformation -->
<!-- growth_curve -->

<!-- # Example of how easily sophisticated plots can be created with data in this format -->
<!-- growth_curve %>%  -->
<!--   ggplot(aes(x = sampling_date_time, y = optical_density)) +  -->
<!--   geom_line() +  -->
<!--   facet_wrap(~ experimental_group) -->
<!-- ``` -->
<!-- In later chapters, we'll discuss R's "tidyverse", a collection of tools within R -->
<!-- that facilitate analyzing and visualizing data once they've been read into R. Here, we -->
<!-- only aim to give an example of how little R code is needed to create useful output from -->
<!-- the data, with the only requirement for gaining this power being that the data need to  -->
<!-- be collected in a format that is "tidy" or close enough to easily read into R. -->

<!-- ### Example---Data on bacteria colony forming units -->

<!-- ### Example---Data from multiple related experiments -->



<!-- ### Issues with these data sets -->

<!-- 1. Issues related to using a spread sheet program -->
<!--     - Embedded macros -->
<!--     - Use of color to encode information -->
<!-- 2. Issues related to non-structured / non-two-dimensional data -->
<!--     - Added summary row -->
<!--     - Multiple tables in one sheet -->
<!--     - One cell value is meant to represent values for all rows below, until next -->
<!--       non-missing row -->
<!-- 3. Issues with data being non-"tidy" -->

<!-- ### Final "tidy" examples -->

<!-- ### Options for recording tidy data -->

<!-- **Spreadsheet program.** -->

<!-- **Spreadsheet-like interface in R.** -->

<!-- ### Examples of how "tidy" data can be easily analyzed / visualized -->




<!-- ### Discussion questions -->

