<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="3.1 Principles of pre-processing experimental data | Improving the Reproducibility of Experimental Data Recording and Pre-Processing" />
<meta property="og:type" content="book" />

<meta property="og:description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing." />
<meta name="github-repo" content="rstudio/bookdown-demo" />

<meta name="author" content="Brooke Anderson, Michael Lyons, Mercedes Gonzalez-Juarrero, Marcela Henao-Tamayo, and Gregory Robertson" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing.">

<title>3.1 Principles of pre-processing experimental data | Improving the Reproducibility of Experimental Data Recording and Pre-Processing</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#rigor-and-reproducibility-in-computation" id="toc-rigor-and-reproducibility-in-computation"><span class="toc-section-number">1</span> Rigor and reproducibility in computation</a></li>
<li><a href="2-experimental-data-recording.html#experimental-data-recording" id="toc-experimental-data-recording"><span class="toc-section-number">2</span> Experimental Data Recording</a></li>
<li><a href="3-experimental-data-preprocessing.html#experimental-data-preprocessing" id="toc-experimental-data-preprocessing"><span class="toc-section-number">3</span> Experimental Data Preprocessing</a></li>
<li><a href="4-references.html#references" id="toc-references"><span class="toc-section-number">4</span> References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="module12" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Principles of pre-processing experimental data</h2>
<p>The experimental data collected for biomedical research often requires
pre-processing before it can be analyzed. This stage of working with
experimental data has critical implications for the reproducibility and rigor of
later data analysis. Use of point-and-click software and/or propritary software
can limit the transparency and reproducibility of this analysis stage and is
time-consuming for repeated tasks. In this module, we will explain how
preprocessing can be broken into common themes and processes. In the next
module, we will explain how scripted pre-processing, especially using open
source software, can improve transparency and reproducibility fo this
stage of working with biomedical data.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Define “pre-processing” of experimental data</li>
<li>Understand key themes and processes in pre-processing and identify these
processes in their own pipelines</li>
</ul>
<div id="what-is-data-preprocessing" class="section level3" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> What is data preprocessing?</h3>
<p>Let’s start by talking about what data pre-processing is, as well as why
biomedical data often need preprocessing. When you run an assay, you are aiming
to gain knowledge and insights on a scientific question. For example, does a new
drug help in curing a disease? There are some measures that you can collect that
are directly linked to the scientific question. For example, if you track people
who take a drug versus a placebo for a terminal disease, the length of time that
each subject survives might be a direct measure of the success of that drug.
When you have direct measurements like this, your data may not require much
preprocessing before you incorporate it in a statistical analysis.</p>
<p>However, there are often cases where we collect data that are not as immediately
linked to the scientific question. Instead, these data may require
preprocessing before they can be used to test meaningful scientific hypotheses.
Data <em>preprocessing</em> is a term that covers steps must be done after data are
collected but before they are used for further analysis. After the data are
appropriately pre-processed, you can use them for statistical tests—for
example, to determine if metabolite profiles are different between experimental
groups—and also combine them with other data collected from the
experiment—for example, to see whether certain metabolite levels are
correlated with the bacterial load in a sample.</p>
<p>Some types of preprocessing fulfill the need of translating from the equipment
measurements to measures that are directly relevant to the scientific question.
Other preprocessing tasks are more housekeeping (e.g., reading data into memory
so the software can use it) and others aim to digest complexity in the data or
reduce or handle noise and bias in the data collection. It is particularly
common that pre-processing is necessary for data extracted using complex
equipment. Equipment like mass spectrometers and flow cytometers leverage
physics, chemistry, and biology in clever ways to help us derive more
information from samples, but one tradeoff is that the data from such equipment
often require a bit of work to move into a format that is useful for answering
scientific questions.</p>
<p>In any scientific field, when you work with data, it will often take much more
time to prepare the data for analysis than it takes to set up and run the
statistical analysis itself <span class="citation">(D. Robinson 2014)</span>. This is certainly true with
complex biomedical data, including data for flow cytometry, transcriptomics,
proteomics, and metabolomics. It is a worthwhile investment of time to learn
strategies to make preprocessing of this data more efficient and reproducible,
and it is critical—for the rigor of the entire experiment—to ensure that the
preprocessing is done correctly.</p>
</div>
<div id="common-themes-and-processes-in-data-preprocessing" class="section level3" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Common themes and processes in data preprocessing</h3>
<p>This preprocessing will vary depending on the way the data were collected and
the scientific questions you hope to answer, and often it will take a lot of
work to develop a solid pipeline for preprocessing data from a specific assay.
However, there are some common themes that drive the need for such preprocessing
of data, as well as some common steps in preprocessing that come up for many
datasets, across types of data collection and research questions. These common
themes provide a framework that can help as you design data
preprocessing pipelines, or as you interpret and apply pipelines that were
developed by other researchers. The rest of this module will describe
several of the most common themes and process in data preprocessing.</p>
<p><strong>Data input</strong></p>
<p>To be able to work with data—whether to preprocess it or to analyze it—you
will first need to provide a way for the software to access the data. For some
software programs, this will require reading the data into memory for the
program. Other software might be able to access data in a streaming fashion,
working with only part in memory at a time, or be able to read into memory
specific pieces from a file while leaving most of the data on disk. Regardless
of the method, the software needs some kind of access the data. Providing
this access is typically the first step in most data preprocessing pipelines.</p>
<p>When working with biomedical research data, you will typically have it stored in
some type of computer file. Computer files are saved in various file formats.
File formats define for the computer the way that the data are stored in the
file, and different formats are used for biomedical data. For example, data that
you enter and save in Excel may be saved to an “.xls” or “.xlsx” file
format. Some types of equipment, like plate readers, may also allow you to
output data in this file format. An even more basic file format is a plain text
file, in which data can be “delimited” (divided into sections, like columns)
using commas or other marks. These files often have an extension like “.csv” (if
commas are used as the delimiter) or “.tsv” (if tabs are used as the delimiter),
or a more generic file extension, like “.txt”.
It is usually straightforward to read data from these simper file formats
into R and other similar software. In later modules, we’ll provide examples
of how to read in from these types of file formats.</p>
<p>For other assays, especially those that involve more complicated equipment, the
file format will be more complex than a basic spreadsheet or other tabular
format. For example, mass spectrometers, which are used to collect data for
proteomics and metabolomics, often output data in specific file formats, like
the mzML file format <span class="citation">(Deutsch 2012)</span>. Flow cytometers often output data in
the Flow Cytometry Standard (FCS) file format <span class="citation">(Spidlen et al. 2021)</span>. In many cases,
these file formats will be well-defined and standardized across equipment, which
makes it easier for open source developers to build functions to input data from
those file formats.</p>
<p>Often, the initial download of R or similar software will not include tools
to read in data from these more complex and specialized file formats. However,
developers have often created extensions to the software (packages in R)
that can handle these more specialized file types. If a file format is
standardized and well-defined to the public, it allows people to write code
that can access and load data in that format, even if it is very complex.
For example, the Bioconductor project includes extensions that help read data
into R from some of the specific file formats for biomedical data.</p>
<p><strong>Extracting scientifically-relevant measurement</strong></p>
<p>Another common purpose of preprocessing is to translate the measurements that
you directly collect into measurements that are meaningful for your scientific
research question. Scientific research uses a variety of complex techniques and
equipment to initially collect data. As a result of these inventions and
processes, the data that are directly collected in the laboratory by a person or
piece of equipment might require quite a bit of preprocessing to be translated
into a measure that meaningfully describes a process of interest. A key element
of preprocessing data are to translate the acquired data into a format that can
more directly answer scientific questions.</p>
<p>This type of preprocessing will vary substantially from assay to assay, with
algorithms that are tied to the methodology of the assay itself. We’ll describe
some examples of this idea, moving from very simple translation to processes
that are much more complex (and more typical of the data collected at present in
many types of biomedical research assays).</p>
<p>As a basic example, some assays will use equipment that can measure the
intensity of color of a sample or the sample’s opacity. Some of these measures
might be directly (or at least proportionally) interpretable. For example,
opacity might provide information about how high the concentration of
bacteria are in a sample. Others might need more interpretation, based on
the scientific underpinnings of the assay. For example, in ELISA, antibody
levels are detected as a measure of the intensity of color of a sample at
various dilutions, but to interpret this correctly, you need to know the
exact process that was used for that assay, as well as the dilutions that were
measured.</p>
<p><label for="tufte-mn-28" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-28" class="margin-toggle"><span class="marginnote"><span style="display: block;">“Meaningful interpretation of sequencing data has become particularly
important. Yet such interpretation relies heavily on complex
computation—a new and unfamiliar domain to many of our biomedical
colleagues—which, unlike data generation, is not universally accessible
to everyone.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Nekrutenko and Taylor 2012)</span></span></span></span></p>
<p>The complexity of this “translation” scales up as you move to data that are
collected using more complex processes. Biomedical research today leverages
extraordinarily complex equipment and measurement processes to learn more about
health and disease. These invented processes of measurement can
provide extraordinarily detailed and informative data, allowing us to “see”
elements of biological processes that could not be seen at that level before.
However, they all require steps to translate the data that are directly
recorded by equipment into data that are more scientifically meaningful.</p>
<p>One example is flow cytometry, which can characterize immune cell populations.
In flow cytometry, immune cells are characterized based on proteins that are
present both within and on the surface of each cell, as well as properties like
cell size and granularity <span class="citation">Barnett et al. (2008)</span>. Flow
cytometry identifies these proteins through a complicated process that involves
lasers and fluorescent tags and that leverages a key biological process—that
an antibody can have a very specific affinity for one specific protein
<span class="citation">(Barnett et al. 2008)</span>.</p>
<p>The process starts by identifying proteins that can help to
identify specific immune cell populations (e.g., CD3 and CD4 proteins in
combination can help identify helper T cells). This collection of proteins is
the basis of a panel that’s developed for that flow cytometry experiment. For
each of the proteins on the panel, you will incorporate an antibody with a
specific affinity for that protein. If that antibody sticks to the cell in a
substantial number, it indicates the presence of its associated protein on the
cell. To be able to measure which of the antibodies stick to which cells, each
type of antibody is attached to a specific fluorescent tag (each of these is
often referred to as a “color” in descriptions of flow cytometry)
<span class="citation">(Benoist and Hacohen 2011)</span>.</p>
<p>Each fluorescent tag included in the panel will emit wavelength in a certain
well-defined range after it is exposed to light at wavelengths of a certain
range. As each cell passes through the flow cytometer, lasers activate these
fluorescent tags, and you can measure the intensity of light emitted at specific
wavelengths to identify which proteins in the panel are present on or in each
cell <span class="citation">(Barnett et al. 2008)</span>.</p>
<p>This is an extraordinarily clever way to identify cells, but the complexity of
the process means that a lot of preprocessing work must be done on the resulting
measurements. To interpret the data that are recorded by a flow cytometer
(intensity of light at different wavelengths)—and to generate a
characterization of immune cell populations from these data—you need to
incorporate a number of steps of translation. These include steps that
incorporate information about which fluorescent tags were attached to which
antibodies, which proteins in the cell each of those antibodies attach to, which
immune cells those proteins help characterize, what wavelength each fluorescent
tag emits at, and so on. In some cases, the measuring equipment will provide
software that performs some of this preprocessing before you get the first
version of the data, but some may need to be performed by hand, especially if
you need to customize based on your research question. Further, it’s critical to
understand to process, to decide if it’s apporpriate for your specific
scientific question.</p>
<p>Similarly complex processes are used to collect data for many single-cell and
high throughput assays, including transcriptomics, metabolomics, proteomics,
and single cell RNA-sequencing. It can require complex and sometimes lengthy
algorithms and pipelines to extract direct scientifically-relevant measures
from the measures that the laboratory equipment captures in these cases.
Depending on the assay, this preprocessing can include sequence alignment
and assembly (if sequencing data were collected) or peak identification and
alignment (if data was collected using mass spectrometry, for example).</p>
<p><label for="tufte-mn-29" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-29" class="margin-toggle"><span class="marginnote"><span style="display: block;">“To reap the full benefits of the omics revolution, we need
information technology tools capable of making sense of the vast data
sets generated by omics experiments. In fact, the development of such
tools has become a discipline unto itself, called bioinformatics. And
only with those tools can researchers hope to clear another obstacle to
drug development: that posed by so-called emergent properties—behaviors
of biological systems that cannot be predicted from the basic
biochemical properties of their components.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Barry and Cheung 2009)</span></span></span></span></p>
<p><label for="tufte-mn-30" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-30" class="margin-toggle"><span class="marginnote"><span style="display: block;">“One thing that has not changed in the last 10 years is that the
individual outputs of the sequence machines are essentially worthless by
themselves. … Fundamental to creating biological understanding from the
increasing piles of sequence data is the development of analysis
algorithms able to assess the success of the experiments and synthesize
the data into manageable and understandable pieces.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Flicek and Birney 2009)</span></span></span></span></p>
<p>The discipline of bioinformatics works to develop these types of algorithms
<span class="citation">(Barry and Cheung 2009)</span>, and many of them are available through open-source, scripted
software like R and Python. These types of preprocessing algorithms are often
also available as proprietary software, sometimes sold by equipment
manufacturers and sometimes separately.</p>
<p><strong>Quality assessment and control</strong></p>
<p>Another common step in preprocessing is to identify and resolve quality control
issues. These are cases where some error or problem occurred in the data
recording and measurement, or some of the samples are poor quality and need to
be discarded. One way you can help to identify potential quality issues in
research data is through basic exploratory analysis. Basic distribution plots or
scatterplots of the data, for example, can often help to identify unusual
outliers, which may be the result of a data recording error or an oddity in one
of the samples.</p>
<p>There are a variety of reasons why biomedical data might have quality control
issues. First, when data are recorded “by hand” (including into a spreadsheet),
the person who is recording the data can miss a number or mis-type a number. For
example, if you are recording the weights of mice for an experiment, you may
forget to include a decimal in one recorded value, or invert two numbers. These
types of errors include recording errors (reading the value from the instrument
incorrectly), typing errors (making a mistake when entering the value into a
spreadsheet or other electronic record), and copying errors (introduced when
copying from one record to another) <span class="citation">(Chatfield 1995)</span>.</p>
<p>While some of these can be hard to identify later, in many cases you can
identify and fix recording errors through exploratory analysis of the data. For
example, if most recorded mouse weights are around 25 grams, but one is recorded
as 252 grams, you may be able to identify that the recorder missed a decimal point
when recorded one weight. In this case, you could identify the error as
an extreme outlier—in fact, beyond a value that would make physical sense.</p>
<p>Other quality control issues may come in the form of missing data (e.g., you
forget to measure one mouse at one time point), or larger issues, like a quality
problem with a whole sample. In these cases, it is important to
identify missingness in the data, so that as a next step you can try to
determine why certain data points are missing (e.g., are they missing at random,
or is there some process that makes certain data points more likely to be
missing, in which case this missingness may bias later analysis), to help you
decide how to handle those missing values <span class="citation">(Chatfield 1995)</span>.</p>
<p>Some quality control issues will be very specific to a type of data or assay.
For example, one common theme in quality control repeats across methods
that measure data at the level of the single cell. Some examples of this type of
single-cell resolution measurement include flow cytometry and single-cell
RNA-seq. In these cases, some of the measurements might be made on cells that
are in some way problematic. This can include cells that are dead or damaged
<span class="citation">(Ilicic et al. 2016)</span>, and it can also include cases where a measurement
that was meant to be taken on a single cell was instead taken on two or more
cells that were stuck together, or on a piece of debris or, in the case of
droplet-based single cell RNA-seq, an empty droplet.</p>
<p>Quality control steps can help to identify and remove these problematic
observations. For example, flow cytometry panels will often include a marker for
dead cells, which can then be used when the data are gated to identify and
exclude these cells, while the size measure made of the cells (forward scatter)
can identify cases where two or more cells were stuck together and passed
through the equipment at the same time. In scRNA-seq, low quality cells may be
identified based on a relatively high mitochondrial DNA expression compared to
expression of other genes, potentially because if a cell ruptured before it was
lysed for the assay, much of the cytoplasm and its messenger RNA would have
escaped, but not RNA from the mitochondria <span class="citation">(Ilicic et al. 2016)</span>. Cells
can be removed in the preprocessing of scRNA-seq data based on this and related
criteria (low number of detected genes, small relative library size)
<span class="citation">(Ilicic et al. 2016)</span>.</p>
<p><strong>Addressing technical noise</strong></p>
<p>The second common purpose of preprocessing data is to address, to the extent
that it is possible, unwanted noise in the data. When we collect biomedical
research data, we are often collecting data in the hope that it will measure
some meaningful biological variation between two or more conditions. For
example, we may measure it in the hope that there is a meaningful difference in
gene expression between a sample taken from an animal that is diseased versus
one that is healthy, with the aim of finding a biomarker of the disease.</p>
<p>There are, however, several sources of variation in data we collect. The first
of these is variation that comes from meaningful biological variation between
different samples—the type of variation that we are trying to measure and
use to answer scientific questions.</p>
<blockquote>
<p>“Statistical models usually contain one or more ssytematic components as well
as a random (or stochastic) component. The random component, sometimes called
the noise, arises for a variety of reasons, and it is sometimes helpful to
distinguish between (i) measurement error and (ii) natural random variability
arising from differences between experimental units and from changes in
experimental conditions which cannot be controlled. The systematic component,
sometimes called the signal, may be deterministic, but there is increasing
interest in the case where the signal evolves through time according to
probabilistic laws. In engineering parlance, a statistical analysis can be
regarded as extracting information about the signal in the presence of noise.”
<span class="citation">(Chatfield 1995)</span></p>
</blockquote>
<p>There are other sources of variation, however. These sources are irrelevant to our
scientific question, and so we often call them “noise”—in other words, they
cause our data to change from one sample to the next in a way that might blur
the signal that we care about (variation from meaningful biological
differences). We therefore often take steps in preprocessing to try to limit or
remove this varation to help us see the meaningful biological variation more
clearly.</p>
<p>There are two main sources of this other variation or noise: biological and
technical. Biological noise in data comes from biological processes, but from
ones that are irrelevant to the process that we care about in our particular
experiment. For example, cells express different genes depending on where they
are in the cell cycle. However, if you are trying to use single cell
RNA-sequencing to explore variation in gene expression by cell type, you might
consider this growth-related variation as noise, even though it represents a
biological process.</p>
<p>The second source of noise is technical. Technical noise comes from variation
that is introduced in the process of collecting data, rather than from
biological processes. For example, part of the process of single cell RNA-seq
involves amplifying complementary DNA that are developed from the messenger RNA
in each cell in the sample. How much the complementary DNA are amplified in this
process, however, varies across cells <span class="citation">(J. M. Perkel 2017)</span>. If this isn’t
addressed in preprocessing, then this “amplification bias” prevents any
meaningful comparison across cells.</p>
<blockquote>
<p>“In many cases… the tools used in bulk RNA-seq can be applied to scRNA-seq.
But fundamental differences in the data mean that this is not always possible.
For one thing, single-cell data are noisier… With so little RNA to work with,
small changes in amplification and capture efficiencies can produce large
differences from cell to cell and day to day and have nothing to do with
biology. Researchers must therefore be vigilant for ‘batch effects’, in which
seemingly identical cells prepared on different days differ for purely technical
reasons, and for ‘dropouts’—genes that are expressed in the cell but not
picked up in the sequence data. Another challenge is the scale… A typical bulk
RNA-seq experiment involves a handful of samples, but scRNA-seq studies can
involve thousands. Tools that can handle a dozen samples often slow to a crawl
when confronted with ten or a hundred times as many.” <span class="citation">(J. M. Perkel 2017)</span></p>
</blockquote>
<blockquote>
<p>“Methods to quantify mRNA abundance introduce systematic sources of variation
that can obscure signals of interest. Consequently, an essential first step in
most mRNA-expression analyses is normalization, whereby systemic variations
are adjusted to make expression counts comparable across genes and / or samples.
Within-sample normalization methods adjust for gene-specific features, such
as GC content and gene length, to facilitate comparisons of a gene’s expression
within an individual sample; whereas between-sample normalization methods adjust
for sample-specific features, such as sequencing depth, to allow for comparisons of
a gene’s expression across samples.” <span class="citation">(Bacher et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“A number of methods are available for between-sample normalization in bulk
RNA-seq experiments. Most of these methods calculate global scale factors (one
factor is applied to each sample, and this same factor is applied to all genes
in the sample) to adjust for sequencing depth. These methods demonstrate
excellent performance in bulk RNA-seq, but they are compromised in the
single-cell setting because of an abundance of zero-expression values and
increased technical variability.” <span class="citation">(Bacher et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“scRNA-seq data show systematic variation in the relationship between
transcript-specific expression and sequencing depth (which we refer to as the
count-depth relationship) that is not accommodated by a single scale factor
common to all genes in a cell. Global scale factors adjust for a count-depth
relatinoship that is assumed to be common across genes. When this relationship
is not common across genes, normalization via global scale factors leads to
overcorrection for weakly and moderately expressed genes and, in some cases,
undernormalization of highly expressed genes. To address this, SCnorm uses
quantile regression to estimate the dependence of transcript expression on
sequencing depth for every gene. Genes with similar dependence are then grouped,
and a second quantile regression is used to estimate scale factors within each
group. Within-group adjustment for sequencing depth is then performed using the
estimated scale factors to provide normalized estimates of expression.”
<span class="citation">(Bacher et al. 2017)</span></p>
</blockquote>
<p>In some cases, there are ways to reduce some of the variation that comes from
processes that aren’t of interest for your scientific question, either from
biological or technical sources. Some of this variation might just lower the
statistical power of the analysis, but some can go further and bias the results.</p>
<p>One example of a process that can help adjust for unwanted variation is
normalization. Let’s start with a very simple example to explain what
normalization does. Say that you wanted to measure the height of three people,
so you can compare to determine who is tallest and who is shortest.
However, rather than standing on an even surface, they are all standing on
ladders that are different heights. If you measure the height of the top of
each person’s head from the ground, you will not be able to compare their
heights correctly, because each has the height of their ladder incorporated
into the measure. If you knew the height of each person’s ladder, though,
you could normalize your measure by subtracting each ladder’s height from
the total measurement, and then you could meaningfully compare the heights
to determine which person is tallest.</p>
<p>Normalization plays a similar role in preprocessing many forms of biomedical
data. One simple example is when comparing the weights of two groups of
mice. Often, a group of mice might be measured collectively in their cage,
rather than taken out and weighed individually. Say that you have three
treated mice in one cage and four control mice in another cage. You can
weigh both cages of mice, but to compare these weights, you will need to
normalize the measurement by dividing by the total number of mice that are
in each cage (in other words, taking the average weight per mouse). This
type of averaging is a very simple example of normalizing data.</p>
<p>As another example, scRNA-seq data can be prone to something called
amplification bias. This occurs because, while the different fragments are all
amplified before their sequences are read, some fragments are amplified more
times than others. If two fragments had the exact same abundence in the original
cell, but one was amplified more than the other, that one would be measured as
having a higher level in the sample if this amplification bias were not
accounted for. One way to adjust for this is … [Sequencing depth / read depth
in other sequencing data analysis?]</p>
<blockquote>
<p>“Normalization is critical to the development of analysis techniques on
scRNA-seq and to counteract technical noise or bias. Before observed data can be
used to identify differentially expressed genes or potential subpopulations, it
must undergo these corrections, for what is observed is seldom exactly what is
present within the data set.” <span class="citation">(Lytal, Ran, and An 2020)</span></p>
</blockquote>
<blockquote>
<p>“The primary goal of single-cell normalization is to remove the influence
of technical effects in the underlying molecular counts, while preserving
true biological variation.” <span class="citation">(Hafemeister and Satija 2019)</span></p>
</blockquote>
<blockquote>
<p>“In general, the normalized expression level of a gene should not be
correlated with the total sequencing depth of a cell. Downstream analytical
tasks (dimension reduction, differential expression) should also not
be influenced by variation in sequencing depth.” <span class="citation">(Hafemeister and Satija 2019)</span></p>
</blockquote>
<blockquote>
<p>“Scaling normalization is typically required in RNA-seq data analysis to
remove biases caused by differences in sequencing depth, capture efficiency
or composition effects between samples.” <span class="citation">(McCarthy et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“Each count in a count matrix represents the successful capture, reverse transcription and sequencing of a molecule of cellular mRNA (Box 1). Count depths for identical cells can differ due to the variability inherent in each of these steps. Thus, when gene expres- sion is compared between cells based on count data, any difference may have arisen solely due to sampling effects. Normalization addresses this issue by e.g. scaling count data to obtain correct rela- tive gene expression abundances between cells.” <span class="citation">(Luecken and Theis 2019)</span></p>
</blockquote>
<p>Other normalization preprocessing might be used to adjust for sequencing
depth for gene expression data, so that you can meaningfully compare the
measures of a gene’s expression in different samples or treatment
groups, which can be done by calculating and adjusting for a global
scale factor <span class="citation">(Bacher et al. 2017)</span>.</p>
<p>In scRNA-seq, processes like the use of UMIs can allow you to later account for
amplification bias, in which some of the initial mRNA molecules are amplified
more than others <span class="citation">(Haque et al. 2017)</span>.</p>
<p>Another example of technical noise comes from flow cytometry. In this
case, each of the fluorescent tags, rather than emitting at a single
wavelength, has a distribution of wavelengths across which it emits.
These distributions overlap somewhat for some of the fluorescent tags,
especially when measuring many of these “colors” through a multicolor
panel?, causing something called “spillover”?, where emissions from
one tag might be recorded as part of the signal for another tag.</p>
<p>One key example of technical noise is known as “batch effects”—if different
samples are run through the equipment at different times, it can introduce
differences between the samples based on which “batch” they were measured
with. …</p>
<blockquote>
<p>“The steps in a typical flow cytometry experiment … present several
variables that need to be controlled for effective standardization. These variables
include the general areas of reagents, sample handling, instrument setup
and data analysis… The effects of changes in these variables are largely
known. For example, the stabilization and control of staining reagents
through the use of pre-configured lyophilized-reagent plates, and centralized
data analysis, have both been shown to decrease variability in a multi-site
study. However, the wide-spread adoption of standards for controlling such
variables has not taken place. This is in contrast to other technologies, such
as gene expression microarrays, which have achieved a reasonable degree of
standardization in recent years. … Of course, microarray data are less complex
than flow cytometry data, which are based on many hierarchical gates. Still,
a reasonable degree of standardization of flow cytometry assays should be
possible to achieve.” <span class="citation">(Maecker, McCoy, and Nussenblatt 2012)</span></p>
</blockquote>
<blockquote>
<p>“The biggest, the easiest way [for a biologist doing RNA-Seq to tell that
better normalization of the data is needed]—the way that I discovered the
importance of normalization in the microarray context—is the lack of
reproducibility across different studies. You can have three studies that are
all designed to study the same thing, and you just see basically no
reproducibility, in terms of differentially expressed genes. And every time I
encountered that, it could always be traced back to normalization. So, I’d say
that the biggest sign and the biggest reason why you want to use normalization
is to have a clear signal that’s reproducible.” <span class="citation">(Mak 2011)</span></p>
</blockquote>
<p>Some of these procedures can be separated into two groups, normalization
processes and data correction processes, which might include batch correction
and noise correction <span class="citation">(Luecken and Theis 2019)</span>.</p>
<p>Normalization in scRNA-seq can also include gene normalization:</p>
<blockquote>
<p>“In the same way that cellular count data can be normalized to make them
comparable between cells, gene counts can be scaled to improve comparisons
between genes. Gene normalization constitutes scaling gene counts to have zero
mean and unit vari- ance (z scores). … There is currently no consensus on
whether or not to perform normalization over genes.” <span class="citation">(Luecken and Theis 2019)</span></p>
</blockquote>
<p>There can be patterns in the data that result from the way that the
data are collected. For example, the data can have something called batch
effects if they have consistent differences based on who was doing the
measuring or which batch the sample was run with. For example, if two
researchers are working to weigh the mice for an experiment, the
weights recorded by one of the researchers might tend to be, on average, lower
than those recorded by the other researcher, perhaps because the two scales
they are using are calibrated a bit differently. Similarly, settings or
conditions can change in subtle ways between different runs on a piece of
equipment, and so the samples run in different batches might have some
differences in output based on the batch.</p>
<p>These batch effects can often be addressed through statistical modeling,
as long as they are identified and are not aligned with a difference you
are trying to measure (in other words, if all the samples for the control
animals are run in one batch and all those for the treated animals in
another batch, you would not be able to separate the batch effect from
the effect of treatment).</p>
<p>There are some methods that adjust for batch effects by fitting a regression
model that includes the batch as a factor, and then using the residuals from
that model for the next steps of analysis (“regressing out” those batch effects)
<span class="citation">(McCarthy et al. 2017)</span>. You can also incorporate this directly into a statistical
model that is being used for the main statistical hypothesis testing of interest
<span class="citation">(McCarthy et al. 2017)</span>.</p>
<blockquote>
<p>“After scaling normalization, further correction is typically required to
ameliorate or remove batch effects. For example, in the case study dataset,
cells from two patients were each processed on two C1 machines. Although C1
machine is not one of the most important explanatory variables on a per-gene
level, this factor is correlated with the first principal component of the
log-expression data. This effect cannot be removed by scaling normalization
methods, which target cell-specific biases and are not sufficient for removing
large-scale batch effects that vary on a gene-by-gene basis. … For the
dataset here, we fit a linear model to the scran normalized log-expression
values with the C1 machine as an explanatory factor. (We also use the log-total
counts from the endogenous genes, percentage of counts from the top 100 most
highly-expressed genes and percentage of counts from control genes as
additional covariates to control for these other unwanted technical effects.)
We then use the residuals from the fitted model for further analysis. This approach
successfully removes the C1 machine effect as a major source of variation between
cells.” <span class="citation">(McCarthy et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“We emphasize that it is generally preferable to incorporate batch effects or
latent variables into statistical models used for inference. When this is not
possible (e.g., for visualization), directly regressing out these uninteresting
factors is required to obtain ‘corrected’ expression values for further analysis.
Furthermore, a general risk of removing latent factors is that interesting
biological variation may be removed along with the presumed unwanted variation.
Users should therefore apply such methods with appropriate caution, particularly
when an analysis aims to discover biological conditions, such as new cell types.”
<span class="citation">(McCarthy et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“Once identified, important covariates and latent variables can be flagged for
inclusion in downstream statistical models or their effects regressed out of
normalized expression values.” <span class="citation">(McCarthy et al. 2017)</span></p>
</blockquote>
<p><strong>Transforming data</strong></p>
<p>Scaling, normalization, log transformations, calculating time since start
of experiment, etc. Some transformations can help change data that have
a skewed distribution into a more normally-distributed dataset, which can
be helpful in meeting the assumptions that underlie some statistical
tests and models. Some transformations are also helpful in visualizing
the data. For example, if data are extremely right-skewed (that is, have
a few very large outliers), it can be hard to see overall patterns
when plotting the untransformed data, as those outliers force the scale
to expand to fit them, squeezing the bulk of the data into a small
portion of the total scale of the plot. A log transformation can help to
spread the data more evenly across the plot area, so that you can
see patterns in the bulk of the data more easily.</p>
<blockquote>
<p>“There are various reasons for making a transformation, which may also apply to
deriving a new variable: 1. to get a more meaningful variable (the best reason!);
2. to stabilize variance; 3. to achieve normality (or at least symmetry); 4. to
create additive effects (i.e. remove interaction effects); 5. to enable a linear
model to be fitted.” <span class="citation">(Chatfield 1995)</span></p>
</blockquote>
<blockquote>
<p>[For transformations] “Logarithms are often meaningful, particularly with
economic data when proportional, rather than absolute, changes are of interest.
Another application of the logarithmic transformation is … to transform a
severely skewed distribution to normality.” <span class="citation">(Chatfield 1995)</span></p>
</blockquote>
<p>One critical process in this category is the process of <em>normalization</em>. …</p>
<blockquote>
<p>“After normalization, data matrices are typically log(x+1)-trans- formed. This
transformation has three important effects. Firstly, distances between
log-transformed expression values represent log fold changes, which are the
canonical way to measure changes in expression. Secondly, log transformation
mitigates (but does not remove) the mean–variance relationship in single-cell
data (Bren- necke et al, 2013). Finally, log transformation reduces the skew-
ness of the data to approximate the assumption of many downstream analysis tools
that the data are normally distributed. While scRNA-seq data are not in fact
log-normally distributed (Vieth et al, 2017), these three effects make the log
transformation a crude, but useful tool. This usefulness is highlighted by down-
stream applications for differential expression testing (Finak et al, 2015;
Ritchie et al, 2015) or batch correction (Johnson et al, 2006; Buttner et al,
2019) that use log transformation for these purposes. It should however be noted
that log transformation of normalized data can introduce spurious differential
expression effects into the data (preprint: Lun, 2018). This effect is
particularly pronounced when normalization size factor distributions differ
strongly between tested groups.” <span class="citation">(Luecken and Theis 2019)</span></p>
</blockquote>
<p><strong>Digesting complexity in datasets</strong></p>
<p>Biomedical research has dramatically changed in the past couple of decades to
include much more high-dimensional data. These data often include measurements
for each sample on hundreds of thousands of different parameters. For example,
transcriptomics data can include measurements for each sample on the expression
level of tens of thousands of different genes <span class="citation">(J. M. Perkel 2017)</span>. Data
from metabolics, proteomics, and other “omics” similarly create data at
high-dimensional scales.</p>
<p>There are also some cases where data are large because of the number of
observations, rather than (or in addition to) the number of measurements taken
for each observation. One example of this is flow cytometry data, where the
observations are individual cells. Current experiments often capture in the
range of a million cells for each sample in flow cytometry, measuring for each
cell some characteristics that can be used to determine its size, granularity,
and surface proteins, all with the aim of characterizing its cell type. A more
recent example is with single cell RNA-sequencing. Again, with this technique,
observations are taken at the level of the cell, with on the order of at least
10,000 cells processed per sample. [Check with Marcela / Taru on
back-of-envelope estimates in this paragraph]</p>
<p>Whether data is large because it measures many features (e.g., transcriptomics)
or includes many observations (e.g., single-cell data), the sheer size of the
data can require you to digest it somehow before you can use it to answer
scientific questions. There are several preprocessing techniques that can be
used to do this. The way that you digest this size and complexity depends on
whether the data are large because they have many features or because they have
many observations.</p>
<p>For data with many measurements for each observation, the different measurements often
have strong correlation structures across samples. For example, a large
collection of genes may work in concert, and so gene expression across those
genes may be highly correlated, or a metabolite might break down into
multiple measured metabolite features, making the measurements for those
features highly correlated. Therefore, even though there may be thousands
of measurements that are made on each sample in high throughput data, it
is often not the case that they are all providing separate information and
insights.</p>
<p>In some cases, your data may have more measurements than samples. For example,
if you run an assay that measures the level of thousands of metabolite features,
with twenty samples for which you collect data, then you will end up with many
more measurements (columns in your dataset, if it has a tidy structure) than
observations (rows in a tidy data structure). In this case, you may have no
choice but to resolve this before later steps of analysis. This is because a
number of statistical techniques fail or provide meaningless results for
datasets with more columns than rows <span class="citation">(Chatfield 1995)</span>.</p>
<p>Another concern with data that have many measurements per observation is that
the amount of information across the measurements is lower than the number of
measurement—in other words, some of the measures are partially or fully
redundant. To get a basic idea of dimension reduction, consider this example.
Say you have conducted an experiment that includes two species of research mice,
C57 black 6 and BALB/C. You record information about each mouse, including
columns that record both which species the mouse is and what color its coat is.
Since C57 black 6 mice are always black, and BALB/C mice are always white, these
two columns of data will be perfectly correlated. Therefore, one of the two
columns adds no information—once you have one of the measurements for a mouse,
you can perfectly deduce what the other measurement will be. You could
therefore, without any lose of information, reduce the dimensions (number of
columns) of the data you’ve collected in this case by choosing only one of these
two columns to keep and getting rid of the other column.</p>
<p>This same idea scales up to much more complex data—in many high dimensional
datasets, many of the measurements (e.g., levels of metabolite features in
metabolomics data or levels of gene expression in gene expression data) will be
highly correlated with each other, essentially providing the same information
across different measurements. In this case, the complexity of the dataset can
often be substantially reduced by using a dimension reduction technique, and
then conducting analysis to compare values in some of the main principle
components of the data.</p>
<p>Dimension reduction techniques are therefore often a key element of data
preprocessing when working with high-dimensional biological data. Dimension
reduction helps to collect the information that is captured by the dataset into
fewer columns, or “dimensions”—to go, for instance, from columns that measure
the expression of thousands of different genes down to a few “principal
component” columns that capture the key sources of variation across these genes.
One long-standing approach to dimension reduction is principal components
analysis (PCA). Other newer techniques have been developed, as well, such as
t-distributed stochastic neighbor embedding (t-SNE) <span class="citation">(J. M. Perkel 2017)</span>. Newer
techniques often aim to improve on limitations of classic techniques like PCA
under the conditions of current biomedical data—for example, some may help
address problems that arise when applying dimension reduction techniques to very
large datasets [?].</p>
<blockquote>
<p>“Most approaches seek to reduce these ‘multi-dimensional data’, with each
dimension being the expression of one gene, into a very small number of dimensions
that can be more easily visualised and interpreted. Principal component analysis
(PCA) is a mathematical algorithm that reduces the dimensionality of data, and
is a basic and very useful tool for examining heterogeneity in scRNA-seq data.”
<span class="citation">(Haque et al. 2017)</span></p>
</blockquote>
<p>Another approach to digest the complexity of high dimensional data is to remove
some of the features that were measured entirely, an approach that is more
generally called “feature selection” in data science. One example is in
preprocessing single-cell RNA-seq data. In this case, it is common to filter
down to only some of the genes whose expression was measured. One filtering
criterion is to filter out “low quality” genes. These might be genes with low
abundance on average across samples or high dropout rates (which happens if
a transcript is present in the cell but either isn’t captured or isn’t amplified
and so is not present in the sequencing reads) <span class="citation">McCarthy et al. (2017)</span>. Another criterion for filtering genes for single cell
RNA-sequencing is to focus on the genes that vary substantially across different
cell types, removing the “housekeeping” genes with similar expression regardless
of the cell type.</p>
<p>For data with lots of observations, like single-cell data, again the sheer size
of the data can make it difficult to explore and generate knowledge from it.
In this case, you can often reduce complexity by finding a way to group the
observations and then summarizing the size and other characteristics of each
group.</p>
<p>One way of doing this is with clustering techniques, which can be helpful
to explore large-scale patterns across the many observations.
As one example, single cell RNA-seq measures messenger RNA expression for each
cell in a sample of what can be 10,000 or more cells [double-check
with Taru]. One goal of scRNA-seq is
to use gene expression patterns in each cell to identify distinct cell types in
the sample, potentially including cell types that were not known prior to the
experiment <span class="citation">(J. M. Perkel 2017)</span>. To do this, it needs to used measures of the
expression of [hundreds or thousands] of genes in each cell to group the
[hundreds or thousands] of cells into groups with similar patterns of gene
expression. One common approach is to use clustering algorithms to group the
cells based on patterns in their gene expression.</p>
<p>One use of clustering techniques is to group cells into cell types, based
on their gene expression profiles, through scRNA-seq <span class="citation">(Haque et al. 2017)</span>.</p>
<blockquote>
<p>“Dimensionality reduction and visualization are, in many cases, followed by
clustering of cells into subpopulations that represent biologically meaningful
trends in the data, such as functional similarity or developmental
relationship. Owing to the high dimensionality of scRNA-seq data, clustering
often requires special consideration.” <span class="citation">(Haque et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“Cluster analysis aims to partition a group of individuals into groups or clusters
which are in some sense ‘close together’. There is a wide variety of possible
procedures. In my experience the clusters obtained depend to a large extent on the methods
used (except where the clusters are really clear-cut) and users are now aware of
the drawbacks and the precautions which need to be taken to avoid irrelevant or misleading
results.” <span class="citation">(Chatfield 1995)</span></p>
</blockquote>
<p>Flow cytometry often uses a different process to leverage the different measures
taken on each cell to make sense of them. This process is referred to
as “gating”. In gating, each measure taken on the cells is considered one or
two at a time. The cells with with characteristics on that measure or set of
measures that are consistent with a cell type of interest are retained.
The gating process steps through many of these “gates”, filtering out cells and
each step and only retaining the cells with markers or characteristics
that align with a certain cell type, until the researcher is satisfied that they
have identified all the cells of a certain type in the sample (e.g., all
helper T cells in the sample).</p>

</div>
</div>
<p style="text-align: center;">
<a href="3-experimental-data-preprocessing.html"><button class="btn btn-default">Previous</button></a>
<a href="3.2-module12a.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
