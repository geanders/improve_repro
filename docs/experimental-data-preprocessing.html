<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Module 3 Experimental Data Preprocessing | Improving the Reproducibility of Experimental Data Recording and Pre-Processing</title>
  <meta name="description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing." />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="Module 3 Experimental Data Preprocessing | Improving the Reproducibility of Experimental Data Recording and Pre-Processing" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Module 3 Experimental Data Preprocessing | Improving the Reproducibility of Experimental Data Recording and Pre-Processing" />
  
  <meta name="twitter:description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing." />
  

<meta name="author" content="Brooke Anderson, Michael Lyons, Mercedes Gonzalez-Juarrero, Marcela Henao-Tamayo, and Gregory Robertson" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="experimental-data-recording.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Visualization in R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Rigor and reproducibility in computation</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#overview-of-these-modules"><i class="fa fa-check"></i><b>1.1</b> Overview of these modules</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i><b>1.2</b> License</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html"><i class="fa fa-check"></i><b>2</b> Experimental Data Recording</a>
<ul>
<li class="chapter" data-level="2.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module1"><i class="fa fa-check"></i><b>2.1</b> Separating data recording and analysis</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#popularity-of-spreadsheets"><i class="fa fa-check"></i><b>2.1.1</b> Popularity of spreadsheets</a></li>
<li class="chapter" data-level="2.1.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#data-recording-versus-data-analysis"><i class="fa fa-check"></i><b>2.1.2</b> Data recording versus data analysis</a></li>
<li class="chapter" data-level="2.1.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#hazards-of-combining-recording-and-analysis"><i class="fa fa-check"></i><b>2.1.3</b> Hazards of combining recording and analysis</a></li>
<li class="chapter" data-level="2.1.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#approaches-to-separate-recording-and-analysis"><i class="fa fa-check"></i><b>2.1.4</b> Approaches to separate recording and analysis</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module2"><i class="fa fa-check"></i><b>2.2</b> Principles and power of structured data formats</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#data-recording-standards"><i class="fa fa-check"></i><b>2.2.1</b> Data recording standards</a></li>
<li class="chapter" data-level="2.2.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#defining-data-standards-for-a-research-group"><i class="fa fa-check"></i><b>2.2.2</b> Defining data standards for a research group</a></li>
<li class="chapter" data-level="2.2.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#two-dimensional-structured-data-format"><i class="fa fa-check"></i><b>2.2.3</b> Two-dimensional structured data format</a></li>
<li class="chapter" data-level="2.2.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#levels-of-standardizationresearch-group-to-research-community"><i class="fa fa-check"></i><b>2.2.4</b> Levels of standardization—research group to research community</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module3"><i class="fa fa-check"></i><b>2.3</b> The ‘tidy’ data format</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#what-makes-data-tidy"><i class="fa fa-check"></i><b>2.3.1</b> What makes data “tidy”?</a></li>
<li class="chapter" data-level="2.3.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#why-make-your-data-tidy"><i class="fa fa-check"></i><b>2.3.2</b> Why make your data tidy?</a></li>
<li class="chapter" data-level="2.3.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#using-tidyverse-tools-with-data-in-the-tidy-data-format"><i class="fa fa-check"></i><b>2.3.3</b> Using tidyverse tools with data in the tidy data format</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module4"><i class="fa fa-check"></i><b>2.4</b> Designing templates for “tidy” data collection</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#exampledata-on-rate-of-bacterial-growth"><i class="fa fa-check"></i><b>2.4.1</b> Example—Data on rate of bacterial growth</a></li>
<li class="chapter" data-level="2.4.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#features-that-make-data-collection-templates-untidy"><i class="fa fa-check"></i><b>2.4.2</b> Features that make data collection templates untidy</a></li>
<li class="chapter" data-level="2.4.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#converting-to-a-tidier-format-for-data-collection-templates"><i class="fa fa-check"></i><b>2.4.3</b> Converting to a “tidier” format for data collection templates</a></li>
<li class="chapter" data-level="2.4.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#learning-more-about-tidy-data-collection-in-the-laboratory"><i class="fa fa-check"></i><b>2.4.4</b> Learning more about tidy data collection in the laboratory</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module5"><i class="fa fa-check"></i><b>2.5</b> Example: Creating a template for “tidy” data collection</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#example-datadata-on-rate-of-bacterial-growth"><i class="fa fa-check"></i><b>2.5.1</b> Example data—Data on rate of bacterial growth</a></li>
<li class="chapter" data-level="2.5.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#limiting-the-template-to-the-collection-of-data"><i class="fa fa-check"></i><b>2.5.2</b> Limiting the template to the collection of data</a></li>
<li class="chapter" data-level="2.5.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#making-sensible-choices-about-rows-and-columns"><i class="fa fa-check"></i><b>2.5.3</b> Making sensible choices about rows and columns</a></li>
<li class="chapter" data-level="2.5.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#avoiding-problematic-characters-or-formatting"><i class="fa fa-check"></i><b>2.5.4</b> Avoiding problematic characters or formatting</a></li>
<li class="chapter" data-level="2.5.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#separating-data-analysis-from-data-collection"><i class="fa fa-check"></i><b>2.5.5</b> Separating data analysis from data collection</a></li>
<li class="chapter" data-level="2.5.6" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#applied-exercise"><i class="fa fa-check"></i><b>2.5.6</b> Applied exercise</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module6"><i class="fa fa-check"></i><b>2.6</b> Organizing project files</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#organizing-project-files"><i class="fa fa-check"></i><b>2.6.1</b> Organizing project files</a></li>
<li class="chapter" data-level="2.6.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#how-to-organize-project-files"><i class="fa fa-check"></i><b>2.6.2</b> How to organize project files</a></li>
<li class="chapter" data-level="2.6.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#what-is-a-project-template"><i class="fa fa-check"></i><b>2.6.3</b> What is a project template?</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module7"><i class="fa fa-check"></i><b>2.7</b> Creating project directory templates</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#designing-a-project-template"><i class="fa fa-check"></i><b>2.7.1</b> Designing a project template</a></li>
<li class="chapter" data-level="2.7.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#creating-and-using-a-project-template"><i class="fa fa-check"></i><b>2.7.2</b> Creating and using a project template</a></li>
<li class="chapter" data-level="2.7.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#project-directories-as-rstudio-projects"><i class="fa fa-check"></i><b>2.7.3</b> Project directories as RStudio Projects</a></li>
<li class="chapter" data-level="2.7.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#creating-project-templates-in-rstudio"><i class="fa fa-check"></i><b>2.7.4</b> Creating ‘Project’ templates in RStudio</a></li>
<li class="chapter" data-level="2.7.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#discussion-questions"><i class="fa fa-check"></i><b>2.7.5</b> Discussion questions</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module8"><i class="fa fa-check"></i><b>2.8</b> Example: Creating a project template</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#description-of-the-example-set-of-studies"><i class="fa fa-check"></i><b>2.8.1</b> Description of the example set of studies</a></li>
<li class="chapter" data-level="2.8.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#step-1-survey-of-data-collected-for-the-projects"><i class="fa fa-check"></i><b>2.8.2</b> Step 1: Survey of data collected for the projects</a></li>
<li class="chapter" data-level="2.8.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#step-2-organizing-a-project-directory"><i class="fa fa-check"></i><b>2.8.3</b> Step 2: Organizing a project directory</a></li>
<li class="chapter" data-level="2.8.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#step-3-establishing-file-name-conventions"><i class="fa fa-check"></i><b>2.8.4</b> Step 3: Establishing file name conventions</a></li>
<li class="chapter" data-level="2.8.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#step-4-designing-data-collection-templates"><i class="fa fa-check"></i><b>2.8.5</b> Step 4: Designing data collection templates</a></li>
<li class="chapter" data-level="2.8.6" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#step-5-designing-a-report-template"><i class="fa fa-check"></i><b>2.8.6</b> Step 5: Designing a report template</a></li>
<li class="chapter" data-level="2.8.7" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#applied-exercise-1"><i class="fa fa-check"></i><b>2.8.7</b> Applied exercise</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module9"><i class="fa fa-check"></i><b>2.9</b> Harnessing version control for transparent data recording</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#challenges-of-collaborating-on-evolving-research-materials"><i class="fa fa-check"></i><b>2.9.1</b> Challenges of collaborating on evolving research materials</a></li>
<li class="chapter" data-level="2.9.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#recording-data-in-the-laboratoryfrom-paper-to-computers"><i class="fa fa-check"></i><b>2.9.2</b> Recording data in the laboratory—from paper to computers</a></li>
<li class="chapter" data-level="2.9.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#defining-version-and-version-control"><i class="fa fa-check"></i><b>2.9.3</b> Defining “version” and “version control”</a></li>
<li class="chapter" data-level="2.9.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#what-are-the-key-elements-of-version-control"><i class="fa fa-check"></i><b>2.9.4</b> What are the key elements of version control?</a></li>
<li class="chapter" data-level="2.9.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#comparing-git-to-other-tools"><i class="fa fa-check"></i><b>2.9.5</b> Comparing git to other tools</a></li>
<li class="chapter" data-level="2.9.6" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#discussion-questions-1"><i class="fa fa-check"></i><b>2.9.6</b> Discussion questions</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module10"><i class="fa fa-check"></i><b>2.10</b> Enhance the reproducibility of collaborative research with version control platforms</a>
<ul>
<li class="chapter" data-level="2.10.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#what-are-version-control-platforms"><i class="fa fa-check"></i><b>2.10.1</b> What are version control platforms?</a></li>
<li class="chapter" data-level="2.10.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#why-use-version-control-platforms"><i class="fa fa-check"></i><b>2.10.2</b> Why use version control platforms?</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module11"><i class="fa fa-check"></i><b>2.11</b> Using git and GitLab to implement version control</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#how-to-use-version-control"><i class="fa fa-check"></i><b>2.11.1</b> How to use version control</a></li>
<li class="chapter" data-level="2.11.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#leveraging-git-and-github-as-a-project-director"><i class="fa fa-check"></i><b>2.11.2</b> Leveraging git and GitHub as a project director</a></li>
<li class="chapter" data-level="2.11.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#leveraging-git-and-github-as-a-scientist-who-programs"><i class="fa fa-check"></i><b>2.11.3</b> Leveraging git and GitHub as a scientist who programs</a></li>
<li class="chapter" data-level="2.11.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#applied-exercise-2"><i class="fa fa-check"></i><b>2.11.4</b> Applied exercise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html"><i class="fa fa-check"></i><b>3</b> Experimental Data Preprocessing</a>
<ul>
<li class="chapter" data-level="3.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#module12"><i class="fa fa-check"></i><b>3.1</b> Principles of pre-processing experimental data</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#what-is-data-preprocessing"><i class="fa fa-check"></i><b>3.1.1</b> What is data preprocessing?</a></li>
<li class="chapter" data-level="3.1.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#common-themes-and-processes-in-data-preprocessing"><i class="fa fa-check"></i><b>3.1.2</b> Common themes and processes in data preprocessing</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#module12a"><i class="fa fa-check"></i><b>3.2</b> Selecting software options for pre-processing</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#gui-versus-code-script"><i class="fa fa-check"></i><b>3.2.1</b> GUI versus code script</a></li>
<li class="chapter" data-level="3.2.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#open-source-versus-proprietary-software"><i class="fa fa-check"></i><b>3.2.2</b> Open-source versus proprietary software</a></li>
<li class="chapter" data-level="3.2.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#discussion-questions-2"><i class="fa fa-check"></i><b>3.2.3</b> Discussion questions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#module13"><i class="fa fa-check"></i><b>3.3</b> Introduction to scripted data pre-processing in R</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#what-is-a-code-script"><i class="fa fa-check"></i><b>3.3.1</b> What is a code script?</a></li>
<li class="chapter" data-level="3.3.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#how-code-scripts-improve-reproducibility-of-preprocessing"><i class="fa fa-check"></i><b>3.3.2</b> How code scripts improve reproducibility of preprocessing</a></li>
<li class="chapter" data-level="3.3.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#how-to-write-an-r-code-script"><i class="fa fa-check"></i><b>3.3.3</b> How to write an R code script</a></li>
<li class="chapter" data-level="3.3.4" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#how-to-run-code-in-an-r-script"><i class="fa fa-check"></i><b>3.3.4</b> How to run code in an R script</a></li>
<li class="chapter" data-level="3.3.5" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#exercise"><i class="fa fa-check"></i><b>3.3.5</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#module13a"><i class="fa fa-check"></i><b>3.4</b> Tips for improving reproducibility when writing R scripts</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#write-code-for-computers-but-edit-it-for-humans"><i class="fa fa-check"></i><b>3.4.1</b> Write code for computers, but edit it for humans</a></li>
<li class="chapter" data-level="3.4.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#modify-rather-than-start-from-scratch"><i class="fa fa-check"></i><b>3.4.2</b> Modify rather than start from scratch</a></li>
<li class="chapter" data-level="3.4.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#do-not-repeat-yourself"><i class="fa fa-check"></i><b>3.4.3</b> Do not repeat yourself</a></li>
<li class="chapter" data-level="3.4.4" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#discussion-questions-3"><i class="fa fa-check"></i><b>3.4.4</b> Discussion questions</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#module14"><i class="fa fa-check"></i><b>3.5</b> Simplify scripted pre-processing through R’s ‘tidyverse’ tools</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#tools-for-data-input"><i class="fa fa-check"></i><b>3.5.1</b> Tools for data input</a></li>
<li class="chapter" data-level="3.5.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#tools-for-changing-or-creating-columns"><i class="fa fa-check"></i><b>3.5.2</b> Tools for changing or creating columns</a></li>
<li class="chapter" data-level="3.5.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#tools-for-working-with-character-strings"><i class="fa fa-check"></i><b>3.5.3</b> Tools for working with character strings</a></li>
<li class="chapter" data-level="3.5.4" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#tools-for-working-with-dates-and-times"><i class="fa fa-check"></i><b>3.5.4</b> Tools for working with dates and times</a></li>
<li class="chapter" data-level="3.5.5" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#tools-for-statistical-modeling"><i class="fa fa-check"></i><b>3.5.5</b> Tools for statistical modeling</a></li>
<li class="chapter" data-level="3.5.6" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#resources-to-learn-more-on-tidyverse-tools"><i class="fa fa-check"></i><b>3.5.6</b> Resources to learn more on tidyverse tools</a></li>
<li class="chapter" data-level="3.5.7" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#practice-quiz"><i class="fa fa-check"></i><b>3.5.7</b> Practice quiz</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#module15"><i class="fa fa-check"></i><b>3.6</b> Complex data types in experimental data pre-processing</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#how-the-bioconductor-and-tidyverse-approaches-differ"><i class="fa fa-check"></i><b>3.6.1</b> How the Bioconductor and tidyverse approaches differ</a></li>
<li class="chapter" data-level="3.6.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#why-is-the-bioconductor-approach-designed-as-it-is"><i class="fa fa-check"></i><b>3.6.2</b> Why is the Bioconductor approach designed as it is?</a></li>
<li class="chapter" data-level="3.6.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#why-is-it-sometimes-necessary-to-use-a-bioconductor-approach-with-biomedical-data"><i class="fa fa-check"></i><b>3.6.3</b> Why is it sometimes necessary to use a Bioconductor approach with biomedical data</a></li>
<li class="chapter" data-level="3.6.4" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#combining-bioconductor-and-tidyverse-approaches-in-a-workflow"><i class="fa fa-check"></i><b>3.6.4</b> Combining Bioconductor and tidyverse approaches in a workflow</a></li>
<li class="chapter" data-level="3.6.5" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#outlook-for-a-tidyverse-approach-to-biomedical-data"><i class="fa fa-check"></i><b>3.6.5</b> Outlook for a tidyverse approach to biomedical data</a></li>
<li class="chapter" data-level="3.6.6" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#practice-quiz-1"><i class="fa fa-check"></i><b>3.6.6</b> Practice quiz</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#module18"><i class="fa fa-check"></i><b>3.7</b> Introduction to reproducible data pre-processing protocols</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#introducing-reproducible-data-pre-processing-protocols"><i class="fa fa-check"></i><b>3.7.1</b> Introducing reproducible data pre-processing protocols</a></li>
<li class="chapter" data-level="3.7.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#using-knitted-documents-for-protocols"><i class="fa fa-check"></i><b>3.7.2</b> Using knitted documents for protocols</a></li>
<li class="chapter" data-level="3.7.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#advantages-of-using-knitted-documents-for-data-focused-protocols"><i class="fa fa-check"></i><b>3.7.3</b> Advantages of using knitted documents for data-focused protocols</a></li>
<li class="chapter" data-level="3.7.4" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#how-knitted-documents-work"><i class="fa fa-check"></i><b>3.7.4</b> How knitted documents work</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#module19"><i class="fa fa-check"></i><b>3.8</b> RMarkdown for creating reproducible data pre-processing protocols</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#creating-knitted-documents-in-r"><i class="fa fa-check"></i><b>3.8.1</b> Creating knitted documents in R</a></li>
<li class="chapter" data-level="3.8.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#formatting-text-with-markdown-in-rmarkdown"><i class="fa fa-check"></i><b>3.8.2</b> Formatting text with Markdown in Rmarkdown</a></li>
<li class="chapter" data-level="3.8.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#preambles-in-rmarkdown-documents"><i class="fa fa-check"></i><b>3.8.3</b> Preambles in Rmarkdown documents</a></li>
<li class="chapter" data-level="3.8.4" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#executable-code-in-rmarkdown-files"><i class="fa fa-check"></i><b>3.8.4</b> Executable code in Rmarkdown files</a></li>
<li class="chapter" data-level="3.8.5" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#more-advanced-rmarkdown-functionality"><i class="fa fa-check"></i><b>3.8.5</b> More advanced Rmarkdown functionality</a></li>
<li class="chapter" data-level="3.8.6" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#learning-more-about-rmarkdown."><i class="fa fa-check"></i><b>3.8.6</b> Learning more about Rmarkdown.</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#module20"><i class="fa fa-check"></i><b>3.9</b> Example: Creating a reproducible data pre-processing protocol</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#introduction-and-example-data"><i class="fa fa-check"></i><b>3.9.1</b> Introduction and example data</a></li>
<li class="chapter" data-level="3.9.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#advice-on-designing-a-pre-processing-protocol"><i class="fa fa-check"></i><b>3.9.2</b> Advice on designing a pre-processing protocol</a></li>
<li class="chapter" data-level="3.9.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#writing-data-pre-processing-protocols"><i class="fa fa-check"></i><b>3.9.3</b> Writing data pre-processing protocols</a></li>
<li class="chapter" data-level="3.9.4" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#applied-exercise-3"><i class="fa fa-check"></i><b>3.9.4</b> Applied exercise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>4</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Improving the Reproducibility of Experimental Data Recording and Pre-Processing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="experimental-data-preprocessing" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Module 3</span> Experimental Data Preprocessing<a href="experimental-data-preprocessing.html#experimental-data-preprocessing" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This section includes modules on:</p>
<ul>
<li><a href="experimental-data-preprocessing.html#module12">Module 3.1: Principles of pre-processing experimental data</a></li>
<li><a href="experimental-data-preprocessing.html#module12a">Module 3.2: Selecting software options for pre-processing</a></li>
<li><a href="experimental-data-preprocessing.html#module13">Module 3.3: Introduction to scripted data pre-processing in R</a></li>
<li><a href="experimental-data-preprocessing.html#module13a">Module 3.4: Tips for improving reproducibility when writing R scripts</a></li>
<li><a href="experimental-data-preprocessing.html#module14">Module 3.5: Simplify scripted pre-processing through R’s ‘tidyverse’ tools</a></li>
<li><a href="experimental-data-preprocessing.html#module15">Module 3.6: Complex data types in experimental data pre-processing</a></li>
<li><a href="experimental-data-preprocessing.html#module18">Module 3.7: Introduction to reproducible data pre-processing protocols</a></li>
<li><a href="experimental-data-preprocessing.html#module19">Module 3.8: RMarkdown for creating reproducible data pre-processing protocols</a></li>
<li><a href="experimental-data-preprocessing.html#module20">Module 3.9: Example: Creating a reproducible data pre-processing protocol</a></li>
</ul>
<div id="module12" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Principles of pre-processing experimental data<a href="experimental-data-preprocessing.html#module12" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The experimental data collected for biomedical research often requires
pre-processing before it can be analyzed. This stage of working with
experimental data has critical implications for the reproducibility and rigor of
later data analysis. Use of point-and-click software and/or propritary software
can limit the transparency and reproducibility of this analysis stage and is
time-consuming for repeated tasks. In this module, we will explain how
preprocessing can be broken into common themes and processes. In the next
module, we will explain how scripted pre-processing, especially using open
source software, can improve transparency and reproducibility fo this
stage of working with biomedical data.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Define “pre-processing” of experimental data</li>
<li>Understand key themes and processes in pre-processing and identify these
processes in their own pipelines</li>
</ul>
<div id="what-is-data-preprocessing" class="section level3 hasAnchor" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> What is data preprocessing?<a href="experimental-data-preprocessing.html#what-is-data-preprocessing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When you are conducting an experiment that involves work in the wet lab, you
will do a lot of work before you have any data ready for the computer. You may,
for example, have conducted an extensive period of work that involved laboratory
animals or cell cultures. In many cases, you will have run some samples through
very advanced equipment, like cytometers or sequencers. Once you have completed
this long and hard process, you may ask yourself, “I ran the experiment, I ran
the equipment… Aren’t I done with the hard work?”.</p>
<p>For certain types of data, you may be, and you may be able to proceed directly to
statistical analysis. For example, if you collected the weights of lab animals,
you are probably directly using those data to answer questions like whether weight
differed between treatment groups. However, with a lot of biomedical data, you
will not be able to move directly to analyzing the data. Instead, you will need
to start with a stage of <em>pre-processing</em> the data: that is, taking
computational steps to prepare the data before it’s in an appropriate format to
be used in statistical analysis.</p>
<p>There are several reasons that pre-processing is often necessary. The first is
that many biomedical data are collected using extremely complex equipment and
scientific principles. The pre-processing in this case is used to extract scientific
meaning from data that might have been collected using measurements that are
more closely linked to the complex process than to the final scientific question.
Next, there will be some cases where practical concerns made it easier to
collect data in one way and pre-process it later to get it to a format that
aligns with the scientific question. For example, if you want the average weight
of mice in different treatment groups, it may be more practical to weigh the
cage that contains all the mice in each treatment group rather than weigh
each mouse individually. This makes life in the lab easier, but means you’ll need
to do some more computational pre-processing of the data to make sense of it
appropriately. Third, there are now frequent cases where an assay generates
a very large set of measures—for example, expression levels of thousands of
genes for each sample—and some pre-processing might help in digesting the
complexity inherent in this type of high-dimensional data. Finally, preprocessing
is often necessary to check for and resolve quality control issues within the
data.</p>
<p>In any scientific field, when you work with data, it will often take much more
time to prepare the data for analysis than it takes to set up and run the
statistical analysis itself <span class="citation">(D. Robinson 2014)</span>. This is certainly true with
complex biomedical data, including data for flow cytometry, transcriptomics,
proteomics, and metabolomics. It is a worthwhile investment of time to learn
strategies to make preprocessing of this data more efficient and reproducible,
and it is critical—for the rigor of the entire experiment—to ensure that the
preprocessing is done correctly and can be repeated by others.</p>
<p>These preprocessing steps, in fact, should be as clear and practical to follow
as the types of protocols you would follow for a wet lab procedure. Key to
reproducibility is that a procedure is described in enough detail that others
can follow it exactly.</p>
</div>
<div id="common-themes-and-processes-in-data-preprocessing" class="section level3 hasAnchor" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Common themes and processes in data preprocessing<a href="experimental-data-preprocessing.html#common-themes-and-processes-in-data-preprocessing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Exactly what preprocessing you will need to do will vary depending on the way the data
were collected and the scientific questions you hope to answer, and often it
will take a lot of work to develop a solid pipeline for preprocessing data from
a specific assay. However, there are some common themes that drive the need for
such preprocessing of data across types of data collection and research
questions. These common themes provide a framework that can help as you design
data preprocessing pipelines, or as you interpret and apply pipelines that were
developed by other researchers. The rest of this module will describe several of
the most common themes in data preprocessing.</p>
<p><strong>Extracting scientifically-relevant measurement</strong></p>
<p>One common purpose of preprocessing is to translate the measurements that
you directly collect into measurements that are meaningful for your scientific
research question. Scientific research uses a variety of complex techniques and
equipment to initially collect data. As a result of these inventions and
processes, the data that are directly collected in the laboratory by a person or
piece of equipment might require quite a bit of preprocessing to be translated
into a measure that meaningfully describes a scientific process. A key element
of preprocessing data is to translate the acquired data into a format that can
more directly answer scientific questions.</p>
<p>This type of preprocessing will vary substantially from assay to assay, with
algorithms that are tied to the methodology of the assay itself. We’ll describe
some examples of this idea, moving from very simple translation to processes
that are much more complex (and more typical of the data collected at present in
many types of biomedical research assays).</p>
<p>As a basic example, some assays will use equipment that can measure the
intensity of color of a sample or the sample’s opacity. Some of these measures
might be directly (or at least proportionally) interpretable. For example,
opacity might provide information about how high the concentration of bacteria
are in a sample. Others might need more interpretation, based on the scientific
underpinnings of the assay. For example, in an enzyme-linked immunosorbent assay
(ELISA), antibody levels are detected as a measure of the intensity of color of
a sample at various dilutions, but to interpret this correctly, you need to know
the exact process that was used for that assay, as well as the dilutions that
were measured.</p>
<p>The complexity of this “translation” scales up as you move to data that are
collected using more complex processes. Biomedical research today leverages
extraordinarily complex equipment and measurement processes to learn more about
health and disease. These invented processes of measuring can
provide extraordinarily detailed and informative data, allowing us to “see”
elements of biological processes that could not be seen at that level before.
However, they all require steps to translate the data that are directly
recorded by equipment into data that are more scientifically meaningful.</p>
<p>One example is flow cytometry, which can characterize immune cell populations.
In flow cytometry, immune cells are characterized based on proteins that are
present both within and on the surface of each cell, as well as properties like
cell size and granularity <span class="citation">Barnett et al. (2008)</span>. Flow
cytometry identifies these proteins through a complicated process that involves
lasers and fluorescent tags and that leverages a key biological process—that
an antibody can have a very specific affinity for one specific protein
<span class="citation">(Barnett et al. 2008)</span>.</p>
<p>The process starts by identifying proteins that can help to
identify specific immune cell populations (e.g., CD3 and CD4 proteins in
combination can help identify helper T cells). This collection of proteins is
the basis of a panel that’s developed for that flow cytometry experiment. For
each of the proteins on the panel, you will incorporate an antibody with a
specific affinity for that protein. If that antibody sticks to the cell in a
substantial number, it indicates the presence of its associated protein on the
cell.</p>
<p>To be able to measure which of the antibodies stick to which cells, each type of
antibody is attached to a specific fluorescent tag (each of these is often
referred to as a “color” in descriptions of flow cytometry) <span class="citation">(Benoist and Hacohen 2011)</span>.
Each fluorescent tag included in the panel will emit wavelength in a certain
well-defined range after it is exposed to light at wavelengths of a certain
range. As each cell passes through the flow cytometer, lasers activate these
fluorescent tags, and you can measure the intensity of light emitted at specific
wavelengths to identify which proteins in the panel are present on or in each
cell <span class="citation">(Barnett et al. 2008)</span>.</p>
<p>This is an extraordinarily clever way to identify cells, but the complexity of
the process means that a lot of preprocessing work must be done on the resulting
measurements. To interpret the data that are recorded by a flow cytometer
(intensity of light at different wavelengths)—and to generate a
characterization of immune cell populations from these data—you need to
incorporate a number of steps of translation. These include steps that
incorporate information about which fluorescent tags were attached to which
antibodies, which proteins in the cell each of those antibodies attach to, which
immune cells those proteins help characterize, what wavelength each fluorescent
tag emits at, and so on. In some cases, the measuring equipment will provide
software that performs some of this preprocessing before you get the first
version of the data, but some may need to be performed by hand, especially if
you need to customize based on your research question. Further, it’s critical to
understand the process, to decide if it’s appropriate for your specific
scientific question.</p>
<p>Similarly complex processes are used to collect data for many single-cell and
high throughput assays, including transcriptomics, metabolomics, proteomics,
and single cell RNA-sequencing. It can require complex and sometimes lengthy
algorithms and pipelines to extract direct scientifically-relevant measures
from the measures that the laboratory equipment captures in these cases.
Depending on the assay, this preprocessing can include sequence alignment
and assembly (if sequencing data were collected) or peak identification and
alignment (if data was collected using mass spectrometry, for example).</p>
<p>As Anton Nekrutenko and Taylor James note in an article on the reproducibility
of next-generation sequencing:</p>
<blockquote>
<p>“Meaningful interpretation of sequencing data has become particularly
important. Yet such interpretation relies heavily on complex computation—a new
and unfamiliar domain to many of our biomedical colleagues—which, unlike
data generation, is not universally accessible to everyone.”
<span class="citation">(Nekrutenko and Taylor 2012)</span></p>
</blockquote>
<p>In another article, Paul Flicek and Ewan Birney also capture this idea:</p>
<blockquote>
<p>“One thing that has not changed in the last 10 years is that the individual
outputs of the sequence machines are essentially worthless by themselves. …
Fundamental to creating biological understanding from
the increasing piles of sequence data is the development of analysis algorithms
able to assess the success of the experiments and synthesize the data into
manageable and understandable pieces.”
<span class="citation">(Flicek and Birney 2009)</span></p>
</blockquote>
<p>The discipline of bioinformatics works to develop these types of algorithms
<span class="citation">(Barry and Cheung 2009)</span>. Many of them are available through open-source, scripted
software like R and Python. These types of preprocessing algorithms are often
also available as proprietary software, sometimes sold by equipment
manufacturers and sometimes separately.</p>
<p><strong>Addressing practical concerns and limitations in data collection</strong></p>
<p>Another common reason for preprocessing is related to addressing things you
did while collecting the data—specifically, things you did for
practical purposes or under practical limitations. These will they need to
be handled, when possible, in computational preprocessing.</p>
<p>More generally, this type of preprocessing addresses something called
<em>noise</em> in the data. When we collect biomedical
research data, we are often collecting data in the hope that it will measure
some meaningful biological variation between two or more conditions. For
example, we may measure it in the hope that there is a meaningful difference in
gene expression between a sample taken from an animal that is diseased versus
one that is healthy, with the aim of finding a biomarker of the disease.</p>
<p>There are, however, several sources of variation in data we collect. The first
of these is variation that comes from meaningful biological variation between
samples—the type of variation that we are trying to measure and
use to answer scientific questions. We often call this the “signal” in the
data <span class="citation">(Chatfield 1995)</span>.</p>
<p>There are other sources of variation, however. These sources are irrelevant to
our scientific question, and so we often call them “noise”—in other words,
they cause our data to change from one sample to the next in a way that might
blur the signal that we care about. We therefore often take steps in
preprocessing to try to limit or remove this varation to help us see the
meaningful biological variation more clearly.</p>
<p>There are two main sources of this other variation or noise: biological and
technical. Biological noise in data comes from biological processes, but from
ones that are irrelevant to the process that we care about in our particular
experiment. For example, cells express different genes depending on where they
are in the cell cycle. However, if you are trying to use single cell
RNA-sequencing to explore variation in gene expression by cell type, you might
consider this growth-related variation as noise, even though it represents a
biological process.</p>
<p>The second source of noise is technical. Technical noise comes from variation
that is introduced in the process of collecting data, rather than from
biological processes. In the introduction to the module, we brought up the
example of weighing mice by cage rather than individually; one example of
technical noise in this case would be the differences across the samples that’s
based on the number of mice in each cage.</p>
<p>As another example, part of the process of single cell RNA-seq involves
amplifying complementary DNA that are developed from the messenger RNA in each
cell in the sample. How much the complementary DNA are amplified in this
process, however, varies across cells <span class="citation">(J. M. Perkel 2017)</span>. This occurs because,
while the different fragments are all amplified before their sequences are read,
some fragments are amplified more times than others. If two fragments had the
exact same abundence in the original cell, but one was amplified more than the
other, that one would be measured as having a higher level in the sample if this
amplification bias were not accounted for. If this isn’t addressed in
preprocessing, then this “amplification bias” prevents any meaningful comparison
across cells.</p>
<p>Another source of technical noise is something called <em>batch effects</em>. These
occur when data have consistent differences based on who was doing the measuring
or which batch the sample was run with, or which equipment was used for the
measure. For example, if two researchers are
working to weigh the mice for an experiment, the weights recorded by one of the
researchers might tend to be, on average, lower than those recorded by the other
researcher, perhaps because the two scales they are using are calibrated a bit
differently. Similarly, settings or conditions can change in subtle ways between
different runs on a piece of equipment, and so the samples run in different
batches might have some differences in output based on the batch.</p>
<p>In some cases, there are ways to reduce some of the variation that comes from
processes that aren’t of interest for your scientific question, either from
biological or technical sources. This is important to consider doing, because
while some of this variation might just lower the statistical power of the
analysis, some can go further and bias the results.</p>
<p>For example, batch effects can often be addressed through statistical modeling,
as long as they are identified and are not aligned with a difference you
are trying to measure (in other words, if all the samples for the control
animals are run in one batch and all those for the treated animals in
another batch, you would not be able to separate the batch effect from
the effect of treatment).</p>
<p>There are some methods that adjust for batch effects by fitting a regression
model that includes the batch as a factor, and then using the residuals from
that model for the next steps of analysis (“regressing out” those batch effects)
<span class="citation">(McCarthy et al. 2017)</span>. You can also incorporate this directly into a statistical
model that is being used for the main statistical hypothesis testing of interest
<span class="citation">(McCarthy et al. 2017)</span>. In this case, the technical noise isn’t addressed
during the preprocessing phase, but rather as part of the statistical analysis.</p>
<p>Another example of a process that can help adjust for unwanted variation is
normalization. Let’s start with a very simple example to explain what
normalization does. Say that you wanted to measure the height of three people,
so you can compare to determine who is tallest and who is shortest.
However, rather than standing on an even surface, they are all standing on
ladders that are different heights. If you measure the height of the top of
each person’s head from the ground, you will not be able to compare their
heights correctly, because each has the height of their ladder incorporated
into the measure. If you knew the height of each person’s ladder, though,
you could normalize your measure by subtracting each ladder’s height from
the total measurement, and then you could meaningfully compare the heights
to determine which person is tallest.</p>
<p>Normalization plays a similar role in preprocessing many forms of biomedical
data. One article defines normalization as the, “process of accounting for, and
possibly removing, sources of variation that are not of biological interest”
<span class="citation">(Mak 2011)</span>. One simple example is when comparing the weights of two groups
of mice. Often, a group of mice might be measured collectively in their cage,
rather than taken out and weighed individually. Say that you have three treated
mice in one cage and four control mice in another cage. You can weigh both cages
of mice, but to compare these weights, you will need to normalize the
measurement by dividing by the total number of mice that are in each cage (in
other words, taking the average weight per mouse). This type of averaging is a
very simple example of normalizing data.</p>
<p>Other normalization preprocessing might be used to adjust for sequencing depth
for gene expression data, so that you can meaningfully compare the measures of a
gene’s expression in different samples or treatment groups.
This can be done in bulk RNA sequencing by calculating and adjusting for a
global scale factor <span class="citation">(Bacher et al. 2017)</span>. One article highlights the critical
role of normalization in RNA sequencing in the context of reproducibility:</p>
<blockquote>
<p>“The biggest, the easiest way [for a biologist doing RNA-Seq to tell that
better normalization of the data is needed]—the way that I discovered the
importance of normalization in the microarray context—is the lack of
reproducibility across different studies. You can have three studies that are
all designed to study the same thing, and you just see basically no
reproducibility, in terms of differentially expressed genes. And every time I
encountered that, it could always be traced back to normalization. So, I’d say
that the biggest sign and the biggest reason why you want to use normalization
is to have a clear signal that’s reproducible.” <span class="citation">(Mak 2011)</span></p>
</blockquote>
<p>In single-cell RNA sequencing, there’s also a need for normalization, but in
this case the procedures to do it are a bit different. Difference processes are
needed because these data tend to be noisier and have a number of
zero-expression values <span class="citation">(J. M. Perkel 2017; Bacher et al. 2017)</span>. For these assays, therefore, new technologies for
normalization have been developed. For example, in scRNA-seq, processes like the
use of unique molecular identifiers (UMIs) can allow you to later account for
amplification bias <span class="citation">(Haque et al. 2017)</span>.</p>
<p><strong>Digesting complexity in datasets</strong></p>
<p>Biomedical research has dramatically changed in the past couple of decades to
include data with higher dimensions: that is, data that either includes many
samples or many measures per sample, or both.</p>
<p>Examples of high-dimensional data in biomedical data include data with many
<em>measurements</em> (also called <em>features</em>), often in the hundreds or thousands in
terms of the measurements generated per sample. These data often include
measurements for each sample on hundreds of thousands of different parameters.
For example, transcriptomics data can include measurements for each sample on
the expression level of tens of thousands of different genes
<span class="citation">(J. M. Perkel 2017)</span>. Data from metabolics, proteomics, and other “omics”
similarly create data at high-dimensional scales.</p>
<p>There are also some cases where data are large because of the number of
observations, rather than (or in addition to) the number of measurements taken
for each observation. One example of this is flow cytometry data, where the
observations are individual cells. Current experiments often capture in the
range of a million cells for each sample in flow cytometry, measuring for each
cell some characteristics that can be used to determine its size, granularity,
and surface proteins, all with the aim of characterizing its cell type. Another
example or an assay that generates lots of observations is single cell
RNA-sequencing. Again, with this technique, observations are taken at the level
of the cell, with on the order of at least 10,000 cells processed per sample.
[Check with Marcela / Taru on back-of-envelope estimates in this paragraph]</p>
<p>Whether data is large because it measures many features (e.g., transcriptomics)
or includes many observations (e.g., single-cell data), the sheer size of the
data can require you to digest it somehow before you can use it to answer
scientific questions. There are several preprocessing techniques that can be
used to do this. The way that you digest this size and complexity depends on
whether the data are large because they have many features or because they have
many observations.</p>
<p>For data with many measurements for each observation, the different measurements
often have strong correlation structures across samples. For example, a large
collection of genes may work in concert, and so gene expression across those
genes may be highly correlated. As another example, a metabolite might break
down into multiple measured metabolite features, making the measurements for
those features highly correlated. In some cases, your data may even have more
measurements than samples. For example, if you run an assay that measures the
level of thousands of metabolite features, with twenty samples, then you will
end up with many more measurements (columns in your dataset, if it has a tidy
structure) than observations (rows in a tidy data structure).</p>
<p>This case of data with many measurements presents, first, a technical issue. In
the case of data with more measurements than samples, you may have no choice but
to resolve this before later steps of analysis. This is because a number of
statistical techniques fail or provide meaningless results for datasets with
more columns than rows, as the algorithms run into problems
related to singularity and non-uniqueness <span class="citation">(Chatfield 1995)</span>. As Chatfield
notes:</p>
<blockquote>
<p>“It is potentially dangerous to allow the number of variables to exceed the
number of observations because of non-uniqueness and singularity problems. Put
simply, the unwary analyst may try to estimate more parameters than there are
observations.” <span class="citation">(Chatfield 1995)</span></p>
</blockquote>
<p>Another concern with data that have many measurements is that
the amount of information across the measurements is lower than the number of
measurement—in other words, some of the measures are partially or fully
redundant. To get a basic idea of dimension reduction, consider this example.
Say you have conducted an experiment that includes two species of research mice,
C57 black 6 and BALB/C. You record information about each mouse, including
columns that record both which species the mouse is and what color its coat is.
Since C57 black 6 mice are always black, and BALB/C mice are always white, these
two columns of data will be perfectly correlated. Therefore, one of the two
columns adds no information—once you have one of the measurements for a mouse,
you can perfectly deduce what the other measurement will be. You could
therefore, without any loss of information, reduce the number of
columns of the data you’ve collected by choosing only one of these
two columns to keep.</p>
<p>This same idea scales up to much more complex data—in many high dimensional
datasets, many of the measurements (e.g., levels of metabolite features in
metabolomics data or levels of gene expression in gene expression data) will be
highly correlated with each other, essentially providing the same information
across different measurements. In this case, the complexity of the dataset can
often be substantially reduced by using something called <em>dimension reduction</em>.</p>
<p>Dimension reduction helps to collect the information that is captured by the
dataset into fewer columns, or “dimensions”—to go, for instance, from columns
that measure the expression of thousands of different genes down to a few
“principal component” columns that capture the key sources of variation across
these genes. One long-standing approach to dimension reduction is principal
components analysis (PCA) <span class="citation">(Haque et al. 2017)</span>. Other newer techniques have
been developed, as well, such as t-distributed stochastic neighbor embedding
(t-SNE) <span class="citation">(J. M. Perkel 2017)</span>. Newer techniques often aim to improve on
limitations of classic techniques like PCA under the conditions of current
biomedical data—for example, some may help address problems that arise when
applying dimension reduction techniques to very large datasets.</p>
<p>Another approach to digest the complexity of high dimensional data is to remove
some of the features that were measured entirely, an approach that is more
generally called “feature selection” in data science. One example is in
preprocessing single-cell RNA-seq data. In this case, it is common to filter
down to only some of the genes whose expression was measured. One filtering
criterion is to filter out “low quality” genes. These might be genes with low
abundance on average across samples or high dropout rates (which happens if
a transcript is present in the cell but either isn’t captured or isn’t amplified
and so is not present in the sequencing reads) <span class="citation">McCarthy et al. (2017)</span>. Another criterion for filtering genes for single cell
RNA-sequencing is to focus on the genes that vary substantially across different
cell types, removing the “housekeeping” genes with similar expression regardless
of the cell type.</p>
<p>For data with lots of observations, like single-cell data, again the sheer size
of the data can make it difficult to explore and generate knowledge from it. In
this case, you can often reduce complexity by finding a way to group the
observations and then summarizing the size and other characteristics of each
group.</p>
<p>For example, flow cytometry leverages the different measures taken on each cell
to make sense of them with a process referred to as “gating”. In gating, each
measure taken on the cells is considered one or two at a time to filter the data
<span class="citation">(Maecker, McCoy, and Nussenblatt 2012)</span>. The gating process steps through many of these
“gates”, filtering out cells and each step and only retaining the cells with
markers or characteristics that align with a certain cell type, until the
researcher is satisfied that they have identified all the cells of a certain
type in the sample (e.g., all helper T cells in the sample).
This compresses the data to counts of different cell types, from original
data with one observation per cell.</p>
<p>Another way of doing this is with clustering techniques, which can be helpful to
explore large-scale patterns across the many observations. As one example,
single cell RNA-seq measures messenger RNA expression for each cell in a sample
of what can be 10,000 or more cells [double-check with Taru]. One goal of
scRNA-seq is to use gene expression patterns in each cell to identify distinct
cell types in the sample, potentially including cell types that were not known
prior to the experiment <span class="citation">(J. M. Perkel 2017)</span>. To do this, it needs to used
measures of the expression of [hundreds or thousands] of genes in each cell to
group the [hundreds or thousands] of cells by similar patterns of gene
expression. One use of clustering techniques is to group cells into cell types,
based on their gene expression profiles, through scRNA-seq
<span class="citation">(Haque et al. 2017)</span>.</p>
<p><strong>Quality assessment and control</strong></p>
<p>Another common step in preprocessing is to identify and resolve quality control
issues. These are cases where some error or problem occurred in the data
recording and measurement, or some of the samples are poor quality and need to
be discarded.</p>
<p>There are a variety of reasons why biomedical data might have quality control
issues. First, when data are recorded “by hand” (including into a spreadsheet),
the person who is recording the data can miss a number or mis-type a number. For
example, if you are recording the weights of mice for an experiment, you may
forget to include a decimal in one recorded value, or invert two numbers. These
types of errors include recording errors (reading the value from the instrument
incorrectly), typing errors (making a mistake when entering the value into a
spreadsheet or other electronic record), and copying errors (introduced when
copying from one record to another) <span class="citation">(Chatfield 1995)</span>.</p>
<p>While some of these can be hard to identify later, in many cases you can
identify and fix recording errors through exploratory analysis of the data. For
example, if most recorded mouse weights are around 25 grams, but one is recorded
as 252 grams, you may be able to identify that the recorder missed a decimal point
when recorded one weight. In this case, you could identify the error as
an extreme outlier—in fact, beyond a value that would make physical sense.</p>
<p>Other quality control issues may come in the form of missing data (e.g., you
forget to measure one mouse at one time point), or larger issues, like a quality
problem with a whole sample. In these cases, it is important to
identify missingness in the data, so that as a next step you can try to
determine why certain data points are missing (e.g., are they missing at random,
or is there some process that makes certain data points more likely to be
missing, in which case this missingness may bias later analysis), to help you
decide how to handle those missing values <span class="citation">(Chatfield 1995)</span>.</p>
<p>Some quality control issues will be very specific to a type of data or assay.
For example, one common theme in quality control repeats across methods
that measure data at the level of the single cell. Some examples of this type of
single-cell resolution measurement include flow cytometry and single-cell
RNA-seq. In these cases, some of the measurements might be made on cells that
are in some way problematic. This can include cells that are dead or damaged
<span class="citation">(Ilicic et al. 2016)</span>, and it can also include cases where a measurement
that was meant to be taken on a single cell was instead taken on two or more
cells that were stuck together, or on a piece of debris or, in the case of
droplet-based single cell RNA-seq, an empty droplet.</p>
<p>Quality control steps can help to identify and remove these problematic
observations. For example, flow cytometry panels will often include a marker for
dead cells, which can then be used when the data are gated to identify and
exclude these cells, while the size measure made of the cells (forward scatter)
can identify cases where two or more cells were stuck together and passed
through the equipment at the same time. In scRNA-seq, low quality cells may be
identified based on a relatively high mitochondrial DNA expression compared to
expression of other genes, potentially because if a cell ruptured before it was
lysed for the assay, much of the cytoplasm and its messenger RNA would have
escaped, but not RNA from the mitochondria <span class="citation">(Ilicic et al. 2016)</span>. Cells
can be removed in the preprocessing of scRNA-seq data based on this and related
criteria (low number of detected genes, small relative library size)
<span class="citation">(Ilicic et al. 2016)</span>.</p>

</div>
</div>
<div id="module12a" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Selecting software options for pre-processing<a href="experimental-data-preprocessing.html#module12a" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>[Intro]</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Describe software approaches for pre-processing data</li>
<li>Compare the advantages and disadvantages of Graphical User Interface–based
versus scripted approaches and of open-source versus proprietary approaches
to pre-processing</li>
</ul>
<p>The previous module described some common themes and processes in preprocessing
biomedical data. While we’ve covered some key processes of preprocessing, we
haven’t talked yet about the tools you can use to implement it. These are often
combined together into a pipeline (also called a workflow). These pipelines can
become fairly long and complex when you need to preprocess data that are
complex.</p>
<p>Most preprocessing pipelines will be run on the computer, with software tools. An
exception might be for very simple preprocessing tasks—one example is
generating the average cage weight for a group of mice based on the total cage
weight and the number of mice. However, even simple processes like this, which
can be done by hand, can also be done with a computer, and doing so can help
avoid errors and to provide a record of the calculation that was used for the
preprocessing.</p>
<p>You will have a choice about which type of software you use for preprocessing.
There are two key dimensions that separate these choices—first, whether the
software is point-and-click versus script-based, and, second, whether the
software is proprietary versus open-source. It is important to note
that, in some cases, it may make sense to develop a pipeline that chains
together a few different software programs to complete the required
preprocessing.</p>
<p>In this module, we’ll talk about the advantages and disadvantages of these
different types of software. For reproducibility and rigor, there are many
advantages to using software that is script-based and open source for data
preprocessing, and so in later modules, we’ll provide more information on how
you can use this type of software for preprocessing biomedical data. We also
recognize, however, that there are some cases where such software may not be a
viable option for some or all of the data preprocessing for a project.</p>
<div id="gui-versus-code-script" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> GUI versus code script<a href="experimental-data-preprocessing.html#gui-versus-code-script" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When you pick software for preprocessing, the first key dimension to consider
is whether the software is “point-and-click” or script-based.
Let’s start with a definition of each.</p>
<p>Point-and-click software is more formally known as GUI software, where GUI stand
for “graphical user interface”. These are programs where your hand is on the
mouse most of the time, and you use the mouse to select actions and options from
buttons and other widgets that are shown by the software on the screen. This
type of software is also sometimes called “widget-based”, as it is built around
widgets like drop-down menus and slider bars <span class="citation">(J. M. Perkel 2018)</span>.</p>
<p>A basic example of GUI-based software is your computer’s calendar application
(“application” is a common synonym for “software”). To navigate across dates on
your calendar, you use your mouse to click on arrows or dates.
The software includes some text entry—for example, if you add something to
your calendar, you can click on a textbox and enter a description of the
activity using your keyboard. However, the basic way that you navigate and use
the software is via your computer mouse.</p>
<p>Script-based software uses a script, rather than clickable buttons and graphics,
as its main interface. A script, in this case, is a line-by-line set of
instructions describing what actions you want the software to perform. With
script-based software, you typically keep your keys on the keyboard more often
than on the mouse. Many script-based software programs will also allow you to
also send the lines of instructions one at a time in an area referred to as a
<em>console</em>, which will then return the result from each line after you run it.
Script-based software is also sometimes called software that is “used
programatically” <span class="citation">(J. M. Perkel 2018)</span>. Several script-based software programs are
commonly used with biomedical data including R, Python, and Unix bash scripts,
as well as some less common but emerging software programs like Julia.</p>
<p>When comparing point-and-click software to script-based software for
preprocessing, there are a few advantages to point-and-click software, but
many more to script-based software. In terms of code rigor and reproducibility,
script-based software comes out well ahead, especially when used to its
full advantage.</p>
<p>Let’s start, though, by acknowledging some appealing features of point-and-click
software. These features likely contribute to its wide popularity and to the
fact that the vase majority of software that you use in your day-to-day life
outside of research is probably point-and-click.</p>
<p>First, point-and-click software is often easier to learn to use, at least in
terms of basic use. The visual icons help you navigate choices and actions in
the software. Most GUIs are designed to take the underlying processes and
make them easier for a new user to access and use. They do this through
an interface that is visual, rather than language- and script-based.
Further, many people are most familiar with point-and-click
software, since so many everyday applications are of this type, and so its
interface can feel more familiar to users.
They also are easier for a new user to pick up because they typically
provide a much smaller set of options than a full programming language does.</p>
<p>By contrast, coding languages take more investment of time and energy to
initially learn how to use. This is because a coding language is just that—a
language. It is built on a (often large) set of vocabulary that you must learn
to be proficient, as you must learn the names and options for a large set
of functions within the language. Further, it has rules and logic you must learn
in terms of options for how to structure and access data and how the inputs and
outputs of different functions can be chained together to build pipelines for
preprocessing and analysis.</p>
<p>Coding also requires you to be precise in this language. As Brian Kernighan
writes in his book <em>D is for Digital</em>:</p>
<blockquote>
<p>“A computer is the ultimate sorcerer’s apprentice, able to follow instructions
tirelessly and without error, but requiring painstaking accuracy in the
specification of what to do.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<p>However, while there is a higher investment required to learn script-based
software versus point-and-click software, there is also a higher payoff from
that effort. Script-based software creates a full framework for you to
combine tools in interesting ways and to build new tools when you need them.
With point-and-click software, there’s always a layer between the user and
the computer logic, and you are constrained to only use tools that were
designed by the person who programmed the point-and-click software. By
contrast, with script-based software, you have more direct access to the
underlying computer logic, and with many popular script-based languages
(R, Python), you have extraordinary power and flexibility in what you can
ask the program to do.</p>
<p>As an analogy, think about traveling to a country where you don’t yet speak the
language. You have a few choices in how you could communicate. You could
memorize a few key phrases that you think you’ll need, or get a phrase book that
lists these key phrases. Another choice is to try to learn the language,
including learning the grammar of the language, and how thoughts are put
together into phrases. Learning the language, even at a basic level, will take
much more time. However, it will allow you much greater ability to express
yourself. If you only know set phrases, then you may know how to ask someone at
a bakery for loaf of bread, if the person who wrote the phrase book decided to
include that, but not how to ask at a hotel for an extra blanket, if that wasn’t
included. By contrast, if you’ve learned the language, you have learned how to
form a question, and so you can extrapolate to express a great variety of
things.</p>
<p>Point-and-click software is often like using a phrase book for a foreign
language—if the person who developed the tool didn’t imagine something that
you need, you’re stuck. Scripted software is more like learning a language—you
have to learn the rules (grammar) and vocabulary (names of functions and
their parameters), but once you do, you can combine them to address a wide
variety of tasks, including things no one else has yet thought of.</p>
<p>In the late 1990s, a famous computer scientist named Richard Hamming wrote a
book called, “The Art and Science of Engineering”, in which he talks a lot about
the process of building things and the role that programming can play in this
process. He predicted at the time that by 2020, it will be the experts in a
particular field that do programming for that field, rather than experts in
computer programming trying to build tools for other fields <span class="citation">(Hamming 1997)</span>.
He notes:</p>
<blockquote>
<p>“What is wanted in the long run, of course, is that the man with the problem
does the actual writing of the code with no human interface, as we all to often
have these days, between the person who knows the problem and the person who
knows the programming language. This date is unfortunately too far off to
do much good immediately, but I would think by the year 2020 it would be
fairly universal practice for the expert in the field of application to do
the actual program preparation rather than have experts in computers (and
ignorant in the field of application) do the program preparation.” <span class="citation">(Hamming 1997)</span></p>
</blockquote>
<p>The rise of open-source, scripted programs like Python and R is rapidly helping to
achieve this vision—scientists in a variety of fields now write their own
small software programs and tools, building on the framework of larger
open-source languages. Training programs in many scientific fields recommend
or require at least one course in programming in these languages, often
taught in conjunction with data analysis and data management.</p>
<p>Another element that has helped make script-based software more accessible is
the development of programming languages that are easier to learn and use. Very
early programming languages required the programmer to understand a lot about
how the computer was built and organized, including thinking about where and how
data were stored in the computer’s memory. As programming languages have
developed, such “low-level” languages have remained in use, as they often allow
for unmatched speed in processing. However, “higher-level” programming languages
have become more common, and while these might be somewhat slower in
computational processing power, they are much faster for humans to learn and to
create tools with, as they abstract away many of the details that make low-level
programming more difficult.</p>
<p>Because of the development of easier-to-learn high-level programming languages
like R and Python, it is possible for a scientist to become proficient in one of
these script-based programs in about a year. In our own experience, we have
found that often one semester of a dedicated course or serious self-study,
followed with several months of regularly applying the software to research
data, is enough for a scientist to become productive in using a script-based
software like R or Python for research. With another year or so of regular use,
scientists can often start making their own small software extensions to the
language. However, in a 2017 article on analyzing scRNA-seq data, the author
noted that “relatively few biologists are comfortable working in those
environments”, referring to Unix and R <span class="citation">(J. M. Perkel 2017)</span>, and noted that this
was a barrier to using many of the available tools for working with scRNA-seq
data at the time.</p>
<p>It is true that this is a substantially larger investment in training than a
short course or workshop, which might be adequate for learning the basics of
many point-and-click software programs. This can be a
critical barrier, especially for scientists who are advanced in their career and
may have minimal time for further training. Further, it’s more of a barrier in
analyzing some types of biomedical data, due to the extreme size and complexity
of the data <span class="citation">(Nekrutenko and Taylor 2012)</span>.</p>
<p>However, it is much less of a time investment than it takes to become an expert
in a scientific field. It takes years of training to become an expert in
cellular biology or immunology, for example. Richard Hamming’s vision was that
the experts can ask the best and most creative questions of the data, and that
it is best to remove the barrier of a different computer programmer, so that the
expert can directly create the program and leverage the full capabilities of the
computer. Higher-level programming languages now are accessible enough that this
vision is playing out across scientific fields.</p>
<p>Another advantage of script-based software—and one that is related to the idea
of experts in a scientific field directly programming—is that often the most
cutting edge algorithms and pipelines will be available first in scripted
languages, and only later be added into point-and-click software programs. This
means that you may have earlier access to new algorithms and approaches if you
are comfortable coding in a script-based language.</p>
<p>In some cases, biologists aim to analyze data that represents the cutting edge
of equipment and measurement technology, or that is very specialized for a
particular field. In these cases, scripted, open-source packages will often be
the first place where algorithms working with the data are available. For
example, an article about scRNA-seq from 2017 noted that, at the time, there
were “very few, if any, ‘plug-and-play’ packages” for working with scRNA-seq
data, and of those available, they were “user-friendly but have the drawback
that they are to some extent a ‘black box’, with little transparency as to the
precise algorithmic details and parameters employed.” <span class="citation">(Haque et al. 2017)</span>
Similarly, another article in the same year noted that at the time, “most
scRNA-seq tools exist as Unix programs or packages in the programming language
R”, although “some ready-to-use pipelines have been developed”
<span class="citation">(J. M. Perkel 2017)</span>.</p>
<p>Script-based approaches also encourage the user to learn how the underlying process
works. The approach encourages the user to think more like a car owner who gets
under the hood from time to time than like one who only drives the car. This
approach does take more time to learn and develop, but with the upside that the
user will often have a much deeper understanding of what is happenening in each
step, as well as how to fix or adjust different steps to fix a pipeline or
adapt one pipeline to meet another analysis need.</p>
<p>Another key advantage of script-based software is that, in writing the
script, you are thoroughly documenting the steps you took to preprocess the
data. When you create a code script, the script itself includes all
the steps and details of the process. In combination with information about the
version of all software used and the raw data input to the pipeline, it creates
a fully reproducible record of the data preprocessing and analysis.</p>
<p>This means both that you will be able to re-do all the steps
yourself in the future, if you need to, but that also that other researchers can
explore and replicate what you do. You may want to share your process with
others in your laboratory group, for example, so they can understand the choices
you made and steps you took in pre-processing the data. You may also want to
share the process with readers of the articles you publish, and this may in fact
be required by the journal. Further, the use of a code script encourages you to
document this code and this process, even more so when you move beyond a script
and include the code in a reproducible pre-processing protocol. Well-documented
code makes it much easier to write up the method section later in manuscripts
that leveraged the data collected in the experiment.</p>
<p>By contrast, while you could write down the steps that you took and the buttons
you pressed when using point-and-click software, it’s very easy to forget to
record a step. When you use a code script, it will not run if you forget a step
or a detail of that step. Some GUI-based programs are taking steps to try to
ameliorate this, allowing a user to save or download a full record that records
the steps taken in a given pipeline or allow the user to develop a full,
recorded workflow (one example is FlowJo Envoy’s workflow model for analyzing
data from flow cytometry). As a note, there are some movements towards
“integrative frameworks”, which can help improve reproducibility for pipelines
that span different types of software (Galaxy, Gene Prof) <span class="citation">(Nekrutenko and Taylor 2012)</span>.</p>
<p>When you write a script to do a task with data, it is like writing a recipe that
can be applied again and again. By writing a script, you encode the process a
single time, so you can take the time to check and recheck to make sure that
you’ve encoded the process correctly. This helps in avoiding small errors when
you do the preprocessing—if you are punching numbers into a calculator over
and over, it’s easy to mistype a number or forget a step every now and then,
while the code will ensure that the same process is run every time and that it
faithfully uses the numbers saved in the data for each step, rather than relying
on a person correctly entering each number in the calculation.</p>
<p>Scripts can be used across projects, as well, and so they can ensure consistency
in the calculation across projects. If different people do the calculation in
the lab for different projects or experiments, and they are doing the
calculations by hand, they might each do the calculation slightly differently,
even if it’s only in small details like how they report rounded numbers. A
script will do the exact same thing every time it is applied. You can even share
your script with colleagues at other labs, if you want to ensure that your data
preprocessing is comparable for experiments conducted in different research
groups, and many scientific journals will allow supplemental material with
code used for data preprocessing and analysis, or links within the manuscript
to a repository of this code posted online.</p>
<p>There are also gains in efficiency when you use a script. For small
pre-processing steps, these might seem small for each experiment, and certainly
when you first write the script, it will likely take longer to write and test
the script than it would to just do the calculation by hand (even more if
you’re just starting to learn how to write code scripts). However, since the
script can be applied again and again, with very little extra work to apply it
to new data, you’ll save yourself time in the future, and over a lot of
experiments and projects, this can add up. This makes it particularly useful to
write scripts for preprocessing tasks that you find yourself doing again and
again in the lab.</p>
<p>This is often a gain that fully pays back the investment in learning the
software—it can make data preprocessing and analysis much more efficient over
the long term. Code scripts often are easier to automate than a workflow through
a point-and-click system. For example, if you need to process a number of files
that all follow the same format, you can often develop a script using one of
those files, check that script very carefully, and the apply it with minimal
modifications to each of the files in the full set. This allows you to spend
more time on the template script, making sure that it works as you expect, and
then apply it quickly, whereas working through multiple files with
point-and-click software may essentially boil down to a lot of time spent in
repetition. This kind of automation can also help in limiting errors from human
mistakes in by-hand processing <span class="citation">(Gibb 2014)</span>.</p>
</div>
<div id="open-source-versus-proprietary-software" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Open-source versus proprietary software<a href="experimental-data-preprocessing.html#open-source-versus-proprietary-software" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The other dimension to consider for software for preprocessing is whether it is
open-source or proprietary. Open-source software is software where you can
access, explore, and build on all the underlying code for the software. It also
most often is free. By contrast, the code that powers proprietary software is
typically kept private, so you can use the product but cannot explore the way
that it is built or extend it in the same way that you can open-source software.
In biomedical research, many script-based languages are open-source, while many
point-and-click programs are proprietary. However, this is not a hard and fast
rule, and there are examples of open-source point-and-click software (for
example, the Inkscape program for vector graphic design) as well as proprietary
script-based software (for example, Matlab and SAS). There are advantages and
disadvantages to both types of software, but in terms of rigor and
reproducibility, open-source software often has the advantage.</p>
<p>One key advantage of open-source software is that all code is open, so you can
dig down to figure out exactly how each step of the program works. Futher, in
many cases for open-source scientific software, the algorithms and their
principles have gone through peer review as part of the academic publication
process. With proprietary software, on the other hand, details of algorithms may
be considered protected intellectual property, and so it may be hard to find out
the details of how the underlying algorithms work <span class="citation">(Nekrutenko and Taylor 2012)</span>. Also,
the algorithms may not have gone through peer-review, especially if they are
considered private intellectual property.</p>
<p>Transparency is a key element of reproducibility <span class="citation">(Neff 2021)</span>. As Gordon
Lithgow and coauthors note in a commentary on reproducibility, “Improved
reproducibility comes from pinning down methods.” <span class="citation">(Lithgow, Driscoll, and Phillips 2017)</span> If the
algorithms of software can be investigated, then scientists who are using two
different programs (for example, one program in Python and one in R) can
determine if their choice of program is causing differences in their results. By
contrast, if two research groups use two different types of proprietary
software, the algorithms that underlie the processing are often kept secret and
so cannot be compared. In that case, if the two groups conduct the same
experiment and get different results, it’s impossible to rule out whether the
difference was caused by the choice of software.</p>
<p>Gordon Lithgow, Monica Driscoll, and Patrick Phillips wrote a commentary for
<em>Nature</em> describing their experiences in replicating research. They
describe the advice they give students who are trying to do an
experiment that should work:</p>
<blockquote>
<p>“If there is nothing wrong with the reagents and reproducibility is still an
issue, then as I like to tell students, there are two options: (1) the physical
constants of the universe and hence the laws of physics are in a state of flux
in their round-bottomed flask, or (2) the researcher is doing something wrong
and either doesn’t know it or doesn’t want to know it. Then I ask them which
explanation they think I’m leaning towards.” <span class="citation">(Gibb 2014)</span></p>
</blockquote>
<p>If you get different results from another group, it is critical to have a
detailed description of the methods that each group used to figure out why
the groups are getting different results. Open-source software provides this
at the level of computational analysis, because the openness of the software
means anyone can explore the exact details of how each algorithm runs.</p>
<p>Another advantage of open-source software is that older versions of the software
are often well-archived and easily available to reinstall and use if needed to
reproduce an analysis that was done using an earlier version of the software
than the current main version at the time of the replication.</p>
<p>Another advantage is that open-source software is often free. This makes it
economical to test out, and it means that trainees from a lab will have no
problem continuing to use the software as they move to new positions.
The cost with open-source software, then, comes not with the price to buy
the software, but with the investment that is required to learn it.</p>
<p>When data are the output of complex laboratory equipment, there will often be
proprietary software that is available for this pre-processing. This software
may be created by the same company that made the equipment, or it may be created
and sold by other companies. This software might conduct some steps using
defaults, and others based on the user’s specifications. These are often
provided through “GUIs” (graphical user interfaces), where the user does a
series of point-and-click steps to process the data. In some software, this
series of point-and-click steps is recorded as the user does them, so that these
steps can be “re-run” later or on a different dataset.</p>
<p>However, for many types of biological data, including output from equipment like
flow cytometers and mass spectrometers, open-source software has been developed
that can be used for this preprocessing. Often, the most cutting edge methods
for data preprocessing are first available through open-source software
packages, if the methods are developed by researchers rather than by the
companies, and often many of the algorithms that are made available through the
equipment manufacturer’s proprietary software are encoded versions of an
algorithm first shared by researchers as open-source software.</p>
<p>One facet where proprietary software has an advantage is that it will often have
more comprehensive company-based user support than open-source software. The
companies that make and sell proprietary software will usually have a user
support team to answer questions and help develop pipelines and may also offer
training programs or materials.</p>
<p>Some open-source software also has robust user support, although sometimes a bit
less organized under a common source. In some cases, this has developed as a
result of a large community of users who help each other. Message boards like
StackOverflow provides a forum for users to ask and respond to questions. Some
companies also exist that provide, as their business model, user support for
open-source software. While open-source software is usually free, these
companies make money by providing support for that software.</p>
<p>User support is sparser for some of smaller software packages that are developed
as extensions of open-source software. For example, many of the packages for
preprocessing types of biomedical data are built by small bioinformatics teams
or individuals at academic or research institutions. Often this software is
developed by a single person or very small team as one part of their job
profile, with limited resources for user support and for providing training.
These extensions build on larger, more supported open-source software (e.g., R
or Python), but the extension itself is built and maintained by a very small
team that may not have the capacity to respond quickly to user questions. Many
open-source software developers try to create helpful documentation in the form
of help files and package vignettes (tutorials on how to use the software they
created), but from a practical point of view it is difficult for small
open-source developers to provide the same level of user support that a large
proprietary software company can.</p>
<p>This is often the case with cutting-edge open-source software for biomedical
preprocessing. These just-developed software packages are less likely to be
comprehensively documented than longer-established software. Further, it can
take a while for the community of software users to develop once software is
available, and while this is a limitation of new software for both open source
and proprietary languages, it can represent more of a problem for open-source
software, where there is typically not a company-based helpline and so the
community of users often represents one of the main sources for help and
troubleshooting.</p>
</div>
<div id="discussion-questions-2" class="section level3 hasAnchor" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Discussion questions<a href="experimental-data-preprocessing.html#discussion-questions-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>

</div>
</div>
<div id="module13" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Introduction to scripted data pre-processing in R<a href="experimental-data-preprocessing.html#module13" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This module is meant for researchers who do not yet used code scripts but who
are interested in starting or are supervising other researchers who are working
with code for biomedical analysis. Our aim in this module is to provide enough
information that someone without coding experience can gain some comfort in
navigating R code scripts, either to help understand a paper that includes such
scripts as part of its supplemental materials or to help understand the work of
a trainee who is incorporating code in their research. For researchers who are
already using code scripts, we recommend the next module, which provides some
advice on steps that can improve reproducibility when writing scripts for
biomedical data preprocessing.</p>
<p>In this module, we will provide an introduction to scripted pre-processing of
experimental data through R scripts. We will introduce the basic elements of an
R code script as well as the basics of creating and running a script. At the end
of the module, through a video exercise, we will demonstrate how to create,
save, and run an R code script for a simple data preprocessing task.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Describe what an R code script is and how it differs from interactive
coding in R</li>
<li>Explain how code scripts can increase reproducibility of data pre-processing</li>
<li>Create and save an R script to perform a simple data pre-processing task</li>
<li>Run an R script</li>
<li>Work through an example R script using a video exercise</li>
</ul>
<p>Learning to code can seem daunting, but it’s not any more difficult than
learning any new language. Many people from a variety of disciplines have
learned to code to help with their research. Doing so can pay big dividends in
terms of reproducibility and efficiency.</p>
<p>In this section, we’ll provide some tips to make it easier as you get started.
If you are new to coding, these can give you a framework for how to tackle what
can seem the daunting task of learning to code, as well as help you see that
there are approachable techniques. If you already know how to code, these tips
and guidelines can help in improving that code and give you some new directions
in how to code efficiently and reproducibly.</p>
<div id="what-is-a-code-script" class="section level3 hasAnchor" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> What is a code script?<a href="experimental-data-preprocessing.html#what-is-a-code-script" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The simplest method of working with R is through something called <em>interactive
coding</em>. With this style of coding, you enter a single command or function call
at the cursor in the console, tell the program to execute that one element of
code (for example, by pressing the Return key), and then wait until it executes
it before you enter the next command or function call.</p>
<p>A script, on the other hand, is a longer document that gives all the steps in a
process. You can think of a code script as being like a script for a play—it’s
a record of everything that happens over the course of the event. For a play,
the script records the dialogue and stage directions for a play, while for a
data preprocessing task, it can record all the steps from inputting the data
through preprocessing steps and finally saving the data in a processed form for
further analysis, visualization, and statistical testing.</p>
<p>You can run the same code whether you’re using a script or typing in the
commands one at a time in the console as interactive coding. However, when you
code interactively at the console, you’re not making a record of each of your
steps (as a note, there are ways to save the history of commands typed at a
console, but it can be very messy to try to use later to reproduce and remember
what you did originally, so you should consider commands that are typed at the
console to not be recorded for the purposes of reproducibility). When you write
your code in a script, on the other hand, you have a record that you can
later reopen to see what you did or to repeat the steps. In a very broad way,
you can visualize this process as walking in wet sand—you are making a record
(footsteps) of the path you took while you are making that path.</p>
<p>A code script is typically written in a plain text document, and you can create,
edit, and save code scripts in any interactive development environment (like
RStudio if you are programming in R). The program (R for example) can then read
and run this script as a “batch” at any time. In other words, it can walk
through and execute each piece of code that you recorded in the script, rather
than you needing to enter each line of code one at a time in the console. For
many programming languages, you can also run the code in a script in smaller
sections, executing just one or a few lines at a time to explore what’s
happening in each line of the code. With this combination of functionality, as
well as recording of code for future reference or reproduction, code scripts
provide an excellent method for building and using pipelines of code to
preprocess biomedical data.</p>
<p>In later sections of this module, we’ll walk through the practical steps of
writing one of these code scripts. In a video exercise at the end, we’ll look at
an example script for a simple task in biomedical data preprocessing,
calculating the rate of growth of bacteria under different growing conditions.
In this exercise, we’ll walk you through how to open, run, and explore this
script in RStudio.</p>
</div>
<div id="how-code-scripts-improve-reproducibility-of-preprocessing" class="section level3 hasAnchor" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> How code scripts improve reproducibility of preprocessing<a href="experimental-data-preprocessing.html#how-code-scripts-improve-reproducibility-of-preprocessing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the introduction to this book, we provide the definition for computational
reproducibility. Specifically, computational reproducibility typically means
that another researcher could get the exact results of the original study from
the original data collected from a study <span class="citation">(Stark 2018)</span>. Computational
reproducibility, then, requires two main things: the original data and very
thorough instructions that describe how those data were processed and analyzed
<span class="citation">(Nekrutenko and Taylor 2012)</span>.</p>
<p>Neither of these elements is trivial to provide in a thorough way for a complex
biomedical experiment. Raw datasets are often extremely large and complex. To
provide thorough instructions on the processing and analysis requires “access to
… source code or binaries of exact versions of software used to carry out the
initial analysis (this includes all helper scripts that are used to convert
formats, groom data, and so on) and knowing all parameter settings exactly as
they were used” <span class="citation">(Nekrutenko and Taylor 2012)</span>.</p>
<p>By using a code script for data preprocessing (and data analysis and
visualization), you can often substantially improve the computational
reproducibility of your experiment, because the code script itself documents the
exact and precise instructions for how the data are processed and analyzed. For
example, an R script will include all instructions for how the data were loaded
from a file, and will even include the file name where the data are saved, as it
must reference this to input the data. Further, it provides a list of all the
function calls that were run and the order in which they were run. For each
function call, it provides the details on the parameter settings used for that
function. Since R is an open-source language, and its packages are largely
open-source as well, if you know the version of R and each package used in the
script, you can find and read through all the underlying code that defines all
the functions used in the script. In other words, the open-source nature of the
code means that you can, if you want, dig into the algorithms underlying any
step of the process, and so you do not have to consider any step of the script
as a “black box”.</p>
<p>In other words, in the course of writing an executable script to preprocess
data, you are thoroughly documenting each step that you take in that process,
creating one of the key components (clear instructions on how the data were
processed and analyzed) that is necessary to make an experiment computationally
reproducible. Once you have this script, there are only two other elements
that are required to make the experiment fully computationally reproducible:
first, the original, raw data, and second, information on the versions of
any software you used in the code (this would include the version of R that
was used, as well as versions of any R packages that were used to
supplement the base R functions).</p>
</div>
<div id="how-to-write-an-r-code-script" class="section level3 hasAnchor" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> How to write an R code script<a href="experimental-data-preprocessing.html#how-to-write-an-r-code-script" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, we’ll go through some basics to help you get started writing a
code script in R. The process of writing a code script is similar in many other
interpreted languages, like Python and Julia. If you are familiar with writing
code scripts in R, you may find the next module—where we provide some tips on
improving reproducibility when writing scripts—more helpful.</p>
<p>We’ll start with a few basics of the conventions of the R programming language.
If you have never used R before, it is critical to understand these basic
pieces—just enough so you can understand how an R code script is put together
and run. In later modules, we’ll go into some more detail about some helpful
tools in R, including the suite of “tidyverse”
tools that are now taught in most beginner R programming courses. We will, of
course, not have room to provide a full course on how to program in R, but we
are aiming to give you enough of a view that you can understand how R
programming can fit into a data preprocessing and analysis pipeline for
laboratory-based biomedical research projects, as well as how you can navigate
an R script that someone else has written. In module 3.4, we’ll provide
directions to more resources if you would like to continue developing your
expertise in R programming beyond the basics covered in these modules.</p>
<p><strong>What is an R object?</strong></p>
<p>First, you’ll need to understand where R keeps data while you’re working with
it. When you work in R, any piece of data that you work with will be available
in something called an “object”. The simplest way to think of this R object is
simply as a container for data. Different objects can be structured in different
ways, in terms of how they arrange the data—which has implications for how
you can access the data from that object—but regardless of this structure, all
R objects share the same purpose of storing data in a way that’s available to
you as you work in R.</p>
<p>One of the first steps in most R scripts, therefore, will be to create some of
these objects. Until you have some data available, there’s not much interesting
stuff that you can do in R. If you want to work with data that are stored in a
file—for example, data that you recorded in the laboratory and saved in an
Excel file—then you can create an R object with that data by reading in the
data using a specific R function (we’ll cover these in a minute). This will read
the data in R and store it in an object where you can access it later.</p>
<p>To keep track of the objects you have in your R session, you typically assign
each object a name. Any time you want to use the data in that object, or work
with the object in any way, you can then refer to it by that name, rather than
needing to repeat all the code you used to initially create it. You can assign
an object its name using a special function in R called the <em>gets arrow</em> or
<em>assignment operator</em>. It’s an arrow made of the less than and hyphen keys, with
no spaces between the two (<code>&lt;-</code>). You’ll put the name you want to give the object
to the left of this arrow and the code to create the object (for example, to read
in data from a file) to the right. Therefore, the beginning of your R script
will often have one or more lines of code that look like this:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="experimental-data-preprocessing.html#cb1-1" tabindex="-1"></a>my_data <span class="ot">&lt;-</span> <span class="fu">read_excel</span>(<span class="st">&quot;my_recorded_data.xlsx&quot;</span>)</span></code></pre></div>
<p>In this example, the line of code is reading in data from an Excel file named
“my_recorded_data.xlsx” and storing in an R object that is assigned the name
<code>my_data</code>. When you want to work with these data later in the code pipeline, you
can do so with the name <code>my_data</code>, which now stores the data from that file.</p>
<p>In addition to creating objects from the data that you initially read in, you will
likely create more intermediate objects along the way. For example, if you take
your initial data and filter it to a subset, then you might assign that version
of the data to a separate object name, so you can work with that version later in
your code. Alternatively, in some cases you’ll just overwrite the original object
with the new version, using the same object name (for example, creating a subset of
the <code>my_data</code> object and assigning it the same name of <code>my_data</code>). This reassigns the
object name—when you refer to <code>my_data</code> from that point on, it will contain the
subsetted version. However, in some cases this can be useful because it helps keep
the collection of R objects you have in your session a bit smaller and simpler. What’s
more, you can make these changes to simplify the version of the data you’re working
with in R without worrying about it changing your raw data. Once you read the data
in from an outside file, like an Excel file, R will work on a copy of that data, not
the original data. You can make as many changes as you want to the data object in R
without it changing anything in your raw data.</p>
<p><strong>What are R functions and an R function calls?</strong></p>
<p>The next key component of the R programming language is the idea of R functions
and R function calls. These are the parts of R that do things (whereas the objects in R
are the “things” that these functions operate on). An R function is a tool that can
take one or more R objects as inputs, do something based on those inputs, and return a
new R object as the output (occasionally they’ll also have “side effects” beyond returning
this R object—for example, some functions will make a plot and show it in the plotting
window of RStudio).</p>
<p>The R objects that you input can be ones that you’ve assigned to a name (for
example, <code>my_data</code>). They can also be simple objects that you make on the fly,
just to have to input to that function. For example, if you’re reading in data
from a file, one of the R object inputs you’ll need to give the function is the
path to that file, which you could either save as an object (e.g.,
<code>my_data_filepath &lt;- "my_recorded_data.xlsx"</code> and then reference
<code>my_data_filepath</code> when you call the function) or create as an object on the fly
when you call the function (e.g., just put <code>"my_recorded_data.xlsx"</code> directly in
the function call, as shown in the example above).</p>
<p>The function itself is the tool, which encapsulates the code to do something with
input objects. When you use that tool, it’s called <em>calling</em> the function. Therefore,
all of the lines of code in your script will give <em>function calls</em>, where you are
asking R to run a specific function (or, in some cases, a linked set of functions)
based on specified inputs.</p>
<p>For example, the following function call would read in data from the Excel file
“my_recorded_data.xlsx”:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="experimental-data-preprocessing.html#cb2-1" tabindex="-1"></a><span class="fu">read_excel</span>(<span class="st">&quot;my_recorded_data.xlsx&quot;</span>)</span></code></pre></div>
<p>This line of code is calling the function <code>read_excel</code>, which is a tool for inputting
data from an Excel file into an R object with a specific data structure. By running
this line of code, either at the console or in an R script, you are asking R to input
data from the file named “my_recorded_data.xlsx”, which is the R object that you’re
giving as an input to the function. This particular call would only read the data in—it
won’t assign the resulting object to a name, but instead will just print out the data
at the R console.</p>
<p>If you’d like to read the data in and save it in an object to use later, you’ll
want to add another function to this call, so that you assign the output object
a name. For this, you’ll use the gets arrow that we described earlier. This is a
special type of function in R. Most R functions consist of the function’s name,
followed by parentheses inside of which you put the objects to input to the
function (e.g., <code>read_excel("my_recorded_dat.xlsx"</code>). The gets arrow is a
different type of function called an <em>operator</em>. These functions go between two
objects, both of which are input to the operator function. They’re used often
for arithmetic (for example, the <code>+</code> operator adds the values in the objects
before and after it, so that you can call <code>1 + 2</code> to add one and two). For the
gets arrow, it will go between the name that you want to assign to the object
(e.g., <code>my_data</code>) and the function call that creates that object (e.g.,
<code>read_excel("my_recorded_data.xlsx")</code>):</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="experimental-data-preprocessing.html#cb3-1" tabindex="-1"></a>my_data <span class="ot">&lt;-</span> <span class="fu">read_excel</span>(<span class="st">&quot;my_recorded_data.xlsx&quot;</span>)</span></code></pre></div>
<p>In this case, the line that R will execute will include two functions, where the
output of one gets linked straight into the second, and the result will be the
output from the second function (that the data in the Excel file is stored in
an object assigned the name <code>my_data</code>).</p>
<p>As you write an R script, you will use function calls to work through the
steps in your pipeline. You can use different function calls to do things like
apply a transformation, average values across groups, or reduce dimensions of
a high-dimensional dataset. Once you’ve preprocessed the data, you can also use
function calls to run statistical tests with the data and to visualize results
through figures and tables.</p>
<p>The process of writing a script is normally very iterative—you’ll write the
code to do the first few steps (e.g., read in the data), look at what you’ve
got, plan out some next steps, try to write some code for those steps, run it
and check your output, and so on. The process is very similar to drafting a
paper. You can try things out in early steps—and some steps won’t work out at
first, or it will turn out that you don’t need them. As you continue, you’ll
refine the script, editing it down to the essential steps and making sure each
function call within those steps is operating as you intend. While it can be
intimidating to start with a blank file and develop some code—just like it is
with a blank piece of paper when writing a manuscript—just like with writing,
you can start with something rough and then iterate until you arrive at
the version you want.</p>
<p>This process might seem a bit overwhelming when you first learn it, but it
suffices at this point if you understand that, in R code, you’ll be working with
objects (your materials) and functions (your tools). As we look through R
scripts in the video exercise of this module, we’ll see these two pieces—objects and
functions—used again and again in the scripts. They are the building blocks
for your R scripts.</p>
<p><strong>What is an R library?</strong></p>
<p>There’s one last component of R that will be helpful to understand as we move through
the rest of this module and the next few modules. That’s the idea of an R package, and
fortunately, it’s a pretty straightforward one.</p>
<p>We just talked about how functions in R are tools, which you can use to do interesting
things with your data (including all the preprocessing steps we talked about in
modules 3.1). However, the version of R that you initially install to your computer
(available for free for all major operating systems at <a href="https://cran.r-hub.io/" class="uri">https://cran.r-hub.io/</a>) doesn’t
include all the tools that you will likely want to use. The initial download gives you
the base of the programming language, which is called <em>base R</em>, as well as a few
extensions of this for very common tasks, like fitting some common statistical models.</p>
<p>Because R is an open-source software, people who use R can build on top of this
simple base. R users can create new functions that combine more rudimentary
tools in base R to create customized tools suited to their own tasks. R users
can create these tools for their own personal use, and often do, but there is
also a mechanism for them to share these new tools with others if they’d like.
They can bundle a set of R functions they’ve created into an <em>R package</em> and
then post this package on a public repository where others can download it and
use the functions in it. In some of the examples in these modules, we’ll be
using tools from these packages, and it’s rare that someone uses R without using
at least some of these supplementary packages, so it’s good to get an idea of
how to get and use them.</p>
<p>The people who make packages can share them in a number of repositories, but the
most standard repository for sharing R packages widely is the Comprehensive R
Archive Network (CRAN). If a package is shared through CRAN, you can get it
using the function <code>install.packages</code> along with the package’s name. For
example, in the code we showed earlier, the <code>read_excel</code> function does not come
with base R, but instead is part of a package called <code>readxl</code>, which is shared
on CRAN. To download that package so that you can use its functions, you can
run:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="experimental-data-preprocessing.html#cb4-1" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;readxl&quot;</span>)</span></code></pre></div>
<p>This will download the code for the package and unpack it in a special part of
your computer where R can easily find it. You only need to install a package
once, at least until you get a new computer or update your version of base R.
However, to use the functions in that package, you’ll need to <em>load</em> the package
in your current R session. This makes the functions in that package available to
you as you work in that R session. To do this, you use the <code>library</code> function,
along with the name of the package. For example, to load the <code>readxl</code> package in
an R session, you’d need to run:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="experimental-data-preprocessing.html#cb5-1" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;readxl&quot;</span>)</span></code></pre></div>
<p>While you only need to install a package once, you need to load it every
time you open a new R session to do work, if you want to use its functions in
that R session. Therefore, you’ll often see a lot of calls to the <code>library</code>
function in R scripts. You can use this call anywhere in the script as long as
you put it before code where you use the library’s functions, but it’s great to
get in the habit of putting all the <code>library</code> function calls at the start of
your R script. That way, if you share the script with someone else, they can
quickly check to see if they’ll need to install any new packages before they can
run the code in the script.</p>
<p><strong>Creating a script</strong></p>
<p>Based on the points that we’ve just discussed, hopefully you can envision now
that an R script will ultimately include a number of lines of code, covering a
number of R function calls that work with data stored in objects. You can expect
there to be lots of calls that assign objects their own names (with <code>&lt;-</code>), and
the function calls will typically include both a function called by name and
some objects as input to that function, contained inside parentheses after the
function name.</p>
<p>This type of script should be written in plain text, and so the best way to
create an R script is by using a text editor. Your computer likely came with a
text editor as one of the pieces of utility software that was installed by
default. However, with R scripts, it can be easier to use the text editor that
comes as part of RStudio. This allows you to open and edit your scripts in a
nice environment, one that includes a console area where you can test out pieces
of code, a pane for viewing figures, and so on.</p>
<p>In RStudio, you can create a new R script by going to the “File” menu at the top
of the screen, choosing “New File” and then choosing “R Script”. This will open
a new plain text file that, by default, will have the file extension “.R” (e.g.,
“my_file.R”), which is the standard file extension for R scripts. Once you’ve
created an R script file, you can begin writing your script. In the next
section, we’ll walk through how you can run code that you’ve put into your
script. However, we think it’s worth mentioning that, as you get started on this
process, you might find it easiest to start not by writing your own R script
from scratch, but instead by starting with someone else’s and walking through
that. You can explore how it works (reverse engineer it). Then you can try
changing small parts, to see if it acts as you expect when you do. This process
will help you get a feel for how these scripts are organized and how they
operate. In the video exercise for this module, we’ll provide an R script for a
basic laboratory data preprocessing task and walk you through it, so you can use
that as a starting point to understand how it would work to create, edit, and
run your own R script.</p>
</div>
<div id="how-to-run-code-in-an-r-script" class="section level3 hasAnchor" number="3.3.4">
<h3><span class="header-section-number">3.3.4</span> How to run code in an R script<a href="experimental-data-preprocessing.html#how-to-run-code-in-an-r-script" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Once you’ve written code in an R script, you can run (execute) that code in a
number of ways. First, you can run all the code in the script at once, which is
known as <em>batch execution</em>. When you do this, all the code in the script will be
executed by R, and while it’s executed by R one line at a time, you won’t have
the chance to make changes along the way. If you compare it to the idea of a
code script to a play script, you can think of this as being like when the
play is performed for an audience—once you start the play, you don’t have
the chance to stop and work on it as it’s going. Instead, it will go straight
through to the end. If there is an error somewhere along the way, then the code
will stop running at that point and you’ll get an error message, but otherwise
when you run the code as a batch, R won’t stop executing the lines until it gets
to the end. This mode of running the code is great for once you’ve developed a
pipeline that you’re happy with—it quickly runs everything and provides the
output.</p>
<p>The other way that you can execute the code is by running a single line, or a
small set of lines, of the code at a time. In the play analogy, this is similar
to what might happen during rehearsals, when you go through part of the play
script and then stop to get comments from the director, then either re-try that
part with a few changes or move on to the next small part. This mode of running
the code is great for when you’re developing the pipeline. Just like with a
play’s rehearsals, you’ll want a lot of chances to explore and change things as
you develop the final product, and this mode of running code is excellent for
exploration and editing. Often, most of your time when you code will be spent
doing this style of code execution. Running in batch mode will get a lot of work
done, but is very quick for the programmer—developing the code is what takes
time, and just like with writing a manuscript, this time comes from drafting a
rough draft and then editing it until you arrive at a clean and clear final
version.</p>
<p>Both of these methods of code execution are easy to do in RStudio. Since you’ll
usually start by using line-by-line execution, we’ll start with showing how you
can do that. In RStudio, you can open your code script (a file ending in “.R”),
and you will still be able to see the console, which is a space for submitting
function calls to R. To execute the code in the script one line at a time,
there’s a few quick ways that you can tell RStudio to send that line in the
script to the console and run it. Start by putting your cursor on that line of code.
One way to now execute this line (i.e., send it to the console to run) is to
click on the “Run” button in the top right-hand corner of the script file. If
you try this, you should see that this line of code gets sent to the console
pane of RStudio, and the results from running that line are shown in the
console.</p>
<p>Even quicker is a keyboard shortcut that does the same thing. (Keyboard
shortcuts are short control sequences that you type in your keyboard to run a
command. They’re faster than clicking buttons because you can do them without
taking your hands off the keyboard. Ctrl-C is one very common one that you might
have used before, which in most programs will copy the current selection.) For
running a line of R code, with your cursor on the line of the function call that
you want to execute, use the keyboard shortcut Ctrl-Return (depending on your
operating system, you may need to use Command rather than Ctrl).</p>
<p>You can use a similar method to run a few lines of code at once. All you have to
do is highlight the code that you want to run, and then you can use either of
the two methods (click the “Run” button or use the Ctrl-Return keyboard shortcut).
We will show you examples of how to do this in the video exercise at the end
of this module.</p>
<p>To execute an R script in batch mode, there are again a could of ways you can do
it. First, there is a “Source” button in the top right of the R script file when
you open it in RStudio. You can click on this button and it will run the entire
script as a batch. There is also an R command that you can use to source a file
based on its file name, <code>source</code>. If you have a file in your working directory
named “my_pipeline.R”, for example, you can execute the code in it in a batch by
running `source(“my_pipeline.R)”.</p>
<p>To get started, it’s probably easiest to just use the buttons “Run” and “Source”
that RStudio provides in the window for the R script file. As you do more work, you
may find some of these other methods help you work faster, or allow you to do
more interesting things, so it’s good to know they’re there, but you don’t need
to try to navigate them all as you learn how to run code in an R script.</p>
</div>
<div id="exercise" class="section level3 hasAnchor" number="3.3.5">
<h3><span class="header-section-number">3.3.5</span> Exercise<a href="experimental-data-preprocessing.html#exercise" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As we mentioned earlier, it can be helpful as you learn how to navigate R
scripts to start by dissecting and exploring an existing script. In the video
that is embedded here, we give an example of that using a script that captures
an example given in a previous module. We will walk through how this script
captures some of the elements discussed in this module.</p>
<p>[Resources for exercise]</p>
<p>[Video of exercise]</p>

</div>
</div>
<div id="module13a" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Tips for improving reproducibility when writing R scripts<a href="experimental-data-preprocessing.html#module13a" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This module is meant for researchers who are using R already as part of their
research. It is meant as a complement and alternative to the previous script,
which focused on readers who are new to creating code scripts.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Improve the reproducibility of their scripts by leveraging tips meant for
researchers who are already incorporating scripts in their research</li>
</ul>
<p>Some biomedical researchers have already worked quite a bit with a programming
language like R, either in a role that is primarily computational, or as a
way to understand data they’ve collected themselves from the wet lab. While
the last module focused on scientists who are new to coding, to help give them
an entry point into how to write and run a code script in R, this module
focuses on a different audience—scientists who are familiar with coding but
would like to take steps to improve their practice.</p>
<p>We have worked with a number of scientists in this situation. This module
provides a series of tips for how they can improve their coding practice to
make it more rigorous and reproducible. These are tips based on our own
experiences of the things that—in real and regular practice—get in the
way of code being rigorous and reproducible.</p>
<p>We’ll provide advice in three main areas:</p>
<ul>
<li>Write code for computers, but edit it for humans</li>
<li>Modify rather than start from scratch</li>
<li>Do not repeat yourself</li>
</ul>
<div id="write-code-for-computers-but-edit-it-for-humans" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Write code for computers, but edit it for humans<a href="experimental-data-preprocessing.html#write-code-for-computers-but-edit-it-for-humans" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A key requirement for a project to be computationally reproducible is that any
of the code used for pre-processing or analysis in that project is available.
However, even when code for a project is available, it can be hard to understand
and reproduce the analysis. One common culprit is that the code isn’t written
for humans to read. One way to improve the reproducibility of your code,
therefore, is to make sure you edit it so that it’s clear for humans, not just
computers.</p>
<p>During World War I and World War II, the British and US used a special type of
camouflage on some of their ships called “dazzle camouflage”. This type of
camouflage uses large geometric shapes, often in black and white, and it makes
the ships look a bit like zebras. Unlike other types of camouflage, this type
doesn’t conceal the ship—it’s still very clear that it’s there. However, to be
able to hit a ship at sea, people needed to know not only where it was, but also
where it was going. This is because the ship will move between the time that a
ballistic is fired and when it lands, and so people needed to calibrate to aim
where the ship would be by the time the ballistic got to it. Dazzle camouflage
makes it much harder to make out where the ship is headed.</p>
<p>Often, people will write code for research projects that looks like it’s using
dazzle camouflage. It’s easy to see that there’s something there when you look
at the code script, but it’s very hard to figure out what it’s doing or where
it’s trying to go. In other words, it’s hard for a human to quickly digest. This
type of code will be hard for others to figure out, and also it will be hard for
you to figure out when you come back to the code in the future.</p>
<p>The best way to avoid this type of code is to get into the practice of editing
your code. When you first write code, you don’t want to write it slowly and
carefully—rather, you’ll usually be best at figuring out how to get something
to work if you get in the flow and get down some code without worrying about how
legible it is to humans.</p>
<p>This is fine, but get in the habit of thinking of this as just the first step:
in your initial coding, you’re getting the code to work for the computer, but
later you will need to go back and clean up the code so it’s clear for humans,
too. Editing the code will make it easier to understand (both by others and by
you) and will also make the code easier to maintain and extend in the future.</p>
<p>This idea is similar to writing. Many writing experts recommend that you consider
your writing process as having several stages. First, you want to write in a
drafting process, where you get your ideas on paper but without editing yourself
much. This is a stage of getting the ideas out. In a separate stage, you can
edit, and at this stage you want to have your audience clearly in mind, editing
to make the writing clear for them. By separating this, you can use your mind
in a more creative, less constrained way as you create ideas, and then in a
more critical way as you refine those ideas for your audience.</p>
<p>While this practice is familiar to many writers, it’s less well known to
scientists who are also coders. If you don’t already, try incorporating editing
stages as you develop your code. It is most helpful to take time to edit code if
you’re still within a day or two of writing it, so it’s helpful to work in this
editing stage fairly frequently. Since it often requires less energy and brain
power than the initial stage (getting the code to work with the computer), it
can be helpful to incorporate editing time at times in your day when your energy
is otherwise low. For example, taking ten or fifteen minutes to edit existing
code can be a good way to start coding for the day, before you get to the
heavier lifting of writing new code.</p>
<p>As you edit your code, there are a few specific things that you can do to make
it clearer for humans to read. Some editing steps that we will cover in this
module are:</p>
<ul>
<li>Improve the names you’re using within the code</li>
<li>Break up monolithic code</li>
<li>Add useful comments</li>
<li>Remove dead-end code</li>
</ul>
<p>Let’s take a closer look at how you can do each of these steps.</p>
<p><strong>Improve names within the code</strong></p>
<p>One important step when editing your code is to use better names for things like
data objects and columns within dataframes. When you’re coding quickly, you
might often use “placeholder” types of names for objects. For example, a coder
might tend to name objects “df” (for “dataframe”) or “ex” (for “example”) as
they’re first getting the code to work.</p>
<p>There’s no problem in using these types of generic names as you initially
develop your code. In fact, there’s a rich history of these placeholder
object names. They even have a fancy name, <em>metasyntactic variables</em>.
Different coding languages have developed different ones that are popular,
as have coders in different countries. For example, many C programmers will
name things “foo” and “bar” as they initially work on their code, while
Italians often use the Italian words for different Disney characters
(“pippo”, “pluto”, “paperino”).</p>
<p>The problem isn’t in using these placeholder names; the problem is when you
don’t later edit your code to use better names. These generic types of names
will tell you nothing about what’s stored in each object when you go back
and read the code later. With better names for each object, you can read through
the code and in some ways it will document itself, without even needing to
read the code comments to figure out what’s going on.</p>
<p>There is a style guide that is focused on the tidyverse approach available
at <a href="https://style.tidyverse.org/syntax.html" class="uri">https://style.tidyverse.org/syntax.html</a>. It includes guidance on how to
select good names for objects in R within its section on “Syntax”. Generally,
some good principles include that the name of the object should give you an
idea of what’s contained in the object. For example, if you have a dataframe
that has the weights of mice from your experiment, it’s much better to name
it “mouse_weights” rather than something generic like “foo”. Some of the
other guidance will help make your life easier as a coder, including things
like using only lowercase letters.</p>
<p>You can also edit the names of columns within dataframes, which can help improve
the clarity of your code, especially since code will often reference specific
columns by name. Similar principles apply to column names: ideally, you want
their names to describe what they contain. There are also some rules that will
make it easier to work with the column names. For example, column names can
include spaces, but if they do, it makes using them within R harder. Each time
you refer to that column name, you have to surround the name in backticks so R
will process its full name as a single name, rather than thinking its name ends
at the first space. This becomes a pain as you write a lot of code that refers
to that column. It’s also helpful to keep column names fairly short, so you can
see the full name as you work with the dataset and resulting output.</p>
<p>When it comes to column names, some of your editing might be to improve names
that you quickly wrote yourself as you coded. However, a common reason for
ungainly column names is that you’ve read in data from a file format like Excel,
where it was easy for the person who entered the data to include spaces and
special characters in the column names. In this case, there are some tools in
R that can help you quickly improve the column names. In particular, the
<code>janitor</code> package has a function called <code>clean_names</code> that will do a lot of the
work for you, including converting the name to lowercase, removing special
characters (like “*” and “&amp;”), and replacing spaces with underscores. If you
need to make more targeted changes to column names, you can do so using the
<code>rename</code> function from the <code>dplyr</code> package.</p>
<p><strong>Break up monolithic code</strong></p>
<p>The next thing you can do to edit your code is to break up “monolithic”
code—that is, code that isn’t clearly divided to show sections or steps in the
process. When you’re first creating your code, you won’t want to take the time
to nicely organize it into logical sections. However, once you are ready to edit
your code, you will find that breaking into clear sections and labeling them
will help you and others navigate the code first at a higher level
(understanding the big picture of how it works by looking at the major steps it
takes) and only diving into the details of each section once the big picture is
clear.</p>
<p>Again, this process mimics a process used by many writers. It’s common to
create drafts and notes that lack clear organization, but instead are just
collecting the raw material that will be shaped into a final article or book.
However, this raw material then needs to be organized and edited to make it
into something that others can navigate and make sense of. The writer Robert
Caro famously wrote massive books about political figures like Robert Moses and
Lyndon Johnson. To wrangle all his research into books, he noted that,
“I can’t start writing until I’ve thought it through and can see it whole
in my mind.”</p>
<p>In a similar way, once you’ve gotten code to work, you should make sure you have
a clear picture of the whole process and how it tackles the problem at a
“big picture” level. One of these steps might be something like reading in
and cleaning the data, while another step might be identifying and addressing
outliers in the data. Once you’ve identified the big steps, try as much as
possible to group the code into these big steps, then you can use code comments
and blank lines in your code to separate these sections and label them to
describe what they’re doing.</p>
<p>As you do this, you might find that you move some of your code around in the
script, which is fine as long as it doesn’t affect the computer being able to
process the script. For example, one of your big steps might be loading in
packages you’ll need. Rather than having a lot of <code>library</code> calls sprinkled
throughout your code, you can group these all together at the start of your
script in a section called something like “Loading packages”. This will
clean up these calls from other parts of your script; also, by having all
your library calls at the start of the code script, another person could
immediately see which packages they’ll need to have installed to run your code.</p>
<p>Another way that you can break up monolithic code is to split it into more lines.
R will process the code whether it’s all on one line or split into separate
lines: R just keeps reading until it gets to the end of the function call
either way. This means that you can use the “Return” key to break up your code
lines so you’re always able to see the full line of code without scrolling.</p>
<p>One common standard is to keep all of your lines of code to 80 characters or
fewer. RStudio has the functionality to reformat your code to meet this
standard. In the RStudio menus, if you go to the “Code” menu, you can select
to “Reformat Code”. This can help clean up long lines of code in your editing
process.</p>
<p><strong>Add useful comments</strong></p>
<p>As you are breaking up monolithic code, it can also be helpful to use
code comments to add notes about why you are doing certain things. In R, you
can add a code comment anytime after a <code>#</code> on a line; R won’t read anything
that comes after that symbol on a line. You can use this to add small messages
for humans that describes your code.</p>
<p>As you add these comments, keep in mind that it’s often more useful to
describe why you’re doing something rather than what you’re doing. With a lot
of R code—especially in the tidyverse approach—the functions often have
names that clearly describe what they do. For example, the function that
you use to rename a column is called <code>rename</code>, while the function that you
use to select certain columns is called <code>select</code>. Therefore, your code should
do a fairly good job of self-documenting in terms of describing what it’s
doing.</p>
<p>Instead, you can use code comments to remind yourself or others of
why you’re implementing certain steps. For example, rather than having a code
comment that says “Rename columns”, you could say, “The columns that come
from the Excel file generated by the cytometer include a lot of special
characters, which we need to remove to make it easier to work with the data in
R.” By explaining why you’re doing something, you’ll also help yourself when it’s
time to maintain or extend your code. You’ll be able to tell, for example,
whether changing or deleting a certain line of code will cause a big problem
in other areas of code.</p>
<p><strong>Remove dead-end code</strong></p>
<p>Another useful step when you edit your code is to edit out pieces we’ll call
“dead-end code”. These are pieces of code that aren’t contributing to the
process of your script.</p>
<p>There are two main types of this dead-end code that we often see. First, there’s
code that you use during your interactive coding process to check on things. For
example, you might use the <code>View</code> function to take a look at a data frame at a
certain step in your process, or use functions like <code>summary</code> and <code>str</code> to
explore what’s in different objects.</p>
<p>It’s great to do this kind of exploration as you code; in fact, one of the
advantages of interactive software like R is that you can explore as you
develop your scripts. However, these are tools that help you develop a script,
but not ones that are necessary for the final script to run. Instead, they just
gunk up the code that’s doing the real work.</p>
<p>There are two things you can do regarding this type of dead-end code. The first
is that you can get in the habit of running it in your console, rather than
having it in your script, even when you’re developing the code. However, this
does require switching between the console and the script as you write code,
which can interrupt your flow. An alternative is to run these in a script as
you write the code, but then delete any of these exploratory calls as you
edit your script.</p>
<p>There’s also a second type of dead-end code. This is code that you wrote to try
out to solve a particular problem, but that ultimately didn’t work (or that
you replaced with a better approach). Often, you may have worked a long time
on that piece of code, or it might contain some really clever approach that
you’re proud of. However, leaving it your script, if it isn’t contributing to
the ultimate process you ended up with, will only get it the way of understanding
your primary code. It will lead a reader down a rabbit hole, rather than allowing
them to move step by step through your logic.</p>
<p>In writing, there are similarly areas that aren’t contributing to the forward
movement of a piece but that authors are reluctant to remove because they love
them for one reason or another. This has resulted in the famous advice to
authors (from Stephen King, among others) to “murder your darlings”. In other
words, be brave enough to edit out anything that isn’t contributing to the
necessary progress of your piece. Coders should take this advice in a similar
way when it comes to pieces of code in their scripts that don’t ultimately
contribute to the pipeline they’ve developed.</p>
</div>
<div id="modify-rather-than-start-from-scratch" class="section level3 hasAnchor" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Modify rather than start from scratch<a href="experimental-data-preprocessing.html#modify-rather-than-start-from-scratch" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>[Anecdote: reverse engineering? Hardware Hacker?]</p>
<p>[<em>Everyone</em> does this, it’s part of the open-source aesthetic. Also, long
history in science. See anecdotes / quotes about adapting.]</p>
<p>[There are many places that you can find example code to start from:
vignettes, helpfiles, StackOverflow, code included with papers (although
be mindful of attribution / permissions), cheatsheets, even ChatGPT.]</p>
<p>As you code, keep in mind that you shouldn’t reinvent the wheel. There are
excellent resources available that can help provide you with a starting
point for many of the coding tasks you’ll need to do. For example, most
packages have tutorials called vignettes that provide a starting point
in how to use the package, and helpfiles often include short example code
that you can adapt to your own purposes. Online resources like StackOverflow
also provide advice and example code for many challenges you might come up
against as you’re coding. Google can also be used to help you solve coding
problems, especially if you become familiar with some of its special
operators, which can help you refine your search (see <a href="https://support.google.com/websearch/answer/2466433?hl=en" class="uri">https://support.google.com/websearch/answer/2466433?hl=en</a> for more on this).</p>
<p>There’s no problem with using any of these as starting points as you develop
your own code. However, it’s often tempting for a coder to leave some lines
of code “as-is” if they’ve found an example solution that works. Instead, it’s
critical that you make sure you fully understand why each line of code in your
script works the way it does. Further, if you’re adapting example code to
your own problem, you should edit it if possible to use the set of tools you’re
most familar with.</p>
<p>When you find a piece of example code that you think will help with something
you need to do in your own code, you’ll first want to make sure that you can
get it to work with any example data it came with, before you try it out with
your own data. If it won’t run with its own data, there are a few trouble-shooting
steps you can take. First, make sure you have all the required packages
installed and loaded. Second, make sure that you’ve saved the example data
to the right place on your computer if the example code reads in data from
a file. Finally, make sure that you have the same versions of
packages and of R. If the code still doesn’t work after you’ve resolved these
issues, you may want to move on to finding other example code.</p>
<p>Once you’ve gotten the code to run on the example data, walk through it line
by line to understand what it does. For each step, make sure you understand
what the input looks like and what the output looks like. If code is nested
(function calls are placed within function calls), be sure that you understand
the code at each level of nesting. If the code uses piping to move the output
of one call to the input of the next, make sure you’ve worked through each
of the lines in the pipe individually.</p>
<p>There are two tools that can help as you dissect the code in this way. First,
when you work in R study, you can highlight code in a script and then use the
“Run” button to run only that code. This functionality allows you to run a
nested function call without running the whole line of code, or to run only part
of a series of piped calls (by highlighting everything up to the piping symbol
on a line and then running it). The other tool that’s useful is a function in
the <code>dplyr</code> package called <code>pull</code>. This function allows you to extract a column
from a dataframe as a vector. This is helpful when you’re dissecting nested
calls in piped code, as often a function will operate on a single column of the
dataframe. This function allows you to pull out that column and then test the
function call to see what it’s doing with that column.</p>
<p>Once you figure out what the example code does on the data it comes with, you
can adapt it to work with your own data. As you do, pay close attention to
how your data are similar or different to the example data. At this stage, your
goal will be to get the example code to work with your data.</p>
<p>Many researchers stop at this step—they’ve gotten example code to work with
their own data (and hopefully worked through it to understand why). However,
example code often follows a very different style than code you write yourself.
For example, you may use the tidyverse approach, while the example might use
code written in a base R style. Further, different coders think about and
tackle problems in different ways, which can lead to the case where any
example code that you’ve adapted in your script feels different from your usual
code.</p>
<p>This can result in spots of your code that you will later be very worried to
change, because while it works, you don’t understand why well enough to feel
comfortable making any change. This can make your code fragile and hard to
maintain. Instead, take a moment to adapt the logic that you’ve learned from
the example to use your own set of tools. For example, if the example code is
written in base R but you prefer tidyverse tools, rewrite the code’s logic
to use tidyverse tools.</p>
<p>You gain several advantages when you adapt code to use the tools you’re familiar
with. For example, bugs will be less likely, and if there are bugs, you’ll
catch them more quickly, since you’re familiar with the tools that the code is
using. The code will also be much easier for you to understand and maintain in
the future.</p>
<blockquote>
<p>“The sandbox of the budding builder is not <em>making</em> as much as it is
modification: taking something that exists and making it better, either
functionally or aesthetically or both. Often that involves attaching and
securing parts that were not originally intended to go together.” <span class="citation">(Savage 2020)</span></p>
</blockquote>
<p>As you start learning to write code in R, don’t force yourself to stare at an
empty R script file and try to come up with a full script from scratch. One of the
best ways to learn R is to find some scripts that others have written for tasks
that are similar to the ones that you want to do, then work through those to figure
out each function call, and how those function calls add up to the full pipeline.</p>
<p>This method of reverse engineering is useful in many areas when you’re trying to
figure out how things work. …</p>
<p>Once you understand a few other R scripts, you can start trying to modify them and
to pull pieces from different scripts to use as building blocks as you put together
your own script. There’s no need to reinvent the wheel—if someone else has
shared an R script that comes close to doing what you need to do, start there and
then change and evolve that idea to suit your own needs.</p>
<p>To find some starting scripts to learn from, there are a few tactics you can try.
First, check around with colleagues to see if they have R code for data preprocessing
tasks that they do in their lab. If they work with similar types of data, and use R,
they’re likely to have come up with some scripts that achieve tasks you also need to
do.</p>
<p>Another excellent source of example R code are the vignettes and examples that come
with many R packages. If you are using functions from an R package, then there is likely
a vignette that comes with that package, and there may also be examples within the
helpfiles for each of the package’s functions. A package vignette is a tutorial that
walks you through the major functionality of the package, showing how to use the
key functions in the package in an extended example. Some packages will have multiple
vignettes, showing a range of things that you can do with the package.</p>
<p>To find out if there is a vignette for a package that you’re using, you can google the
package name and “vignette”. You can also find out from the console in R using the
function <code>vignette</code>. For example, to find out if the package <code>readxl</code>, which helps read in
data from Excel files, has any vignettes, you can run <code>vignette(package = "readxl")</code>.
This will tell you that the package has two, one called “cell-and-column-types” and
one called “sheet-geometry”. To open one of these, you can again use the <code>vignette</code>
function. For example, <code>vignette("cell-and-column-types", package = "readxl")</code> would
open the first of the two vignettes within your R session.</p>
<p>To open the helpfile for any function in R, at the console type a question mark and then
the function name. For example, <code>?read_excel</code> will open the helpfile for the <code>read_excel</code>
function (you will need to make sure you’ve run <code>library("readxl")</code> to load the package
with this function). The helpfile provides useful information for running the function,
and one of the most useful parts is the “Examples” section. Scroll down to the bottom
of the helpfile to find this section. It includes several examples that you can copy into
your R script or console and try yourself, to figure out the types of inputs that the
function needs and how different options for the function modify how it works.</p>
<p>[How to dissect code: reverse engineering. Steps: (1) run with example
data; (2) understand required input and expected output; (3) for nested code,
word inside out; (4) for piped code, one line at a time]</p>
<p>[How to adapt code for tools you don’t know: adopt <em>idea</em> to tools you do know;
learn any tools as new tools for cases that can’t be adapted.]</p>
<blockquote>
<p>“All creative work builds on what has gone before. When someone declares
that something is original, it’s because they are unaware of the influences.
The creative make the most of things they admire and aren’t ashamed to
be inspired by something they respect. The bad news: everything has already
been done. The good news: it can be done again.” <span class="citation">(Judkins 2016)</span></p>
</blockquote>
<p>[Anecdote: adapting]</p>
<blockquote>
<p>“Extracting penicillin from the mold was no child’s play… Instead of
designing and building a reactor for the chemical reactions from scratch—which
meant more time, money, and uncertainty—[Margaret] Hutchinson opted for something
that was already functional. Some researchers had found that mold from
cantaloupe could be an effective source for penicillin, so she started
there. Her team then revised a fermentation process that Pfizer was using to
produce food additives like citric acid and gluconic acid from sugars, with
the help of microbes. Hutchinson swiftly helped convert a run-down
Brooklyn ice factory into a production facility. The deep-tank fermentation
process produced great quantities of mold by mixing sugar, salt, milk, minerals,
and fodder through a chemical separation process that Hutchinson knew very
well from the refinery business.” <span class="citation">(Madhavan 2015)</span></p>
</blockquote>
<p>[Anecdote: adapting]</p>
<blockquote>
<p>“Johannes Gutenberg invented his printing press by repurposing a wine
press for use with olive oil–based ink and block printing.” <span class="citation">(Madhavan 2015)</span></p>
</blockquote>
<p>However, it is critical that you work through and understand any example code
that you bring in and modify in your own workflow.</p>
<blockquote>
<p>“Appropriate methods are ‘very data-set dependent’… The methods and tuning
parameters may need to be adjusted to account for variable such as sequencing
length. But John Marioni at Cancer Research UK in Cambridge says it’s important not
to put complete faith in the pipeline. ‘Just because the satellite navigation
tells you to drive into the river, you don’t drive into the river,’ he says.”
<span class="citation">(J. M. Perkel 2017)</span></p>
</blockquote>
<blockquote>
<p>“Do not reinvent the wheel. It pays to reuse existing software. Integrative
frameworks and associated application stores already house hundreds of tools
(for example, as of May 2012, Galaxy ToolShed contains ~ 1,700 tools). It is
likely that a script for a particular problem has been already written. Ask
around through existing resources such as SEQanswers43 and BioStar44.”
<span class="citation">(Nekrutenko and Taylor 2012)</span></p>
</blockquote>
<blockquote>
<p>“If you’re going to build a house today, you don’t start by cutting down trees to make
lumber and digging clay to make your own bricks. Instead, you buy prefabricated pieces like
doors, windows, plumbing fixtures, a furnace, and a water heater. House construction is still
a big job, but it’s manageable because you can build on the work of many others and rely
on an infrastructure, indeed an entire industry, that will help. The same is true of
programming. Hardly any significant program is created from nothing. Many components written
by others can be taken off the shelf and used. For instance, if you’re writing a program for
Windows or a Mac, there are libraries of prefabricated menus, buttons, text editors, graphics,
network connections, database access, and so on. Much of the job is understanding the components
and gluing them together in your own way. Of course, many of these components in turn rest on
other simpler and more basic ones, often for several layers. Below that, everything runs on
the operating system, a program that manages the hardware and controls everything that happens.”
<span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“There is also a problem with discovering software that exists; often people
reinvent the wheel just because they don’t know any better. Good repositories
for software and best practice workflows, especially if citable, would be a
start.” — James Taylor in <span class="citation">(Altschul et al. 2013)</span></p>
</blockquote>
</div>
<div id="do-not-repeat-yourself" class="section level3 hasAnchor" number="3.4.3">
<h3><span class="header-section-number">3.4.3</span> Do not repeat yourself<a href="experimental-data-preprocessing.html#do-not-repeat-yourself" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As you become more familiar with programming with R, you can start to evolve your
style of writing scripts in more advanced ways. A key one is to learn how to limit
how often you repeat the same code. As you write data pre-processing pipelines, you’ll
find that you often need to do the same thing, or variations on the same thing, over
and over. For example, you may need to read in and clean several files of the same
type and structure. You will likely, at first at least, find yourself copying and
pasting the same code to several parts of your script, with only minor changes to
that code (e.g., changing the R object that you input each time).</p>
<p>Don’t worry too much about this as you start to learn how to write R scripts. This
is a normal part of the drafting process. However, as you get better at using R, you’ll
want to learn techniques that can help you avoid this repetition.</p>
<p>There are a few reasons that you’ll want to avoid repetition in your code when
possible. First, these repeated copies of the same or similar code will make your
code script much longer and harder to figure out later.
Second, it is hard to keep these copies of code in sync with each other. For example, if
you have several copies of the code you use to check for outliers in your data, and you
decide you want to change how you are doing that, you’ll need to find every copy of
the code in your script and make sure you make the same change in each place. Instead,
if you have less repetition in your code, then you can make the change in a single place
and ensure that the change will be in place everywhere you are doing that process.</p>
<p>There are a few tools that are useful to develop to help avoid repetition. The
first is to learn how to write your own R functions. Any R user can write a new
function. You can write them in packages that you plan to share with others, but
you can also just write them for your personal use. When you wrap a function, it
encapsulates the code for something that you need to do, and it allows you to do
that thing anywhere else in your code just by calling that new function, rather
than copying all the lines of the original code. This is an excellent way to
write the code you need to use often in one place, rather than copying and
pasting the same code throughout your R script.</p>
<p>Since you need to run the code that defines the function before you use it, it
often makes sense to write the code that creates these functions near the top of
your code script. If you find that you’ve written a lot of functions, or that
you’ve written functions that you’d like to use in more than one of your data
preprocessing scripts, you can even save the code that creates the functions in
a separate R script and just source that separate script at the top of each
script that uses the function, using the <code>source</code> call. “Sourcing” a file in
this way simply runs all the code in the file. Eventually, you could
even think of creating your own package with those functions.</p>
<p>There is one other excellent set of tool for avoiding repetition that we want to
mention. Again, it is likely more complex that what you’ll want to start off
with as you learn to write R scripts, but once you are comfortable with the
basics, it’s a powerful tool for creating code scripts that are as short and
simple as possible while doing very powerful things. This set of tools all focus
on iteration. They include <code>for</code> loops, which allow you to step through elements
in a data structure and apply the same code to each. They also include a set of
tools in the <code>purrr</code> library that allow you to apply the same code, through a
function, to each element in a larger data structure. These are excellent tools
when you are doing something like reading in a lot of similar files and
combining them into a single R object for preprocessing.</p>
<p>We will not go into details about how to write R functions or these iteration
tools in these modules, as our aim here is to get you started and give you an
overview of where you might want to go next. If you do want to learn to write
your own R functions, there’s a chapter describing the process in the free
online book “R for Data Science” with guidance on this topic
(<a href="https://r4ds.had.co.nz/functions.html" class="uri">https://r4ds.had.co.nz/functions.html</a>). If you’d like to learn more about tools
for iteration, the same book also has a chapter on that
(<a href="https://r4ds.had.co.nz/iteration.html" class="uri">https://r4ds.had.co.nz/iteration.html</a>).</p>
</div>
<div id="discussion-questions-3" class="section level3 hasAnchor" number="3.4.4">
<h3><span class="header-section-number">3.4.4</span> Discussion questions<a href="experimental-data-preprocessing.html#discussion-questions-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>

</div>
</div>
<div id="module14" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Simplify scripted pre-processing through R’s ‘tidyverse’ tools<a href="experimental-data-preprocessing.html#module14" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The R programming language now includes a collection of ‘tidyverse’ extension
packages that enable user-friendly yet powerful work with experimental data,
including pre-processing and exploratory visualizations. The principle behind
the ‘tidyverse’ is that a collection of simple, general tools can be joined
together to solve complex problems, as long as a consistent format is used for
the input and output of each tool (the ‘tidy’ data format taught in other
modules).</p>
<p>Once data are in the “tidy” data format, you can create a pipeline of code that
uses small tools, each of which does one simple thing, to work with the data.
This work can include cleaning the data, adding values that are functions of the
original values for each observation (e.g., adding a column with BMI based on
values for each observation on height and weight), applying statistical models to
test hypotheses, summarizing data to create tables, and visualizing the data.</p>
<p>In this module, we will explain why this ‘tidyverse’ system is so
powerful and how it can be leveraged within biomedical research, especially for
reproducibly pre-processing experimental data.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Define R’s ‘tidyverse’ system</li>
<li>Explain how the ‘tidyverse’ collection of packages can be both user-friendly
and powerful in solving many complex tasks with data</li>
<li>Describe the difference between base R and R’s ‘tidyverse’.</li>
</ul>
<p>[Anecdote: favorite tools: math chalk, Blackwing pencils, Happy Hacker keyboard]</p>
<blockquote>
<p>“Similar to early many, beginner makers start with a rudimentary set of tools for
basic creative tasks: a hammer (of course), a set of screwdrivers, scissors,
some pliers, maybe a crescent wrench, and some kind of cutting device. Almost
everyone who has strived to make things has some combination of this list. Then,
as we get more experienced, we seek out better versions of the tools we already
have as well as new tools that can facilitate the learning of new techniques—new
ways of cutting things apart, and new ways of putting them back together.” <span class="citation">(Savage 2020)</span></p>
</blockquote>
<blockquote>
<p>“Once we start to expand past the basic complement of tools, what to add to our
collections becomes a multifactor calculus based on reliability, cost, space, time,
repairability, skill, and need. These choices are nontrivial, because the tools we use
are extensions of our hands and our minds. The best tools ‘wear in’ to fit you based
on how you use them, they get smooth where you grab them. They tell the story of their
utility with their patina of use. A toolbox of tools you know well and use lovingly is
a magnificent thing.” <span class="citation">(Savage 2020)</span></p>
</blockquote>
<blockquote>
<p>“The reality is that tool choice is both less important and more important than you
think it is. It is less important to the extent that tool usage is entirely
subjective, which means there is no one right way to do things. But it is more
important, because the best tool for any job is the one you’re most comfortable with,
the one that you can make do what you want it to do, whose movements you fully
understand.” <span class="citation">(Savage 2020)</span></p>
</blockquote>
<p>The best thing that you can do to smooth the path as you learn a coding language
is to start by finding a few general purpose tools and learning to use them
really well. If you ask most good programmers, you will find that a large
amount of their code relies on a fairly small set of general-use tools,
with more specialized tools only used here and there, where a specific
algorithm is necessary.</p>
<p>As you learn to code, then, a good strategy is to start collecting “tools” for
your toolbox in R—functions that you have learned to use very well and that
you understand thoroughly. This will make you proficient in R more quickly, and
it will also limit the chance of bugs and errors in your code, making your data
work more robust and rigorous. When you first start out, though, it is hard to
know which tools are the most important to add early and learn well. In this
section, we’ll cover some tools that we have found helpful for
preprocessing biological data. These are not exhaustive, but may help you to
identify some sets of tools to focus on learning well for data preprocessing and
analysis of biological data.</p>
<p>Some key tools for pre-processing laboratory data are:</p>
<ul>
<li>Tools for data input</li>
<li>Tools for changing columns or creating new columns</li>
<li>Tools for working with character strings</li>
<li>Tools for working with dates and times</li>
<li>Tools for statistical modeling</li>
</ul>
<p>We will concentrate on tools that are drawn from a collection of tools called the
“tidyverse”. The “tidyverse” approach is an approach to using R that has grown
enormously in popularity in recent years. Most R courses and workshops for
beginning programmers are now structured around this approach. It provides a
powerful yet flexible approach for working with data in R, and it one of the
easier ways to start learning R. In a previous module (module 2.3) we described
the tidyverse approach in conjunction with talking about the power of the tidy
data format. In this module, we’ll go deeper into specific tools under this
approach that can be used for common data preprocessing tasks when working with
biomedical data, as well as provide information on more resources that can be
used to continue learning this approach.</p>
<p>The tidyverse functions do not come with base R, but rather are available
through extensions to base R, commonly referred to as “packages”. Like base
R, these are all open-source and free. Many are available through a
repository called CRAN, and you can download them directly from R using the
<code>install.packages</code> function.</p>
<p>The heart of the tidyverse functions are available through an umbrella
package called “tidyverse”. This package includes a number of key tidyverse
packages (e.g., “dplyr”, “tidyr”, “stringr”, “forcats”, “ggplot2”) and allows you
to quickly install this set of packages on your computer. When you are coding
in R, you will then need to load the package in your R session, which you can
do using the <code>library</code> call (e.g., <code>library("tidyverse")</code>).</p>
<p>In addition to the packages that come with the umbrella “tidyverse” package,
there are numerous other packages that build on the tidyverse approach.
Some are created by the creator of the tidyverse approach (Hadley Wickham)
or others on his team, while others are created by other R programmers but
follow the standards of the tidyverse approach. An examples of one of
these extensions that is specifically created for working with biomedical data
is the <code>tidybulk</code> package <span class="citation">(Mangiola et al. 2021)</span>, for working with
transcriptomics data.</p>
<div id="tools-for-data-input" class="section level3 hasAnchor" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> Tools for data input<a href="experimental-data-preprocessing.html#tools-for-data-input" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To be able to work with data in R, you first must load the data into your R
session. Data will typically be saved in some type of file or files, and
so you must instruct R about how to find that data and then read it from
the file into the R session.</p>
<p>There are several key tidyverse tools for inputting data from a file. The
most important is a package in the tidyverse called, <code>readr</code>. This package
allows you to read data from plain text files. Data are often stored in
these plain text files, including in formats like CSV (“comma-separated
values”), tab-separated values, and fixed width files. These are all files
that you can open on your computer with a text editor (for example,
Notepad, Wordpad, or TextEdit).</p>
<p>The <code>readr</code> package includes various functions to read in data from these
types of files, with different functions for different formats of those
files. For example, CSV files separate different pieces of data in the
file with commas, and these can be read into R with the <code>readr</code> function
<code>read_csv</code>.</p>
<p>Some equipment in the laboratory may allow you to save results in a plain
text format. When you export your data from laboratory equipment, you can
check to see if there is an option to outload it to a format like “CSV”
or “txt”, which would allow you to use these <code>readr</code> functions to then
read the data into R.</p>
<p>There are other packages in the tidyverse that allow you to read in data
from other types of file formats. For example, you may have data that
you recorded into an Excel spreadsheet. Excel files are a bit more complex
in their structure than plain text files, and the functions that read
plain text files into R will not work for Excel files. Instead, there are
a series of functions in a package called <code>readxl</code> that you can use to
read in data from Excel files into R. These functions even allow you to
specify which sheet of an Excel file to read data from, as well as which
cells on that sheet, so they allow for very fine control of data input
from an Excel spreadsheet.</p>
<p>In some cases, you may be collecting data with laboratory equipment that does
not export its data to a standard format, like a plain text file or a
basic spreadsheet file. Instead, some equipment will save data into a file
format that has been standardized for a certain type of data (e.g., an mzML
file for metabolomics data) or to a file type that is proprietary to the
company that manufactures the equipment. There is a chance that someone has
created an R package that can input data from these more specialized types of
files. In fact, for common file types from biomedical research, that chance
is high (for example, there are several packages available with functions that
input data from an mzML file). One of the best ways to find an appropriate
tool to input data from more specialized formats is by searching Google for
“R data input” and then the name of the file format. If you use that file
format often in your laboratory, it is worth some research to determine
which R package is a good fit for inputting data from that file format
and then working through vignettes and other helpfiles for that package to
learn how to use it well.</p>
<p>You can learn more about the <code>readr</code> and <code>readxl</code> packages through their
vignettes, which provide tutorials walking through the functionality of
each package. You can find those at:</p>
<ul>
<li><code>readr</code>: <a href="https://readr.tidyverse.org/" class="uri">https://readr.tidyverse.org/</a></li>
<li><code>readxl</code>: <a href="https://readxl.tidyverse.org/" class="uri">https://readxl.tidyverse.org/</a></li>
</ul>
</div>
<div id="tools-for-changing-or-creating-columns" class="section level3 hasAnchor" number="3.5.2">
<h3><span class="header-section-number">3.5.2</span> Tools for changing or creating columns<a href="experimental-data-preprocessing.html#tools-for-changing-or-creating-columns" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are many preprocessing tasks that require creating columns that are
mathematical functions of existing columns. Therefore, you’ll want to have some
tools for changing existing columns or making new column.</p>
<p>One example is when you are scaling or normalizing data. Scaling is often
required before using some of the techniques for dimension reduction (e.g.,
principal components analysis) or clustering, to ensure that the unit of
measurement of each column does not influence its weight in later analysis. For
example, if you were clustering observations using measurements for each subject
that included their weight, you don’t want to get different results depending on
whether their weight is measured in grams versus pounds, and this type of
scaling can help avoid any of those differences based on the units used for
measurements.</p>
<p>There are a range of ways to standardize and normalize different types of
biomedical data, ranging from very simple to much more complex. At the simpler
end is a method called z-score normalization, where the observations for each
feature or column are changed to have an overall mean of 0 and standard
deviation of 1. This can be done by taking each value in a column and
subtracting from it the column-wide mean, then dividing by the standard
deviation. There are also more complex methods for scaling and normalization.
All similarly require mathematical algorithms or functions to be applied to the
original data to create a new column of data that is the scaled or normalized
version of the original.</p>
<p>In R, there are functions that come with the base installation of R (in other
words, don’t require installing extra packages) that can be used for more basic
processes of standardization and normalization. For example, the <code>scale</code>
function can be used for the basic scaling described in the previous paragraph.
You can also directly use math functions (like <code>-</code> for subtraction and <code>/</code> for
division) and very basic functions (like <code>mean</code> to calculate the mean of a
vector of numbers and <code>sd</code> to calculate the standard deviation) to make these
types of calculations from scratch. To apply these, though, you’ll need to know
functions that work with columns in a dataframe.</p>
<p>The <code>dplyr</code> package is a key package to learn from the tidyverse, as it forms
the heart of the tools for cleaning and exploring data that are stored in tidy
dataframes. The package includes not only functions for making changes to a
single column (e.g., the <code>mutate</code> function), but also functions that can be used
to perform the same calculation across many columns (e.g., the <code>across</code>
function). This is an efficient way to do something like scale the data in
multiple columns at once.</p>
<p>These functions can also be used for basic cleaning operations in a dataframe.
For example, data that are recorded for colony-forming units may include “TNTC”
in cells of the spreadsheet where so many bacteria had grown that the individual
colonies were “too numerous to count”. When you read in the data, you may want
to change these values to missing values so that you can run numerical
calculations on the cells that include colony counts. This type of conversion
can easily be done using functions from the <code>dplyr</code> package. They are also
critical for performing processes like scaling / normalization— the <code>mutate</code>
function, for example, can be used to create a new column of scaled data by
applying a scaling function to an existing column.</p>
<p>You can learn more about the <code>dplyr</code> package through its vignette, which is
available at: <a href="https://dplyr.tidyverse.org/" class="uri">https://dplyr.tidyverse.org/</a>.</p>
</div>
<div id="tools-for-working-with-character-strings" class="section level3 hasAnchor" number="3.5.3">
<h3><span class="header-section-number">3.5.3</span> Tools for working with character strings<a href="experimental-data-preprocessing.html#tools-for-working-with-character-strings" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Once you have learned the basic tools for inputting data, as well as basic
manipulations on columns with the <code>dplyr</code> tools, you should take some time to
learn a few other tools that can often be used to make your coding pipelines
much more efficient. One of these is to learn how to work well with character
strings.</p>
<p>Character strings are strings of alphanumerical symbols that are stored
inside quotation marks, like “Mouse-01” or “Control group”. Several tidyverse
packages help you work with this type of data more efficiently, either through
finding and using regular patterns in the data (e.g., the number “01” stored in
“Mouse-01”) or in treating these data as a marker of a set number of groups
(e.g., “Control group” versus “Treated group”). These tools can help you in
processing and exploring the data, and they are also extremely important in
creating figures and tables from the data with clear labels. Once you start
learning to work with character string data, you will realize that it’s not just
within the data, but also that you can treat the file names and directory names
of your project as character strings, and use these tools to embed and use
useful information in them.</p>
<p>The <code>stringr</code> package, which is part of the tidyverse, includes simple but
powerful tools for working with vectors composed of character strings. For
example, the package includes a function that let you extract a subset of each
character string based on the position of the characters in the string, a
function that lets you replace every instance of a pattern with something
else, and a function that will tell you which character strings in the vector
have a match to a certain pattern. It also includes a function that can change
the case of all the letters in each string, either to uppercase, to lowercase,
or to “title case” (the first letter in each word is capitalized).</p>
<p>You likely will not realize how powerful many of these tools are
until you have a time when you need to do one of these tasks, but
then you’ll find they make your life much easier. For example,
say that you have a column in your data that provides the ID of each study
subject (e.g., “Mouse 1A”). If some of the IDs were entered using upper
case (e.g., “MOUSE 1A”), some with lower case (“mouse 1a”), and some with
a mixture (e.g., “Mouse 1A”), then you may find that it is hard
to write code that recognizes that “Mouse 1A” is the same as “mouse 1a” and
“MOUSE 1A”. The functions in the <code>stringr</code> package would let you quickly
convert everything to the same case and so work around this issue.
As another example, you may want to extract certain elements from each
subject ID—for example, you might want to create a column where you
have changed “Mouse 1A” to just “1A” and “Mouse 2B” to just “2B”. The
<code>stringr</code> package has functions that will let you do this in several
ways. For example, it has a function that would let you remove “Mouse”
from each character string, and another function that would let you
extract only the part of the string that starts from the first number.
These types of tools can be invaluable when you need to preprocess or
clean data from the format that it first enters R.</p>
<p>Sometimes, you will want to treat character strings as discrete categories
or values. For example, if part of your data records subject IDs
(e.g., “Mouse 1A”, “Mouse 2B”), you may want to be able to link up all
of the observations that are recorded for each subject. Similarly, you
may want to treat a variable that records treatment (e.g., “treated” / “control”)
as a set of specific categories that each observation belongs to.</p>
<p>In R, you can do this by treating that column as something called a “factor”.
This data type looks like a character string (e.g., “treated”), but R has
recorded that there are only a few set values that values in the column can have
(e.g., “treated” or “control”), and when you summarize or plot the data, you can
group by this variable to get summaries within each category, or align it with
the color or shape of plotted points.</p>
<p>The <code>forcats</code> package includes helpful tools for working with this factor type
of data. When a column is changed into a factor, the possible levels of the
factor (in other words, the possible values it can take) will be given an order,
often alphabetical. You won’t notice this order with many of the processing
you might do, but it will control the order that categories are mentioned when
you summarize or plot the data. The <code>forcats</code> package includes a function that
lets you rearrange this order, and so rearrange the order that each category
is presented in summaries and plots. The package also includes numerous other
tools for working with this type of data. For example, if you have a factor
that takes many different possible values, it will let you to convert to
specify only those that are most common (you can specify how many categories),
and then pool the rest into an “Other” category.</p>
<p>The vignettes for the <code>stringr</code> and <code>forcats</code> packages are available at:</p>
<ul>
<li><code>stringr</code>: <a href="https://stringr.tidyverse.org/" class="uri">https://stringr.tidyverse.org/</a></li>
<li><code>forcats</code>: <a href="https://forcats.tidyverse.org/" class="uri">https://forcats.tidyverse.org/</a></li>
</ul>
</div>
<div id="tools-for-working-with-dates-and-times" class="section level3 hasAnchor" number="3.5.4">
<h3><span class="header-section-number">3.5.4</span> Tools for working with dates and times<a href="experimental-data-preprocessing.html#tools-for-working-with-dates-and-times" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Another handy set of tools are those for working with dates and times. Often, you
will record the date that an observation is collected, or the date and time if
data are being collected at a fine time scale. Although you record these as a
character string (e.g., “August 1, 2019”), you’ll want to be able to use the
quantitative information within the date. For example, you may want to be able
to tell if the date of each observation is before a certain date, or determine
how many days there are between two date.</p>
<p>The tidyverse includes a package for working with dates and times called
<code>lubridate</code>. This package includes functions that allow you to change a column
in your data to have a date or date-time data type. This will allow you to
do operations on those values as dates—in other words, do things like determine
the number of days between two dates. The <code>lubridate</code> package also includes
functions for these operations on dates, including determining if one date is
larger or smaller than another and whether it’s within an interval of two dates,
as well as determining the difference between two dates or finding out which
date is a certain number of days after a given date. There are also functions
to extract certain elements from each date, like the day of the week or the
month of the year.</p>
<p>The functions in the <code>lubridate</code> package can be very useful for preprocessing
data. For example, you may record the date of each measurement that you take,
but also need to determine how much time has passed between the start of the
experiment and that measurement. The <code>lubridate</code> package has a function that
will allow you to calculate the time since a recorded start time, and so this
allows you to record only the date and time of each measurement, and then
determine the time since the start of the experiment within reproducible code
once you read the recorded data into R.</p>
<p>To find out more about the <code>lubridate</code> package, you can read its vignette
at <a href="https://lubridate.tidyverse.org/" class="uri">https://lubridate.tidyverse.org/</a>.</p>
</div>
<div id="tools-for-statistical-modeling" class="section level3 hasAnchor" number="3.5.5">
<h3><span class="header-section-number">3.5.5</span> Tools for statistical modeling<a href="experimental-data-preprocessing.html#tools-for-statistical-modeling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Often, analysis of biomedical data will include some statistical hypothesis
testing or model building. For example, if you have collected bacterial
loads in two groups of animals with different treatment assignments
(treated and control), you may want to test the hypothesis that the average
bacterial load in the two groups is the same. If the treatment was successful
and the experiment had adequate power, then the data will hopefully show that
this null hypothesis should be rejected.</p>
<p>R has a number of functions that can run the most common statistical
hypothesis tests (e.g., Student’s t-test) as well as fit commonly-used
statistical models (e.g., linear regression models). Many of the tools
for common tests and model building are included with your initial installation
of R. This means that you can use them without installing or loading additional
packages.</p>
<p>Further, there are many additional packages that are available that run
less common statistical tests or fit less common statistical model frameworks.
Part of R’s strength is in its deep availability of these packages for
statistical analysis. You can often use a Google search to determine if
there is a function or package for a statistical analysis that you would
like to perform in R, and it is rare to not find at least one package with
the appropriate algorithm. To help you select among different packages, check
out the article “Ten Simple Rules for Finding and Selecting R Packages”
<span class="citation">(Wendt and Anderson 2022)</span>.</p>
<p>In addition to learning the tools for the types of statistical analysis
that you do often in your research, it is also helpful to learn some tools
that help you incorporate that statistical analysis into your workflow.
Many of the tools in R for statistical analysis were originally focused on
being an endpoint of a code pipeline. For example, many of them will result
in a print-out summary of the results of the statistical test or model fit.
This is fine if you only want to record that result, but often you will
want to use the results in further R code, for example to add to plots or
tables or to combine with other results.</p>
<p>There are a couple of packages that can help with this. First, there is a
package called <code>broom</code> that can conver the output of many statistical
tests and models into a tidy dataframe. If you have focused on learning
tidyverse tools, then this functionality makes it much easier for you to
continue working with the output. The <code>tidymodels</code>
package extends on this idea by creating a common interface for fitting
a variety of statistical models and extracting results in a tidy format.</p>
<p>You can read the vignettes for the <code>broom</code> and <code>tidymodels</code> packages at:</p>
<ul>
<li><code>broom</code>: <a href="https://cran.r-project.org/web/packages/broom/vignettes/broom.html" class="uri">https://cran.r-project.org/web/packages/broom/vignettes/broom.html</a></li>
<li><code>tidymodels</code>: <a href="https://www.tidymodels.org/" class="uri">https://www.tidymodels.org/</a></li>
</ul>
</div>
<div id="resources-to-learn-more-on-tidyverse-tools" class="section level3 hasAnchor" number="3.5.6">
<h3><span class="header-section-number">3.5.6</span> Resources to learn more on tidyverse tools<a href="experimental-data-preprocessing.html#resources-to-learn-more-on-tidyverse-tools" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here we have introduced the tidyverse approach, as well as covered some key
tools within it for biomedical data preprocessing. However, we strongly
recommend that you continue to learn more in this approach. In this section,
we’ll point you to resources that you can use to continue to learn this approach
to working with data in R.</p>
<p>The tidyverse approach is now widely taught, both in in-person courses at
universities and through a variety of online resources.
Since there are so many excellent resources available—many for free—to learn
how to code in R using the tidyverse approach, we consider it beyond the scope
of these modules to go more deeply into these instructions. Rather, we’ll
point you to some excellent references that go deeply into the tidyverse
approach to coding, its set of tools, and how they can be applied when
working with biomedical data.</p>
<p><strong>Classes and workshops</strong></p>
<p>Most R programming classes at universities, as well as workshops at conferences
and other venues, now focus on the tidyverse approach, especially if they are
geared to new R users. An R programming class can be a worthwhile investment of
time if this resource is available to you, and if you head a research group and
do not have time to take one yourself, you could instead encourage
trainees in your research group to take this type of class. Programming in other
scripted languages, like Python and Julia, provides similar skills, although the
collection of extension packages that are available for biomedical data tends to
be most extensive for R (at least at this time). Classes in programming
languages like Java or C++, on the other hand, would have less immediate
relevance for most biologists and other bench scientists, and so if you would
like to become better at working with biomedical data, it would be worthwhile to
focus on programming languages that are scripted.</p>
<p><strong>Online books</strong></p>
<p>There are a number of excellent free online books that are available to help
you learn more about R (many of which can also be purchased as a hard copy, if
you prefer that format). These typically include lots of examples of code that
help you try out concepts as you learn them.</p>
<p>One key resource for learning the tidyverse approach for R is the book <em>R for
Data Science</em> by Hadley Wickham (the primary developer of the tidyverse) and
Garrett Grolemund. This book is available as a print edition through O’Reilly
Media. It is also freely available online at <a href="https://r4ds.had.co.nz/" class="uri">https://r4ds.had.co.nz/</a>. This book
is geared to beginners in R, moving through to get readers to an intermediate
stage of coding expertise, which is a level that will allow most scientific
researchers to powerfully work with their experimental data. The book includes
exercises for practicing the concepts, and a separate online book is available
with solutions for the exercises
(<a href="https://jrnold.github.io/r4ds-exercise-solutions/" class="uri">https://jrnold.github.io/r4ds-exercise-solutions/</a>).</p>
<p>Another online book that is an excellent tool—particularly for those using
R for biomedical research—is <em>Modern Statistics for Modern Biology</em>, by
Susan Holmes and Wolfgang Huber. These book shows how the tidyverse approach
can be combined with tools from Bioconductor that are custom built to
work with bioinformatics data. It also provides an excellent overview of
statistical methods for working with biomedical data and how those can be
applied using R. The book is available online at <a href="https://www.huber.embl.de/msmb/" class="uri">https://www.huber.embl.de/msmb/</a>.</p>
<p><strong>Cheatsheets</strong></p>
<p>For many of the key tidyverse packages, there are two-page “cheatsheets” that
have been developed by the package creators to help users learn and remember
the functions that are available in the package. These are available here:
<a href="https://posit.co/resources/cheatsheets/" class="uri">https://posit.co/resources/cheatsheets/</a>.</p>
<p>Each cheatsheet includes numerous working examples. One excellent way to
familiarize yourself with the tools in a package, then, is to work through the
examples on the cheatsheet one at a time, making sure that you understand the
inputs and outputs to the function and how the function has created the output.
Once you have worked through a cheatsheet in this way, you can keep it close
to your desk to serve as a quick reminder of the names and uses of different
functions in the package, until you have used them enough that you don’t need
this memory jog.</p>
<p>For deeper tutorials of each tidyverse package, you can explore the
package’s vignette. We’ve provided links to several of these throughout this
module.</p>
</div>
<div id="practice-quiz" class="section level3 hasAnchor" number="3.5.7">
<h3><span class="header-section-number">3.5.7</span> Practice quiz<a href="experimental-data-preprocessing.html#practice-quiz" class="anchor-section" aria-label="Anchor link to header"></a></h3>

</div>
</div>
<div id="module15" class="section level2 hasAnchor" number="3.6">
<h2><span class="header-section-number">3.6</span> Complex data types in experimental data pre-processing<a href="experimental-data-preprocessing.html#module15" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Raw data from many biomedical experiments, especially those that use
high-throughput techniques, can be very large and complex. Because of the scale
and complexity of these data, software for pre-processing the data in R often
uses complex, ‘untidy’ data formats. While these formats are necessary for
computational efficiency, they add a critical barrier for researchers wishing to
implement reproducibility tools. In this module, we will explain why use of
complex data formats is often necessary within open source pre-processing
software and outline the hurdles created in reproducibility tool use among
laboratory-based scientists.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Explain why R software for pre-processing biomedical data often stores
data in complex, ‘untidy’ formats</li>
<li>Describe how these complex data formats can create barriers to
laboratory-based researchers seeking to use reproducibility tools for
data pre-processing</li>
</ul>
<p>In previous modules, we have gone into a lot of detail about all of the
advantages of the tidyverse approach. In some cases, though, data might be
poorly suited to a tidyverse approach at some part of your pipeline. However as
you work with biomedical data, you may find that it is unreasonable to start
with a tidyverse approach from the first steps of pre-processing the data. This
is particularly the case if you are working with data from complex research
equipment, liek mass spectrometers and flow cytometers.</p>
<p>It can be frustrating to realize that you can’t use your standard tools
in some steps of working with the data you collect in your experiments.
For example, you may have taken an R course or workshop, and be at the
point where you are starting to feel pretty comfortable with how to use
R to work with standard datasets. You can feel like you’re starting at
square one when you realize that approach won’t work for some steps of
working with the data you’re collecting for your own research.</p>
<p>This module aims to help you navigate this process.
It is helpful to understand how the Bioconductor approach differs from
the tidyverse approach, to start developing a framework and tools for
navigating both approaches.</p>
<p>The primary difference between the two approaches is how the data objects
are structured. When you work with data in R, it is kept in an “object”,
which you can think of as a structured container for the data. In the
tidyverse approach, the primary data container is the dataframe. A
dataframe is made up of a set of object types called vectors (each column
in the dataframe is a vector). Therefore, to navigate the tidyverse
approach, the only data structures you need to understand well are the
dataframe structure and the vector structure. Tools in the tidyverse
use these simple structures over and over.</p>
<p>By contrast, the Bioconductor approach uses a collection of more complex
structured containers to store data as it’s used. There are a number of
reasons for this, which we’ll discuss in this module.</p>
<p>As a note, it is very possible that in the near future, all steps of even
complex pipelines will be manageable with a tidyverse approach. More
R developers are embracing the tidyverse approach and making tools and packages
within its framework. In some areas with complex data, there have been
major inroads, allowing a tidyverse approach throughout the pipeline even
when working with complex data. One example of this is with spatial data,
where the <code>sf</code> package, and related tools, now make it possible to stay in
a tidyverse framework when working with large and complex geospatial data.
We will end this module by discussing the outlook for similar developments
in the area of biomedical data.</p>
<div id="how-the-bioconductor-and-tidyverse-approaches-differ" class="section level3 hasAnchor" number="3.6.1">
<h3><span class="header-section-number">3.6.1</span> How the Bioconductor and tidyverse approaches differ<a href="experimental-data-preprocessing.html#how-the-bioconductor-and-tidyverse-approaches-differ" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The heart of the difference between the tidyverse and Bioconductor approaches
comes down to how data are structured within pipelines in the two approaches.
There are more differences than this one, but most of the other differences
result from this main difference.</p>
<p>As we’ve described in detail in earlier modules (modules 2.3 and 3.5), in the
tidyverse approach, data are stored throughout the pipeline in a dataframe
structure. These dataframes are composed of vectors, which make up their
columns. Almost every function in the tidyverse is designed to input either a
dataframe or a vector. And almost every function is designed to output the same
type of data container (dataframe or vector) that it inputs. As a result, the
tidyverse approach can mix and match functions in different orders to tackle
complex processes through a chain of many small steps.</p>
<p>By contrast, most packages in Bioconductor use more complex data structures
to store data. Often, a Bioconductor pipeline will use different data
structures at different points in its pipeline. For example, your data might
be stored in one type of a data container after it’s first read into R, and
another type once you’ve done some pre-processing.</p>
<p>As a result, with the Bioconductor approach, there will be more types of data
structures that you will have to learn how to use and navigate. Another
result is that, often, the functions that you use in your
pipeline will only work with a specific data structure. You therefore will need
to keep track of which type of data structure is required as the input to each
function.</p>
<p>This also means that you are more constrained in how you chain together
different functions to make a pipeline. In the tidyverse approach, you can often
chain the functions in any order, since each function inputs and outputs the
same data structure. With a Bioconductor pipeline, however, there
will be functions that input one data structure and output a different one.
As a result, Bioconductor functions, instead of being “small”
functions that do one simple thing, often carry out a number of complex
steps within each function call.</p>
<p>This difference will also make a difference in how you work when you modify a
pipeline of code. In the tidyverse approach, you will change the functions you
include and the order in which you call them, rearranging the small tools to
create different pipelines. For a Bioconductor pipeline, it’s more common that
to customize it, you will adjust parameter settings within functions, but will
still call a standard series of functions in a standardized order.</p>
<p>Because of those differences, it can be hard to pick up the Bioconductor
approach if you’re used to the tidyverse approach. However, Bioconductor is
critical to learn if you are working with many types of biomedical data, as many
of the key tools and algorithms for genomic data are shared through that
project. This means that, for many biomedical researchers who are now generating
complex, high-throughput data, it is worth learning how to use complex data
structures in R.</p>
<p>To be clear, a pipeline in R that includes these complex data structures
will typically still be modular, in the sense that you can adapt and
separate specific parts of the pipeline. However, they tend to be much
less flexible than pipelines developed with a tidyverse approach. The
data structure changes often, with certain functions outputing a data
structure that is needed for the next step, then the function of the
next step outputting the data in a different structure, and so on. This
changing data structure means that the functions for each step often are
constrained to always be put in the same order. By comparison, the small
tools that make up tidyverse functions can often be combined in many different
orders, letting you build a much larger variety of pipelines with them.
Also, many of the functions that work with complex data types will do
many things within one function, so they can be harder to learn and
understand, and they are often much more customized to a specific action,
which means that you have to learn more functions (since each does one
specific thing).</p>
</div>
<div id="why-is-the-bioconductor-approach-designed-as-it-is" class="section level3 hasAnchor" number="3.6.2">
<h3><span class="header-section-number">3.6.2</span> Why is the Bioconductor approach designed as it is?<a href="experimental-data-preprocessing.html#why-is-the-bioconductor-approach-designed-as-it-is" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>It can be helpful to understand why the Bioconductor approach is designed
in the way it is. First, there are some characteristics of complex data
that can make it unsuitable for a tidyverse approach. In the next section
of the module, we’ll discuss some of these characteristics, as well as
provide examples of how biomedical data can have these characteristics.</p>
<p>However, there are also some historical and cultural reasons for the
Bioconductor design. It is helpful to have an introduction to this, as it
can help you navigate as you work within the Bioconductor framework.</p>
<p>Bioconductor predates the tidyverse approach. In fact, it has been around
almost as long as R itself—the first version of R was first released
in 2000, and Bioconductor started in 2003.</p>
<p>The Bioconductor project had an ambitious aim—allow people around the world to
coordinate to make tools for preprocessing and analyzing genomic and other
high-throughput data. Anyone is allowed to make their own extension to R as a
package, including a Bioconductor package. This is similar to the approach taken
by Andy Warhol, the famous pop artist. He had a studio where the door was always
open, and people were free to walk in off the street to help create new things
<span class="citation">(Judkins 2016)</span>.</p>
<p>Imagine how complex it is to try to harness all these contributions. Within
the Bioconductor project, this is managed by using some general design principles,
centered on some standard data structures. Each person who writes
code for Bioconductor can use these data structures, writing functions that
input and output data within these defined structures. If they are working
on something where there isn’t yet a defined structure, they can define new
ones within their package, which others can then use in their own packages.</p>
<p>The different Bioconductor data structures, then, were implemented to help many
people coordinate to make software extensions to R to handle complex biomedical
data. As Susan Holmes and Wolfgang Huber note in their book <em>Modern Statistics
for Modern Biology</em>:</p>
<blockquote>
<p>“The Bioconductor project has defined specialized data containers to represent
complex biological datasets. These help to keep your data consistent, safe and
easy to use.” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<p>Indeed, in an article on software for computational biology, Robert Gentleman—one
of the developers of R and founders of the Bioconductor project—is quoted
as saying:</p>
<blockquote>
<p>“We defined a handful of data structures that we expected people to use. For
instance, if everybody puts their gene expression data into the same kind of
box, it doesn’t matter how the data came about, but that box is the same and can
be used by analytic tools. Really, I think it’s data structures that drive
interoperability.” — Robert Gentlemen in <span class="citation">(Altschul et al. 2013)</span></p>
</blockquote>
</div>
<div id="why-is-it-sometimes-necessary-to-use-a-bioconductor-approach-with-biomedical-data" class="section level3 hasAnchor" number="3.6.3">
<h3><span class="header-section-number">3.6.3</span> Why is it sometimes necessary to use a Bioconductor approach with biomedical data<a href="experimental-data-preprocessing.html#why-is-it-sometimes-necessary-to-use-a-bioconductor-approach-with-biomedical-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are some characteristics of some types of biomedical data that make
non-tidy data structures sometimes very useful. Specifially, there are two main
features—of data collected from complex laboratory equipment like flow
cytometers and mass spectrometers, in particular—that make it useful to use
more complex data structures in R in the earlier stages of preprocessing the
data rather directly using a tidy data structure. First, the data are often very
large, in some cases so large that it is difficult to read them into R. Second,
the data might combine various elements, each with their own natural structures,
that you’d like to keep together as you move through the steps of preprocessing
the data.</p>
<p>The first reason why dataframe structures don’t always work for data from
biological experiments has to do with the size of data (and so how much
memory it requires). Very large datasets are common in biomedical data,
including genomics data. As Holmes and Huber note:</p>
<blockquote>
<p>“Biology, formerly a science with sparse, often only qualitative data, has
turned into a field whose production of quantitative data is on par with high
energy physics or astronomy and whose data are wildly more heterogeneous and
complex.” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<p>A computer has several ways that it can store data. The primary storage is
closely connected with the computer’s processing unit, where calculations are
made, and so data stored in this primary storage can be processed by code very
quickly. This storage is called the computer’s random access memory, or RAM. R
uses this approach, and so when you load data in R to be stored in one of its
traditional data structures, that data is moved into part of the computer’s RAM
<span class="citation">(Burns 2011; Gillespie and Lovelace 2016)</span>.</p>
<p>Data can also be stored in other devices on a computer, including hard drives
and solid state drives that are built into the computer or even onto storage
devices that can be removed from the computer, like USB drives or external hard
drives. The size of available storage in these devices tends to be much, much
larger than the storage size of the computer’s RAM. However, it takes longer to
access data in these secondary storage devices because they aren’t directly
connected to the processor, and instead require the data to move into RAM before
it can be accessed by the processor, which is the only part of the computer that
can do things to analyze, modify, or otherwise process the data.</p>
<p>The traditional dataframe structure in R is built after
reading data into RAM. However, many biological experiments now create
data that is much too large to read into memory for R in a reasonable way
<span class="citation">(Lawrence and Morgan 2014; Hicks et al. 2021)</span>. If you try to read in a dataset
that’s too large for the RAM, R can’t handle it. As Roger Peng notes in
<em>R Programming for Data Science</em>:</p>
<blockquote>
<p>“Reading in a large dataset for which you do not have enough RAM is one easy
way to freeze up your computer (or at least your R session). This is usually an
unpleasant experience that usually requires you to kill the R process, in the
best case scenario, or reboot your computer, in the worst case.” <span class="citation">(R. D. Peng 2016)</span></p>
</blockquote>
<p>More complex data structures can allow more sophisticated ways to handle massive
data, and so they are often necessary when working with massive biological
datasets, particularly early in pre-processing, before the data can be
summarized in an efficient way. For example, a more complex data structure could
allow much of the data to be left on disk, and only read into memory on demand,
as specific portions of the data are needed <span class="citation">(Gatto 2013; Hicks et al. 2021)</span>. This approach can be used to iterate across subsets of the
data, only reading parts of the data into memory at a time
<span class="citation">(Lawrence and Morgan 2014)</span>. Such structures can be designed to work in a way that,
if you are the user, you won’t notice the difference in where the data is kept
(on disk versus in memory)—this means you won’t have to worry about these
memory management issues, but instead can just gain from everything going
smoothly, even as datasets get very large <span class="citation">(Gatto 2013)</span>.
As one article notes:</p>
<blockquote>
<p>“These advances have helped to ensure that R and Bioconductor remain relevant
in the age of high-throughput sequencing. We plan to continue in this direction
by designing and implementing abstractions that enable user code to be agnostic
to the mode of data storage, whether it be memory, files or databases. This will
bring much needed agility to resource allocation and will enable the user to be
more resourceful, without the burden of increased complexity.”
<span class="citation">(Lawrence and Morgan 2014)</span></p>
</blockquote>
<p>The second reason that tidy dataframes aren’t always best for biomedical data
has to do with the complexity of the data. Dataframes are very clearly and
simply organized. However, they can be too restrictive in some cases. Sometimes,
you might have data that do not fit well within the two-dimensional, non-ragged
structure that is characteristic of the dataframe structure. For example, some
biomedical data may have data that records characteristics at several levels of
the data. It may have records on the levels of gene expression within each
sample, separate information about each gene that was measured, and another
separate set of information that characterizes each of the samples. While it is
critical to keep “like” measurements aligned with data like this—in other
words, to ensure that you can connect the data that characterizes a gene with
the data that provides measures of the level of expression of that gene in each
sample—these data do not naturally have a two-dimensional structure and so do
not fit naturally into a dataframe structure.</p>
<p>Finally, one of the advantages of these complex data structures for biomedical
data preprocessing is that they can be leveraged to develop very powerful
algorithms for working with complex biomedical data. These include reading data
in from the specialized file formats that are often output by laboratory
equipment. As Holmes and Huber note:</p>
<blockquote>
<p>“Bioconductor packages support the reading of many of the data types and formats
produced by measurement instruments used in modern biology, as well as the
needed technology-specific ‘preprocessing’ routines. This community is
actively keeping these up-to-date with the rapid developments in the
instrument market.” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
</div>
<div id="combining-bioconductor-and-tidyverse-approaches-in-a-workflow" class="section level3 hasAnchor" number="3.6.4">
<h3><span class="header-section-number">3.6.4</span> Combining Bioconductor and tidyverse approaches in a workflow<a href="experimental-data-preprocessing.html#combining-bioconductor-and-tidyverse-approaches-in-a-workflow" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Work with research data will typically require a series of steps for
pre-processing, analysis, exploration, and visualization. Collectively, these
form a <em>workflow</em> or <em>pipeline</em> for the data analysis. With large, complex
biological data, early steps in this workflow might require a Bioconductor
approach, given the size and complexity of the data, or because you’d like to
use a method or algorithm available through Bioconductor. However, this doesn’t
mean that you must completely give up the power and efficiency of the tidyverse
approach described in earlier modules.</p>
<p>Instead, you can combine the two, in a workflow like that shown in Figure
<a href="experimental-data-preprocessing.html#fig:combinedworkflow">3.1</a>. In this combined approach, you start the workflow
in the Bioconductor approach and transition when possible to a tidyverse
approach, transitioning by “tidying” from a more complex data structure to a
simpler dataframe data structure along the way. In this module, we will describe
how you can make this transition to create this type of combined workflow. This
is a useful approach, because once your workflow has advanced to a stage where
it is straightforward to store the data in a a dataframe, there are a large
advantages to shifting into the tidyverse approach as compared to using more
complex object-oriented classes for storing the data, in particular when it
comes to data analysis and visualization at later stages in your workflow.</p>
<div class="figure"><span style="display:block;" id="fig:combinedworkflow"></span>
<img src="figures/workflow.png" alt="An overview of a workflow that moves from a Bioconductor approach---for pre-processing of the data---through to a tidyverse approach one pre-processing has created smaller, simpler data that can be reasonably stored in a dataframe structure." width="\textwidth" />
<p class="caption">
Figure 3.1: An overview of a workflow that moves from a Bioconductor approach—for pre-processing of the data—through to a tidyverse approach one pre-processing has created smaller, simpler data that can be reasonably stored in a dataframe structure.
</p>
</div>
<p>Key to this kind of combined pipeline are tools that can convert between
specialized data structures for Bioconductor to tidy dataframes. A set of tools
for doing this are available through the <code>biobroom</code> package. You will use
functions in this package applied to certain types of Bioconductor data objects,
and the function will be able to extract parts of the data into a tidy data
frame.</p>
<p>The <code>biobroom</code> package includes three main generic functions (also called
“methods”), which can be used on a number of Bioconductor object classes. When
applied to object stored in one of these Bioconductor classes, these functions
will extract part of the data into a tidy dataframe format. In this format, it
is easy to use the tools from the tidyverse to further explore, analyze, and
visualize the data.</p>
<p>The three generic functions of <code>biobroom</code> are the functions <code>tidy</code>, <code>augment</code>,
and <code>glance</code>. These function names mimic the names of the three main functions
in the <code>broom</code> package, which is a more general purpose package for extracting
tidy datasets from more complex R object containers <span class="citation">(D. Robinson 2014)</span>. The
<code>broom</code> package focuses on the output from functions in R for statistical
testing and modeling, while the newer <code>biobroom</code> package replicates this idea,
but for many of the common object classes used to store data through
Bioconductor packages and workflows.</p>
<p>As an example, we can look at how the <code>biobroom</code> package can be used to convert
output generated by functions in the <code>edgeR</code> package. The <code>edgeR</code> package is a
popular Bioconductor package that can be used on gene expression data to explore
which genes are expressed differently across experimental groups (<em>differential
expression analysis</em>) <span class="citation">(M. D. Robinson, McCarthy, and Smyth 2010)</span>. Before using the functions in the package, the
data must be preprocessed to align sequence reads from the raw data and then to
create a table with the counts of each read at each gene across each sample. The
<code>edgeR</code> package includes functions for pre-processing through its own functions,
as well, including capabilities for filtering out genes with low read counts
across all samples and model-based normalization across samples to help handle
technical bias, including differences in sequencing depth <span class="citation">(Chen et al. 2014)</span>.</p>
<p>The <code>edgeR</code> package operates on data stored in a special object class defined by
the package, the <code>DGEList</code> object class <span class="citation">(Chen et al. 2014)</span>. This object class
includes areas for storing the table of read counts, in the form of a matrix
appropriate for analysis by other functions in the package, as well as other
spots for storing information about each sample and, if needed, a space to store
annotations of the genes <span class="citation">(Chen et al. 2014)</span>. Then functions from the <code>edgeR</code>
package can perform differential expression analysis on the data in the
<code>DGEList</code> class. The result is an object in the <code>DGEExact</code> class, which is
defined by the <code>edgeR</code> package. To extract data from this class in a tidy
format, you can use the <code>tidy</code> and <code>glance</code> functions from <code>biobroom</code>.</p>
</div>
<div id="outlook-for-a-tidyverse-approach-to-biomedical-data" class="section level3 hasAnchor" number="3.6.5">
<h3><span class="header-section-number">3.6.5</span> Outlook for a tidyverse approach to biomedical data<a href="experimental-data-preprocessing.html#outlook-for-a-tidyverse-approach-to-biomedical-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Finally, it is quite likely better tools will continue to evolve, and that in
the future there might be tidy dataframe formats that are adaptable enough to
handle earlier stages in the data preprocessing for genomics data. The tidyverse
dataframe approach has already been adapted to enable tidy dataframes to include
more complex types of data within certain columns of the data frame as a special
list-type column. This functionality is being leveraged through the <code>sf</code>
package, for example, to enable a tidy approach to working with geographical
data. This allows those who are working with geographical data, for example data
from shapefiles for creating maps, to use the standard tidyverse approaches
while still containing complex data needed for this geographical information
within a tidy dataframe. It seems very possible that similar approaches may be
adapted in the near future to allow for biomedical or genomic data to be stored
in a way that both accounts for complexity early and pre-processing of these
data but also allows for a more natural integration with the wealth of powerful
tools available through the tidyverse approach.</p>
<hr />
<p>Many excellent
resources exist for learning the tidyverse approach, and so we won’t recover that
information here. Instead, we will focus on how to interface between this
approach and the object-based approach that’s more common with Bioconductor
packages. Bioconductor packages often take an object-based approach, and with
good reason because of the complexity and size of many early versions of
biomedical data in the preprocessing process. There are also resources for
learning to use specific Bioconductor packages, as well as some general
resources on Bioconductor, like <em>R Programming for Bioinformatics</em> [ref].
However, there are fewer resources available online that teach how to coordinate
between these two approaches in a pipeline of code, so that you can leverage the
needed power of Bioconductor approaches early in your pipeline, as you
preprocess large and complex data, and then shift to use a tidyverse approach
once your data is amenible to this more straightforward approach to analysis and
visualization.</p>
<p>The heart of making this shift is learning how to convert data, when possible,
from a more complex, class-type data structure (built on the flexible list
data structure) to the simpler, more standardized two-dimensional dataframe
structure that is required for the tidyverse approach. In this subsection, we’ll
cover approaches for converting your data from Bioconductor data structures to
dataframes.</p>
<p>If you are lucky, this might be very straightforward. A pair of packages called
<code>broom</code> and <code>biobroom</code> have been created specifically to facilitate the conversion
of data from more complex structures to dataframes. The <code>broom</code> package was
created first, by David Robinson, to convert the data stored in the objects that
are created by fitting statistical models into tidy dataframes. Many of the functions
in R that run statistical tests or fit statistical models output results in a
more complex, list-based data structure. These structures have nice “print” methods,
so if fitting the model or running the test is the very last step of your pipeline,
you can just read the printed output from R. However, often you want to include
these results in further code—for example, creating plots or tables that show
results from several statistical tests or models. The <code>broom</code> package includes
several functions for pulling out different bits of data that are stored in the
complex data structure created by fitting the model or running the test and convert
those pieces of data into a tidy dataframe. This tidy dataframe can then be
easily used in further code using a tidyverse approach.</p>
<p>The <code>biobroom</code> package was created to meet a similar need with data stored in some
of the complex structures commonly used in Bioconductor packages.</p>
<hr />
<p>Some of the most important data structures in Bioconductor are <span class="citation">(Huber et al. 2015)</span> (from Table 2 in this reference):</p>
<ul>
<li><code>ExpressionSet</code> (<code>Biobase</code> package)</li>
<li><code>SummarizedExperiment</code> (<code>GenomicRanges</code> package)</li>
<li><code>GRanges</code> (<code>GenomicRanges</code> package)</li>
<li><code>VCF</code> (<code>VariantAnnotation</code> package)</li>
<li><code>VRanges</code> (<code>VariantAnnotation</code> package)</li>
<li><code>BSgenome</code> (<code>BSgenome</code> package)</li>
</ul>
<blockquote>
<p>“The Bioconductor project distributes the software as a number of different R
packages, including Rsamtools, IRanges, GenomicRanges, GenomicAlignments,
Biostrings, rtracklayer, biovizBase and BiocParallel. The software enables the
analyst to conserve computational resources, iteratively generate summaries and
visualize data at arbitrary levels of detail. These advances have helped to
ensure that R and Bioconductor remain relevant in the age of high-throughput
sequencing. We plan to continue in this direction by designing and implementing
abstractions that enable user code to be agnostic to the mode of data storage,
whether it be memory, files or databases. This will bring much needed agility to
resource allocation and will enable the user to be more resourceful, without the
burden of increased complexity.” <span class="citation">(Lawrence and Morgan 2014)</span></p>
</blockquote>
<blockquote>
<p>“The biobroom package contains methods for converting standard objects in Bioconductor into a ‘tidy format’. It serves as a complement to the popular broom package, and follows the same division (tidy/augment/glance) of tidying methods.”
<span class="citation">(Bass et al. 2020)</span></p>
</blockquote>
<blockquote>
<p>“Tidying data makes it easy to recombine, reshape and visualize bioinformatics analyses. Objects that can be tidied include: ExpressionSet object,
GRanges and GRangesList objects, RangedSummarizedExperiment object, MSnSet object,
per-gene differential expression tests from limma, edgeR, and DESeq2, qvalue object for multiple hypothesis testing.” <span class="citation">(Bass et al. 2020)</span></p>
</blockquote>
<blockquote>
<p>“We are currently working on adding more methods to existing Bioconductor objects.” <span class="citation">(Bass et al. 2020)</span></p>
</blockquote>
<blockquote>
<p>“All biobroom tidy and augment methods return a tbl_df by default (this prevents them from printing many rows at once, while still acting like a traditional data.frame).” <span class="citation">(Bass et al. 2020)</span></p>
</blockquote>
<blockquote>
<p>“The concept of ‘tidy data’ offers a powerful framework for structuring data
to ease manipulation, modeling and visualization. However, most R functions,
both those builtin and those found in third-party packages, produce output that
is not tidy, and that is therefore difficult to reshape, recombine, and
otherwise manipulate. Here I introduce the broom package, which turns the output
of model objects into tidy data frames that are suited to further analysis,
manipulation, and visualization with input-tidy tools.” <span class="citation">(D. Robinson 2014)</span></p>
</blockquote>
<blockquote>
<p>“Tools are classified as ‘messy-output’ if their output does not fit into this
[tidy] framework. Unfortunately, the majority of R modeling tools, both from the
built-in stats package and those in common third party packages, are
messy-output. This means the data analyst must tidy not only the original data,
but the results at each intermediate stage of an analysis.” <span class="citation">(D. Robinson 2014)</span></p>
</blockquote>
<blockquote>
<p>“The broom package is an attempt to solve this issue, by bridging the gap from
untidy outputs of predictions and estimations to create tidy data that is easy
to manipulate with standard tools. It centers around three S3 methods, tidy,
augment, and glance, that each take an object produced by R statistical
functions (such as lm, t.test, and nls) or by popular third-party packages (such
as glmnet, survival, lme4, and multcomp) and convert it into a tidy data frame
without rownames (Friedman et al., 2010; Therneau, 2014; Bates et al., 2014;
Hothorn et al., 2008). These outputs can then be used with input-tidy tools such
as dplyr or ggplot2, or downstream statistical tests. broom should be
distinguished from packages such as reshape2 and tidyr, which rearrange and
reshape data frames into different forms (Wickham, 2007b, 2014b). Those packages
perform essential tasks in tidy data analysis but focus on manipulating data
frames in one specific format into another. In contrast, broom is designed to
take data that is not in a data frame (sometimes not anywhere close) and convert
it to a tidy data frame.” <span class="citation">(D. Robinson 2014)</span></p>
</blockquote>
<blockquote>
<p>“<code>tidy</code> constructs a data frame that summarizes the model’s statistical
components, which we refer to as the component level. In a regression such as
the above it may refer to coefficient estimates, p-values, and standard errors
for each term in a regression. The tidy generic is flexible- in other models it
could represent per-cluster information in clustering applications, or per-test
information for multiple comparison functions. … <code>augment</code> add columns to the
original data that was modeled, thus working at the observation level. This
includes predictions, residuals and prediction standard errors in a regression,
and can represent cluster assignments or classifications in other applications.
By convention, each new column starts with . to ensure it does not conflict with
existing columns. To ensure that the output is tidy and can be recombined,
rownames in the original data, if present, are added as a column called
.rownames. … Finally, <code>glance</code> constructs a concise one-row summary of the
model level values. In a regression this typically contains values such as R2 ,
adjusted R2 , residual standard error, Akaike Information Criterion (AIC), or
deviance. In other applications it can include calculations such as cross
validation accuracy or prediction error that are computed once for the entire
model. … These three methods appear across many analyses; indeed, the fact
that these three levels must be combined into a single S3 object is a common
reason that model outputs are not tidy. Importantly, some model objects may have
only one or two of these methods defined. (For example, there is no sense in
which a Student’s T test or correlation test generates information about each
observation, and therefore no augment method exists).” <span class="citation">(D. Robinson 2014)</span></p>
</blockquote>
<blockquote>
<p>“While model inputs usually require tidy inputs, such attention to detail
doesn’t carry over to model outputs. Outputs such as predictions and estimated
coefficients aren’t always tidy. For example, in R, the default representation
of model coefficients is not tidy because it does not have an explicit variable
that records the variable name for each estimate, they are instead recorded as
row names. In R, row names must be unique, so combining coefficients from many
models (e.g., from bootstrap resamples, or subgroups) requires workarounds to
avoid losing important information. This knocks you out of the flow of analysis
and makes it harder to combine the results from multiple models.”
<span class="citation">(Wickham 2014)</span></p>
</blockquote>
<blockquote>
<p>“Right now, in labs across the world, machines are sequencing the genomes of the life
on earth. Even with rapidly decreasing costs and huge technological advancements in
genome sequencing, we’re only seeing a glimpse of the biological information contained
in every cell, tissue, organism, and ecosystem. However, the smidgen of total biological
information we’re gathering amounts to mountains of data biologists need to work with. At
no other point in human history has our ability to understand life’s complexities been so
dependent on our skills to work with and analyze data.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Bioinformaticians are concerned with deriving biological understanding from large
amounts of data with specialized skills and tools. Early in biology’s history, the
datasets were small and manageable. Most biologists could analyze their own data after
taking a statistics course, using Microsoft Excel on a personal desktop computer.
However, this is all rapidly changing. Large sequencing datasets are widespread, and will
only become more common in the future. Analyzing this data takes different tools, new skills,
and many computers with large amounts of memory, processing power, and disk space.”
<span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Unfortunately, many of the biologist’s common computational tools can’t scale to the
size and complexity of modern biological data. Complex data formats, interfacing
numerous programs, and assessing software and data make large bioinformatics datasets
difficult to work with.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Bioconductor’s pakcage system is a bit different than those on the Comprehensive R
Archive Network (CRAN). Bioconductor packages are released on a set schedule, twice
a year. Each release is coordinated with a version of R, making Bioconductor’s versions
tied to specific R versions. The motivation behind this strict coordination is that it
allows for packages to be thoroughly tested before being released for public use.
Additionally, because there’s considerable code re-use within the Bioconductor project,
this ensures that all package versions within a Bioconductor release are compatible
with one another. For users, the end result is that packages work as expected and
have been rigorously tested before you use it (this is good when your scientific
results depend on software reliability!). If you need the cutting-edge version of a
package for some reason, it’s always possible to work with their development branch.”
<span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“When installing Bioconductor packages, we use the <code>biocLite()</code> function. <code>biocLite()</code>
installs the correct version of a package for your R version (and its corresponding
Bioconductor version).” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
</div>
<div id="practice-quiz-1" class="section level3 hasAnchor" number="3.6.6">
<h3><span class="header-section-number">3.6.6</span> Practice quiz<a href="experimental-data-preprocessing.html#practice-quiz-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>

</div>
</div>
<div id="module18" class="section level2 hasAnchor" number="3.7">
<h2><span class="header-section-number">3.7</span> Introduction to reproducible data pre-processing protocols<a href="experimental-data-preprocessing.html#module18" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Reproducibility tools can be used to create reproducible data pre-processing
protocols—documents that combine code and text in a “knitted” document, which
can be re-used to ensure data pre-processing is consistent and reproducible
across research projects. In this module, we will describe how reproducible data
pre-processing protocols can improve reproducibility of pre-processing
experimental data, as well as to ensure transparency, consistency, and
reproducibility across the research projects conducted by a research team.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Define a “reproducible data pre-processing protocol”</li>
<li>Explain how such protocols improve reproducibility at the data pre-processing
phase</li>
<li>List other benefits, including improving efficiency and consistency of data
pre-processing</li>
<li>Understand how a “knitted” document can be used to combine text and
executable code to create a reproducible data pre-processing protocol</li>
</ul>
<div id="introducing-reproducible-data-pre-processing-protocols" class="section level3 hasAnchor" number="3.7.1">
<h3><span class="header-section-number">3.7.1</span> Introducing reproducible data pre-processing protocols<a href="experimental-data-preprocessing.html#introducing-reproducible-data-pre-processing-protocols" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If you have ever worked in a laboratory, you are likely familiar with protocols.
For a wet lab, protocols are used as “recipes” for conducting certain
experiments or processes. They are written to be clear enough that everyone in
the lab could follow the same steps in the process by following the protocol. In
this way, they help to standardize processes done in the laboratory, and they
can also play a role in improving safety and the quality of data collection.
Protocols are similarly used for medical procedures and
tests, as well as for clinical trials. In all cases, they help to define in
detail the steps of the procedure, so they can be done in a way that is
comparable from one case to the next and with high precision.</p>
<p>Placeholder (you should not see this)</p>
<p>You can apply a similar idea to pre-processing and analyzing the data that you
collect in a laboratory. Just as a wet lab protocol can help standardize your
data collection to the point that the data are recorded, a separate protocol
can help define how you manage and work with that data.
The basic content of a data-focused protocol will include a description of the type of data
you expect to input, the type of data you expect at the end of the process,
and the steps you take to get from the input to the output.
A data-focused protocol can include steps for quality control of the
collected data, as well as pre-processing steps like transformations and
scaling of the data.</p>
<p>In module 3.9, we’ll walk through an example of creating a data pre-processing
protocol that focuses on data collected by plating samples to estimate
bacterial load. In this case, a key step in pre-processing the data is to
identify a “good” dilution to be used for estimating bacterial load in each
sample—each sample is plated at several dilutions, and to work with the data,
you must identify a dilution for each sample for which enough bacteria grew to
be countable, but not so many that there are too many colonies to count. In high
throughput experiments, like RNA-seq experiments, there may be important steps
in the data pre-processing that help check for batch effects across samples, for
signs of a poor-quality sample, or for normalizing and scaling the data in
preparation for applying other algorithms, like algorithms to estimate
differential expression across samples or to identify clusters within the data.</p>
<p>A data-focused protocol brings many of the same advantages as wet lab protocols.
It can help standardize the process of data pre-processing across members of
the laboratory, as well as from experiment to experiment. It can also help
ensure the quality of the data collection, by defining clear rules,
steps, and guidelines for completing the data pre-processing. Finally, it can
help ensure that someone else could recreate the process at a later time, and
so can improve the reproducibility of the experiment.
Not only do data-focused protocols help with improving quality and reproducibility,
but they also help improve efficiency. These protocols should include
clearly defined steps, as well as explanations for each step, and they should illustrate
these with example data. By having this “recipe”, a new lab member can quickly
learn how to do the data pre-processing, and a long-term lab member remember
the exact steps more quickly.</p>
<p>You can create a data pre-processing protocol using any document processing
program that you’d like. For example, you could write one in Google Docs or in
Word. However, there is a better format. With programming languages like R and
Python, you can created a type of document called a <strong>knitted document</strong>. A
knitted document interweaves two elements: first, text written for humans and
second, executable code meant for the computer. These documents can be
“rendered” in R or another programming language, which executes all the code and
adds all the output from that code at the appropriate place in the text. The end
result is a document in a format that is easy to share and read (PDF, Word, or
HTML), which includes text, example code, and output. You can use these
documents to record the data pre-processing process for a type of data in your
laboratory, and by using a knitted document, you ensure that the code is
“checked” every time you render the document. In this module,
we will give an overview of how these knitted documents work, as well as how they
can improve the reproducibility and efficiency of experimental work. In the
next module, we’ll show how you can make them in the free RStudio software.
Finally, in module 3.9, we’ll walk through a full example of writing a data pre-processing
protocol in this way—you can take a look now to get an idea by downloading the
example protocol
<a href="https://github.com/geanders/improve_repro/raw/master/data/bactcountr_example_data/example_protocol.pdf">here</a>.
There are also some excellent data-focused protocols that have been published in
journals like <em>Nature Protocols</em>. Some recent examples of such protocols include
<span class="citation">Schrode et al. (2021)</span>, <span class="citation">Quintelier et al. (2021)</span>, and <span class="citation">Majumder et al. (2021)</span>. You
may find it useful to take a look at one or more to get an idea of how
data-focused protocols can be useful.</p>
<!-- ### Improving reproducibility with protocols -->
<!-- Just like a protocol for a laboratory procedure, a protocol for data -->
<!-- pre-processing should be thorough and detailed. In a protocol, every step of the -->
<!-- process should be clearly documented [@thomas2015write]. The steps should be -->
<!-- detailed enough that the reader can start from the example input data and get -->
<!-- the same resulting output data. -->
<!-- One thing that helps to ensure this level of detail is to do all pre-processing -->
<!-- steps using code scripts and include this code in the protocol. While there  -->
<!-- are a number of software tools available that let you pre-process or analyze -->
<!-- data using point-and-click software, data pre-processing that is done with  -->
<!-- this type of software can be hard to document and reproduce. As an alternative,  -->
<!-- you could use code for a program like R or Python to complete the same steps,  -->
<!-- and this allows you to record each step that was taken in the process.  -->
<!-- As an example, let's consider the process of gating flow cytometry data. Flow -->
<!-- cytometry data is often collected in immunology research, where it can be used -->
<!-- to characterize the population size of different types of immune cells, like T -->
<!-- cells and B cells. The data can therefore be used to help measure an animal's -->
<!-- immune response following vaccination or a challenge with a pathogen. Flow -->
<!-- cytometry data is collected by ... There are some steps that a researcher must -->
<!-- take to get from the raw data that is output by the flow cytometry equipment to -->
<!-- the measurement they're interested in, which might be something like the -->
<!-- population size of CD4+ T cells in the sample. The researcher must go through -->
<!-- the data in steps, comparing along one or two of the measured [markers?] at each -->
<!-- step, looking at the univariate or bivariate distributions along those markers. -->
<!-- As they do this, they'll apply "gates" to isolate the cells in a specific part -->
<!-- of the distribution, and only use those cells in continuing steps. For example, -->
<!-- they might start by isolating only cells that are live and are "singlets" -->
<!-- (measurements of single cells, rather than "doublets", which represent two or -->
<!-- more cells stuck together).  -->
<!-- This gating process can be done with point-and-click software. When such -->
<!-- software is used, the researcher will sit at the computer and go through a -->
<!-- series of screens, using their mouse at each stage to add a gate to subset to -->
<!-- certain cells. As an alternative, a researcher could conduct this process using -->
<!-- a code script. The script would use algorithms to apply specific rules to the -->
<!-- gating process at each step. It will be much easier to make a reproducible data -->
<!-- pre-processing protocol if the researcher uses a code script for the process -->
<!-- rather than the point-and-click software. With code, all relevant details of the -->
<!-- process can be found by investigating either the code or the software that the -->
<!-- code draws on (which, for software like R and Python, is open-source and so can -->
<!-- be explored). Therefore, the code itself provides thorough and detailed -->
<!-- documentation of the process. By contrast, to document the point-and-click -->
<!-- method, the researcher would need to describe each screen they see during the -->
<!-- process and where they click on each screen. This could be done with screenshots -->
<!-- and explanations, but it can be hard to describe thoroughly enough to ensure -->
<!-- that the process could be exactly reproduced. Further, the layout of some  -->
<!-- screens may change in the future, in which case the described process may  -->
<!-- no longer work. While the open-source software behind coding-based approaches -->
<!-- can also change, all former versions tend to be archived so they can be  -->
<!-- accessed in the future if needed to reproduce an earlier process.  -->
<!-- (Fortunately, some point-and-click software systems have begun creating a way to  -->
<!-- save workflows and apply them later, which is more similar to the process -->
<!-- of writing a script to do the analysis. These do help with reproducibility,  -->
<!-- including consistency across samples for an experiment and across analyses  -->
<!-- done by different people. However, they may not always be as transparent  -->
<!-- and easy to decipher as a well-documented code script, and they won't interface -->
<!-- as well with other valuable tools, like the RMarkdown system for creating  -->
<!-- reproducible reports.) -->
<!-- A data pre-processing protocol can also help in improving the quality of the -->
<!-- pre-processing. If your research group uses the protocol to explain and  -->
<!-- document why each step is taken, the process of writing the protocol helps -->
<!-- to think deeply about each step of pre-processing. You can include references -->
<!-- in the protocol to recent papers to help justify certain steps, and you can -->
<!-- revisit and update the protocol at regular periods to make sure that it  -->
<!-- reflects the current best practices [@thomas2015write]. In doing so, you can create versions -->
<!-- of the protocol, so that you can both reproduce work done using an earlier -->
<!-- version, while still allowing the protocol to be a "living" document that  -->
<!-- changes as new data pre-processing methods become available.  -->
<!-- [Example---scRNA-seq, normalization?] -->
<!-- ### Improving efficiency with protocols -->
<!-- Data pre-processing protocols can also help improve the efficiency of working -->
<!-- with biomedical data. As an example, let's revisit the flow cytometry example -->
<!-- that we discussed in the last section. If the process has been documented using -->
<!-- a code script, then the entire process can often be redone on a new sample just -->
<!-- by changing the input data for the code. By contrast, if data pre-processing was -->
<!-- done using point-and-click software, the researcher typically must go through -->
<!-- each step anew with the data for each sample. The use of code---including code -->
<!-- that is embedded in a reproducible protocol---can therefore save the researcher -->
<!-- a lot of time in this scenario. While the use of code for the pre-processing -->
<!-- plays a large role in this gain in efficiency, there is also some gain from -->
<!-- using not just a code script but one within a protocol, as the protocol can help -->
<!-- the researcher quickly figure out any changes that should be made for a new -->
<!-- sample compared to the example data used within the protocol---the text and -->
<!-- explanations in the protocol can help in figuring out if anything about -->
<!-- processing the new sample should be done differently from the standard process. -->
<!-- ... -->
</div>
<div id="using-knitted-documents-for-protocols" class="section level3 hasAnchor" number="3.7.2">
<h3><span class="header-section-number">3.7.2</span> Using knitted documents for protocols<a href="experimental-data-preprocessing.html#using-knitted-documents-for-protocols" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When it comes to protocols that are focused on data pre-processing and
analysis, there are big advantages to creating them as something called
<strong>knitted documents</strong>. In this section, we’ll walk through what a knitted
document is, and in the next section we’ll cover some of the advantages of using this format to
create data-focused protocols.</p>
<p>A knitted document is one that is written in plain text in a way that “knits”
together text with executable code. Once you have written the document, you can
render it, which executes the code, adds to the document results from this
execution (figures, tables, and code output, for example), and formats all text
using the formatting choices you’ve specified. The end result is a nicely format
document, which can be in one of several output formats, including PDF, Word, or
HTML. Since the code was executed to create the document, you can ensure that
all the code has worked as intended.</p>
<p>If you have coded using a scripting language like R or Python, you likely have
already seen many examples of knitted documents. For both these languages, there
are many tutorials available that are created as knitted documents. Figure
<a href="experimental-data-preprocessing.html#fig:xcmsexample">3.2</a> shows an example from the start of a vignette for the
<code>xcms</code> package in R. This is a package that helps with pre-processing and
analyzing data from liquid chromatography–mass spectrometry (LC–MS)
experiments. You can see that this document includes text to explain the package
and also example code and the output from that code.
As a larger example, all the modules in this online book were written as knitted
documents.</p>
<div class="figure"><span style="display:block;" id="fig:xcmsexample"></span>
<img src="figures/vignette_example_annotated.png" alt="An example of a knitted document. This shows a section of the online vignette for the `xcms` package from Bioconductor. The two types of content are highlighted: formatted text for humans to read, and executable computer code." width="\textwidth" />
<p class="caption">
Figure 3.2: An example of a knitted document. This shows a section of the online vignette for the <code>xcms</code> package from Bioconductor. The two types of content are highlighted: formatted text for humans to read, and executable computer code.
</p>
</div>
<p>You can visualize the full process of creating and rendering a knitted document
in the following way. Imagine that you write a document by hand on sheets of
paper. There are parts where you need a team member to add their data or to run
a calculation, so you include notes in square brackets telling your team member
where to do these things. Then, you use some editing marks to show where
text should be italicized and which text should be section a header:</p>
<pre><code># Results

We measured the bacterial load of 
*Mycobacterium tuberculosis* for each 
sample. 

[Kristina: Calculate bacterial loads for 
each sample based on dilutions and
add table with results here.]</code></pre>
<p>You send the document to your team member Kristina first, and she does her
calculations and adds the results at the indicated spot in the paper, so that
the note to her gets replaced with results. She focuses on the notes to her in
square brackets and ignores the rest of the document. Next, Kristina sends the
document, with her additions, to an assistant, Tom, to type up the document. Tom
types the full document, paying attention to any indications that are included
for formatting. For example, he sees that “Results” is meant to be a section
heading, since it is on a line that starts with “#”, your team’s convention for
section headings. He therefore types this on a line by itself in larger font. He
also sees that “Mycobacterium tuberculosis” is surrounded by asterisks, so he
types this in italics.</p>
<p>Knitted documents work in the same way, but the computer does the steps that
Kristina and Tom did in this toy example. The way the document was written in
this example is analogous to writing up a knitted document in plain text with
appropriate “executable” sections, designated with special markings, and with
other markings used to show how the text should be formatted in its final
version. When Kristina looked for the section that was marked for her, generated
results in that section, and replaced the note with the results, it was
analogous to the first stage of rendering a knitted document, where the document
is passed through software that looks for executable code and ignores everything
else, executing that code and adding in results in the right place.
When Tom took that output and used formatting marks in the text to create a
nicely formatted final report, the step was analogous to the second stage of
rendering a formatted document, when a software program takes the output of the
first stage and formats the full document into an attractive, easy-to-read final
document, using any markings you include to format the document.</p>
<p>Knitted documents therefore build on two key techniques. The first is the
ability to include executable code in a document, in a way that a computer can
go through the document, find that code, execute it, and fill in the results at
the appropriate spot in the document. The second is a set of conventions for
formatting marks that can be put in the plain text of the document to indicate
formatting that should be added, like headers and italic text. Let’s take a closer
look at each of these necessary techniques.</p>
<p>The first technique that’s needed to create knitted documents is the ability to
include executable code within the plain text version of the document.
The idea here is that you can use special markers to indicate in the document
where code starts and where it ends. With these markings, a computer program can
figure out the lines of the document that it should run as code, and the ones it
should ignore when it’s looking for executable code. In the toy example above,
notes to Kristina were put in square brackets, with content that started with
her name and a colon. To “process” this document, then, she could just scan
through it for square brackets with her name inside and ignore everything else
in the document.</p>
<p>The same idea happens with knitted documents, but a computer program takes the
place of Kristina in the example. With markings in place to indicate executable
code, the document will be run through two separate programs as it is rendered.
The first program will look for code to execute and ignore any other lines of
the file. It will execute this code and then place any results, like figures,
tables, or code output, into the document right after that piece of code. We
will talk about the second program in just a minute, when we talk about markup
languages.</p>
<p>This technique comes from an idea that you could include code to be executed in
a document that is otherwise easy for humans to read. This is an incredibly
powerful idea. It originated with a famous computer scientist named Donald
Knuth, who realized that one key to making computer code sound is to make sure
that it is clear to humans what the code is doing. Computers will faithfully do
exactly what you tell them to do, so they will do what you’re hoping they will
as long as you provide the correct instructions. The greatest room for error,
then, comes from humans not giving the right instructions to computers. To
write sound code, and code that is easy for yourself and others to maintain and
extend, you must make sure that you and other humans understand what it is
asking the computer to do. Donald Knuth came up with a system called “literate
programming” that allows programmers to write code in a way that focuses on
documenting the code for humans, while also allowing the computer to easily
pull out just the parts that it needs to execute, while ignoring all the text
meant for humans. This process flips the idea of documenting code by including
plain text comments in the code—instead of the code being the heart of the
document, the documentation of the code is the heart, with the code provided
to illustrate the implementation. When used well, this technique results in
beautiful documents that clearly and comprehensively document the intent and
the implementation of computer code. The knitted documents that we can build
with R or Python through systems like RMarkdown and Jupyter Notebooks build
on these literate programming ideas, applying them in ways that complement
programming languages that can be run interactively, rather than needing to
be compiled before they’re run.</p>
<p>The second technique required for knitted documents is one that allows you to
write text in plain text, include formatting specifications in that plain text,
and render this to an attractive output document in PDF, Word, or HTML. This
part of the process uses a tool from a set of tools called <strong>Markup languages</strong>.
Here, we will use a markup language called <strong>Markdown</strong>. It is one of the easiest
markup languages to learn, as it has a fairly small set of formatting indicators
that can be used to “markup” the formatting in a document. This small set,
however, covers much of the formatting you might want to do, and so this
language provides an easy introduction to markup languages while still providing
adequate functionality for most purposes.</p>
<p>The Markdown markup language evolved starting in spaces where people could
communicate in plain text only, without point-and-click methods for adding
formatting like bold or italic type <span class="citation">(Buffalo 2015)</span>. For example,
early versions of email only allowed users to write using plain text. These
users eventually evolved some conventions for how to “mark-up” this plain text,
to serve the purposes normally served by things like italics and bold in formatted text
(e.g., emphasis, highlighting). For example, to emphasize a word, a user could
surround it with asterisks, like:</p>
<pre><code>I just read a *really* interesting article!</code></pre>
<p>In this early prototype for a markup language, the reader’s mind was doing
the “rendering”, interpreting these markers as a sign that part of the text
was emphasized. In Markdown, the text can be rendered into more attractive
output documents, like PDF, where the rendering process has actually
changed the words between asterisks to print in italics.</p>
<p>Placeholder (you should not see this)</p>
<p>The Markdown language has developed a set of these types of marks—like
asterisks—that are used to “mark up” the plain text with the formatting that
should be applied when the text is rendered. There are marks that you can use
for a number of formatting specifications, including: italics, bold, underline,
strike-through, bulleted lists, numbered lists, web links, headers of different
levels (e.g., to mark off sections and subsections), horizontal
rules, and block quotes. Details and examples of the Markdown syntax can be
found on the Markdown Guide page at <a href="https://www.markdownguide.org/basic-syntax/" class="uri">https://www.markdownguide.org/basic-syntax/</a>,
and we’ll cover more examples of using Markdown in the next two modules. Once a
document is run through a program to execute any code, it will then be run
through a program that interprets this formatting markup (a markup renderer),
which will format the document based on any of the mark up indications and will
output an attractive document in a format like PDF, Word, or HTML.</p>
</div>
<div id="advantages-of-using-knitted-documents-for-data-focused-protocols" class="section level3 hasAnchor" number="3.7.3">
<h3><span class="header-section-number">3.7.3</span> Advantages of using knitted documents for data-focused protocols<a href="experimental-data-preprocessing.html#advantages-of-using-knitted-documents-for-data-focused-protocols" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are several advantages to using knitted documents when writing code to
pre-process or analyze research data. These include improvements in terms of
reliability, efficiency, transparency, and reproducibility.</p>
<p>First, when you have written your code within a knitted document, this code is
checked every time you render the document. In other words, you are checking
your code to ensure it operates as you intend throughout the process of writing
and editing your document, checking the code each time you render the document
to its formatted version. This helps to increase the <strong>reliability</strong> of the code
that you have written. Open-source software evolves over time, and by continuing
to check code as you work on protocols and reports with your data, you can
ensure that you will quickly identify and adapt to any such changes. Further,
you can quickly identify if updates to your research data introduce any issues
with the code. Again, by checking the code frequently, you can identify any
issues quickly, and this often will allow you to easily pinpoint and fix these
issues. By contrast, if you only identify a problem after writing a lot of code,
it is often difficult to identify the source of the issue. By including code
that is checked each time of document is rendered, you can quickly identify when
a change in open source software affects the analysis that you were conducting
or the pre-processing and work to adapt to any changes quickly.</p>
<p>Second, when you write a document that includes executable code, it allows you
to easily rerun the code as you update your research data set, or adopt the code
to work with a new data set. If you are not using a knitted document to write
pre-processing protocols and research reports, then your workflow is probably to
run all your code—either from a script or the command line—and copy the
results into a document in a word processing program like Word or Google Docs.
If you do that, you must recopy all your results every time you adapt any part
of the code or add new data. By contrast, when you use a knitted document, the
rendering process executes the code and incorporates the results directly and
automatically into a nicely formatted final document. The use of knitted
documents therefore can substantially improve the <strong>efficiency</strong> of
pre-processing and analyzing your data and generating the reports that summarize
this process.</p>
<p>Third, documents that are created in knitted format are created using plain
text. Plain text files can easily be tracked well and clearly using version
control tools like git, and associated collaboration tools like GitHub, as
discussed in earlier modules (modules 2.9–2.11). This substantially increases the <strong>transparency</strong>
of the data pre-processing and analysis. It allows you to clearly document
changes you or others make in the document, step-by-step. You can document who
made the change, and that person can include a message about why they made the
change. This full history of changes is recorded and can be searched to explore
how the document has evolved and why.</p>
<p>The final advantage of using knitted documents, especially for pre-processing
research data, is that it allows the code to be clearly and thoroughly
documented. This can help increase the <strong>reproducibility</strong> of the process. In
other words, it can help ensure that another researcher could repeat the same
process, making adaptations as appropriate for their own data set, or ensuring
they arrive at the same results if using the original data. It also ensures that
you can remember exactly what you did, which is especially useful if you plan to
reuse or adopt the code to work with other data sets, as will often be the case
for a pre-processing protocol. If you are not using a knitted document, but are
using code for preprocessing, then as an alternative you may be documenting your
code through comments in a code script. A code script does allow you to include
documentation about the code through these code comments, which are demarcated
from code in the script through a special symbol (<code>#</code> in R). However these code
comments are much less expressive and harder to read than nicely formatted text,
and it is hard to include elements like mathematical equations and literature
citations in code comments. A knitted document allows you to write the
documentation in a format that is clear and attractive for humans to read, while
including code that is clear and easy for a computer to execute.</p>
</div>
<div id="how-knitted-documents-work" class="section level3 hasAnchor" number="3.7.4">
<h3><span class="header-section-number">3.7.4</span> How knitted documents work<a href="experimental-data-preprocessing.html#how-knitted-documents-work" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now that we’ve gotten a top-level view of the idea of knitted documents, let’s take a
closer look at how they work. We’ll wrap up this module by covering some of
the mechanics of how all knitted documents work, and then in the next module (3.8)
we’ll look more closely at how you can leverage these techniques in the
RMarkdown system specifically.</p>
<p>There are seven components of how these documents work. It is helpful to
understand these to understand these to begin creating and adapting knitted
documents. Knitted documents can be created through a number of programs, and
while we will later focus on Rmarkdown, these seven components are in play
regardless of the exact system used to create a knitted document, and therefore
help in gaining a general understanding of this type of document. We have listed
the seven components here and in the following paragraphs will describe each
more fully:</p>
<ol style="list-style-type: decimal">
<li>Knitted documents start as plain text;</li>
<li>A special section at the start of the document (<strong>preamble</strong>) gives some
overall directions about the document;</li>
<li>Special combinations of characters indicate where the executable code starts;</li>
<li>Other special combinations show where the regular text starts (and the
executable code section ends);</li>
<li>Formatting for the rest of the document is specified with a <strong>markup
language</strong>;</li>
<li>You create the final document by <strong>rendering</strong> the plain text document. This
process runs through two software programs; and</li>
<li>The final document is attractive and <strong>read-only</strong>—you should never make
edits to this output, only to your initial plain text document.</li>
</ol>
<p>First, a knitted document should be written in plain text. In an earlier module,
we described some of the advantages of using plain text file formats, rather
than proprietary and/or binary file formats, especially in the context of saving
research data (e.g., using csv file formats rather than Excel file formats).
Plain text can also be used to write documentation, including through knitted
documents. Figure <a href="experimental-data-preprocessing.html#fig:xcmsexampleplain">3.3</a> shows an example of what the plan text might look like for the
start of the <code>xcms</code> tutorial shown in Figure <a href="experimental-data-preprocessing.html#fig:xcmsexample">3.2</a>.</p>
<div class="figure"><span style="display:block;" id="fig:xcmsexampleplain"></span>
<img src="figures/plaintext_vignette_example.png" alt="An example of a the plain text used to write a knitted document. This shows a section of the plain text used to write the online vignette for the `xcms` package from Bioconductor. The full plain text file used for the vignette can be viewed on GitHub [here](https://github.com/sneumann/xcms/blob/master/vignettes/xcms.Rmd)." width="\textwidth" />
<p class="caption">
Figure 3.3: An example of a the plain text used to write a knitted document. This shows a section of the plain text used to write the online vignette for the <code>xcms</code> package from Bioconductor. The full plain text file used for the vignette can be viewed on GitHub <a href="https://github.com/sneumann/xcms/blob/master/vignettes/xcms.Rmd">here</a>.
</p>
</div>
<p>There are a few things to keep in mind when writing plain text. First, you
should always use a text editor rather than a word processor when you are
writing a document in plain text. Text editors can include software programs
like Notepad on Microsoft operating systems and TextEdit on Mac operating
systems. You can also use a more advanced text editor, like vi/vim or emacs.
Rstudio can also serve as a text editor, and if you are doing other work in
Rstudio, this is often the most obvious option as a text editor to use to write
knitted documents.</p>
<p>You must use a text editor to write plain text for knitted documents for the
same reasons that you must use one to write code scripts. Word processors often
introduce formatting that is saved through underlying code rather than clearly
evident on the document that you see as you type. This hidden formatting can
complicate the written text. Conversely, text written in a text editor will not
introduce such hard-to-see formatting. Word processing programs also tend to
automatically convert some symbols into slightly fancier versions of the symbol.
For example, they may change a basic quotation symbol into one with shaping,
depending on whether the mark comes at the beginning or end of a quotation. This
subtle change in formatting can cause issues in both the code and the formatting
specifications that you include in a knitted document.</p>
<p>Further, when are writing plain text, typically you should only use characters
from the American Standard Code for Information Interchange, or ASCII. This is a
character set from early in computing that includes 128 characters. Such a small
character set enforces simplicity: this character set mostly includes what you
can see on your keyboard, like the digits 0 to 9, the lowercase and uppercase
alphabet, some symbols, including punctuation symbols like the exclamation point
and quotation marks, some mathematical symbols like plus, minus, and division,
and some control codes, including ones for a new line, a tab, and even ringing a
bell. The full set of characters included in ASCII can be found in a number of
sources including a very thorough Wikipedia page on this character set (<a href="https://en.wikipedia.org/wiki/ASCII" class="uri">https://en.wikipedia.org/wiki/ASCII</a>).</p>
<p>Because the character set available for plain text files is so small, you will
find that it becomes important to leverage the limited characters that are
available. One example is <strong>white space</strong>. White space can be created in ASCII
with both the space character and with the new line command. It is an important
component that can be used to make plain text files clear for humans to read. As
we begin discussing the convention for markdown languages, we will find that
white space is often used to help specify formatting as well.</p>
<p>The second component of how knitted documents work is that each knitted document
will have a special section at its start called the <strong>preamble</strong>. This preamble
will give some overall directions regarding the document, like its title and
authors and the format to which it should be rendered. Knitted documents are
created using a <strong>markup language</strong> to specify formatting for the document, and
there are a number of different markup languages including HTML, LaTeX, and
Markdown. The specifications for the document’s preamble will depend on the
markup language being used.</p>
<p>In Rmarkdown, we will be focusing on Markdown, for which the preamble is
specified using something called YAML (short for YAML Ain’t Markup Language).
Here is an example of the YAML for a sample pre-processing protocol created
using RMarkdown:</p>
<pre><code>  ---
  title: &quot;Preprocessing Protocol for LC-MS Data&quot;
  author: &quot;Jane Doe&quot;
  date: &quot;1/25/2021&quot;
  output: pdf_document
  ---</code></pre>
<p>This YAML preamble specifies information about the document with <strong>keys</strong> and
<strong>values</strong>. For example, the title is specified using the YAML key <code>title</code>,
followed by a colon and a space, and then the desired value for that
component of the document, <code>"Preprocessing Protocol for LC-MS Data"</code>.
Similarly, the author is specified with the <code>author</code> key and the desired
value for that component, and the date with the <code>date</code> key and associated
component.</p>
<p>Different keys can take different types of values in the YAML
(this is similar to how different parameters in a function can take different values). For example, the keys of <code>author</code>, <code>title</code>, and <code>date</code> all take
a character string with any desired character combination, and the quotation
marks surrounding the values for each of these keys denote those character strings. By contrast, the <code>output</code> key—which specifies the format that
that the knitted document should be rendered to—can only take one of a
few set values, each of which is specified without surrounding
quotation marks (<code>pdf_document</code> in this case, to render the document
as a PDF report).</p>
<p>The rules for which keys can be included in the preamble will depend on the
markup language being used. Here, we are showing an example in Markdown, but you
can also use other markup languages like LaTeX and HTML, and these will have
their own convention for specifying the preamble. In the next module, when we
talk more specifically about Rmarkdown, we will give some resources where you
can find more about how to customize the preamble in Rmarkdown specifically. If
you are using a different markup language, there are numerous websites,
cheatsheets, and other resources you can use to find which keywords are
available for the preamble in that markup language, as well as the possible
values those keywords can take.</p>
<p>The next characteristic of knitted documents is that they need to clearly
demarcate where executable code starts and where regular formatted text starts
(in other words, where the executable code section ends). To do this, knitted
documents have two special combination of characters, one that can be used in
the plain text to indicate where executable code starts and and one to indicate
where it ends. For example, Figure <a href="experimental-data-preprocessing.html#fig:demarcatecode">3.4</a> shows the plain text
that could be used in an Rmarkdown document to write some regular text, then
some executable code, and then indicate the start of more regular text:</p>
<div class="figure"><span style="display:block;" id="fig:demarcatecode"></span>
<img src="figures/demarcating_code.png" alt="An example of how special combinations of characters are used to demarcate code in an RMarkdown file. The color formatting here is applied automatically by RStudio; all the text in this example is written in plain text." width="\textwidth" />
<p class="caption">
Figure 3.4: An example of how special combinations of characters are used to demarcate code in an RMarkdown file. The color formatting here is applied automatically by RStudio; all the text in this example is written in plain text.
</p>
</div>
<p>The combination that indicates the start of executable code will vary depending
on the markup language being.
You may have noticed that these markers, which indicate the beginning and end of
executable code, seem like very odd character combination. There is a good
reason for this. By making this character combination unusual, there will be
less of a chance that it shows up in regular text. This way there are fewer
cases where the writer unintentionally indicate the start of a new section of
executable code when trying to write regular text in the knitted document.</p>
<p>The next characteristic of knitted documents is that formatting for the regular
text in the document—that is, everything that is not executable code—is
specified using what is called a <strong>markup language</strong>. When you were writing in
plain text, you do not have buttons to click on for formatting, for example, to
specify words or phrases that should be in bold or italics, font size, headings,
and so on. Instead you use special characters or character combinations to
specify formatting in the final document. These character combinations are
defined based on the markup language you use. As mentioned earlier, Rmarkdown
uses the Markdown language; other knitted documents can be created using LaTeX
or HTML. As an example of how these special character combinations work, in
Markdown, you place two asterisks around a word or phrase to make it bold. To
write <strong>“this”</strong> in the final document, in other words, you’ll write
<code>**"this"**</code> in the plain text in the initial document.</p>
<p>You can start to see how this works by looking at the example of the <code>xcms</code>
vignette shown earlier in Figures <a href="experimental-data-preprocessing.html#fig:xcmsexample">3.2</a> and
<a href="experimental-data-preprocessing.html#fig:xcmsexampleplain">3.3</a>. In Figure <a href="experimental-data-preprocessing.html#fig:xcmsbothversions">3.5</a>, we’ve
recreated these two parts side-by-side, so they’re easier to compare.</p>
<div class="figure"><span style="display:block;" id="fig:xcmsbothversions"></span>
<img src="figures/originalandfinal.png" alt="The original plain text for a knitted document and the final output, side by side. These examples are from the `xcms` package vignette, a package available on Bioconductor. The left part of the figure shows the plain text that was written to create the output, which is shown in the left part of the figure. You can see how elements like sections headers and different font styles are indicated in the original plain text through special characters or combinations of charaters, using the Markdown language syntax." width="\textwidth" />
<p class="caption">
Figure 3.5: The original plain text for a knitted document and the final output, side by side. These examples are from the <code>xcms</code> package vignette, a package available on Bioconductor. The left part of the figure shows the plain text that was written to create the output, which is shown in the left part of the figure. You can see how elements like sections headers and different font styles are indicated in the original plain text through special characters or combinations of charaters, using the Markdown language syntax.
</p>
</div>
<p>You can look for several formatting elements here. First, the section is headed
“Initial data inspection”. You can see that in the original plain text document,
this is marked using a <code>#</code> to start the line with the text for the header. You
can also see that words or phrases that are formatted in a computer-style font
in the final document—to indicate that they are values from computer code,
rather than regular English words—are surrounded by backticks in the plain
text file.</p>
<p>The final characteristics of knitted documents is that, to create the final
document, you will render the plain text document. That is the process that will
create an attractive final document. To visualize this, <strong>rendering</strong> is the
process that takes the document from the plain text format, as shown in the left
of Figure <a href="experimental-data-preprocessing.html#fig:xcmsbothversions">3.5</a>, to the final format, shown in the right
of that figure.</p>
<p>When you render the document, it will be run through two software programs, as
described earlier. The first will look only for sections with executable code,
based on the character combination that is used to mark these executable code
sections. This first software will execute that code and take any
output—including data results, figures, and tables—and insert those at the
relevant spot in the document’s file. Next, the output file from this software
will be run through another software program. This second program will look
for all the formatting instructions and render the final document in an
attractive format. This final output can be in a number of file formats,
depending what you specify in the preamble, including a PDF document, an HTML
file, or a Word document.</p>
<p>You should consider the final document, regardless of the output format, as
read-only. This means that you should never make edits or changes to the final
version of the document. Instead you should make any changes to your initial
plain text file. This is because the rendering process will overwrite any
previous versions of the final document. Therefore any changes that you have
made to your final document will be overwritten anytime you re-render from the
original plain text document.</p>
<!-- ### Discussion questions -->

</div>
</div>
<div id="module19" class="section level2 hasAnchor" number="3.8">
<h2><span class="header-section-number">3.8</span> RMarkdown for creating reproducible data pre-processing protocols<a href="experimental-data-preprocessing.html#module19" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The R extension package RMarkdown can be used to create documents that combine
code and text in a ‘knitted’ document, and it has become a popular tool for
improving the computational reproducibility and efficiency of the data analysis
stage of research. This tool can also be used earlier in the research process,
however, to improve reproducibility of pre-processing steps. In this module, we
will provide detailed instructions on how to use RMarkdown in RStudio to create
documents that combine code and text. We will show how an RMarkdown document
describing a data pre-processing protocol can be used to efficiently apply the
same data pre-processing steps to different sets of raw data.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Define RMarkdown and the documents it can create</li>
<li>Explain how RMarkdown can be used to improve the reproducibility of research
projects at the data pre-processing phase</li>
<li>Create a document in RStudio using RMarkdown</li>
<li>Describe more advanced features of Rmarkdown and where you can find out more
about them</li>
</ul>
<div id="creating-knitted-documents-in-r" class="section level3 hasAnchor" number="3.8.1">
<h3><span class="header-section-number">3.8.1</span> Creating knitted documents in R<a href="experimental-data-preprocessing.html#creating-knitted-documents-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the last module (3.7), we described what knitted documents are, as well as the
advantages of using knitted documents to create data pre-processing protocols
for common pre-processing tasks in your research group. We also described the
key elements of creating a knitted document, regardless of the software system
you are using. In this module, we will
go into more detail about how you can create these documents using R and
RStudio, and in the next module (3.9) we will walk through an example data
pre-processing protocol created using this method. We strongly recommend that
you read the previous module (3.7) before working through this one.</p>
<p>R has a special format for creating knitted documents called <strong>Rmarkdown</strong>.
In the previous module, we talked about the elements of a knitted document, and
later in this module we’ll walk through how they apply to Rmarkdown. However,
the easiest way to learn how to use Rmarkdown is to try an example, so we’ll
start with a very basic one. If you’d like to try it yourself, you’ll need
to download R and RStudio. The RStudio IDE can be
downloaded and installed as a free software, as long as you use the personal
version (RStudio creates higher-powered versions for corporate use).</p>
<p>Like other plain text documents, an Rmarkdown file should be edited using a text
editor, rather than a word processor like Word or Google Docs. It is easiest to
use the Rstudio IDE as the text editor when creating and editing an R markdown
document, as this IDE has incorporated some helpful functionality for working
with plain text documents for Rmarkdown. In RStudio, you can create a number of
types of new files through the “File” menu. To create a new R markdown file,
open RStudio and then choose “New File”, then choose “Rmarkdown” from the
choices in that menu. Figure <a href="experimental-data-preprocessing.html#fig:rmarkdownnewfile">3.6</a> shows an example of
what this menu option looks like.</p>
<div class="figure"><span style="display:block;" id="fig:rmarkdownnewfile"></span>
<img src="figures/rmarkdown_newfile.png" alt="RStudio pull-down menus to help you navigate to open a new Rmarkdown file." width="\textwidth" />
<p class="caption">
Figure 3.6: RStudio pull-down menus to help you navigate to open a new Rmarkdown file.
</p>
</div>
<p>This will open a window with some options you can specify some of the overall
information about the document (Figure <a href="experimental-data-preprocessing.html#fig:rmarkdownchoices">3.7</a>), including
the title and the author. You can specify the output format that you would
like. Possible output formats include HTML, Word, and PDF. You should be able to
use the HTML and Word output formats without any additional software, so we’ll start
there with this example. If you
would like to use the PDF output, you will need to install one other piece of
software: Miktex for Windows, MacTex for Mac, or TeX Live for Linux. These are
all pieces of software with an underlying TeX engine and all are open-source and
free. The example in the next module was created as a PDF using one of these tools.</p>
<div class="figure"><span style="display:block;" id="fig:rmarkdownchoices"></span>
<img src="figures/rmarkdown_choices.png" alt="Options available when you create a new Rmarkdown file in RStudio. You can specify information that will go into the document's preamble, including the title and authors and the format that the document will be output to (HTML, Word, or PDF)." width="\textwidth" />
<p class="caption">
Figure 3.7: Options available when you create a new Rmarkdown file in RStudio. You can specify information that will go into the document’s preamble, including the title and authors and the format that the document will be output to (HTML, Word, or PDF).
</p>
</div>
<p>Once you have selected the options in this menu you can choose the “Okay” button
(Figure <a href="experimental-data-preprocessing.html#fig:rmarkdownchoices">3.7</a>). This will open a new document. This
document, however, won’t be blank. Instead it will include an example document
written in Rmarkdown (Figure <a href="experimental-data-preprocessing.html#fig:rmarkdowntemplate">3.8</a>). This example
document helps you navigate how the Rmarkdown process works, by letting you test
out a sample document. It also gives you a starting point—once you understand
how the example document works, you can edit it and change it to convert it
into the document you would like to create.</p>
<div class="figure"><span style="display:block;" id="fig:rmarkdowntemplate"></span>
<img src="figures/rmarkdown_template.png" alt="Example of the template Rmarkdown document that you will see when you create a new Rmarkdown file in RStudio. You can explore this template and try rendering (knitting) it. Once you are familiar with how this example works, you can edit the text and code to adapt it for your own document." width="\textwidth" />
<p class="caption">
Figure 3.8: Example of the template Rmarkdown document that you will see when you create a new Rmarkdown file in RStudio. You can explore this template and try rendering (knitting) it. Once you are familiar with how this example works, you can edit the text and code to adapt it for your own document.
</p>
</div>
<p>If you have not used Rmarkdown before, it is very helpful to try knitting this
example document before making changes, to explore how pieces in the document
align with elements in the rendered output document. Once you are
familiar with the line-up between elements in this file in the output document,
you can delete parts of the example file and insert your own text and code.</p>
<p>Let’s walk through and explore this example document, aligning it
with the formatted output document (Figure <a href="experimental-data-preprocessing.html#fig:rmarkdownoriginalfinal">3.9</a>).
First, to render this or any Rmarkdown document, if you are in RStudio you can
use the “Knit” button at the top of the file, as shown in Figure
<a href="experimental-data-preprocessing.html#fig:rmarkdowntemplate2">3.10</a>. When you click on this button, it will render the
entire document to the output format you’ve selected (HTML, PDF, or Word). This
rendering process will both run the executable code and apply all formatting.
The final output (Figure <a href="experimental-data-preprocessing.html#fig:rmarkdownoriginalfinal">3.9</a>, right) will pop up
in a new window. As you start with Rmarkdown, it is useful to look at this
output to see how it compares with the plain text Rmarkdown file (Figure
<a href="experimental-data-preprocessing.html#fig:rmarkdownoriginalfinal">3.9</a>, left).</p>
<div class="figure"><span style="display:block;" id="fig:rmarkdownoriginalfinal"></span>
<img src="figures/rmarkdownoriginalfinal.png" alt="Example of the template Rmarkdown document that you will see when you create a new Rmarkdown file in RStudio. You can explore this template and try rendering (knitting) it. Once you are familiar with how this example works, you can edit the text and code to adapt it for your own document." width="\textwidth" />
<p class="caption">
Figure 3.9: Example of the template Rmarkdown document that you will see when you create a new Rmarkdown file in RStudio. You can explore this template and try rendering (knitting) it. Once you are familiar with how this example works, you can edit the text and code to adapt it for your own document.
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:rmarkdowntemplate2"></span>
<img src="figures/rmarkdown_template2.png" alt="Example of the template Rmarkdown document, highlighting buttons in RStudio that you can use to facilitate working with the document. The 'knit' button, highlighted at the top of the figure, will render the entire document. The green arrow, highlighted lower in the figure within a code chunk, can be used to run the code in that specific code chunk." width="\textwidth" />
<p class="caption">
Figure 3.10: Example of the template Rmarkdown document, highlighting buttons in RStudio that you can use to facilitate working with the document. The ‘knit’ button, highlighted at the top of the figure, will render the entire document. The green arrow, highlighted lower in the figure within a code chunk, can be used to run the code in that specific code chunk.
</p>
</div>
<p>You will also notice, after you first render the document, that your working
directory has a new file with this output document. For example, if you are
working to create an HTML document using an Rmarkdown file called
“my_report.Rmd”, once you knit your Rmarkdown file, you will notice a new file
in your working directory called “my_report.html”. This new file is your output
file, the one that you would share with colleagues as a report. You should
consider this output document to be read only—in other words, you can read and
share this document, but you should not make any changes directly to this
document, since they will be overwritten anytime you re-render the original
Rmarkdown document.</p>
<p>Next, let’s compare the example Rmarkdown document (the one that is given when
you first open an Rmarkdown file in RStudio) with the output file that is
created when you render this example document (Figure
<a href="experimental-data-preprocessing.html#fig:rmarkdownoriginalfinal">3.9</a>). If you look at the output document (Figure
<a href="experimental-data-preprocessing.html#fig:rmarkdownoriginalfinal">3.9</a>, right), you can notice how different elements
align with pieces in the original Rmarkdown file (Figure
<a href="experimental-data-preprocessing.html#fig:rmarkdownoriginalfinal">3.9</a>). For example, the output document includes a
header with the text “R Markdown”. This second-level header is created by the
Markdown notation in the original file of:</p>
<pre><code>## R Markdown</code></pre>
<p>This header is formatted in a larger font than other text, and on a separate
line—the exact formatting is specified within the style file for the Rmarkdown
document, and will be applied to all second-level headers in the document. You
can also see formatting specified through things like bold font for the word
“Knit”, through the Markdown syntax <code>**Knit**</code>, and a clickable link specified
through the syntax <code>&lt;http://rmarkdown.rstudio.com&gt;</code>. At the beginning of the
original document, you can see how elements like the title, author, date, and
output format are specified in the YAML. Finally, you can see that special
character combinations demarcate sections of executable code.</p>
<p>Let’s look a little more closely in the next part of the module at how these
elements of the Rmarkdown document work.</p>
</div>
<div id="formatting-text-with-markdown-in-rmarkdown" class="section level3 hasAnchor" number="3.8.2">
<h3><span class="header-section-number">3.8.2</span> Formatting text with Markdown in Rmarkdown<a href="experimental-data-preprocessing.html#formatting-text-with-markdown-in-rmarkdown" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If you remember from the last module, one element of knitted documents is
that they are written in plain text, with all the formatting specified
using a markup language.
For the main text in an Rmarkdown document, all formatting is done using
Markdown as the markup language. Markdown is a popular markup language, in part
because it is a good bit simpler than other markup languages like HTML or LaTeX.
This simplicity means that it is not quite as expressive as other markup
languages. However, Markdown probably provides adequate formatting for at least 90% of the
formatting you will typically want to do for a research report or
pre-processing protocol, and by staying simpler, it is much easier to learn the
Markdown syntax quickly compared to other markup languages.</p>
<p>As with other markup languages, Markdown uses special characters or combinations of characters to indicate formatting within the plain text of the original document. When the document is rendered, these markings are used by the software to create the formatting that you have specified in the final output document. Some example formatting symbols and conventions for Markdown include:</p>
<ul>
<li>to format a word or phrase in bold, surround it with two asterisks (<code>**</code>)</li>
<li>to format a word or phrase in italics, surround it with one asterisk (<code>*</code>)</li>
<li>to create a first-level header, put the header text on its own line, starting the line with <code>#</code></li>
<li>to create a second-level header, put the header text on its own line, starting the line with <code>##</code></li>
<li>separate paragraphs with empty lines</li>
<li>use hyphens to create bulleted lists</li>
</ul>
<p>One thing to keep in mind when using Markdown, in terms of formatting, is that
white space can be very important in specifying the formatting. For example when
you specify a new paragraph, you must leave a blank line from your previous
text. Similarly when you use a hash (<code>#</code>) to indicate a header, you must leave a
blank space after the hash before the word or phrase that you want to be used in
that header. To create a section header, you would write:</p>
<pre><code># Initial Data Inspection</code></pre>
<p>On the other hand, if you forgot the space after the hash sign, like this:</p>
<pre><code>#Initial Data Inspection</code></pre>
<p>then in your ouput document you would get this:</p>
<p>#Initial Data Inspection</p>
<p>Similarly, white space is needed to separate paragraphs. For example, this would create two paragraphs:</p>
<pre><code>This is a first paragraph. 

This is a second.</code></pre>
<p>Meanwhile this would create one:</p>
<pre><code>This is a first paragraph.
This is still part of the first paragraph.</code></pre>
<p>The syntax of Markdown is fairly simple and can be learned quickly. For more
details on this syntax, you can refer to the Rmarkdown reference guide at
<a href="https://rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf" class="uri">https://rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf</a>. The
basic formatting rules for Markdown are also covered in some more
extensive resources for Rmarkdown that we will point you to later in this
module.</p>
</div>
<div id="preambles-in-rmarkdown-documents" class="section level3 hasAnchor" number="3.8.3">
<h3><span class="header-section-number">3.8.3</span> Preambles in Rmarkdown documents<a href="experimental-data-preprocessing.html#preambles-in-rmarkdown-documents" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the previous module, we explained how knitted documents include a
preamble to specify some metadata about the document, including elements
like the title, authors, and output format. In R, this preamble is
created using YAML. In this subsection, we provide some more details
on using this YAML section in Rmarkdown documents.</p>
<p>In an Rmarkdown document, the YAML is a special section
at the top of an RMarkdown document (the original, plain text file, not the
rendered version). It is set off from the rest of the document using a special
combination of characters, using a process very similar to how executable code
is set off from other text with a special set of characters so it can be easily
identified by the software program that renders the document. For the YAML, this
combination of characters is three hyphens (<code>---</code>) on a line by themselves to
start the YAML section and then another three on a line by themselves to end it.
Here is an example of what the YAML might look like at the top of an RMarkdown
document:</p>
<pre><code>---
title: &quot;Laboratory report for example project&quot;
author: &quot;Brooke Anderson&quot;
date: &quot;1/12/2020&quot;
output: word_document
---</code></pre>
<p>Within the YAML itself, you can specify different options for your document.
You can change simple things like the title, author, and date, but you can
also change more complex things, including how the output document is rendered.
For each thing that you want to specify, you specify it with a special
keyword for that option and then a valid choice for that keyword. The idea
is very similar to setting parameter values in a function call in R. For
example, the <code>title:</code> keyword is a valid one in RMarkdown YAML. It allows you
to set the words that will be printed in the title space, using title formatting,
in your output document. It can take any string of characters, so you can put in
any text for the title that you’d like, as long as you surround it with quotation
marks. The <code>author:</code> and <code>date:</code> keywords work in similar ways. The <code>output:</code>
keyword allows you to specify the output that the document should be rendered to.
In this case, the keyword can only take one of a few set values, including
<code>word_document</code> to output a Word document, <code>pdf_document</code> to output a pdf
document (see later in this section for some more set-up required to make that
work), and <code>html_document</code> to output an HTML document.</p>
<p>As you start using RMarkdown, you will be able to do a lot without messing with
the YAML much. In fact, you can get a long way without ever changing the values
in the YAML from the default values they are given when you first create an
RMarkdown document. As you become more familiar with R, you may want to learn
more about how the YAML works and how you can use it to customize your
document—it turns out that quite a lot can be set in the YAML to do very
interesting customizations in your final rendered document. The book <em>R
Markdown: The Definitive Guide</em> <span class="citation">(Xie, Allaire, and Grolemund 2018)</span>, which is available free online, has
sections discussing YAML choices for both HTML and pdf output, at
<a href="https://bookdown.org/yihui/rmarkdown/html-document.html" class="uri">https://bookdown.org/yihui/rmarkdown/html-document.html</a> and
<a href="https://bookdown.org/yihui/rmarkdown/pdf-document.html" class="uri">https://bookdown.org/yihui/rmarkdown/pdf-document.html</a>, respectively. There is
also a talk that Yihui Xie, the creator of RMarkdown, gave on this topic at a
past RStudio conference, available at
<a href="https://rstudio.com/resources/rstudioconf-2017/customizing-extending-r-markdown/" class="uri">https://rstudio.com/resources/rstudioconf-2017/customizing-extending-r-markdown/</a>.</p>
</div>
<div id="executable-code-in-rmarkdown-files" class="section level3 hasAnchor" number="3.8.4">
<h3><span class="header-section-number">3.8.4</span> Executable code in Rmarkdown files<a href="experimental-data-preprocessing.html#executable-code-in-rmarkdown-files" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the previous module, we described how knitted documents use special markers
to indicate where sections of executable code start and stop. In RMarkdown,
the markers you will use to indicate executable code look like this:</p>
<pre><code>```r{}
my_object &lt;- c(1, 2, 3)
```</code></pre>
<p>In Rmarkdown, the following combination indicates
the start of executable code:</p>
<p><code>```{r}</code></p>
<p>while this combination indicates the end of executable code (in other
words the start of regular text):</p>
<p><code>```</code></p>
<p>In the example above, we have shown the most basic
version of the markup character combination used to specify the start of
executable code (<code>```{r}</code>). This character combination can be expanded,
however, to include some specifications for how you want the code in the section
following it to be run, as well as how you want output to be shown. For example,
you could use the following indications to specify that the code should be run,
but the code itself should not be printed in the final document, by specifying
<code>echo = FALSE</code>, as well as that the created figure should be centered on the
page, by specifying <code>fig.align = "center"</code>:</p>
<p><code>```{r echo = FALSE, fig.align = "center"}</code></p>
<p>There are numerous options that can be used to specify how the code will be run.
These specifications are called
<strong>chunk options</strong>, and you specify them in the special character combination
where you mark the start of executable code. For example, you can specify that
the code should be printed in the document, but not executed, by setting the
<code>eval</code> parameter to <code>FALSE</code> with <code>```{r eval = FALSE}</code> as the marker to
start the code section.</p>
<p>The chunk options also include <code>echo</code>, which can be used to specify whether to
print the code in that code chunk when the document is rendered. For some
documents, it is useful to print out the code that is executed, where for other
documents you may not want that printed. For example, for a pre-processing
protocol, you are aiming to show yourself and others how the pre-processing was
done. In this case, it is very helpful to print out all of the code, so that
future researchers who read that protocol can clearly see each step. By
contrast, if you are using Rmarkdown to create a report or an article that is
focused on the results of your analysis, it may make more sense to instead hide
the code in the final document.</p>
<p>As part of the code options, you can also specify whether messages and warnings
created when running the code should be included in the document output, and
there are number of code chunk options that specify how tables and figures
rendered by the code should be shown. For more details on the possible options
that can be specified for how code is evaluated within an executable chunk of
code, you can refer to the Rmarkdown cheat sheet available at
<a href="https://rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf" class="uri">https://rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf</a></p>
<p>RStudio has some functionality that is useful when you are working with code in
Rmarkdown documents. Within each code chuck are some buttons that can be used to
test out the code in that chunk of executable code. One is the green right arrow
key to the right at the top of the code chunk, highlighted in Figure
<a href="experimental-data-preprocessing.html#fig:rmarkdowntemplate2">3.10</a>. This button will run all of the code in that
chunk and show you the output in an output field that will open directly below
the code chunk. This functionality allows you to explore the code in your
document as you build it, rather than waiting until you are ready to render the
entire document. The button directly to the left of that button, which looks
like an upward-pointing arrow over a rectangle, will execute all code that comes
before this chunk in the document. This can be very helpful in making sure that
you have set up your environment to run this particular chunk of code.</p>
</div>
<div id="more-advanced-rmarkdown-functionality" class="section level3 hasAnchor" number="3.8.5">
<h3><span class="header-section-number">3.8.5</span> More advanced Rmarkdown functionality<a href="experimental-data-preprocessing.html#more-advanced-rmarkdown-functionality" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The details and resources that we have covered so far focus on the basics of
Rmarkdown. You can get a lot done just with these basics. However, the Rmarkdown
system is very rich and allows complex functionality beyond these basics. In
this subsection, we will highlight just a few of the ways Rmarkdown can be used
in a more advanced way. Since this topic is so broad, we will focus on elements
that we have found to be particularly useful for biomedical researchers as they
become more advanced Rmarkdown users. For the most part, we will not go into
extensive detail about how to use these more advanced features in this module,
but instead point to resources where you can learn more as you are ready. If you
are just learning Rmarkdown, at this point it will be helpful to just know that
some of these advanced features are available, so you can come back and explore
them when you become familiar with the basics. However, we will provide more
details for one advanced element that we find particularly useful in creating data
pre-processing protocols: including bibliographical references.</p>
<p><strong>Including bibliographical references.</strong></p>
<p>To include references in RMarkdown documents, you can use something called
<strong>BibTeX</strong>. This is a software system that is free and open source and works in
concert with LaTeX and other markup languages. It allows you to save bibliographical information in a plain
text file—following certain rules—and then reference that information in a
document. In this way, it can serve the role of a bibliographical reference
manager (like Endnote or Mendeley) while being free and keeping all information
in plain text files, where they can easily be tracked with version control like
git. By using BibTeX with RMarkdown, you can include bibliographical references
in the documents that you create, and Rmarkdown will handle the creation of the
references section and the numbering of the documents within your text.</p>
<p>To use BibTeX to add references to an RMarkdown document, you’ll need to take
three steps:</p>
<ol style="list-style-type: decimal">
<li>Create a plain text file with listings for each of
your references (<strong>BibTeX file</strong>). Save this file with the
extension <code>.bib</code>. These listings need to follow a special format, which
we’ll describe in just a minute.</li>
<li>In your RMarkdown document, include the filepath
to this BibTeX file, so that RMarkdown will be able to find the bibliographical
listings.</li>
<li>In the text of the RMarkdown file, include a key and special character
combination anytime you want to reference a paper. This referencing also
follows a special format, which we’ll describe below.</li>
</ol>
<p>Let’s look at each of these steps in a bit more detail. The first step is
to create a plain text file with a listing for each of the documents that
you’d like to cite. The plain text document should be saved with the file
extension <code>.bib</code> (for example, “mybibliography.bib”), and the listings for
each document in the file must follow specific rules.</p>
<p>Let’s take a look at one to explore these rules. Here’s an example of a BibTeX
listing for a scientific article:</p>
<pre><code>@article{fox2020,
  title={Cyto-feature engineering: A pipeline for flow cytometry 
    analysis to uncover immune populations and associations with 
    disease},
  author={Fox, Amy and Dutt, Taru S and Karger, Burton and Rojas, 
    Mauricio and Obreg{\&#39;o}n-Henao, Andr{\&#39;e}s and 
    Anderson, G Brooke and Henao-Tamayo, Marcela},
  journal={Scientific Reports},
  volume={10},
  number={1},
  pages={1--12},
  year={2020}
}</code></pre>
<p>You can see that this listing is for an article, because it starts with the
keyword <code>@article</code>. BibTeX can record a number of different types of documents,
including articles, books, and websites. You start by specifying the document
type because different types of documents need to include different elements
in their listings. For example, a website should include the date when it was
last accessed, while an article typically will not.</p>
<p>Within the curly brackets for the listing shown above, there are key-value
pairs—elements where the type of value is given with a keyword (e.g.,
<code>title</code>), and then the value for that element is given after an equals sign.
For example, to specify the journal in which the article was published, this
listing has <code>journal={Scientific Reports}</code>. Finally, the listing has a key
that you will use to identify the listing in the main text. In this case,
the listing is given the key <code>fox2020</code>, which combines the first author and
publication year. You can use any keys you like for the items in the bibliography,
as long as they are different for every listing, so that the computer can
identify which bibliographical listing you are referring to when you use a key.</p>
<p>This format may seem overwhelming, but fortunately you will rarely have to
create these listings by hand. Instead, you can get them directly from Google
Scholar. To do this, look up the paper on Google Scholar (Figure
<a href="experimental-data-preprocessing.html#fig:googlebibtex">3.11</a>). When you see it, look for a small quotation mark
symbol at the bottom of the article listing (shown with the top red arrow in
Figure <a href="experimental-data-preprocessing.html#fig:googlebibtex">3.11</a>). If you click on this, it will open a pop-up
with the citation for the article. At the bottom of that pop-up is a link that
says “BibTeX” (bottom red arrow in Figure <a href="experimental-data-preprocessing.html#fig:googlebibtex">3.11</a>). If you click
on that, it will take you to a page that gives the full BibTex listing for that
article, and you can just copy and paste this into your plain text BibTeX file.</p>
<div class="figure"><span style="display:block;" id="fig:googlebibtex"></span>
<img src="figures/google_scholar.png" alt="Example of using Google Scholar to get bibliographical information for a BibTeX file. When you look up an article on Google Scholar, there is an option (the quotation mark icon under the article listing) to open a pop-up window with bibliographical information. At the bottom of this pop-up box, you can click on 'BibTeX' to get a plain text version of the BibTeX entry for the article. You can copy and paste this into you BibTeX file." width="\textwidth" />
<p class="caption">
Figure 3.11: Example of using Google Scholar to get bibliographical information for a BibTeX file. When you look up an article on Google Scholar, there is an option (the quotation mark icon under the article listing) to open a pop-up window with bibliographical information. At the bottom of this pop-up box, you can click on ‘BibTeX’ to get a plain text version of the BibTeX entry for the article. You can copy and paste this into you BibTeX file.
</p>
</div>
<p>Once you have this plain text BibTeX file, you will tell your computer how to
find it by including its path in the YAML. For example, if you created a BibTeX
file called “mybibliography.tex” and saved it in the same directory as a
RMarkdown document, you could use the following to indicate this file for
the RMarkdown document:</p>
<pre><code>  ---
  title: &quot;Reproducible Research with R&quot;
  author: &quot;Brooke Anderson&quot;
  date: &quot;1/25/2021&quot;
  output: beamer_presentation
  bibliography: mybibliography.bib
  ---</code></pre>
<p>This shows the YAML for the document—the part that goes at the beginning of
the RMarkdown document and gives some metadata and overall instructions for the
document. In this example, we’ve added an extra line: <code>bibliography: mybibliography.bib</code>. This says that you’d like to link to a BibTeX file when
this document is rendered, as well as where to find that file (the file named
“mybibliography.bib” in the directory of the RMarkdown file).</p>
<p>Now that you have created the BibTeX file and told the RMarkdown file where to
find it, you can connect the two. As you write in the RMarkdown file, you can
refer to any of your BibTeX listings by using the key that you set for that
document. For example, if you wanted to reference the Fox et al. paper we
used in the example listing above, you would use the key that we set for
that listing, <code>fox2020</code>.
You will follow a special convention when you reference this key: you’ll use the
<code>@</code> symbol directly followed by that key. Typically, you will surround
this with square brackets. Therefore, to reference the Fox et al. paper,
you’d use <code>[@fox2021]</code>.</p>
<p>Here’s how that might look in practice. If you write in the RMarkdown document:</p>
<pre><code>This technique follows earlier work [@fox2020].</code></pre>
<p>In the output from rendering that RMarkdown document you’d get:</p>
<blockquote>
<p>``This technique follows earlier work (Fox et al. 2020).”</p>
</blockquote>
<p>The full paper details will then be included at the end of the document, in a reference
section.</p>
<p><strong>Other advanced Rmarkdown functionality</strong></p>
<p>There are a number of other advanced things that you can do with Rmarkdown, once
you have mastered the basics. First, you can use Rmarkdown to build different
types of documents, not just reports in Word, PDF, or HTML. For example, you can
use the <code>bookdown</code> package to create entire online and print books using the
Rmarkdown framework. This book of modules was created using this system.
You can also create websites and web dashboards, using the <code>blogdown</code> and
<code>flexdashboard</code> packages, repectively. The <code>blogdown</code> package allows you to
create professionally-styled websites, including blog sections where you can
include R code and results. Figure <a href="experimental-data-preprocessing.html#fig:blogdown">3.12</a> gives an example of a
website created using <code>blogdown</code>—you can see the full website
<a href="https://kind-neumann-789611.netlify.app/">here</a> if you’d like to check out some
of the features that this framework provides. The <code>flexdashboard</code> package lets
you create “dashboards” with data, similar to the dashboards that many public
health departments using during the COVID-19 pandemic to share case numbers in
specific counties and states.</p>
<div class="figure"><span style="display:block;" id="fig:blogdown"></span>
<img src="figures/blogdownexample.png" alt="Example of a website created using blogdown, leveraging the Rmarkdown framework." width="\textwidth" />
<p class="caption">
Figure 3.12: Example of a website created using blogdown, leveraging the Rmarkdown framework.
</p>
</div>
<p>With Rmarkdown, you can also create reports that are more customized than the
default style that we explored above. First, you can create templates that add
customized styling to the document. In fact, many journals have created
journal-specific templates that you can use in Rmarkdown. With these templates,
you can write up your research results in a reproducible way, using Rmarkdown,
and submit the resulting document directly to the journal, in the correct
format. An example of the first page of an article created in Rmarkdown using
one of these article templates is shown in Figure <a href="experimental-data-preprocessing.html#fig:rticleexample">3.13</a>
<span class="citation">(Wendt and Anderson 2022)</span>. The <code>rticles</code> package in R provides these templates for several
different journal families.</p>
<div class="figure"><span style="display:block;" id="fig:rticleexample"></span>
<img src="figures/rticles_example.png" alt="Example of a manuscript written in Rmarkdown using a templat. This figure shows the first page of an article written for submission to PLoS Computation Biology, written in Rmarkdown while using the PLoS template from the `rticles` package (Wendt and Anderson 2022)." width="\textwidth" />
<p class="caption">
Figure 3.13: Example of a manuscript written in Rmarkdown using a templat. This figure shows the first page of an article written for submission to PLoS Computation Biology, written in Rmarkdown while using the PLoS template from the <code>rticles</code> package (Wendt and Anderson 2022).
</p>
</div>
<p>Rmarkdown also has some features that make it easy to run code that is
computationally expensive or code that is written in another programming
language. If code takes a long time to run, there are options in Rmarkdown to
<strong>cache</strong> the results—that is, run the code once when you render the document,
and then only re-run it in later renderings if the inputs have changed. Rmarkdown
does this through by saving intermediate results, as well as using a system to
remember which pieces of code depend on which earlier code. With very computationally
expensive code, it can be a big time saver, although it can also use more storage,
since it is saving more results. To include code in languages other than R, you can
change something called the <strong>engine</strong> of the code chunk. Essentially, this is the
language that your computer will use to run the code in that chunk. You can change
the engine so that certain chunks of code are run using Python, Julia, and other
languages by specifying the engine you’d like to use in the marker in the document
that indicates the start of a piece of executable code. Earlier in this module,
we showed you that executable code is normally introduced in Rmarkdown with
<code>```{r}</code>. The <code>r</code> in this string is specifying that the R engine should be
used to run the code.</p>
<p>Finally, Rmarkdown allows you to create very customized formatting, as you move into
more advanced ways to use the framework.
As mentioned earlier, Markdown is a fairly simple markup language. Occasionally,
this simplicity means that you might not be able to create fancier formatting
that you might desire. There is a method that allows you to work around this
constraint in RMarkdown.</p>
<p>In Rmarkdown documents, when you need more complex formatting, you can shift
into a more complex markup language for part of the document. Markup languages
like LaTeX and HTML are much more expressive than Markdown, with many more
formatting choices possible. However,
there is a downside—when you include formatting specified in these
more complex markup languages, you will limit the output formats that you can
render the document to. For example, if you include LaTeX formatting within
an RMarkdown document, you must output the document to PDF, while if you
include HTML, you must output to an HTML file. Conversely, if you stick with
the simpler formatting available through the Markdown syntax, you can easily
switch the output format for your document among several choices.</p>
<p>One area of customization that is particularly useful and simple to implement is
with customized tables. The Markdown syntax can create very simple tables, but
does not allow the creation of more complex tables. There is an R package called
<code>kableExtra</code> that allows you to create very attractive and complex tables
in RMarkdown documents.
This package leverages more of the power of underlying markup languages, rather
than the simpler Markdown language.
The
<code>kableExtra</code> package is extensively documented through two vignettes that come
with the package, one if the output will be in pdf
(<a href="https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_pdf.pdf" class="uri">https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_pdf.pdf</a>)
and one if it will be in HTML
(<a href="https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html" class="uri">https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html</a>).</p>
</div>
<div id="learning-more-about-rmarkdown." class="section level3 hasAnchor" number="3.8.6">
<h3><span class="header-section-number">3.8.6</span> Learning more about Rmarkdown.<a href="experimental-data-preprocessing.html#learning-more-about-rmarkdown." class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To learn more about RMarkdown, you can explore a number of excellent resources.
The most comprehensive are shared by RStudio, where RMarkdown’s developer
and maintainer, Yihui Xie, works. These resources are all freely available
online, and some are also available to buy as print books, if you prefer that
format.</p>
<p>First, you should check out the online tutorials that are provided by RStudio on
RMarkdown. These are available at RStudio’s RMarkdown page:
<a href="https://rmarkdown.rstudio.com/" class="uri">https://rmarkdown.rstudio.com/</a>. The page’s “Getting Started” section
(<a href="https://rmarkdown.rstudio.com/lesson-1.html" class="uri">https://rmarkdown.rstudio.com/lesson-1.html</a>) provides a nice introduction you
can work through to try out RMarkdown and practice the overview provided in the
last subsection of this module. The “Articles” section
(<a href="https://rmarkdown.rstudio.com/articles.html" class="uri">https://rmarkdown.rstudio.com/articles.html</a>) provides a number of other
documents to help you learn RMarkdown. RStudio’s RMarkdown page also includes a
“Gallery” (<a href="https://rmarkdown.rstudio.com/gallery.html" class="uri">https://rmarkdown.rstudio.com/gallery.html</a>). This resource allows you
to browse through example documents, so you can get a visual idea of what you
might want to create and then access the example code for a similar document.
This is a great resource for exploring the variety of documents that you can
create using RMarkdown.</p>
<p>To go more deeply into RMarkdown, there are two online books from some of the
same team that are available online. The first is <em>R Markdown: The Definitive
Guide</em> by Yihui Xie, J. J. Allaire, and Garrett Grolemund <span class="citation">(Xie, Allaire, and Grolemund 2018)</span>. This book is
available free online at <a href="https://bookdown.org/yihui/rmarkdown/" class="uri">https://bookdown.org/yihui/rmarkdown/</a>. It moves from
basics through very advanced functionality that you can implement with
RMarkdown, including several of the topics we highlight later in this
subsection.</p>
<p>The second online book to explore from this team is <em>R Markdown Cookbook</em>, by
Yihui Xie, Christophe Dervieux, and Emily Riederer <span class="citation">(Xie, Dervieux, and Riederer 2020)</span>. This book is available
free online at <a href="https://bookdown.org/yihui/rmarkdown-cookbook/" class="uri">https://bookdown.org/yihui/rmarkdown-cookbook/</a>. This book is a
helpful resource for dipping in to a specific section when you want to learn how
to achieve a specific task. Just like a regular cookbook has recipes that you
can explore and use one at a time, this book does not require a comprehensive
end-to-end read, but instead provides “recipes” with advice and instructions for
doing specific things. For example, if you want to figure out how to align a
figure that you create in the center of the page, rather than the left, you can
find a “recipe” in this book to do that.</p>

</div>
</div>
<div id="module20" class="section level2 hasAnchor" number="3.9">
<h2><span class="header-section-number">3.9</span> Example: Creating a reproducible data pre-processing protocol<a href="experimental-data-preprocessing.html#module20" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will walk through an example of creating a reproducible data pre-processing
protocol. As an example, we will look at how to pre-process and analyze data
that are collected in the laboratory to estimate bacterial load in samples.
These data come from plating samples from an immunological experiment at serial
dilutions, using data from an experiment lead by one of the coauthors. This data
pre-processing protocol was created using RMarkdown and allows the efficient,
transparent, and reproducible pre-processing of plating data for all experiments
in the research group. We will go through how RMarkdown techniques can be
applied to develop this type of data pre-processing protocol for a laboratory
research group.</p>
<p><strong>Objectives.</strong> After this module, you should be able to:</p>
<ul>
<li>Explain how a reproducible data pre-processing protocol can be developed for
a real research project</li>
<li>Understand how to design and implement a data pre-processing protocol to
replace manual or point-and-click data pre-processing tools</li>
<li>Apply techniques in RMarkdown to develop your own reproducible data
pre-processing protocols</li>
</ul>
<div id="introduction-and-example-data" class="section level3 hasAnchor" number="3.9.1">
<h3><span class="header-section-number">3.9.1</span> Introduction and example data<a href="experimental-data-preprocessing.html#introduction-and-example-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this module, we’ll provide advice and an example of how you can use the
tools for knitted documents to create a reproducible data preprocessing
protocol. This module builds on ideas and techniques that were introduced
in the last two modules (3.7 and 3.8), to help you put them into practical use for
data preprocessing that you do repeatedly for research data in your
laboratory.</p>
<p>In this module, we will use an example of a common pre-processing task in
immunological research: estimating the bacterial load in samples by plating at
different dilutions. For this type of experiment, the laboratory researcher
plates each of the samples at several dillutions, identifies a good dilution for
counting colony-forming units (CFUs), and then back-calculates the estimated
bacterial load in the original sample based on the colonies counted at this “good”
dilution. This experimental technique dates back to the late 1800s, with Robert
Koch, and continues to be widely used in microbiology research and applications
today <span class="citation">(Ben-David and Davidson 2014)</span>. These data are originally from an experiment in one
of our authors’ laboratory and are also available as example data for an R
package called <code>bactcountr</code>, currently under development at
<a href="https://github.com/aef1004/bactcountr/tree/master/data" class="uri">https://github.com/aef1004/bactcountr/tree/master/data</a>.</p>
<p>These data are representative of data often collected in immunological research.
For example, you may be testing out some drugs against an infectious bacteria
and want to know how successful different drugs are in limiting bacterial load.
You run an experiment and have samples from animals treated with different drugs
or under control and would then want to know how much viable (i.e., replicating)
bacteria are in each of your samples.</p>
<p>You can find out by plating the sample at different dilutions and
counting the colony-forming units (CFUs) that are cultured on each plate.
You put a sample on a plate with a medium they can grow on and then give them
time to grow. The idea is that individual bacteria from the original sample end
up randomly around the surface of the plate, and any that are viable (able to
reproduce) will form a new colony that, after a while, you’ll be able to see.</p>
<p>To get a good estimate of bactieral load from this process, you need to count
CFUs on a “countable” plate—one with a “just right” dilution (and you
typically won’t know which dilution this is for a sample until after plating).
If you have too high of a dilution (i.e., one with very few viable bacteria),
randomness will play a big role in the CFU count, and you’ll estimate the
original bacterial load with more variability. If you have too low of a dilution (i.e., one
with lots of viable bacteria), it will be difficult to identify separate
colonies, and they may complete for resources. To translate from diluted
concentration to original concentration, you can then do a back-calculation,
incorporating both the number of colonies counted at that dilution and how
dilute the sample was. There is therefore some pre-processing required (although
it is fairly simple) to prepare the data collected to get an estimate of
bacterial load in the original sample. This estimate of bacterial load can then be used in
statistical testing and combined with other experimental data to explore
questions like whether a candidate vaccine reduces bacterial load when a research
animal is challenged with a pathogen.</p>
<p>We will use this example of a common data pre-processing task to show how to
create a reproducible pre-processing protocol in this module. If you would like,
you can access all the components of the example pre-processing protocol and
follow along, re-rendering it yourself on your own computer. The example data
are available as a csv file, downloadable
<a href="https://raw.githubusercontent.com/geanders/improve_repro/master/data/bactcountr_example_data/cfu_data.csv">here</a>.
You can open this file using spreadsheet software, or look at it directly in
RStudio. The final pre-processing protocol for these data can also be
downloaded, including both <a href="https://raw.githubusercontent.com/geanders/improve_repro/master/data/bactcountr_example_data/example_protocol.Rmd">the original RMarkdown
file</a>
and <a href="https://github.com/geanders/improve_repro/raw/master/data/bactcountr_example_data/example_protocol.pdf">the output PDF
document</a>.
Throughout this module, we will walk through elements of this document, to
provide an example as we explain the process of developing data pre-processing
modules for common tasks in your research group. We recommend that you go ahead and
read through the output PDF document, to get an idea for the example protocol that
we’re creating.</p>
<p>This example is intentionally simple, to allow a basic introduction to the
process using pre-processing tasks that are familiar to many laboratory-based
scientists and easy to explain to anyone who has not used plating in experimental
work. However, the same general process can also be used to create
pre-processing protocols for data that are much larger or more complex or for
pre-processing pipelines that are much more involved.
For example, this process could be used to create data pre-processing protocols
for automated gating of flow cytometry data or for pre-processing data
collected through single cell RNA sequencing.</p>
</div>
<div id="advice-on-designing-a-pre-processing-protocol" class="section level3 hasAnchor" number="3.9.2">
<h3><span class="header-section-number">3.9.2</span> Advice on designing a pre-processing protocol<a href="experimental-data-preprocessing.html#advice-on-designing-a-pre-processing-protocol" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before you write your protocol in a knitted document, you should decide on the
content to include in the protocol. This section provides tips on this design
process. In this section, we’ll describe some key steps in designing a
data pre-processing protocol:</p>
<ol style="list-style-type: decimal">
<li>Defining input and output data for the protocol;</li>
<li>Setting up a project directory for the protocol;</li>
<li>Outlining key tasks in pre-processing the input data; and</li>
<li>Adding code for pre-processing.</li>
</ol>
<p>We will illustrate these design steps using the example protocol on
pre-processing plating data.</p>
<p><strong>Defining input and output data for the protocol.</strong></p>
<p>The first step in designing the data pre-processing protocol is to decide on the
starting point for the protocol (the data input) and the ending point (the data
output). It may make sense to design a separate protocol for each major type of
data that you collect in your research laboratory. Your input data for the
protocol, under this design, might be the data that is output from a specific
type of equipment (e.g., flow cytometer) or from a certain type of sample or
measurement (e.g., metabolomics run on a mass spectrometer), even if it is a
fairly simple type of data (e.g., CFUs from plating data, as used in the example
protocol for this module). For example, say you are working with three types of
data for a research experiment: data from a flow cytometer, metabolomics data
measured with a mass spectrometer, and bacterial load data measured by plating
data and counting colony forming units (CFUs). In this case, you may want to
create three pre-processing protocols: one for the flow data, one for the
metabolomics data, and one for the CFU data. These protocols are modular and can
be re-used with other experiments that use any of these three types of data.</p>
<p>With an example dataset, you can begin to create a pre-processing protocol
before you collect any of your own research data for a new experiment. If the
format of the initial data is similar to the format you anticipate for your
data, you can create the code and explanations for key steps in your
pre-processing for that type of data. Often, you will be able to adapt the
RMarkdown document to change it from inputting the example data to inputting
your own experimental data with minimal complications, once your data comes in.
By thinking through and researching data pre-processing options before the data
is collected, you can save time in analyzing and presenting your project results
once you’ve completed the experimental data collection for the project. Further,
with an example dataset, you can get a good approximation of the format in which
you will output data from the pre-processing steps. This will allow you to begin
planning the analysis and visualization that you will use to combine the
different types of data from your experiment and use it to investigate important
research hypotheses. Again, if data follow standardized formats across steps in
your process, it will often be easy to adapt the code in the protocol to input
the new dataset that you created, without major changes to the code developed
with the example dataset.</p>
<p>While pre-processing protocols for some types of data might be very complex,
others might be fairly simple. However, it is still worthwhile to develop a
protocol even for simple pre-processing tasks, as it allows you to pass along
some of the details of pre-processing the data that might have become “common
sense” to longer-tenured members of your research group. For example, the
pre-processing tasks in the example protocol are fairly simple. This protocol
inputs data collected in a plain-text delimited file (a csv file, in the
example). Within the protocol, there are steps to convert initial measurements
from plating at different dilutions into an estimate of the bacterial load in
each sample. There are also sections in the protocol for exploratory data
analysis, to allow for quality assessment and control of the collected data as
part of the preprocessing. The output of the protocol is a simple data object (a
dataframe, in this example) with the bacterial load for each original sample.
These data are now ready to be used in tables and figures in the research report
or manuscript, as well as to explore associations with the experimental design
details (e.g., comparing bacterial load in treated versus untreated animals) or
merged with other types of experimental data (e.g., comparing immune cell
populations, as measured with flow cytometry data, with bacterial loads, as
measured from plating and counting CFUs).</p>
<p>Once you have identified the input data type to use for the protocol, you should
identify an example dataset from your laboratory that you can use to create the
protocol. This could be a dataset that you currently need to pre-process, in
which case the development of the protocol will serve a second purpose, allowing
you to complete this task at the same time. However, you may not have a new set
of data of this type that you currently need to pre-process, and in this case
you can build your protocol using a dataset from a previous experiment in your
laboratory. In this case, you may already have a record of the steps that you
used to pre-process the data previously, and these can be helpful as a starting
point as you draft the more thorough pre-processing protocol. You may want to
select an example dataset that you have already published or are getting ready
to publish, so you won’t feel awkard about making the data available for people
to practice with. If you don’t have an example dataset from your own laboratory,
you can explore example datasets that are already available, either as data
included with existing R packages or through open repositories, including those
hosted through national research institutions like the NIH. In this case, be
sure to cite the source of the data and include any available information about
the equipment that was used to collect it, including equipment settings used
when the data were collected.</p>
<p>For the example protocol for this module, we want to pre-process data that were
collected “by hand” by counting CFUs on plates in the laboratory. These counts
were recorded in a plain text delimited file (a csv file) using spreadsheet
software. The spreadsheet was set up to ensure the data can easily be converted
to a “tidy” format, as described in module 2.3. The first few rows of the input
data look like this:</p>
<pre><code>## # A tibble: 6 × 6
##   group replicate dilution_0 dilution_1 dilution_2 dilution_3
##   &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;
## 1     2 2-A       26         10                  0          0
## 2     2 2-B       TNTC       52                 10          5
## 3     2 2-C       0          0                   0          0
## 4     3 3-A       0          0                   0          0
## 5     3 3-B       TNTC       TNTC               30         10
## 6     3 3-C       0          0                   0          0</code></pre>
<p>Each row represents the number of bacterial colonies counted after plating a
certain sample, where each sample represents one experimental animal and several
experimental animals (replicates) were considered for each experimental group.
Columns are included with values for the experimental group of the sample
(<code>group</code>), the specific ID of the sample within that experimental group
(<code>replicate</code>, e.g., <code>2-A</code> is mouse A in experimental group 2), and the
colony-forming units (CFUs) counted at each of several dilutions. If a cell has
the value “TNTC”, this indicates that CFUs were too numerous to count for that
sample at that dilution.</p>
<p>When you have identified the input data type you will use for the protocol,
as well as selected an example dataset of this type to use to create the
protocol, you can include a section in the protocol
that describes these input data, what file format they are in, and how they
can be read into R for pre-processing (Figure <a href="experimental-data-preprocessing.html#fig:protocoldatainput">3.14</a>).</p>
<div class="figure"><span style="display:block;" id="fig:protocoldatainput"></span>
<img src="figures/protocol_data_input.png" alt="Providing details on input data in the pre-processing protocol. Once you have an example data file for the type of data that will be input for the protocol, you can add a section that provides the code to read the data into R. You can also add code that will show the first few rows of the example dataset, as well as a description of the data. This figure shows examples of how these elements can be added to an RMarkdown file for a pre-processing protocol, and the associated elements in the final pdf of the protocol, using the example protocol for this module." width="\textwidth" />
<p class="caption">
Figure 3.14: Providing details on input data in the pre-processing protocol. Once you have an example data file for the type of data that will be input for the protocol, you can add a section that provides the code to read the data into R. You can also add code that will show the first few rows of the example dataset, as well as a description of the data. This figure shows examples of how these elements can be added to an RMarkdown file for a pre-processing protocol, and the associated elements in the final pdf of the protocol, using the example protocol for this module.
</p>
</div>
<p>For the data output, it often makes sense to plan for data in a format that is
appropriate for data analysis and for merging with other types of data collected
from the experiment. The aim of pre-processing is to get the data from the
format in which they were collected into a format that is meaningful for
combining with other types of data from the experiment and using in statistical
hypothesis testing.</p>
<p>In the example pre-processing protocol, we ultimately
output a simple dataset, with one row for each of the original samples. The
first few rows of this output data are:</p>
<pre><code>## # A tibble: 6 × 3
##   group replicate cfu_in_organ
##   &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;
## 1     2 2-A                260
## 2     2 2-B               2500
## 3     2 2-C                  0
## 4     3 3-A                  0
## 5     3 3-B               7500
## 6     3 3-C                  0</code></pre>
<p>For each original sample, an estimate of the CFUs of <em>Mycobacterium
tuberculosis</em> in the full spleen is given (<code>cfu_in_organ</code>). These data
can now be merged with other data collected about each animal in the experiment.
For example, they could be joined with data that provide measures of the
immune cell populations for each animal, to explore if certain immune
cells are associated with bacterial load. They could also be joined with
experimental information and then used in hypothesis testing. For example,
these data could be merged with a table that describes which groups were
controls versus which used a certain vaccine, and then a test could be
conducted exploring evidence that bacterial loads in animals given a
vaccine were lower than in control animals.</p>
<p><strong>Setting up a project directory for the protocol</strong></p>
<p>Once you have decided on the input and output data formats, you will next want
to set up a file directory for storing all the inputs needed in the protocol.
You can include the project files for the protocol in an RStudio Project (see
module 2.6) and post this either publicly or privately on GitHub (see modules
2.9–2.11). This creates a “packet” of everything that a reader needs to use to
recreate what you did—they can download the whole GitHub repository and will
have a nice project directory on their computer with everything they need to try
out the protocol.</p>
<p>Part of the design of the protocol involves deciding on the files that should be
included in this project directory. Figure <a href="experimental-data-preprocessing.html#fig:protocolprojectfiles">3.15</a>
provides an example of the initial files included in the project directory for
the example protocol for this module. The left side of the figure shows the
files that are initially included, while the left side shows the files in the
project after the code in the protocol is run.</p>
<p>Generally, in the project directory you should include a file with the input
example data, in whatever file format you will usually collect this type of
data. You will also include an RMarkdown file where the protocol is written. If
you are planning to cite articles and other references, you can include a BibTeX
file, with the bibliographical information for each source you plan to cite (see module 3.8).
Finally, if you would like to include photographs or graphics, you can include
these image files in the project directory. Often, you might want to group these
together in a subdirectory of the project named something like “figures”.</p>
<p>Once you run the RMarkdown file for the protocol, you will generate additional
files in the project. Two typical files you will generate will be the output
file for the protocol (in the example, this is output to a pdf file). Usually,
the code in the protocol will also result in output data, which is pre-processed
through the protocol code and written into a file to be used in further
analysis.</p>
<div class="figure"><span style="display:block;" id="fig:protocolprojectfiles"></span>
<img src="figures/protocol_project_files.png" alt="Example of files in the project directory for a data pre-processing protocol. On the left are the files initially included in the project directory for the example protocol for this module. These include a file with the input data (cfu\_data.csv), a BibTeX file with bibliographical information for references (example\_bib.bib), the RMarkdown file for the protocol (example\_protocol.Rmd), and a subdirectory with figures to include in the protocol (figures). On the right is shown the directory after the code in the protocol RMarkdown document is run, which creates an output pdf with the protocol (example\_protocol.pdf) as well as the output data (processed\_cfu\_estimates.csv)." width="\textwidth" />
<p class="caption">
Figure 3.15: Example of files in the project directory for a data pre-processing protocol. On the left are the files initially included in the project directory for the example protocol for this module. These include a file with the input data (cfu_data.csv), a BibTeX file with bibliographical information for references (example_bib.bib), the RMarkdown file for the protocol (example_protocol.Rmd), and a subdirectory with figures to include in the protocol (figures). On the right is shown the directory after the code in the protocol RMarkdown document is run, which creates an output pdf with the protocol (example_protocol.pdf) as well as the output data (processed_cfu_estimates.csv).
</p>
</div>
<p><strong>Outlining key tasks in pre-processing the input data.</strong></p>
<p>The next step is to outline the key tasks that are involved in moving from the
data input to the desired data output. For the plating data we are using for our
example, the key tasks to be included in the pre-processing protocol are:</p>
<ol style="list-style-type: decimal">
<li>Read the data into R</li>
<li>Explore the data and perform some quality checks</li>
<li>Identify a “good” dilution for each sample—one at which you have a
countable plate</li>
<li>Estimate the bacterial load in each original sample based on the CFUs counted
at that dilution</li>
<li>Output data with the estimated bacterial load for each sample</li>
</ol>
<p>Once you have this basic design, you can set up the RMarkdown file for the
pre-processing protocol to include a separate section for each task, as well as
an “Overview” section at the beginning to describe the overall protocol, the
data being pre-processed, and the laboratory procedures used to collect those
data. In RMarkdown, you can create first-level section headers by putting the
text for the header on its own line and beginning that line with <code>#</code>, followed
by a space. You should include a blank line before and after the line with this
header text. Figure <a href="experimental-data-preprocessing.html#fig:protocolsections">3.16</a> shows how this is done in the
example protocol for this module, showing how text in the plain text RMarkdown
file for the protocol align with section headers in the final pdf output of the
protocol.</p>
<div class="figure"><span style="display:block;" id="fig:protocolsections"></span>
<img src="figures/protocol_sections.png" alt="Dividing an RMarkdown data pre-processing protocol into sections. This shows an example of creating section headers in a data pre-processing protocol created with RMarkdown, showing section headers in the example pre-procotcol for this module." width="\textwidth" />
<p class="caption">
Figure 3.16: Dividing an RMarkdown data pre-processing protocol into sections. This shows an example of creating section headers in a data pre-processing protocol created with RMarkdown, showing section headers in the example pre-procotcol for this module.
</p>
</div>
<p><strong>Adding code for pre-processing.</strong></p>
<p>For many of these steps, you likely have code—or can start drafting the
code—required for that step.
In RMarkdown, you can test this code as you write it. You insert each piece
of executable code within a special section, separated from the regular
text with special characters, as described in previous modules.</p>
<p>For any pre-processing steps that are straightforward (e.g., calculating the
dilution factor in the example module, which requires only simple mathematical
operations), you can directly write in the code required for the step.
For other pre-processing steps, however, the algorithm may be a bit more
complex. For example, complex algorithms have been developed for steps like
peak identification and alignment that are required when
pre-processing data from a mass spectrometer.</p>
<p>For these more complex tasks, you can start to explore available R packages for
performing the task. There are thousands of packages available that extend the
basic functionality of R, providing code implementations of algorithms in a
variety of scientific fields. Many of the R packages relevant for biological
data—especially high-throughput biological data—are available through a
repository called Bioconductor. These packages are all open-source (so you can
explore their code if you want to) and free. You can use vignettes and package
manuals for Bioconductor packages to identify the different functions you can
use for your pre-processing steps. Once you have identified a function for the
task, you can use the helpfile for the function to see how to use it. This help
documentation will allow you to determine all of the function’s parameters and
the choices you can select for each.</p>
<p>You can add each piece of code in the RMarkdown version of the protocol using
the standard method for RMarkdown (module 3.8). Figure <a href="experimental-data-preprocessing.html#fig:protocolcode">3.17</a>
shows an example from the example protocol for this module. Here, we are using
code to help identify a “good” dilution for counting CFUs for each sample. The
code in included in an executable code chunk, and so it will be run each time
the protocol is rendered. Code comments are included in the code to provide
finer-level details about what the code is doing.</p>
<div class="figure"><span style="display:block;" id="fig:protocolcode"></span>
<img src="figures/protocol_code.png" alt="Example of including code in a data pre-processing protocol created with RMarkdown. This figure shows how code can be included in the RMarkdown file for a pre-processing protocol (right), and the corresponding output in the final pdf of the protocol (left), for the code to identify a 'good' dilution for counting CFUs for each sample. Code comments are included to provide finer-level details on the code." width="\textwidth" />
<p class="caption">
Figure 3.17: Example of including code in a data pre-processing protocol created with RMarkdown. This figure shows how code can be included in the RMarkdown file for a pre-processing protocol (right), and the corresponding output in the final pdf of the protocol (left), for the code to identify a ‘good’ dilution for counting CFUs for each sample. Code comments are included to provide finer-level details on the code.
</p>
</div>
<p>For each step of the protocol, you can also include potential problems
that might come up in specific instances of the data you get from
future experiments. This can help you adapt the code in the protocol in
thoughtful ways as you apply it in the future to new data collected
for new studies and projects.</p>
</div>
<div id="writing-data-pre-processing-protocols" class="section level3 hasAnchor" number="3.9.3">
<h3><span class="header-section-number">3.9.3</span> Writing data pre-processing protocols<a href="experimental-data-preprocessing.html#writing-data-pre-processing-protocols" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now that you have planned out the key components of the pre-processing protocol,
you can use RMarkdown’s functionality to flesh it out into a full pre-processing
protocol. This gives you the chance to move beyond a simple code script, and
instead include more thorough descriptions of what you’re doing at each step and
why you’re doing it. You can also include discussions of potential limitations
of the approach that you are taking in the pre-processing, as well as areas
where other research groups might use a different approach. These details can
help when it is time to write the Methods section for the paper describing your
results from an experiment using these data. They can also help your research
group identify pre-processing choices that might differ from other research
groups, which opens the opportunity to perform sensitivity analyses regarding
these pre-processing choices and ensure that your final conclusions are robust
across multiple reasonable pre-processing approaches.</p>
<p>Protocols are common for wet lab techniques, where they provide a “recipe” that
ensures consistency and reproducibility in those processes. Computational tasks,
including data pre-processing, can also be standardized through the creation and
use of protocol in your research group. While code scripts are becoming more
common as a means of recording data pre-processing steps, they are often not
as clear as a traditional protocol, in particular in terms of providing a
thorough description of what is being done at each step and why it is being
done that way. Data pre-processing protocols can provide these more thorough
descriptions, and by creating them with RMarkdown or with similar types
of “knitted” documents (modules 3.7 and 3.8), you can combine the executable code
used to pre-process the data with extensive documentation. As a further
advantage, the creation of these protocols will ensure that your research
group has thought carefully about each step of the process, rather than
relying on cobbling together bits and pieces of code they’ve found but don’t
fully understand. Just as the creation of a research protocol for a
clinical trial requires a careful consideration of each step of the ultimate
trial <span class="citation">(Al-JunDi and SAkkA 2016)</span>, the creation of data pre-processing protocols ensure
that each step in the process is carefully considered, and so helps to
ensure that each step of this process is conducted as carefully as the
steps taken in designing the experiment as a whole and each wet lab technique
conducted for the experiment.</p>
<p>Placeholder (you should not see this)</p>
<p>A data-preprocessing protocol, in the sense we use it here, is essentially an
annotated recipe for each step in preparing your data from the initial, “raw”
state that is output from the laboratory equipment (or collected by hand) to a
state that is useful for answering important research questions. The exact
implementation of each step is given in code that can be re-used and adapted
with new data of a similar format. However, the code script is often not enough
to helpfully understand, share, and collaborate on the process. Instead, it’s
critical to also include descriptions written by humans and for humans. These
annotations can include descriptions of the code and how certain parameters are
standardized the algorithms in the code. They can also be used to justify
choices, and link them up both with characteristics of the data and equipment
for your experiment as well as with scientific principles that underlie the
choices. Protocols like this are critical to allow you to standardize the
process you use across many samples from one experiment, across different
experiments and projects in your research laboratory, and even across different
research laboratories.</p>
<p>As you begin adding text to your pre-processing protocol, you should keep in
mind these general aims. First, a good protocol provides adequate detail that
another researcher can fully reproduce the procedure <span class="citation">(Al-JunDi and SAkkA 2016)</span>. For a
protocol for a trial or wet lab technique, this means that the protocol should
allow another researcher to reproduce the process and get results that are
<em>comparable</em> to your results <span class="citation">(Al-JunDi and SAkkA 2016)</span>; for a data pre-processing
protocol, the protocol must include adequate details that another researcher,
provided they start with the same data, gets <em>identical</em> results (short of any
pre-processing steps that include some element of sampling or random-number
generation, e.g., Monte Carlo methods). This idea—being able to exactly
re-create the computational results from an earlier project—is referred to as
<strong>computational reproducibility</strong> and is considered a key component in
ensuring that research is fully reproducible.</p>
<p>Placeholder (you should not see this)</p>
<p>By creating the data pre-processing protocol as a knitted document
using a tool like RMarkdown (modules 3.7 and 3.8), you can ensure that the protocol is
computationally reproducible. In an RMarkdown document, you include the code
examples as <em>executable</em> code—this means that the code is run every time you
render the document. You are therefore “checking” your code every time that you
run it. As the last step of your pre-processing protocol, you should output the
copy of the pre-processed data that you will use for any further analysis for
the project. You can use functions in R to output this to a plain text format,
for example a comma-separated delimited file (modules 2.4 and 2.5). Each time you render
the protocol, you will re-write this output file, and so this provides assurance
that the code in your protocol can be used to reproduce your output data (since
that’s how you yourself created that form of the data).</p>
<p>Figure <a href="experimental-data-preprocessing.html#fig:protocoloutput">3.18</a> provides an example from the example protocol
for this module. The RMarkdown file for the protocol includes code to write out
the final, pre-processed data to a comma-separated plain text file called
“processed_cfu_estimates.csv”. This code writes the output file into the same
directory where you’ve saved the RMarkdown file. Each time the RMarkdown file is
rendered to create the pdf version of the protocol, the input data will be
pre-processed from scratch, using the code throughout the protocol, and this
file will be overwritten with the data generated. This guarantees that the code
in the protocol can be used by anyone—you or other researchers—to reproduce
the final data from the protocol, and so guarantees that these data are
computationally reproducible.</p>
<div class="figure"><span style="display:block;" id="fig:protocoloutput"></span>
<img src="figures/protocol_output_data.png" alt="Example of using code in pre-processing protocol to output the final, pre-processed data that will be used in further analysis for the research project. This example comes from the example protocol for this module, showing both the executable code included in the RMarkdown file for the protocol (right) and how this code is included in the final pdf of the protocol. Outputting the pre-processed data into a plain text file as the last step of the protocol helps ensure computational reproducibility for this step of working with experimental data." width="\textwidth" />
<p class="caption">
Figure 3.18: Example of using code in pre-processing protocol to output the final, pre-processed data that will be used in further analysis for the research project. This example comes from the example protocol for this module, showing both the executable code included in the RMarkdown file for the protocol (right) and how this code is included in the final pdf of the protocol. Outputting the pre-processed data into a plain text file as the last step of the protocol helps ensure computational reproducibility for this step of working with experimental data.
</p>
</div>
<p>In your data pre-processing protocol, show the code that you use to implement
this choice and also explain clearly in the text why you made this choice and
what alternatives should be considered if data characteristics are different.
Write this as if you are explaining to a new research group member (or your
future self) how to think about this step in the pre-processing, why you’re
doing it the way your doing it, and what code is used to do it that way. You
should also include references that justify choices when they are
available—include these using BibTeX (module 3.8). By doing this, you will make it much
easier on yourself when you write the Methods section of papers that report on
the data you have pre-processed, as you’ll already have draft information on
your pre-processing methods in your protocol.</p>
<p>Good protocols include not only <em>how</em> (for data pre-processing protocols, this
is the code), but also <em>why</em> each step is taken. This includes explanations that are both higher-level
(i.e., why a larger question is being asked) and also at a fine level, for each
step in the process. A protocol should include some background, the
aims of the work, hypotheses to be tested, materials and methods, methods of
data collection and equipment to analyze samples <span class="citation">(Al-JunDi and SAkkA 2016)</span>.</p>
<p>This step of documentation and explanation is very important to creating a
useful data pre-processing protocol. Yes, the code itself allows someone else to
replicate what you did. However, only those who are very, very familiar with the
software program, including any of the extension packages you include, can
“read” the code directly to understand what it’s doing. Further, even if you
understand the code very well when you create it, it is unlikely that you will
stay at that same level of comprehension in the future, as other tasks and
challenges take over that brain space. Explaining for humans, in text that
augments and accompanies the code, is also important because function names and
parameter names in code often are not easy to decipher. While excellent
programmers can sometimes create functions with clear and transparent names,
easy to translate to determine the task each is doing, this is difficult in
software development and is rare in practice. Human annotations, written by and
for humans, are critical to ensure that the steps will be clear to you and
others in the future when you revisit what was done with this data and what you
plan to do with future data.</p>
<p>The process of writing a protocol in this way forces you to think about each
step in the process, why you do it a certain way (include parameters you choose
for certain functions in a pipeline of code), and include justifications from
the literature for this reasoning. If done well, it should allow you to quickly
and thoroughly write the associated sections of Methods in research reports and
manuscripts and help you answer questions and challenges from reviewers. Writing
the protocol will also help you identify steps for which you are uncertain how
to proceed and what choices to make in customizing an analysis for your research
data. These are areas where you can search more deeply in the literature to
understand implications of certain choices and, if needed, contact the
researchers who developed and maintained associated software packages to get
advice.</p>
<p>For example, the example protocol for this module explains how to pre-process
data collected from counting CFUs after plating serial dilutions of samples. One
of the steps of pre-processing is to identify a dilution for each sample at
which you have a “countable” plate. The protocol includes an explanation of why
it is important to identify the dilution for a countable plate and also gives
the rules that are used to pick a dilution for each sample, before including the
code that implements those rules. This allows the protocol to provide research
group members with the logic behind the pre-processing, so that they can adapt
if needed in future experiments. For example, the count range of CFUs used for
the protocol to find a good dilution is about a quarter of the typically
suggested range for this process, and this is because this experiment plated
each sample on a quarter of a plate, rather than using the full plate. By
explaining this reasoning, in the future the protocol could be adapted when
using a full plate rather than a quarter of a plate for each sample.</p>
<p>One tool in Rmarkdown that is helpful for this process is its built-in
referencing system. In the previous module, we showed how you can include
bibliographical references in an Rmarkdown file. When you write a protocol
within RMarkdown, you can include references in this way to provide background
and support as you explain why you are conducting each step of the
pre-processing. Figure <a href="experimental-data-preprocessing.html#fig:protocolreferences">3.19</a> shows an example of the
elements you use to do this, showing each element in the example protocol for
this module.</p>
<div class="figure"><span style="display:block;" id="fig:protocolreferences"></span>
<img src="figures/protocol_references.png" alt="Including references in a data pre-processing protocol created with RMarkdown. RMarkdown has a built-in referencing system that you can use, based on the BibTeX system for LaTeX. This figure shows examples from the example protocol for this module of the elements used for referencing. You create a BibTeX file with information about each reference, and then use the key for the reference within the text to cite that reference. All cited references will be printed at the end of the document; you can chose the header that you want for this reference section in the RMarkdown file ('References' in this example). In the YAML of the RMarkdown file, you specify the path to the BibTeX file (with the 'bibliography: ' key), so it can be linked in when the RMarkdown file is rendered." width="\textwidth" />
<p class="caption">
Figure 3.19: Including references in a data pre-processing protocol created with RMarkdown. RMarkdown has a built-in referencing system that you can use, based on the BibTeX system for LaTeX. This figure shows examples from the example protocol for this module of the elements used for referencing. You create a BibTeX file with information about each reference, and then use the key for the reference within the text to cite that reference. All cited references will be printed at the end of the document; you can chose the header that you want for this reference section in the RMarkdown file (‘References’ in this example). In the YAML of the RMarkdown file, you specify the path to the BibTeX file (with the ‘bibliography:’ key), so it can be linked in when the RMarkdown file is rendered.
</p>
</div>
<p>Other helpful tools in RMarkdown are tools for creating equations and tables. As
described in the previous module, RMarkdown includes a number of formatting
tools. You can create simple tables through basic formatting, or more complex
tables using add-on packages like <code>kableExtra</code>. Math can be typeset using
conventions developed in the LaTeX mark-up language. The previous module
provided advice and links to resources on using these types of tools. Figure
<a href="experimental-data-preprocessing.html#fig:protocolequations">3.20</a> gives an example of them in use within the example
protocol for this module.</p>
<div class="figure"><span style="display:block;" id="fig:protocolequations"></span>
<img src="figures/protocol_equations_tables.png" alt="Example of including tables and equations in an RMarkdown data pre-processing protocol." width="\textwidth" />
<p class="caption">
Figure 3.20: Example of including tables and equations in an RMarkdown data pre-processing protocol.
</p>
</div>
<p>You can also include figures, either figures created in R or outside figure files.
Any figures that are created by code in the RMarkdown document will automatically
be included in the protocol. For other graphics, you can include image files
(e.g., png and jpeg files) using the <code>include_graphics</code> function from the
<code>knitr</code> package. You can use options in the code chunk options to specify
the size of the figure in the document and to include a figure caption. The
figures will be automatically numbered in the order they appear in the protocol.</p>
<p>Figure <a href="experimental-data-preprocessing.html#fig:protocolfigures">3.21</a> shows an example of how external figure
files were included in the example protocol. In this case, the functionality
allowed us to include an overview graphic that we created in PowerPoint and
saved as an image as well as a photograph taken by a member of our research
group.</p>
<div class="figure"><span style="display:block;" id="fig:protocolfigures"></span>
<img src="figures/protocol_figures.png" alt="Example of including figures from image files in an RMarkdown data pre-processing protocol." width="\textwidth" />
<p class="caption">
Figure 3.21: Example of including figures from image files in an RMarkdown data pre-processing protocol.
</p>
</div>
<p>Finally, you can try out even more complex functionality for RMarkdown as you
continue to build data pre-processing protocols for your research group. Figure
<a href="experimental-data-preprocessing.html#fig:protocolyaml">3.22</a> shows an example of using R code within the YAML of the
example protocol for this module; this allows us to include a “Last edited” date
that is updated with the day’s date each time the protocol is re-rendered.</p>
<div class="figure"><span style="display:block;" id="fig:protocolyaml"></span>
<img src="figures/protocol_yaml_date.png" alt="Example of using more advanced RMarkdown functionality within a data pre-processing protocol. In this example, R code is incorporated into the YAML of the document to include the date that the document was last rendered, marking this on the pdf output as the *Last edited* date of the protocol." width="\textwidth" />
<p class="caption">
Figure 3.22: Example of using more advanced RMarkdown functionality within a data pre-processing protocol. In this example, R code is incorporated into the YAML of the document to include the date that the document was last rendered, marking this on the pdf output as the <em>Last edited</em> date of the protocol.
</p>
</div>
</div>
<div id="applied-exercise-3" class="section level3 hasAnchor" number="3.9.4">
<h3><span class="header-section-number">3.9.4</span> Applied exercise<a href="experimental-data-preprocessing.html#applied-exercise-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To wrap up this module, try downloading both the source file and the output of this example
data pre-processing protocol. Again, you can find the source code (the RMarkdown file)
<a href="https://raw.githubusercontent.com/geanders/improve_repro/master/data/bactcountr_example_data/example_protocol.Rmd">here</a> and the output file <a href="https://github.com/geanders/improve_repro/raw/master/data/bactcountr_example_data/example_protocol.pdf">here</a>. If you would like to try re-running the file, you can get all the additional files
you’ll need (the original data file, figure files, etc.) <a href="https://github.com/geanders/improve_repro/tree/master/data/bactcountr_example_data">here</a>. See if
you can compare the elements of the RMarkdown file with the output they produce in the
PDF file. Read through the descriptions of the protocol. Do you think that you could recreate
the process if your laboratory ran a new experiment that involved plating samples to
estimate bacterial load?</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="experimental-data-recording.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/12-scripted_preprocessing.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["improve_repro.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
