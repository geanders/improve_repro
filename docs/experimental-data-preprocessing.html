<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Experimental Data Preprocessing | Improving the Reproducibility of Experimental Data Recording and Pre-Processing</title>
  <meta name="description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Experimental Data Preprocessing | Improving the Reproducibility of Experimental Data Recording and Pre-Processing" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Experimental Data Preprocessing | Improving the Reproducibility of Experimental Data Recording and Pre-Processing" />
  
  <meta name="twitter:description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing." />
  

<meta name="author" content="Brooke Anderson, Michael Lyons, Mercedes Gonzalez-Juarrero, Marcela Henao-Tamayo, and Gregory Robertson" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="experimental-data-recording.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Visualization in R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Overview</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i><b>1.1</b> License</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html"><i class="fa fa-check"></i><b>2</b> Experimental Data Recording</a><ul>
<li class="chapter" data-level="2.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#separating-data-recording-and-analysis"><i class="fa fa-check"></i><b>2.1</b> Separating data recording and analysis</a><ul>
<li class="chapter" data-level="2.1.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#data-recording-versus-data-analysis"><i class="fa fa-check"></i><b>2.1.1</b> Data recording versus data analysis</a></li>
<li class="chapter" data-level="2.1.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#hazards-of-combining-recording-and-analysis"><i class="fa fa-check"></i><b>2.1.2</b> Hazards of combining recording and analysis</a></li>
<li class="chapter" data-level="2.1.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#approaches-to-separate-recording-and-analysis"><i class="fa fa-check"></i><b>2.1.3</b> Approaches to separate recording and analysis</a></li>
<li class="chapter" data-level="2.1.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#discussion-questions"><i class="fa fa-check"></i><b>2.1.4</b> Discussion questions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#principles-and-power-of-structured-data-formats"><i class="fa fa-check"></i><b>2.2</b> Principles and power of structured data formats</a><ul>
<li class="chapter" data-level="2.2.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#data-recording-standards"><i class="fa fa-check"></i><b>2.2.1</b> Data recording standards</a></li>
<li class="chapter" data-level="2.2.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#defining-data-standards-for-a-research-group"><i class="fa fa-check"></i><b>2.2.2</b> Defining data standards for a research group</a></li>
<li class="chapter" data-level="2.2.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#two-dimensional-structured-data-format"><i class="fa fa-check"></i><b>2.2.3</b> Two-dimensional structured data format</a></li>
<li class="chapter" data-level="2.2.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#saving-two-dimensional-structured-data-in-plain-text-file-formats"><i class="fa fa-check"></i><b>2.2.4</b> Saving two-dimensional structured data in plain text file formats</a></li>
<li class="chapter" data-level="2.2.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#occassions-for-more-complex-data-structures-and-file-formats"><i class="fa fa-check"></i><b>2.2.5</b> Occassions for more complex data structures and file formats</a></li>
<li class="chapter" data-level="2.2.6" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#levels-of-standardizationresearch-group-to-research-community"><i class="fa fa-check"></i><b>2.2.6</b> Levels of standardization—research group to research community</a></li>
<li class="chapter" data-level="2.2.7" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#applied-exercise"><i class="fa fa-check"></i><b>2.2.7</b> Applied exercise</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#the-tidy-data-format"><i class="fa fa-check"></i><b>2.3</b> The ‘tidy’ data format</a><ul>
<li class="chapter" data-level="2.3.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#what-makes-data-tidy"><i class="fa fa-check"></i><b>2.3.1</b> What makes data “tidy”?</a></li>
<li class="chapter" data-level="2.3.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#why-make-your-data-tidy"><i class="fa fa-check"></i><b>2.3.2</b> Why make your data “tidy”?</a></li>
<li class="chapter" data-level="2.3.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#using-tidyverse-tools-with-data-in-the-tidy-data-format"><i class="fa fa-check"></i><b>2.3.3</b> Using tidyverse tools with data in the “tidy data” format</a></li>
<li class="chapter" data-level="2.3.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#practice-quiz"><i class="fa fa-check"></i><b>2.3.4</b> Practice quiz</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#designing-templates-for-tidy-data-collection"><i class="fa fa-check"></i><b>2.4</b> Designing templates for “tidy” data collection</a><ul>
<li class="chapter" data-level="2.4.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#creating-the-rules-for-collecting-data-in-the-same-time-each-time"><i class="fa fa-check"></i><b>2.4.1</b> Creating the rules for collecting data in the same time each time</a></li>
<li class="chapter" data-level="2.4.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-1"><i class="fa fa-check"></i><b>2.4.2</b> Subsection 1</a></li>
<li class="chapter" data-level="2.4.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#dont-repeat-yourself"><i class="fa fa-check"></i><b>2.4.3</b> Don’t Repeat Yourself!</a></li>
<li class="chapter" data-level="2.4.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#dont-repeat-your-report-writing"><i class="fa fa-check"></i><b>2.4.4</b> Don’t repeat your report-writing!</a></li>
<li class="chapter" data-level="2.4.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#automating-reports"><i class="fa fa-check"></i><b>2.4.5</b> Automating reports</a></li>
<li class="chapter" data-level="2.4.6" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#scripts-and-automated-reports-as-simple-pipelines"><i class="fa fa-check"></i><b>2.4.6</b> Scripts and automated reports as simple pipelines</a></li>
<li class="chapter" data-level="2.4.7" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#applied-exercise"><i class="fa fa-check"></i><b>2.4.7</b> Applied exercise</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#example-creating-a-template-for-tidy-data-collection"><i class="fa fa-check"></i><b>2.5</b> Example: Creating a template for “tidy” data collection</a><ul>
<li class="chapter" data-level="2.5.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-1"><i class="fa fa-check"></i><b>2.5.1</b> Subsection 1</a></li>
<li class="chapter" data-level="2.5.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-2"><i class="fa fa-check"></i><b>2.5.2</b> Subsection 2</a></li>
<li class="chapter" data-level="2.5.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#example-datasets"><i class="fa fa-check"></i><b>2.5.3</b> Example datasets</a></li>
<li class="chapter" data-level="2.5.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#issues-with-these-data-sets"><i class="fa fa-check"></i><b>2.5.4</b> Issues with these data sets</a></li>
<li class="chapter" data-level="2.5.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#final-tidy-examples"><i class="fa fa-check"></i><b>2.5.5</b> Final “tidy” examples</a></li>
<li class="chapter" data-level="2.5.6" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#options-for-recording-tidy-data"><i class="fa fa-check"></i><b>2.5.6</b> Options for recording tidy data</a></li>
<li class="chapter" data-level="2.5.7" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#examples-of-how-tidy-data-can-be-easily-analyzed-visualized"><i class="fa fa-check"></i><b>2.5.7</b> Examples of how “tidy” data can be easily analyzed / visualized</a></li>
<li class="chapter" data-level="2.5.8" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#discussion-questions"><i class="fa fa-check"></i><b>2.5.8</b> Discussion questions</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#power-of-using-a-single-structured-project-directory-for-storing-and-tracking-research-project-files"><i class="fa fa-check"></i><b>2.6</b> Power of using a single structured ‘Project’ directory for storing and tracking research project files</a><ul>
<li class="chapter" data-level="2.6.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#organizing-project-files-through-the-file-system"><i class="fa fa-check"></i><b>2.6.1</b> Organizing project files through the file system</a></li>
<li class="chapter" data-level="2.6.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#organizing-files-within-a-project-directory"><i class="fa fa-check"></i><b>2.6.2</b> Organizing files within a project directory</a></li>
<li class="chapter" data-level="2.6.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#using-rstudio-projects-with-project-file-directories"><i class="fa fa-check"></i><b>2.6.3</b> Using RStudio Projects with project file directories</a></li>
<li class="chapter" data-level="2.6.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-1"><i class="fa fa-check"></i><b>2.6.4</b> Subsection 1</a></li>
<li class="chapter" data-level="2.6.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-2"><i class="fa fa-check"></i><b>2.6.5</b> Subsection 2</a></li>
<li class="chapter" data-level="2.6.6" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#practice-quiz"><i class="fa fa-check"></i><b>2.6.6</b> Practice quiz</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#creating-project-templates"><i class="fa fa-check"></i><b>2.7</b> Creating ‘Project’ templates</a><ul>
<li class="chapter" data-level="2.7.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#making-an-existing-file-directory-an-rstudio-project"><i class="fa fa-check"></i><b>2.7.1</b> Making an existing file directory an RStudio Project</a></li>
<li class="chapter" data-level="2.7.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#making-an-rstudio-project-template"><i class="fa fa-check"></i><b>2.7.2</b> Making an RStudio Project Template</a></li>
<li class="chapter" data-level="2.7.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#discussion-questions"><i class="fa fa-check"></i><b>2.7.3</b> Discussion questions</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#example-creating-a-project-template"><i class="fa fa-check"></i><b>2.8</b> Example: Creating a ‘Project’ template</a><ul>
<li class="chapter" data-level="2.8.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-1"><i class="fa fa-check"></i><b>2.8.1</b> Subsection 1</a></li>
<li class="chapter" data-level="2.8.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-2"><i class="fa fa-check"></i><b>2.8.2</b> Subsection 2</a></li>
<li class="chapter" data-level="2.8.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#applied-exercise"><i class="fa fa-check"></i><b>2.8.3</b> Applied exercise</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#harnessing-version-control-for-transparent-data-recording"><i class="fa fa-check"></i><b>2.9</b> Harnessing version control for transparent data recording</a><ul>
<li class="chapter" data-level="2.9.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#what-is-version-control"><i class="fa fa-check"></i><b>2.9.1</b> What is version control?</a></li>
<li class="chapter" data-level="2.9.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#recording-data-in-the-laboratoryfrom-paper-to-computers"><i class="fa fa-check"></i><b>2.9.2</b> Recording data in the laboratory—from paper to computers</a></li>
<li class="chapter" data-level="2.9.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#discussion-questions"><i class="fa fa-check"></i><b>2.9.3</b> Discussion questions</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#enhance-the-reproducibility-of-collaborative-research-with-version-control-platforms"><i class="fa fa-check"></i><b>2.10</b> Enhance the reproducibility of collaborative research with version control platforms</a><ul>
<li class="chapter" data-level="2.10.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#git-and-github-features"><i class="fa fa-check"></i><b>2.10.1</b> git and GitHub features</a></li>
<li class="chapter" data-level="2.10.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-1"><i class="fa fa-check"></i><b>2.10.2</b> Subsection 1</a></li>
<li class="chapter" data-level="2.10.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-2"><i class="fa fa-check"></i><b>2.10.3</b> Subsection 2</a></li>
<li class="chapter" data-level="2.10.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#discussion-questions"><i class="fa fa-check"></i><b>2.10.4</b> Discussion questions</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#using-git-and-gitlab-to-implement-version-control"><i class="fa fa-check"></i><b>2.11</b> Using git and GitLab to implement version control</a><ul>
<li class="chapter" data-level="2.11.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#how-to-use-version-control"><i class="fa fa-check"></i><b>2.11.1</b> How to use version control</a></li>
<li class="chapter" data-level="2.11.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#leveraging-git-and-github-as-a-project-director"><i class="fa fa-check"></i><b>2.11.2</b> Leveraging git and GitHub as a project director</a></li>
<li class="chapter" data-level="2.11.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#leveraging-git-and-github-as-a-scientist-who-programs"><i class="fa fa-check"></i><b>2.11.3</b> Leveraging git and GitHub as a scientist who programs</a></li>
<li class="chapter" data-level="2.11.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#notes"><i class="fa fa-check"></i><b>2.11.4</b> Notes</a></li>
<li class="chapter" data-level="2.11.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#applied-exercise"><i class="fa fa-check"></i><b>2.11.5</b> Applied exercise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html"><i class="fa fa-check"></i><b>3</b> Experimental Data Preprocessing</a><ul>
<li class="chapter" data-level="3.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#principles-and-benefits-of-scripted-pre-processing-of-experimental-data"><i class="fa fa-check"></i><b>3.1</b> Principles and benefits of scripted pre-processing of experimental data</a><ul>
<li class="chapter" data-level="3.1.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#what-is-pre-processing"><i class="fa fa-check"></i><b>3.1.1</b> What is pre-processing?</a></li>
<li class="chapter" data-level="3.1.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#approaches-to-simple-preprocessing-tasks"><i class="fa fa-check"></i><b>3.1.2</b> Approaches to simple preprocessing tasks</a></li>
<li class="chapter" data-level="3.1.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#approaches-to-more-complex-preprocessing-tasks"><i class="fa fa-check"></i><b>3.1.3</b> Approaches to more complex preprocessing tasks</a></li>
<li class="chapter" data-level="3.1.4" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#scripting-preprocessing-tasks"><i class="fa fa-check"></i><b>3.1.4</b> Scripting preprocessing tasks</a></li>
<li class="chapter" data-level="3.1.5" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#potential-quotes"><i class="fa fa-check"></i><b>3.1.5</b> Potential quotes</a></li>
<li class="chapter" data-level="3.1.6" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#discussion-questions"><i class="fa fa-check"></i><b>3.1.6</b> Discussion questions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#introduction-to-scripted-data-pre-processing-in-r"><i class="fa fa-check"></i><b>3.2</b> Introduction to scripted data pre-processing in R</a><ul>
<li class="chapter" data-level="3.2.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-1"><i class="fa fa-check"></i><b>3.2.1</b> Subsection 1</a></li>
<li class="chapter" data-level="3.2.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-2"><i class="fa fa-check"></i><b>3.2.2</b> Subsection 2</a></li>
<li class="chapter" data-level="3.2.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#applied-exercise"><i class="fa fa-check"></i><b>3.2.3</b> Applied exercise</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#simplify-scripted-pre-processing-through-rs-tidyverse-tools"><i class="fa fa-check"></i><b>3.3</b> Simplify scripted pre-processing through R’s ‘tidyverse’ tools</a><ul>
<li class="chapter" data-level="3.3.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#limitations-of-object-oriented-programming"><i class="fa fa-check"></i><b>3.3.1</b> Limitations of object-oriented programming</a></li>
<li class="chapter" data-level="3.3.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#the-tidyverse-approach"><i class="fa fa-check"></i><b>3.3.2</b> The “tidyverse” approach</a></li>
<li class="chapter" data-level="3.3.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#how-to-tidyverse"><i class="fa fa-check"></i><b>3.3.3</b> How to “tidyverse”</a></li>
<li class="chapter" data-level="3.3.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-1"><i class="fa fa-check"></i><b>3.3.4</b> Subsection 1</a></li>
<li class="chapter" data-level="3.3.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-2"><i class="fa fa-check"></i><b>3.3.5</b> Subsection 2</a></li>
<li class="chapter" data-level="3.3.6" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#practice-quiz"><i class="fa fa-check"></i><b>3.3.6</b> Practice quiz</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#complex-data-types-in-experimental-data-pre-processing"><i class="fa fa-check"></i><b>3.4</b> Complex data types in experimental data pre-processing</a><ul>
<li class="chapter" data-level="3.4.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-1"><i class="fa fa-check"></i><b>3.4.1</b> Subsection 1</a></li>
<li class="chapter" data-level="3.4.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-2"><i class="fa fa-check"></i><b>3.4.2</b> Subsection 2</a></li>
<li class="chapter" data-level="3.4.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#subsection-3"><i class="fa fa-check"></i><b>3.4.3</b> Subsection 3</a></li>
<li class="chapter" data-level="3.4.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#practice-quiz"><i class="fa fa-check"></i><b>3.4.4</b> Practice quiz</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#complex-data-types-in-r-and-bioconductor"><i class="fa fa-check"></i><b>3.5</b> Complex data types in R and Bioconductor</a><ul>
<li class="chapter" data-level="3.5.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-1"><i class="fa fa-check"></i><b>3.5.1</b> Subsection 1</a></li>
<li class="chapter" data-level="3.5.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#exploring-and-extracting-data-from-r-list-data-structures"><i class="fa fa-check"></i><b>3.5.2</b> Exploring and extracting data from R list data structures</a></li>
<li class="chapter" data-level="3.5.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#interfacing-between-object-based-and-tidyverse-workflows"><i class="fa fa-check"></i><b>3.5.3</b> Interfacing between object-based and tidyverse workflows</a></li>
<li class="chapter" data-level="3.5.4" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#extras"><i class="fa fa-check"></i><b>3.5.4</b> Extras</a></li>
<li class="chapter" data-level="3.5.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-2"><i class="fa fa-check"></i><b>3.5.5</b> Subsection 2</a></li>
<li class="chapter" data-level="3.5.6" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#applied-exercise"><i class="fa fa-check"></i><b>3.5.6</b> Applied exercise</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#example-converting-from-complex-to-tidy-data-formats"><i class="fa fa-check"></i><b>3.6</b> Example: Converting from complex to ‘tidy’ data formats</a><ul>
<li class="chapter" data-level="3.6.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-1"><i class="fa fa-check"></i><b>3.6.1</b> Subsection 1</a></li>
<li class="chapter" data-level="3.6.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-2"><i class="fa fa-check"></i><b>3.6.2</b> Subsection 2</a></li>
<li class="chapter" data-level="3.6.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#applied-exercise"><i class="fa fa-check"></i><b>3.6.3</b> Applied exercise</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#introduction-to-reproducible-data-pre-processing-protocols"><i class="fa fa-check"></i><b>3.7</b> Introduction to reproducible data pre-processing protocols</a><ul>
<li class="chapter" data-level="3.7.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#introducing-reproducible-data-pre-processing-protocols"><i class="fa fa-check"></i><b>3.7.1</b> Introducing reproducible data pre-processing protocols</a></li>
<li class="chapter" data-level="3.7.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#technique-to-create-reproducible-pre-processing-protocols"><i class="fa fa-check"></i><b>3.7.2</b> Technique to create reproducible pre-processing protocols</a></li>
<li class="chapter" data-level="3.7.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#advantages-of-reproducible-pre-processing-protocols"><i class="fa fa-check"></i><b>3.7.3</b> Advantages of reproducible pre-processing protocols</a></li>
<li class="chapter" data-level="3.7.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-1"><i class="fa fa-check"></i><b>3.7.4</b> Subsection 1</a></li>
<li class="chapter" data-level="3.7.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-2"><i class="fa fa-check"></i><b>3.7.5</b> Subsection 2</a></li>
<li class="chapter" data-level="3.7.6" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#discussion-questions"><i class="fa fa-check"></i><b>3.7.6</b> Discussion questions</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#rmarkdown-for-creating-reproducible-data-pre-processing-protocols"><i class="fa fa-check"></i><b>3.8</b> RMarkdown for creating reproducible data pre-processing protocols</a><ul>
<li class="chapter" data-level="3.8.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-1"><i class="fa fa-check"></i><b>3.8.1</b> Subsection 1</a></li>
<li class="chapter" data-level="3.8.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-2"><i class="fa fa-check"></i><b>3.8.2</b> Subsection 2</a></li>
<li class="chapter" data-level="3.8.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#applied-exercise"><i class="fa fa-check"></i><b>3.8.3</b> Applied exercise</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#example-creating-a-reproducible-data-pre-processing-protocol"><i class="fa fa-check"></i><b>3.9</b> Example: Creating a reproducible data pre-processing protocol</a><ul>
<li class="chapter" data-level="3.9.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-1"><i class="fa fa-check"></i><b>3.9.1</b> Subsection 1</a></li>
<li class="chapter" data-level="3.9.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-2"><i class="fa fa-check"></i><b>3.9.2</b> Subsection 2</a></li>
<li class="chapter" data-level="3.9.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#practice-quiz"><i class="fa fa-check"></i><b>3.9.3</b> Practice quiz</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>4</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Improving the Reproducibility of Experimental Data Recording and Pre-Processing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="experimental-data-preprocessing" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Experimental Data Preprocessing</h1>
<div id="principles-and-benefits-of-scripted-pre-processing-of-experimental-data" class="section level2">
<h2><span class="header-section-number">3.1</span> Principles and benefits of scripted pre-processing of experimental data</h2>
<p>The experimental data collected for biomedical research often requires
pre-processing before it can be analyzed (e.g., gating of flow cytometry data,
feature finding / quantification for mass spectrometry data). Use of
point-and-click software can limit the transparency and reproducibility of this
analysis stage and is time-consuming for repeated tasks. We will explain how
scripted pre-processing, especially using open source software, can improve
transparency and reproducibility.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Define ‘pre-processing’ of experimental data</li>
<li>Describe an open source code script and explain how it can increase
reproducibility of data pre-processing</li>
</ul>
<div id="what-is-pre-processing" class="section level3">
<h3><span class="header-section-number">3.1.1</span> What is pre-processing?</h3>
<p>Some data collected through laboratory experiments is very straightforward and
requires little or no pre-processing before it’s used in analysis. For example,
[example]. Other data may require some minimal pre-processing. For example, if
you plate bacteria from a sample at a variety of dilutions, you might count each
plate and determine a measure of Colony Forming Units from the set of plates
with different dilutions by deciding which dilution provides the clearest count
and then back-calculating based on its dilution to get the total number of
colony-forming units in the original sample.</p>
<p>This step of pre-processing data can become much more complex with data that was
collected using complex equipment, like a flow cytometer or a mass spectrometer.
In these cases, there are often steps required to extract from the machine’s
readings a biologically-relevant measurement. For example, the data output from
a mass spectrometer must be processed to move from measurements of mass and
retention time to estimates of concentrations of different molecules in the
sample. If you want to compare across multiple samples, then the preprocessing
will also involve steps to align the different samples (in terms of …), as
well as to standardize the measurements for each sample, to make the
measurements from the different samples comparable. For data collected from a
flow cytometer, preprocessing may include steps to disentangle the florescence
from different markers to ensure that the read for one marker isn’t inflated by
spillover florescence from a different marker.</p>
</div>
<div id="approaches-to-simple-preprocessing-tasks" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Approaches to simple preprocessing tasks</h3>
<p>There are several approaches for tackling this type of data preprocessing, to
get from the data that you initial observe (or that is measured by a piece of
laboratory equipment) to meaningful biological measurements that can be analyzed
and presented to inform explorations of a scientific hypothesis. While there are
a number of approaches that don’t involve writing code scripts for this
preprocessing, there are some large advantages to scripting preprocessing any
time you are preprocessing experimental data prior to including it in figures or
further analysis. In this section, we’ll describe some common non-scripted
approaches and discuss the advantages that would be brought by instead using a
code script. In the next module, we’ll walk through an example of how scripts
for preprocessing can be created and applied in laboratory research.</p>
<p>In cases where the pre-processing is mathematically straightforward and the
dataset is relatively small, many researchers do the preprocessing by hand in a
laboratory notebook or through an equation or macro embedded in a spreadsheet.
For example, if you have plated samples at different dilutions and are trying to
calculate from these the CFUs in the original sample, this calculation is simple
enough that it could be done by hand. However, there are advantages to instead
writing a code script to do this simple preprocessing.</p>
<p>When you write a script to do a task with data, it is like writing a recipe that
can be applied again and again. By writing a script, you encode the process a
single time, so you can take the time to check and recheck to make sure that
you’ve encoded the process correctly. This helps in avoiding small errors when
you do the preprocessing—if you are punching numbers into a calculator over
and over, it’s easy to mistype a number or forget a step every now and then,
while the code will ensure that the same process is run every time and that it
faithfully uses the numbers saved in the data for each step, rather than relying
on a person correctly entering each number in the calculation.</p>
<p>Scripts can be used across projects, as well, and so they can ensure consistency
in the calculation across projects. If different people do the calculation in
the lab for different projects or experiments, and they are doing the
calculations by hand, they might each do the calculation slightly differently,
even if it’s only in small details like how they report rounded numbers. A
script will do the exact same thing every time it is applied. You can even share
your script with colleagues at other labs, if you want to ensure that your data
preprocessing is comparable for experiments conducted in different research
groups, and many scientific journals will allow supplemental material with
code used for data preprocessing and analysis, or links within the manuscript
to a repository of this code posted online.</p>
<p>There are also gains in efficiency when you use a script. For small
pre-processing steps, these might seem small for each experiment, and certainly
when you first write the script, it will likely take longer to write and test
the script than it would to just do the calculation by hand (even more if
you’re just starting to learn how to write code scripts). However, since the
script can be applied again and again, with very little extra work to apply it
to new data, you’ll save yourself time in the future, and over a lot of
experiments and projects, this can add up. This makes it particularly useful to
write scripts for preprocessing tasks that you find yourself doing again and
again in the lab.</p>
</div>
<div id="approaches-to-more-complex-preprocessing-tasks" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Approaches to more complex preprocessing tasks</h3>
<p>Other preprocessing tasks can be much more complex, particularly those that need
to conduct a number of steps to extract biologically meaningful measurements
from the measurements made by a complex piece of laboratory equipment, as well
as steps to make sure these measurements can be meaningfully compared across
samples.</p>
<p>For these more complex tasks, the equipment manufacturer will often provide
software that can be used for the preprocessing. This software might conduct
some steps using defaults, and others based on the user’s specifications. These
are often provided through “GUIs” (graphical user interfaces), where the user
does a series of point-and-click steps to process the data. In some software,
this series of point-and-click steps is recorded as the user does them, so that
these steps can be “re-run” later or on a different dataset.</p>
<p>For many types of biological data, including output from equipment like flow
cytometers and mass spectrometers, open-source software has been developed
that can be used for this preprocessing. Often, the most cutting edge methods
for data preprocessing are first available through open-source software packages,
if the methods are developed by researchers rather than by the companies, and
often many of the algorithms that are made available through the equipment
manufacturer’s proprietary software are encoded versions of an algorithm
first shared by researchers as open-source software.</p>
<p>It can take a while to develop a code script for preprocessing the raw data from
a piece of complex equipment like a mass spectrometer. However, the process of
developing this script requires a thoughtful consideration of the steps of
preprocessing, and so this is often time well-spent. Again, this initial time
investment will pay off later, as the script can then be efficiently applied to
future data you collect from the equipment, saving you time in pointing and
clicking through the GUI software. Further, it’s easier to teach someone else
how to conduct the preprocessing that you’ve done, and apply it to future
experiments, because the script serves as a recipe.</p>
<p>When you conduct data preprocessing in a script, this also gives you access to
all the other tools in the scripting language. For example, as you work through
preprocessing steps for a dataset, if you are doing it through an R script, you
can use any of the many visualization tools that are available through R. By
contrast, in GUI software, you are restricted to the visualization and other
tools included in that particular set of software, and those software developers
may not have thought of something that you’d like to do. Open-source scripting
languages like R, Python, and Julia include a huge variety of tools, and once
you have loaded your data in any of these platforms, you can use any of these
tools.</p>
<p>If you have developed a script for preprocessing your raw data, it also becomes
much easier to see how changes in choices in preprocessing might influence your
final results. It can be tricky to guess whether your final results are sensitive,
for example, to what choice you make for a particular tranform for part of your
data, or in how you standardize data in one sample to make different samples
easier to compare. If the preprocessing is in a script, then you can test making
these changes and running all preprocessing and analysis scripts, to see if it
makes a difference in the final conclusions. If it does, then it helps you
identify parts of preprocessing that need to be deeply thought through for the
type of data you’re collecting, and you may want to explore the documentation on
that particular step of preprocessing to determine what choice is best for your
data, rather than relying on defaults.</p>
</div>
<div id="scripting-preprocessing-tasks" class="section level3">
<h3><span class="header-section-number">3.1.4</span> Scripting preprocessing tasks</h3>
<p>Code scripts can be developed for any open-source scripting languages, including
Python, R, and Julia. These can be embedded in or called from literate programming
documents, like RMarkdown and Julia, which are described in other modules. The
word “script” is a good one here—it really is as if you are providing the script
for a play. In an interactive mode, you can send requests to run in the programming
language step by step using a console, while in a script you provide the whole list
of all of your “lines” in that conversation, and the programming language will run
them all in order without you needing to interact from the console.</p>
<p>For preprocessing the data, the script will have a few predictible parts. First,
you’ll need to read the data in. There are different functions that can be used
to read in data from different file formats. For example, data that is stored in
an Excel spreadsheet can be loaded into R using functions in a package called
<code>readxl</code>. Data that is stored in a plain-text delimited format (like a csv file)
can be loaded into R using functions in the <code>readr</code> package.</p>
<p>When preprocessing data from complex equipment, you can determine how to read the
data into R by investigating the file type that is output by the equipment.
Fortunately, many types of scientific equipment follow standardized file formats.
This means that open-source developers can develop a single package that can
load data from equipment from multiple manufacturers. For example, flow cytometry
data is often stored in [file format]. Other biological datasets use file
formats that are appropriate for very large datasets and that allow R to work
with parts of the data at a time, without loading the full data in. [netCDF?]
In these cases, the first step in a script might not be to load in all the data,
but rather to provide R with a connection to the larger datafile, so it can
pull in data as it needs it.</p>
<p>Once the data is loaded or linked in the script, the script can proceed through
steps required to preprocess this data. These steps will often depend on the type
of data, especially the methods and equipment used to collect it. For example, for
mass spectrometry data, these steps will include … . For flow cytometry data,
these steps would include … .</p>
<p>The functions for doing these steps will often come from extensions that
different researchers have made for R. Base R is a simpler collection of data
processing and statistics tools, but the open-source framework of R has allowed
users to make and share their own extensions. In R, these are often referred to
as “packages”. Many of these are shared through the Comprehensive R Archive
Network (CRAN), and packages on CRAN can be directly installed using the
<code>install.packages</code> function in R, along with the package’s names. While CRAN
is the common spot for sharing general-purpose packages, there is a specialized
repository that is used for many genomics and other biology-related R packages
called Bioconductor. These packages can also be easily installed through a call
in R, but in this case it requires an installation function from the <code>BiocManager</code>
package. Many of the functions that are useful for preprocessing biological
data from laboratory experiments are available through Bioconductor.</p>
<p>Table [x] includes some of the primary R packages on Bioconductor that can be
used in preprocessing different types of biological data. There are often
multiple choices, developed by different research groups, but this list provides
a starting point of several of the standard choices that you may want to
consider as you start developing code.</p>
<p>Much of the initial preprocessing might use very specific functions that are
tailored to the format that the data takes once it is loaded. Later in the
script, there will often be a transfer to using more general-purpose tools in
that coding language. For example, once data is stored in a “dataframe” format
in R, it can be processed using a powerful set of general purpose tools
collected in a suite of packages called the “tidyverse”. This set of packages
includes functions for filtering to specific subsets of the data, merging
separate datasets, adding new measurements for each observation that are
functions of the initial measurements, summarizing, and visualizing. The
tidyverse suite of R tools is very popular in general R use and is widely
taught, including through numerous free online resources. By moving from
specific tools to these more general tools as soon as possible in the script, a
researcher can focus his or her time in learning these general purpose tools
well, as these can be widely applied across many types of data.</p>
<p>By the end of the script, data will be in a format that has extracted
biologically relevant measurements. Ideally, this data will be in a general
purpose format, like a dataframe, to make it easier to work with using general
purpose tools in the scripting language when the data is used in further data
analysis or to create figures for reports, papers, and presentations. Often, you
will want to save a version of this preprocessed version of the data in your
project files, and so the last step of the script might be to write out the
cleaned data in a file that can be loaded in later scripts for analysis and
visualization. This is especially useful if these data preprocessing steps are
time consuming, as is often the case for the large raw datasets output by
laboratory equipment like flow cytometers and mass spectrometers.</p>
<p>Figure [x] gives an example of a data preprocessing script, highlighting these
different common areas that often show up in these scripts.</p>
<p>[Some data may be incorporated into the preprocessing by downloading it from
databases or other online sources. These data downloads can be automated and
recorded by using scripted code for the download in many cases, as long as the
database or online source offers web services or another API for this type of
scripted data access. In this case, you can incorporate the script in a
RMarkdown document to record the date the data was downloaded, as well as the
code used to download it. R is able to run system calls, and one of these
will provide the current date, so this can be included in an RMarkdown file
to record the date the file is run. Further, there may be a call that can
be made to the online data source’s API that returns the working version of
the database or source, and if so this can also be included in the RMarkdown
code used to access the data.]</p>
<p>RMarkdown files can be used to combine both code and more manual document
(for example, a record of which collaborator provided each type of data file).
While traditionally this more manual documentation was recommended to be
recorded in plain-text README files in a project’s directory and subdirectories
<span class="citation">(Buffalo 2015)</span>, RMarkdown files provide some advantages over
this traditional approach. First, RMarkdown files are themselves in plain
text, and so they offer the advantages of simple plain text documentation
files (e.g., ones never rendered to another format) in terms of being able
to use script-based tools to search them. Further, they can be rendered into
attractive formatted documents that may be easier to share with project
team members who do not code.</p>
<p>[Example of a function: recipe for making a vinaigrette. There will be a
“basic” way that the function can run, which uses its default parameters.
However, you can also specify and customize certain inputs (for example,
using walnut oil instead of olive oil, or adding mustard) to tweak the
recipe in slight ways each time you use it, and to get customized outputs.]</p>
<p>[History of the mouse—enable GUIs, before everything was from the terminal.]</p>
</div>
<div id="potential-quotes" class="section level3">
<h3><span class="header-section-number">3.1.5</span> Potential quotes</h3>
<blockquote>
<p>For bioinformatics, “all too often the software is developed without
thought toward future interoperability with other software products. As a
result, the bioinformatics software landscape is currently characterized
by fragmentation and silos, in which each research group develops and uses
only the tools created within their lab.” <span class="citation">(Barga et al. 2011)</span></p>
</blockquote>
<blockquote>
<p>“The group also noted the lack of agility. Although they may be aware of
a new or better algorithm they cannot easily integrate it into their
analysis pipelines given the lack of standards across both data formats
and tools. It typically requires a complete rewrite of the code in order
to take advntge of a new technique or algorithm, requiring time and often
funding to hire developers.” <span class="citation">(Barga et al. 2011)</span></p>
</blockquote>
<blockquote>
<p>“The benefit of working with a programming language is that you have the code in
a file. This means that you can easily reuse that code. If the code has
parameters it can even be applied to problems that follow a similar pattern.”
<span class="citation">(Janssens 2014)</span></p>
</blockquote>
<blockquote>
<p>“Data exploration in spreadsheet software is typically conducted via menus and
dialog boxes, which leaves no record of the steps taken.” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“One reason Unix developers have been cool toward GUI interfaces is that, in their
designers’ haste to make them ‘user-friendly’ each one often becomes frustratingly
opaque to anyone who has to solve user problems—or, indeed, interact with it anywhere
outside the narrow range predicted by the user-interface designer.” <span class="citation">(Raymond 2003)</span></p>
</blockquote>
<blockquote>
<p>“Many operating systems touted as more ‘modern’ or ‘user friendly’ than Unix achieve their
surface glossiness by locking users and developers into one interface policy, and offer an
application-programming interface that for all its elaborateness is rather narrow and rigid.
On such systems, tasks the designers have anticipated are very easy—but tasks they have
not anticipated are often impossible or at best extremely painful. Unix, on the other hand, has
flexibility in depth. The many ways Unix provides to glue together programs means that components
of its basic toolkit can be combined to produce useful effects that the designers of the individual
toolkit parts never anticipated.” <span class="citation">(Raymond 2003)</span></p>
</blockquote>
<blockquote>
<p>“The good news is that a computer is a general-purpose machine, capable of performing
any computation. Although it only has a few kinds of instructions to work with, it can
do them blazingly fast, and it can largely control its own operation. The bad news is
that it doesn’t do anything itself unless someone tells it what to do, in excruciating
detail. A computer is the ultimate sorcere’s apprentice, able to follow instructions
tirelessly and without error, but requiring painstaking accuracy in the
specification of what to do.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“<em>Software</em> is the general term for sequences of instructions that make a computer
do something useful. It’s ‘soft’ in contrast with ‘hard’ hardware, because it’s
intangible, not easy to put your hands on. Hardware is quite tangible: if you drop
a computer on your foot, you’ll notice. Not true for software.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“Modern system increasingly use general purpose hardware—a processor, some memory,
and connections to the environment—and create specific behaviors by software. The
conventional wisdom is that software is cheaper, more flexible, and easier to change than
hardware is (especially once some device has left the factory).” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“An algorithm is a precise and unambiguous recipe. It’s expressed in terms of a fixed
set of basic operations whose meanings are completely known and specified; it spells out
a sequence of steps using those operations, with all possible situations covered; it’s
guaranteed to stop eventually. On the other hand, a <em>program</em> is the opposite of
abstract—it’s a concrete statement of the steps that a real computer must perform to
accomplish a task. The distinction between an algorithm and a program is like the difference
between a blueprint and a building; one is an idealization and the other is the real thing.”
<span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“One way to view a program is as one or more algorithms expressed in a form that a computer
can process directly. A program has to worry about practical problems like inadequate memory,
limited processor speed, invalid and even malicious input data, faulty hardware, broken
network connections, and (in the background and often exacerbating the other problems)
human frailty. So if an algorithm is an idealized recipe, a program is the instructions for
a cooking robot preparing a month of meals for an army while under enemy attack.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“During the late 1950s and early 1960s, another step was taken towards getting the
computer to do more for programmers, arguably the most important step in the history of
programming. This was the development of ‘high-level’ programming languages that were
independent of any particular CPU architecture. High-level languages make it possible to
express computations in terms that are closer to the way a person might express them.”
<span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“Programming in the real world tends to happen on a large scale. The strategy is similar
to what one might use to write a book or undertake any other big project: figure out what
to do, starting with a broad specification that is broken into smaller and smaller pieces,
then work on the pieces separately, while making sure that they hang together. In programming,
pieces tend to be of a size such that one person can write the precise computational steps
in some programming language. Ensuring that the pieces written by different programmers
work together is challenging, and failing to get this right is a major source of errors.
For instance, NASA’s Mars Climate Orbiter failed in 1999 because the flight system software
used metric units for thrust, but course correction data was entered in English units,
causing an erroneous trajectory that brought the Orbiter too close to the planet’s
surface.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“If you’re going to build a house today, you don’t start by cutting down trees to make
lumber and digging clay to make your own bricks. Instead, you buy prefabricated pieces like
doors, windows, plumbing fixtures, a furnace, and a water heater. House construction is still
a big job, but it’s manageable because you can build on the work of many others and rely
on an infrastructure, indeed an entire industry, that will help. The same is true of
programming. Hardly any significant program is created from nothing. Many components written
by others can be taken off the shelf and used. For instance, if you’re writing a program for
Windows or a Mac, there are libraries of prefabricated menus, buttons, text editors, graphics,
network connections, database access, and so on. Much of the job is understanding the components
and gluing them together in your own way. Of course, many of these components in turn rest on
other simpler and more basic ones, often for several layers. Below that, everything runs on
the operating system, a program that manages the hardware and controls everything that happens.”
<span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“At the simplest level, programming languages provide a mechanism called functions that make
it possible for one programmer to write code that performs a useful a useful task, then package
it in a form that other programmers can use in their programs without having to know how it
works.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“A function has a name and a set of input data values that it needs to do its job; it does
a computation and returns a result to the part of the program that called it. … Functions
make it possible to create a program by building on components that have been created separately
and can be used as necessary by all programmers. A collection of related functions is usually
called a <em>library</em>. … The services that a function library provides are described to programmers
in terms of an <em>Application Programming Interface</em>, or <em>API</em>, which lists the functions, what
they do, how to use them in a program, what input data they require, and what values they
produce. The API might also describe data structures—the organization of data that is passed
back and forth—and various other bits and pieces that all together define what a programmer
has to do to request services and what will be computed as a result. This specification must
be detailed and precise, since in the end the program will be interpreted by a dumb literal
computer, not by a friendly and accomodating human.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“The code that a programmer writes, whether in assembly language or (much more likely) in
a high-level language, is called <em>source code</em>. … Source code is readable by other programmers,
though perhaps with some effort, so it can be studied and adapted, and any innovations or ideas
it contains are visible.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“In early times, most software was developed by companies and most source code was
unavailable, a trade secret of whoever developed it.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“An <em>operating system</em> is the software underpinning that manages the hardware of a
computer and makes it possible to run other programs, which are called <em>applications</em>.
… It’s a clumsy but standard terminology for programs that are more or less self-contained
and focused on a single task.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“Software, like many other things in computing, is organized into layers, analogous to
geological strata, that separate one concern from another. Layering is one of the important
ideas that help programmers to manage complexity.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“I think that it’s important for a well-informed person to know something about
programming, perhaps only that it can be surprisingly difficult to get very simple
programs working properly. There is nothing like doing battle with a computer to teach
this lesson, but also to give people a taste of the wonderful feeling of accomplishment
when a program does work for the first time. It may also be valuable to have enough
programming experience that you are cautious when someone says that programming is easy,
or that there are no errors in a program. If you have trouble making 10 lines of code
work after a day of struggle, you might be legitimately skeptical of someone who claims
that a million-line program will be delivered on time and bug-free.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“Programming languages share certain basic ideas, since they are all notations for spelling
out a computation as a sequence of steps. Every programming language thus will provide ways
to get input data upon which to compute; do arithmetic; store and retrieve intermediate
values as computation proceeds; display results along the way; decide how to proceed on the basis
of previous computations; and save results when the computation is finished. Languages have
<em>syntax</em>, that is, rules that define what is grammatically legal and what is not.
Programming languages are picky on the grammatical side: you have to say it right or there
will be a complaint. Languages also have <em>semantics</em>, that is, a defined meaning for every
construction in the language.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“In programming, a <em>library</em> is a collection of related pieces of code. A library typically
includes the code in compiled form, along with needed source code declarations [for C++].
Libraries can include stand-alone functions, classes, type declarations, or anything else that
can appear in code.” <span class="citation">(Spraul 2012)</span></p>
</blockquote>
<blockquote>
<p>“One way to write R code is simply to enter it interactively at the command line… This
interactivity is beneficial for experimenting with R or for exploring a data set in a casual
manner. … However, interactively typing code at the R command line is a very bad approach from
the perspective of recording and documenting code because the code is lost when R is shut down.
A superior approach in general is to write R code in a file and get R to read the code from the file.” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“The features of R are organized into separate bundles called <em>packages</em>. The standard R
installation includes about 25 of those packages, but many more can be downloaded from CRAN and
installed to expand the things that R can do. … Once a package is installed, it must be
<em>loaded</em> within an R session to make the extra features available. … Of the 25 packages
that are installed by default, nine packages are <em>loaded</em> by default when we start a new
R session; these provide the basic functionality of R. All other packages must be loaded
before the relevant features can be used.” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“The R environment is the software used to run R code.” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“<em>Document your methods and workflows.</em> This should include full command lines (copied
and pasted) that are run through the shell that generate data or intermediate results.
Even if you use the default values in software, be sure to write these values down;
later versions of the program may use different default values. Scripts naturally
document all steps and parameters …, but be sure to document any command-line options
used to run this script. In general, any command that produces results in your work needs
to be documented somewhere.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“<em>Document the version of the software that you ran.</em> This may seem unimportant, but
remember the example from ‘Reproducible Research’ on page 6 where my colleagues and I
traced disagreeing results down to a single piece of software being updated. These
details matter. Good bioinformatices software usually has a command-line option to
return the current version. Software managed with a version control system such as
Git has explicit identifiers to every version, which can be used to document the
precise version you ran… If no version information is available, a release date,
link to the software, and download date will suffice.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“<em>Document when you downloaded data.</em> It’s important to include when the data was downloaded,
as the external data source (such as a website or server) might change in the future. For example,
a script that downloads data directly from a database might produce different results if
rerun after the external database is updated. Consequently, it’s important to document
when data came into your repository.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“All of this [documentation] information is best stored in plain-text README files.
Plain text can easily be read, searched, and edited directly from the command line,
making it the perfect choice for portable and accessible README files. It’s also available
on all computer systems, meaning you can document your steps when working directly on
a server or computer cluster. Plain text also lacks complex formatting, which can create
issues when copying and pasting commands from your documentation back into the command
line.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“The computer is a very flexible and powerful tool, and it is a tool that is ours
to control. Files and documents, especially those in open standard formats, can be
manipulated using a variety of software tools, not just one specific piece of software.
A programming lanuage is a tool that allows us to manipulate data stored in files and
to manipulate data held in RAM in unlimited ways. Even with a basic knowledge of
programming, we can perform a huge variety of data processing tasks.” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“Computer code is the preferred approach to communicating our instructions to the
computer. The approach allows us to be precise and expressive, it provides a complete
record of our actions, and it allows others to replicate our work.” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“Programming in R is carried out, primarily, by manipulating and modifying data structures.
These different transformations are carried out using functions and operators. In R,
virtually every operation is a function call, and though we separate our discussion into
operators and function calls, the distinction is not strong … The R evaluator and
many functions are written in C but most R functions are written in R itself.”
<span class="citation">(Gentleman 2008)</span></p>
</blockquote>
<blockquote>
<p>“Many biologists are first exposed to the R language by following a cookbook-type
approach to conduct a statistical analysis like a t-test or an analysis of
variance (ANOVA). ALthough R excels at these and more complicated statistical
tests, R’s real power is as a data programming lanugage you can use to explore and
understand data in an open-ended, highly interactive, iterative way. Learning R as a
data programming language will give you the freedom to experiment and problem solve
during data analysis—exactly what we need as bioinformaticians.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Popularized by statistician John W. Tukey, EDA is an approach that emphasizes
understanding data (and its limitations) through interactive investigation
rather than explicit statitical modeling. In his 1977 book <em>Exploratory Data
Analysis</em>, Tukey described EDA as ‘detective work’ involved in ‘finding and
revealing the clues’ in data. As Tukey’s quote emphasizes, EDA is much more an approach
to exploring data than using specific statistical methods. In the face of rapidly
changing sequencing technologies, bioinformatics software, and statistical methods,
EDA skills are not only widely applicable and comparatively stable—they’re also
essential to making sure that our analyses are robust to these new data and methods.”
<span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Developing code in R is a back-and-forth between writing code in a rerunnable script
and exploring data interactively in the R interpreter. To be reproducible, all steps
that lead to results you’ll use later must be recorded in the R script that accompanies
your analysis and interactive work. While R can save a history of the commands you’ve
entered in the interpreter during a session (with the command <code>savehistory()</code>),
storing your steps in a well-commented R script makes your life much easier when you
need to backtrack to understand what you did or change your analysis.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“It’s a good idea to avoid referring to specific dataframe rows in your analysis code.
This would produce code fragile to row permutations or new rows that may be generated
by rerunning a previous analysis step. In every case in which you might need to refer
to a specific row, it’s avoidable by using subsetting… Similarly, it’s a good idea
to refer to columns by their column name, <em>not</em> their position. While columns may be
less likely to change across dataset versions than rows, it still happens. Column names
are more specific than positions, and also lead to more readable code.”
<span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“In bioinformatics, we often need to extract data from strings. R has several
functions to manipulate strings that are handy when working with bioinformatics data in
R. Note, however, that for most bioinformatics text-processing tasks, R is <em>not</em>
the preferred language to use for a few reasons. First, R works with all data stored
in memory; many bioinformatics text-processing tasks are best tackled with the
stream-based approaches…, which explicityly avoid loading all data in memory at
once. Second, R’s string processing functions are admittedly a bit clunky compared
to Python’s.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Versions fo R and any R pakcages installed change over time. This can lead to
reproducibility headaches, as the results of your analyses may change with the
changing version of R and R packages. … you should always record the versions
of R and any packages you use for analysis. R actually makes this incrediably
easy to do—just call the <code>sessionInfo()</code> function.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Bioconductor is an open source R software project focused on developing tools
for high-throughput genomics and molecular biology data.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Bioconductor’s pakcage system is a bit different than those on the Comprehensive R
Archive Network (CRAN). Bioconductor packages are released on a set schedule, twice
a year. Each release is coordinated with a version of R, making Bioconductor’s versions
tied to specific R versions. The motivation behind this strict coordination is that it
allows for packages to be thoroughly tested before being released for public use.
Additionally, because there’s considerable code re-use within the Bioconductor project,
this ensures that all package versions within a Bioconductor release are compatible
with one another. For users, the end result is that packages work as expected and
have been rigorously tested before you use it (this is good when your scientific
results depend on software reliability!). If you need the cutting-edge version of a
package for some reason, it’s always possible to work with their development branch.”
<span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“When installing Bioconductor packages, we use the <code>biocLite()</code> function. <code>biocLite()</code>
installs the correct version of a package for your R version (and its corresponding
Bioconductor version).” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“In addition to a careful release cycle that fosters package stability, Bioconductor
also has extensive, excellent documentation. The best, most up-to-date documentation
for each package will always be a Bioconductor [web address]. Each package has a full
reference manual covering all functions and classes included in a package,
as well as one or more in-depth vignettes. Vignettes step through many examples and
common workflows using packages.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
</div>
<div id="discussion-questions" class="section level3">
<h3><span class="header-section-number">3.1.6</span> Discussion questions</h3>

</div>
</div>
<div id="introduction-to-scripted-data-pre-processing-in-r" class="section level2">
<h2><span class="header-section-number">3.2</span> Introduction to scripted data pre-processing in R</h2>
<p>We will show how to implement scripted pre-processing of experimental data
through R scripts. We will demonstrate the difference between interactive coding
and code scripts, using R for examples. We will then demonstrate how to create,
save, and run an R code script for a simple data cleaning task.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Describe what an R code script is and how it differs from interactive
coding in R</li>
<li>Create and save an R script to perform a simple data pre-processing task</li>
<li>Run an R script</li>
<li>List some popular packages in R for pre-processing biomedical data</li>
</ul>
<div id="subsection-1" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Subsection 1</h3>
</div>
<div id="subsection-2" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Subsection 2</h3>
</div>
<div id="applied-exercise" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Applied exercise</h3>

</div>
</div>
<div id="simplify-scripted-pre-processing-through-rs-tidyverse-tools" class="section level2">
<h2><span class="header-section-number">3.3</span> Simplify scripted pre-processing through R’s ‘tidyverse’ tools</h2>
<p>The R programming language now includes a collection of ‘tidyverse’ extension
packages that enable user-friendly yet powerful work with experimental data,
including pre-processing and exploratory visualizations. The principle behind
the ‘tidyverse’ is that a collection of simple, general tools can be joined
together to solve complex problems, as long as a consistent format is used for
the input and output of each tool (the ‘tidy’ data format taught in other
modules). In this module, we will explain why this ‘tidyverse’ system is so
powerful and how it can be leveraged within biomedical research, especially for
reproducibly pre-processing experimental data.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Define R’s ‘tidyverse’ system</li>
<li>Explain how the ‘tidyverse’ collection of packages can be both user-friendly
and powerful in solving many complex tasks with data</li>
<li>Describe the difference between base R and R’s ‘tidyverse’.</li>
</ul>
<div id="limitations-of-object-oriented-programming" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Limitations of object-oriented programming</h3>
<p>In previous sections, we described how the R programming language allows for
object-oriented programming, and how customized objects are often used in
preprocessing for biological data. This is a helpful approach for preprocessing,
because it can handle complexities in biological data at its early stages of
preprocessing, when R must handle complex input formats from equipment like
flow cytometers or mass spectrometers, and data sizes that are often very large.</p>
<p>However, once you have preprocessed your data, it is often possible to work with it
in a smaller, more consistent object type. This will give you a lot of flexibility
and power. While object-oriented approaches can handle complex data, it can be a
little hard to write and work with code that is built on an object oriented approach.
Working with this type of code requires you to keep track of what object type your
data is in at each stage of a code pipeline, as well as which functions can work with
that type of object.</p>
<p>Further, this type of coding, in practice at least, can be a bit inflexible.
Often, specific functions only work with a single or few types of functions. In
theory, object-oriented programming allows for <em>methods</em> that work in customized
ways with different types of objects to apply customized code to that type of
object for similar, common-sense results. For example, there are often <code>summary</code>
and <code>plot</code> methods for most types of objects, and these apply code that is
customized to that object type and output, respectively, summarized information
about the data in the object and a plot of the data in the object. However, when
you want to do more with the object that summarize it or create its default
plot, you often end up needing to move to more customized functions that work
only with a single or few object types. When you get to this point, you find that
you have to remember which functions work with which object type, and you have to
use different functions at different stages of your code pipeline, as your code
changes from one object class to another.</p>
<p>Further, many of these functions input one object type and output a different
one. This evolution of object types for storing data can be difficult to navigate
and keep track of. Different object types store data in different ways, and so
this evolution of data object types for storage can make it tricky to figure out
how to extract and explore data along the pipeline. It makes it hard to write your
own code to explore and visualize the data along the way, as well, and so users
are often restricted to the visualization and analysis functions pre-made and
shared in packages when working with data in complex object types, especially
until the user becomes very comfortable with coding in R.</p>
<p>Overall, what does this all mean? Object-oriented approaches offer real advantages
early in the process of pre-processing biological data, especially complex and
large data output from complex laboratory equipment. However, once this pre-processing
is completed, there is a big advantage in moving the data into a simple format
and then continuing coding, data analysis, and visualization using tools that
work with this simple format. This is the approach taken by a suite of R packages
called the “tidyverse”, as well as extensions that build off the approach that
this suite of tools embraces. This “tidyverse” approach is described in the
next section.</p>
</div>
<div id="the-tidyverse-approach" class="section level3">
<h3><span class="header-section-number">3.3.2</span> The “tidyverse” approach</h3>
<p>The term “elegance” often captures styles and approaches that are beautiful and
functional without unneeded extras or complexity. Engineers and scientists
sometimes use this term to capture approaches that achieve a desired result with
minimal complexity and friction. A coding problem, for example, could be solved
by an average coder with a hundred lines of code that get the job done, but a
very good coder might be able to solve the same problem with five lines of code
that are easy to follow. The second approach would be applauded as the “elegant”
solution. In mathematics, similarly, proofs can be complex and unwieldy, or they
can be simple and elegant—this idea was beautifully captured by the Hungarian
mathematician Paul Erdos, who famously described very elegant mathematical proofs
as being from “The Book”—that is, God’s own version of the proof of the
mathematical idea.</p>
<blockquote>
<p>“Paul Erdos liked to talk about The Book, in which God maintains the perfect
proofs for mathematical theorems, following the dictum of G. H. Hardy that
there is no permanent place for ugly mathematics. Erdos also said that you
need not believe in God but, as a mathematician, you should believe in
The Book.” [Proofs from the Book, Third Edition, Preface]</p>
</blockquote>
<p>The “tidyverse” approach in R is elegant. It is powerful, and gives you immense
flexibility once you’ve gotten the hang of it, but it’s also so straightforward
that the basics can be quickly taught to and applied by beginning coders. It
focuses on keeping data in a simple, standard format called “tidy” dataframes.
By keeping data in this format while working with it, common tools can be applied
that work with the data at any stage of a “tidy” coding pipeline. These tools take
a “tidy” dataframe as their input, and they also output a “tidy” dataframe, with
whatever change the function implements applied. Because each of these “tidyverse”
tools input and output data in the same standard format, they can be strung together
in order you want. By contrast, when functions input and output data in different
object types, they can only be joined in a specified order, because you can only
apply certain functions to certain object types.</p>
<p>Since the “tidyverse” tools can be strung together in any order, they can be
used very flexibly to build up to do interesting tasks. The tidyverse tools
generally each do very small and simple things. For example, one function
(<code>select</code>) just limits the data to a subset of its original columns; another
(<code>mutate</code>) adds or changes values in columns of the dataset, while another
(<code>distinct</code>) limits the dataframe to remove any rows that are duplicates. These
small, simple steps can be combined together in different patterns to add up to
complex operations on the data, while keeping each step very simple and clear.
Since the data stays in a standard and simple object type, it is easy to check
in on your data at any stage, as the common visualization tools for this
approach (from the <code>ggplot2</code> package and its extensions) can be always be
applied to data stored in a tidy dataframe.</p>
<p>The centralizing principal of the tidyverse approach is the format in which data
is stored throughout “tidyverse” coding—the tidy dataframe. We’ve described
this data type, including its rules and advantages, in an earlier module of this
book. Briefly, you can think of this format in two parts. First, there’s the R
object type that the data should be stored in—a basic “dataframe” object. The
dataframe object type is a very basic two-dimensional format for storing data in
R. When you print it out, it will remind you of looking at data in a
spreadsheet. The two dimensions—rows and columns—allow you to include data
for one or more observations, with different values that were measured for each.
For example, if you were conducting a study of children’s BMI and blood sugar,
you might have an observation for each child in the study, and values measured
for each child of height, weight, a blood sugar measure, study ID, and date of
the observation.</p>
<p>The two-dimensional structure of a dataframe keeps the values
measured for each observation lined up with each other, and lets you keep them
aligned as you work with the data. You could also store data for each value as
separate objects, in one-dimensional vectors, which you can visualize as strings
of values of the same data type, like the dates that each observation was made,
or the weight of each study subject. However, when the data is in separate
vectors, it is easy to make coding mistakes, and coding is often less efficient.
If you want to remove one observation, for example, because you find it is a
duplicate, you would need to carefully make sure you remove it correctly from
each vector. When data are stored in a dataframe, you can remove the row for
that observation with one command, and you can be sure that you’ve removed the
value you meant to from each of the measured values.</p>
<p>Sometimes, you’ll see that data in a tidyverse approach are stored in a special
type of dataframe called a “tibble”—this isn’t very different from a
dataframe, and in fact is a special type of dataframe. It’s only differences in
practice are that it has a slightly different <code>print</code> method. The <code>print</code> method
is the method that’s run, by default, when you just type the R object’s name
at the console. A tibble prints more nicely than a basic dataframe. By default,
it will only print the first few lines. By contrast, a dataframe will, by default,
print everything—if you have a lot of data, this can create an overwhelming
amount of output when you just want to check out what the data looks like. The
printout of a tibble will also include some interesting annotations to help you
see what’s in the data, including the dimensions of the full dataframe and the
data type of each column in the data.</p>
<p>The R object class—dataframe, and more specifically, tibble—of the standard
format for data for a tidyverse approach is just the first part of the standard
data format for the tidyverse approach. The second part of the standard format is
how you organize your data in this format. To easily work with tidyverse functions,
you’ll want to make sure that your data is stored within that dataframe following
“tidy” data principals. These are fully described in an earlier module in this
book [which module]. If you use this data format to initially collect your
data, as described in an earlier module, you will find it very easy to read the
data into R and work within the tidyverse approach. When working with larger and
more complex data collected from laboratory equipment, you may find you need to
do some preprocessing of the data using an object-oriented approach before you
can move the data into this tidy format, but at that point, you can continue with
analysis and visualization of your data using a tidyverse approach.</p>
</div>
<div id="how-to-tidyverse" class="section level3">
<h3><span class="header-section-number">3.3.3</span> How to “tidyverse”</h3>
<p>Once data are in the “tidy” data format, you can create a pipeline of code that
uses small tools, each of which does one simple thing, to work with the data.
This work can include cleaning the data, adding values that are functions of the
original values for each observation (e.g., adding a column with BMI based on
values for each observation on height and weight), applying statistical models to
test hypotheses, summarizing data to create tables, and visualizing the data.</p>
<p>The tidyverse approach is now widely taught, both in in-person courses at
universities and through a variety of online resources. One key resource for
learning the tidyverse approach for R is the book <em>R for Data Science</em> by Hadley
Wickham (the primary developer of the tidyverse) and Garrett Grolemund. This
book is available as a print edition through O’Reilly Media. It is also freely
available online at <a href="https://r4ds.had.co.nz/" class="uri">https://r4ds.had.co.nz/</a>. This book is geared to beginners in
R, moving through to get readers to an intermediate stage of coding expertise,
which is a level that will allow most scientific researchers to powerfully
work with their experimental data. The book includes exercises for practicing the
concepts, and a separate online book is available with solutions for the
exercises (<a href="https://jrnold.github.io/r4ds-exercise-solutions/" class="uri">https://jrnold.github.io/r4ds-exercise-solutions/</a>).</p>
<p>[More on other resources for learning the tidyverse.]</p>
<p>Since there are so many excellent resources available—many for free—to learn
how to code in R using the tidyverse approach, we consider it beyond the scope
of these modules to go more deeply into these instructions. However, we do think
it is critical that biological researchers learn how to connect this approach to
the type of coding that is often necessary for pre-processing large and complex
data that is output from laboratory equipment. Through many of the modules in this book,
we provide advice on how to make these connections, so that data from different
sources—including different types of laboratory equipment and hand-recorded data
collected by personnel in the lab, like colony forming units measured from plating
samples—can all be connected in a tidyverse pipeline by recording hand-recorded
data following a tidy format and by pre-processing data with the aim of moving
data toward a tidy dataframe that can be integrated with other “tidy” data for
analysis and visualization.</p>
</div>
<div id="subsection-1" class="section level3">
<h3><span class="header-section-number">3.3.4</span> Subsection 1</h3>
<blockquote>
<p>“There is a now-old trope in the world of programming. It’s called the ‘worse is
better’ debate; it seeks to explain why the Unix operating systems (which include
Mac OS X these days), made up of so many little interchangeable parts, were so much
more successful in the marketplace than LISP systems, which were ideologically pure,
based on a single languagae (again, LISP), which itself was exceptionally simple,
a favorite of ‘serious’ hackers everywhere. It’s too complex to rehash here, but one
of the ideas inherent within ‘worse is better’ is thata systems made up of many
simple pieces that can be roped together, even if those pieces don’t share a consistent
interface, are likely to be more successful than systems that are designed with consistency
in every regard. And it strikes me that this is a fundamental drama of new technologies.
Unix beat out the LISP machines. If you consider mobile handsets, many of which run
descendants of Unit (iOS and Andriod), Unix beat out Windows as well. And HTML5 beat out
all of the various initiatives to create a single unified web. It nods to accessibility:
it doesn’t get in the way of those who want to make something huge and interconnected.
But it doesn’t enforce; it doesn’t seek to change the behavior of page creators in the
same way that such lost standards as XHTML 2.0 (which eremged from the offices of
the World Wide Web Consortium, and then disappeared under the weight of its own
intentions) once did. It’s not a bad place to end up. It means that there is no
single framework, no set of easy rules to lear, no overarching principles that,
once learned, can make the web appear like a golden statue atop a mountain. There
are just components: HTML to get the words on the page, forms to get people to
write in, videos and images to put up pictures, moving or otherwise, and
JavaScript to make everything dance.” <span class="citation">(Ford 2014)</span></p>
</blockquote>
<blockquote>
<p>“One of the fundamental contributions of the Unix system [is] the idea of a <em>pipe</em>.
A pipe is a way to connect the output of one program to the input of another program
without any temporary file; a <em>pipeline</em> is a connection of two or more programs through
pipes. … Any program that reads from a terminal can read from a pipe instead; any program
that writes on the terminal can write to a pipe. … The programs in a pipeline actually
run at the same time, not one after another. This means that the programs in a pipeline
can be interactive; the kernel looks after whatever scheduling and synchronization is needed
to make it all work. As you probably suspect by now, the shell arranges things when you
ask for a pipe; the individual programs are oblivious to the redirection.” <span class="citation">(Kernighan and Pike 1984)</span></p>
</blockquote>
<blockquote>
<p>“Even though the Unix system introduces a number of innovative programs and techniques,
no single program or idea makes it work well. Instead, what makes it effective is an approach
to programming, a philosophy of using the computer. Although that philosophy can’t be written
down in a single sentence, at its heart is the idea that the power of a system comes more from
the relationships among programs than from the programs themselves. Many Unix programs do
quite trivial things in isolation, but, combined with other programs, become general and
useful tools.” <span class="citation">(Kernighan and Pike 1984)</span></p>
</blockquote>
<blockquote>
<p>“What is ‘Unix’? In the narrowest sense, it is a time-sharing operating system <em>kernel</em>:
a program that controls the resources of a computer and allocates them among its users.
It lets users run their programs; it controls the peripheral devices (discs, terminals,
printers, and the like) connected to the machine; and it provides a file system that
manages the long-term storage of information such as programs, data, and documents.
In a broader sense, ‘Unix’ is often taken to include not only the kernel, but also
essential programs like compiles, editors, command languages, programs for copying and
printing files, and so on. Still more broadly, ‘Unix’ may even include programs
develpoed by you or others to be run on your system, such as tools for document
preparation, routines for statistical analysis, and graphics packages.” <span class="citation">(Kernighan and Pike 1984)</span></p>
</blockquote>
</div>
<div id="subsection-2" class="section level3">
<h3><span class="header-section-number">3.3.5</span> Subsection 2</h3>
</div>
<div id="practice-quiz" class="section level3">
<h3><span class="header-section-number">3.3.6</span> Practice quiz</h3>

</div>
</div>
<div id="complex-data-types-in-experimental-data-pre-processing" class="section level2">
<h2><span class="header-section-number">3.4</span> Complex data types in experimental data pre-processing</h2>
<p>Raw data from many biomedical experiments, especially those that use
high-throughput techniques, can be very large and complex. Because of the scale
and complexity of these data, software for pre-processing the data in R often
uses complex, ‘untidy’ data formats. While these formats are necessary for
computational efficiency, they add a critical barrier for researchers wishing to
implement reproducibility tools. In this module, we will explain why use of
complex data formats is often necessary within open source pre-processing
software and outline the hurdles created in reproducibility tool use among
laboratory-based scientists.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Explain why R software for pre-processing biomedical data often stores
data in complex, ‘untidy’ formats</li>
<li>Describe how these complex data formats can create barriers to
laboratory-based researchers seeking to use reproducibility tools for
data pre-processing</li>
</ul>
<div id="subsection-1" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Subsection 1</h3>
<p>When you process data using a programming language, there will be different
structures that you can use to store data as you work with it. In other modules,
we’ve discussed the “tidyverse” approach to processing data in R—this approach
emphasizes the dataframe as a way to store data while you’re working with it.
In fact, its use of this data structure for data storage is one of the defining
features of the “tidyverse” approach.</p>
<p>Data in R can be stored in a variety of other formats, too. When you are working
with biological data—in particular, complex or large data output from laboratory
equipment—there can be advantages to using data structures besides dataframes.
In this section, we’ll discuss some of the complex characteristics of biomedical
data that recommend the use of data structures in R beyond the dataframe. We’ll
also discuss how the use of these other data structures can complicate the use of
“tidyverse” functions and principles that you might learn in beginning R programming
courses and books. In later modules, we’ll discuss how to connect your work in R
to clean and analyze data by performing earlier pre-processing steps using more
complex data structures and then transferring when possible to dataframes for
storing data, to allow you to take advantage of the power and ease of the
“tidyverse” approach as early as possible in your pipeline.</p>
</div>
<div id="subsection-2" class="section level3">
<h3><span class="header-section-number">3.4.2</span> Subsection 2</h3>
<p>There are two main features of biomedical data—in particular, data collected from
laboratory equipment like flow cytometers and mass spectrometers—that make it
useful to use more complex data structures in R in the earlier stages of preprocessing
the data. First, the data are often very large, in some cases so large that it is
difficult [or impossible?] to read them into R. Second, the data might combine
various elements, each with their own natural structures, that you’d like to keep
together as you move through the steps of preprocessing the data.</p>
<p>[Data size]</p>
<p>[Different elements in the data]
Most laboratory equipment can output a raw data file that you can then read into R.
For many types of laboratory equipment, these raw data files follow a strict format.
For example [flow cytometery format…]…</p>
<p>The file formats will often have different pieces of data stored in specific spots.
For example, the equipment might record not only the measurements taken for the
sample, but also information about the setting that were applied to the equipment
while the measurements were taken, the date of the measurements, and other metadata
that may be useful to access when preprocessing the data. Each piece of data may
have different “dimensions”. For example, the measurements might provide one
measurement per metabolite feature or per marker. Some metadata might also be
provided with these dimensions (e.g., metadata about the markers for flow
cytometry data), but other metadata might be provided a single time per sample
or even per experiment—for example, the settings on the equipment when the
sample or samples were run.</p>
<p>When it comes to data structures, dataframes and other two-dimensional data storage
structures (you can visualize these as similar to the format of data in a spreadsheet,
with rows and columns) work well to store data where all data conform to a common
dimension. For example, a dataframe would work well to store the measurements
for each marker in each sample in a flow cytometry experiment. In this case,
each column could store the values for a specific marker and each row could
provide measurements for a sample. In this way, you could read the measurements
for one marker across all samples by reading down a column, or read the measurements
across all markers for one sample by reading across a row.</p>
<p>When you have data that doesn’t conform to these common dimensions [unit of
measurement?] however, a dataframe may work poorly to store the data. For
example, if you have measurements taken at the level of the equipment settings
for the whole experiment, these don’t naturally fit into the dataframe format.
In the “tidyverse” approach, one approach to handling data with different units
of measurement is to store data for each unit of measurement in a different
dataframe and to include identifiers that can be used to link data across the
dataframes. More common, however, in R extensions for preprocessing biomedical
data is to use more complex data structures that can store data with different
units of measurement in different slots within the data structure, and use these
in conjunction with specific functions that are built to work with that specific
data structure, and so know where to find each element within the data
structure.</p>
</div>
<div id="subsection-3" class="section level3">
<h3><span class="header-section-number">3.4.3</span> Subsection 3</h3>
</div>
<div id="practice-quiz" class="section level3">
<h3><span class="header-section-number">3.4.4</span> Practice quiz</h3>

</div>
</div>
<div id="complex-data-types-in-r-and-bioconductor" class="section level2">
<h2><span class="header-section-number">3.5</span> Complex data types in R and Bioconductor</h2>
<p>Many R extension packages for pre-processing experimental data use complex
(rather than ‘tidy’) data formats within their code, and many output data in
complex formats. Very recently, the <em>broom</em> and <em>biobroom</em> R
packages have been developed to extract a ‘tidy’ dataset from a complex data
format. These tools create a clean, simple connection between the complex data
formats often used in pre-processing experimental data and the ‘tidy’ format
required to use the ‘tidyverse’ tools now taught in many introductory R courses.
In this module, we will describe the ‘list’ data structure, the common backbone
for complex data structures in R and provide tips on how to explore and extract
data stored in R in this format, including through the <em>broom</em> and
<em>biobroom</em> packages.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Describe the structure of R’s ‘list’ data format</li>
<li>Take basic steps to explore and extract data stored in R’s complex, list-based
structures</li>
<li>Describe what the <em>broom</em> and <em>biobroom</em> R packages can do</li>
<li>Explain how converting data to a ‘tidy’ format can improve reproducibility</li>
</ul>
<div id="subsection-1" class="section level3">
<h3><span class="header-section-number">3.5.1</span> Subsection 1</h3>
<p>When you are writing scripts in R to work with your code, if you are at a
point in your pipeline when you can use a “tidyverse” approach, then you will
“keep” your data in a dataframe, as your data structure, throughout your
work. However, at earlier stages in your preprocessing, you may need to use
tools that use other data structures. It’s helpful to understand the
basic building blocks of R data structures, so you can find elements of your
data in these other, more customized data structures.</p>
<p>Many R data structures are built on a general structure called a “list”. This
data structure is a useful basic general data structure, because it is
extraordinarily flexible. The list data structure is flexible in two
important ways: it allows you to include data of different <em>types</em> in the
same data structure, and it allows you to include data with different
dimensions—and data stored hierarchically, including various other data
structures—within the list structure. We’ll cover each of these points a bit
more below and describe why they’re helpful in making the list a very good
general purpose data structure.</p>
<p>In R, your data can be stored as different <em>types</em> of data: whole numbers can be
stored as an <em>integer</em> data type, continuous [?] numbers through a few types of
<em>floating</em> data types, character strings as a <em>character</em> data type, and logical
data (which can only take the two values of “TRUE” and “FALSE”) as a <em>logical</em>
data type. More complex data types can be built using these—for example,
there’s a special data type for storing dates that’s based on a combination of
an [integer?] data type, with added information counting the number of days [?]
from a set starting date (called the [Unix epoch?]), January 1, 1970. (This
set-up for storing dates allows them to be printed to look like dates, rather
than numbers, but at the same time allows them to be manipulated through
operations like finding out which date comes earliest in a set, determining the
number of days between two dates, and so on.) R uses these different data types
for several reasons. First, by using different data types, R can improve its
efficiency [?] in storing data. Each piece of data must—as you go deep in the
heart of how the computer works—as a series of binary digits (0s and 1s).
Some types of data can be stored using fewer of these <em>bits</em> (<em>bi</em>nary dig<em>its</em>).
Each measurement of logical data, for example, can be stored in a single bit,
since it only can take one of two values (0 or 1, for FALSE and TRUE, respectively).
For character strings, these can be divided into each character in the string
for storage (for example, “cat” can be stored as “c”, “a”, “t”). There is a set
of characters called the ASCII character set that includes the lowercase and
uppercase of the letters and punctuation sets that you see on a standard
US keyboard [?], and if the character strings only use these characters, they
can be stored in [x] bits per character. For numeric data types, integers can
typically be stores in [x] bits per number, while continuous [?] numbers,
stored in single or double floating point notation [?], are stored in [x]
and [x] bits respectively. When R stores data in specific types, it can be
more memory efficient by packing the types of data that can be stored in less
space (like logical data) into very compact structures.</p>
<p>The second advantage of the list structure in R is that it has enormous
flexibility in terms of storing lots of data in lots of possible places. This
data can have different types and even different substructures. Some data
structures in R are very constrained in what type of data they can store and
what structure they use to store it. For example, one of the “building block”
data structures in R is the vector. This data structure is one dimensional and
can only contain data that have the same data type—you can think of this as a
bead string of values, each of the same type. For example, you could have a
vector that gives a series of names of study sites (each a character string), or
a vector that gives the dates of time points in a study (each a date data type),
or a vector that gives the weights of mice in a study (each a numeric data
type). You cannot, however, have a vector that includes some study site names
and then some dates and then some weights, since these should be in different
data types. Further, you can’t arrange the data in any structure except a
straight, one-dimensional series if you are using a vector. The dataframe
structure provides a bit more flexibility—you can expand into two dimensions,
rather than one, and you can have different data types in different columns of
the dataframe (although each column must itself have a single data type).</p>
<p>The list data structure is much more flexible. It essentially allows you to
create different “slots”, and you can store any type of data in each of these
slots. In each slot you can store any of the other types of data structures in
R—for example, vectors, dataframes, or other lists. You can even store unusual
things like R <em>environments</em> [?] or <em>pointers</em> that give the directions to where
data is stored on the computer without reading the data into R (and so saving
room in the RAM memory, which is used when data is “ready to go” in R, but which
has much more limited space than the mass [?] storage on your computer).</p>
<p>Since you can put a list into the slot of a list, it allows you to create deep,
layered structures of data. For example, you could have one slot in a list where
you store the metadata for your experiment, and this slot might itself be a list
where you store one dataframe with some information about the settings of the
laboratory equipement you used to collect the data, and another dataframe that
provides information about the experimental design variables (e.g., which animal
received which treatment). Another slot in the larger list then might have
experimental measurements, and these might either be in a dataframe or, if the
data are very large, might be represented through pointers to where the data is
stored in memory, rather than having the data included directly in the data
structure.</p>
<p>Given all these advantages of the list data structure, then, why not use it all
the time? While it is a very helpful building block, it turns out that its flexibility
can have some disadvantages in some cases. This flexibility means that you can’s
always assume that certain bits of data are in a certain spot in each instance
of a list in R. Conversely, if you have data stored in a less flexible structure,
you can often rely on certain parts of the data always being in a certain part of
the data structure. In a “tidy” dataframe, for example, you can always assume
that each row represents the measurements for one observation at the unit of
observation for that dataframe, and that each column gives values across all
observations for one particular value that was measured for all the observations.
For example, if you are conducting an experiment with mice, where a certain number
of mice were sacrificed at certain time points, with their weight and the bacteria
load in their lungs measured when the mouse was sacrificed, then you could store
the data in a dataframe, with a row for each mouse, and columns giving the
experimental characteristics for each mouse (e.g., treatment status, time point
when the mouse was sacrificed), the mouse’s weight, and the mouse’s bacteria
load when sacrificed. You could store all of this information in a list, as
well, but the defined, two-dimensional structure of the dataframe makes it much
more clearly defined where all the data goes in the dataframe structure, while
you could order the data in many ways within a list.</p>
<p>There is a big advantage to having stricter standards for what parts of data go
where when it comes to writing functions that can be used across a lot of data.
You can think of this in terms of how cars are set up versus how kitchens are
set up. Cars are very standardized in the “interface” that you get when you sit
down to drive them. The gas and brakes are typically floor pedals, with the gas
to the right of the brake. The steering is almost always provided through a wheel
centered in front of the driver’s torso. The mechanism for shifting gears (e.g.,
forward, reverse) is typically to the right of the steering wheel, while
mecahnisms for features like lights and windshield wipers, are typically to the
left of the steering wheel. Because this interface is so standardized, you can
get into a car you’ve never driven before and typically figure out how to
drive it very quickly. You don’t need a lot of time exploring where everything
is or a lot of directions from someone familiar with the car to figure out where
things are. Think of the last time that you drove a rental car—within five minutes,
at most, you were probably able to orient yourself to figure out where everything
you needed was. This is like a dataframe in R—you can pretty quickly figure out
where everything you might need is stored in the data structure, and people can
write functions to use with these dataframes that work well generally across lots
of people’s data because they can assume that certain pieces of data are in
certain places.</p>
<p>By contrast, think about walking into someone else’s kitchen and orienting yourself
to use that. Kitchen designs do tend to have some general features—most will have
a few common large elements, like a stove somewhere, a refrigerator somewhere,
a pantry somewhere, and storage for pots, pans, and utensils somewhere. However,
there is a lot of flexibility in where each of these are in the kitchen design,
and further flexibility in how things are organized within each of these structures.
If you cook in someone else’s kitchen, it is easy to find yourself disoriented in the
middle of cooking a recipe, where a utensil that you can grab almost without
thinking in your own kitchen requires you to stop and search many places in
someone else’s kitchen. This is like a list in R—there are so many places that
you can store data in a list, and so much flexibility, that you often find yourself
having to dig around to find a certain element in a list data structure that someone
else has created, and you often can’t assume that certain pieces are in certain
places if you are writing your own functions, so it becomes hard to write
functions that are “general purpose” for generic list structures in R.</p>
<p>There is a way that list structures can be used in R in a way that retains some
of their flexibility while also leveraging some of the benefits of
standardization. This is R’s system for creating <em>objects</em>. These object
structures are built on the list data structure, but each object is constrained
to have certain elements of data in certain structures of the data. These
structures cannot be used as easily as dataframes in a “tidyverse” approach,
since the tidyverse tools are built based on the assumption that data is stored
in a tidy dataframe. However, they are used in many of the Bioconductor
approaches that allow powerful tools for the earlier stages in preprocessing
biological data. The types of standards that are imposed in the more specialized
objects include which slots the list can have, the names they have, what order
they’re in (e.g., in a certain object, the metadata about the experiment might
always be stored in the first slot of the list), and what structures and/or data
types the data in each slot should have.</p>
<p>R programmers get a lot of advantages from using these classes because they can
write functions under the assumption that certain pieces of the data will always
be in the same spot for that type of object. There is still flexibility in the
object, in that it can store lots of different types of data, in a variety of
different structures. While this “object oriented” approach in R data structures
does provide great advantages for programmers, and allow them to create powerful
tools for you to use in R, it does make it a little trickier in some cases for
you to explore your data by hand as you work through preprocessing. This is
because there typically are a variety of these object classes that your data
will pass through as you go through different stages of preprocessing, because
different structures are suited to different stages of analysis. Functions often
can only be used for a single class of objects, and so you have to keep track of
which functions pair up with which classes of data. Further, it can be a bit
tricky—at least in comparison to when you have data in a dataframe—to
explore your data by hand, because you have to navigate through different slots
in the object. By contrast, a dataframe always has the same two-dimensional,
rectangular structure, and so it’s very easy to navigate and explore data in
this structure, and there are a large number of functions that are built to be
used with dataframes, providing enormous flexibility in what you can do with
data stored in this structure.</p>
<p>While it is a bit trickier to explore your data when it is stored in a
list—either a general list you created, or one that forms the base for a
specialized class structure through functions from a Bioconductor package—you
can certainly learn how to do this navigation. This is a powerful and critical
tool for you to learn as you learn to preprocess your data in R, as you should
<em>never</em> feel like you data is stored in a “black box” structure, where you can’t
peek in and explore it. You should <em>always</em> feel like you can take a look at any
part of your data at any step in the process of preprocessing, analyzing, and
visualizing it.</p>
<hr />
<blockquote>
<p>“There are four primary types of atomic vectors: logical, integer, double,
and character (which contains strings). Collectively, integer and double
vectors are known as numeric vectors.” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“… the most important family of data types in base R [is] vectors. …
Vectors come in two flavours: atomic vectors and lists. They differ in terms of
their elements’ types: for atomic vectors, all elements must have the
same type; for lists, elements can have different types. …
Each vector can also have <strong>attributes</strong>, which you can think of as a
named list of arbitrary metadata. Two attributes are particularly important.
The <strong>dimension</strong> attribute turns vectors into matrices and arrays and
the <strong>class</strong> attribute powers the S3 object system.” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“A few places in R’s documentation call lists generic vectors to emphasise
their difference from atomic vectors.” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“Some of the most important S3 vectors [are] factors, dates and times,
data frames, and tibbles.” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<p>You can use <code>typeof</code> to determine the data type and <code>is.[x]</code> (<code>is.logical</code>,
<code>is.character</code>, <code>is.double</code>, and <code>is.integer</code>) to test if data has a certain
type <span class="citation">(Wickham 2019)</span>.</p>
<blockquote>
<p>“You may have noticed that the set of atomic vectors does not include a
number of important data structures like matrices, arrays, factors, or
date-times. These types are all built on top of atomic vectors by
adding attributes.” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“Adding a <code>dim</code> attribute to a vector allows it to behave like a 2-dimensional
<strong>matrix</strong> or a multi-dimensional <strong>array</strong>. Matrices and arrays are primarily
mathematical and statistical tools, not programming tools…” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“One of the most important vector attributes is <code>class</code>, which underlies the
S3 object system. Having a class attribute turns an object into a <strong>S3 object</strong>,
which means it will behave differently from a regular vector when passed to
a <strong>generic</strong> function. Every S3 object is built on top of a base type, and often
stores additional information in other attributes.” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“Lists are a step up in complexity from atomic vectors: each element can be
any type, not just vectors.” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“Lists are sometimes called <strong>recursive</strong> vectors because a list can contain
other lists. This makes them fundamentally different from atomic vectors.”
<span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“The two most important S3 vectors built on top of lists are data frames and
tibbles. If you do data analysis in R, you’re going to be using data frames.
A data frame is a named list of vectors with attributes for (column) <code>names</code>,
<code>row.names</code>, and its class,”data.frame“… In contrast to a regular list, a
data frame has an additional constraint: the length of each of its vectors must
be the same. This gives data frames their rectangular structure…” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“Data frames are one of the biggest and most important ideas in R, and one of the
things that makes R different from other programming languages. However, in the
over 20 years since their creation, the ways that people use R have changed, and
some of the design decisions that made sense at the time data frames were created
now cause frustration. This frustration led to the creation of the tibble
[Muller and Wickham, 2018], a modern reimagining of the data frame. Tibbles are
meant to be (as much as possible) drop-in replacements for data frames that fix
those frustrations. A concise, and fun, way to summarise the main differences is
that tibbles are lazy and surly: they do less and complain more.” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“Tibbles are provided by the tibble package and share the same structure as data
frames. The only difference is that the class vector is longer, and includes <code>tbl_df</code>.
This allows tibbles to behave differentlyin [several] key ways. … tibbles never
coerce their input (this is one feature that makes them lazy)… Additionally, while
data frames automatically transform non-syntactic names (unless <code>check.names = FALSE</code>),
tibbles do not… While every element of a data frame (or tibble) must have the
same length, both <code>data.frame()</code> and <code>tibble()</code> will recycle shorter inputs. However,
while data frames automatically recycle columns that are an integer multiple of the
longest column, tibbles will only recycle vectors of length one. … There is one
final different: <code>tibble()</code> allows you to refer to variables created during
construction. … [Unlike data frames,] tibbles do not support row names. … One
of the most obvious differences between tibbles and data frames is how they
print… Tibbles tweak [a data frame’s subsetting] behaviours so that a <code>[</code> always
returns a tibble, and a <code>$</code> doesn’t do partial matching and warns if it can’t find
a variable (this is what makes tibbles surly). … List columns are easier to use
with tibbles because they can be directly included inside <code>tibble()</code> and they
will be printed tidily.” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“Since the elements of lists are references to values, the size of a list might
be much smaller than you expect.” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“[The] behavior [of environments] is different from that of other objects:
environments are always modified in place. This property is sometimes described
as <strong>reference semantics</strong> because when you modify an environment all existing
bindings to that environment continue to have the same reference. … This
basic idea can be used to create functions that ‘remember’ their previous
state… This property is also used to implement the R6 object-oriented
programming system…” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
</div>
<div id="exploring-and-extracting-data-from-r-list-data-structures" class="section level3">
<h3><span class="header-section-number">3.5.2</span> Exploring and extracting data from R list data structures</h3>
<p>To feel comfortable exploring your data at any stage during the preprocessing
steps, you should learn how to investigate and explore data that’s stored
in a list structure in R. Because the list structure is the building block
for complex data structures, including Bioconductor class structures, this will
serve you well throughout your work. You should get in the habit of checking
the structure and navigating where each piece of data is stored in the data
structure at each step in preprocessing your data. Also, by checking your data
throughout preprocessing, you might find that there are bits of information
tucked in your data at early stages that you aren’t yet using. For example,
many file formats for laboratory equipment include slots for information about
the equipment and its settings during when running the sample. This information
might be read in from the file into R, but you might not know it’s there for you
to use if you’d like, to help you in creating reproducible reports that include
this metadata about the experimental equipment and settings.</p>
<p>First, you will want to figure out whether your data is stored in a generic
list, or if it’s stored in a specific class-based data structure, which means it
will have a bit more of a standardized structure. To do this, you can run the
<code>class</code> function on your data object. The output of this might be a single value
(e.g., “list” [?]) or a short list. If it’s a short list, it will include both
the specific class of the object and, as you go down the list, the more
general data structure types that this class is built on. For example, if the
<code>class</code> function returns this list:</p>
<pre><code>[Example list of data types---maybe some specific class, then &quot;list&quot;?]</code></pre>
<p>it means that the data’s in a class-based structure called … which is built on
the more general structure of a list. You can apply to this data any of the functions
that are specifically built for … data structures, but you can also apply
functions built for the more general list data structure.</p>
<p>There are several tools you can use to explore data structured as lists in R.
R lists can sometimes be very large—in terms of the amount of data stored in
them—particularly for some types of biomedical data. With some of the tools
covered in this subsection, that will mean that your first look might seem
overwhelming. We’ll also cover some tools, therefore, that will let you peel
away levels of the data in a bit more manageable way, which you can use when
you encounter list-structured data that at first feels overwhelming.</p>
<p>First, if your data is stored in a specific class-based data structure, there
likely will also be help files specifically for the class structure that can
help you navigate it and figure out where things are. [Example]</p>
<p>[More about exploring data in list structures.]</p>
<hr />
<blockquote>
<p>“Use <code>[</code> to select any number of elements from a vector. … <strong>Positive
integers</strong> return elements at the specified positions. … <strong>Negative
integers</strong> exclude elements at the specified positions…
<strong>Logical vectors</strong> select elements where the corresponding logical vector
is <code>TRUE</code>. This is probably the most useful type of subsetting
because you can write an expression that uses a logical vector… If the
vector is named, you can also use <strong>character vectors</strong> to return elements
with matching names.” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“Subsetting a list works in the same way as subsetting an atomic vector.
Using <code>[</code> always returns a list; <code>[[</code> and <code>$</code> … lets you pull out
elements of a list.” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“<code>[[</code> is most important when working with lists because subsetting a list
with <code>[</code> always returns a smaller list. … Because <code>[[</code> can return only
a single item, you must use it with either a single positive integer or
a single string.” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“<code>$</code> is a shorthand operator: <code>x$y</code> is roughly equivalent to <code>x[["y"]]</code>.
It’s often used to access variables in a data frame… The one important
difference between <code>$</code> and <code>[[</code> is that <code>$</code> does (left-to-right) partial
matching [which you likely want to avoid to be safe].” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“There are two additional subsetting operators, which are needed for
S4 objects: <code>@</code> (equivalent to <code>$</code>), and <code>slot()</code> (equivalent to <code>[[</code>).”
<span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“The environment is the data structure that powers scoping. … Understanding
environments is not necessary for day-to-day use of R. But they are important to
understand because they power many important features like lexical scoping,
name spaces, and R6 classes, and interact with evaluation to give you powerful
tools for making domain specific languages, like <code>dplyr</code> and <code>ggplot2</code>.”
<span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“The job of an environment is to associate, of <strong>bind</strong>, a set of names to a
set of values. You can think of an environment as a bag of names, with no
implied order (i.e., it doesn’t make sense to ask which is the first element
in an environment).” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“… environments have reference semantics: unlike most R objects, when you
modify them, you modify them in place, and don’t create a copy.” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“As well as powering scoping, environments are also useful data structures in
their own right because they have reference semantics. There are three common
problems that they can help solve: <strong>Avoiding copies of large data.</strong> Since
environments have reference semantics, you’ll never accidentally create a copy.
But bare environments are painful to work with, so I instead recommend using R6
objects, which are built on top of environments. …” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“Generally in R, functional programming is much more important than object-oriented
programming, because you typically solve complex problems by decomposing them
into simple functions, not simple objects. Nevertheless, there are important reasons
to learn each of the three [object-oriented programming] systems [S3, R6, and S4]:
S3 allows your functions to return rich results with user-friendly display and
programmer-friendly internals. S3 is used throughout base R, so it’s important to
master if you want to extend base R functions to work with new types of input.
R6 provides a standardised way to escape R’s copy-on-modify semantics. This is
particularly important if you want to model objects that exist independently
of R. Today, a common need for R6 is to model data that comes from a web API,
and where changes come from inside or outside R. S4 is a rigorous system that
forces you to thing carefully about program design. It’s particularly well-suited
for building large systems that evolve over time and will receive contributions
from many programmers. This is why it’s used by the Bioconductor project, so another
reason to learn S4 is to equip you to contribute to that project.” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“The main reason to use OOP is <strong>polymorphism</strong> (literally: many shapes).
Polymorphism means that a developer can consider a function’s interface
separately from its implementation, making it possible to use the same function
form for different types of input. This is closely related to the idea of
<strong>encapsulation:</strong> the user doesn’t need to worry about details of an object
because they are encapsulated behind a standard interface. … To be more precise,
<strong>OO</strong> systems call the type of an object its <strong>class</strong>, and an implementation for
a specific class is called a <strong>method</strong>. Roughly speaking, a class defines what an
object <em>is</em> and methods describe what that object can <em>do</em>. The class defines
the <strong>fields</strong>, the data possessed by every instance of that class. Classes
are organised in a hierarchy so that if a method does not exist for one
class, its parent’s method is used, and the child is said to <strong>inherit</strong> behaviour.
… The process of finding the correct method given a class is called
<strong>method dispatch</strong>.” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“There are two main paradigms of object-oriented programming which differ in how
methods and classes are related. In this book, we’ll borrow the terminology of
<em>Extending R</em> [Chambers 2016] and call these paradigms encapsulated and
functional: In <strong>encapsulated</strong> OOP, methods belong to objects or classes, and
method calls typically look like <code>object.method(arg1, arg2)</code>. This is called
encapsulated because the object encapsulates both data (with fields) and
behaviour (with methods), and is the paradigm found in most popular languages.
In <strong>functional</strong> OOP, methods belong to <strong>generic</strong> functions, and method calls
look like ordinary function calls: <code>generic(object, arg2, arg3)</code>. This is called
functional because from the outside it looks like a regular function call, and
internally the components are also functions.” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“<strong>S3</strong> is R’s first OOP system… S3 is an informal implementation of functional
OOP and relies on common conventions rather than ironclad guarantees.
This makes it easy to get started with, providing a low cost way of solving many
simple problems. … <strong>S4</strong> is a formal and rigorous rewrite of S3…
It requires more upfront work than S3, but in return provides more guarantees and greater
encapsulation. S4 is implemented in the base <strong>methods</strong> package, which is
always installed with R.” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“While everything <em>is</em> an object, not everything is object-oriented. This confusion
arises because the base objects come from S, and were developed before anyone
thought that S might need an OOP system. The tools and nomenclature evolved
organically over many years without a single guiding principle. Most of the time,
the distinction between objects and object-oriented objects is not important. But
here we need to get into the nitty gritty details so we’ll use the terms
<strong>base objects</strong> and <strong>OO objects</strong> to distinguish them. … Techinally, the difference
between base and OO objects is that OO objects have a ‘class attribute’.”
<span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“An S3 object is a base type with at least a <code>class</code> attribute (other attributes
may be used to store other data). … An S3 object behaves differently from its
underlying base type whenever it’s passed to a <strong>generic</strong> (short for generic
function). … A generic function defines an interface, which uses a different
implementation depending on the class of an argument (almost always the first
argument). Many base R functions are generic, including the important <code>print</code>…
The generic is a middleman: its job is to define the interface (i.e., the arguments)
then find the right implements for the job. The implementation for a specific class
is called a <strong>method</strong>, and the generic finds the method by performing <strong>method
dispatch</strong>.” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“If you have done object-oriented programming in other languages, you may be
surprised to learn that S3 has no formal definition of a class: to make an object
an instance of a class, you simply set the <strong>class attribute</strong>. … You can determine
the class of an S3 object with <code>class(x)</code>, and see if an object is an instance of
a class using <code>inherits(x, "classname")</code>.” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“The job of an S3 generic is to perform method dispatch, i.e., find the specific
implementation for a class.” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“An important new componenet of S4 is the <strong>slot</strong>, a named component of the
object that is accessed using the specialised subsetting operator <code>@</code> (pronounced
‘at’). The set of slots, and their classes, forms an important part of the
definition of an S4 class.” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“Given an S4 object you can see its class with <code>is()</code> and access slots with <code>@</code>
(equivalent to <code>$</code>) and <code>slot()</code> (equivalent to <code>[[</code>) … Generally, you should only
use <code>@</code> in your methods. If you’re working with someone else’s class, look for
<strong>accessor</strong> functions that allow you to safely set and get slot values. … Accessors
are typically S4 generics allowing multiple classes to share the same external
interface.” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“If you’re using an S4 class defined in a package, you can get help on it
with <code>class?Person</code>. To get help for a method, put <code>?</code> in front of a call (e.g.,
<code>?age(john)</code>) and <code>?</code> will use the class of the arguments to figure out which help
file you need.” <span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“Slots [in S4 objects] should be considered an internal implementation detail:
they can change without warning and user code should avoid accessing them
directly. Instead, all user-accessible slots should be accompanied by a pair
of <strong>accessors</strong>. If the slot is unique to the class, this can just be a function…
Typically, however, you’ll define a generic so that multiple classes can used
the same interface” <span class="citation">(<span class="citeproc-not-found" data-reference-id="wickham2019advancedr"><strong>???</strong></span>)</span></p>
</blockquote>
<blockquote>
<p>“The strictness and formality of S4 make it well suited for large teams. Since more
structure is provided by the system itself, there is less need for convention, and new
contributors don’t need as much training. S4 tends to require more upfront design
than S3, and this investment is more likely to pay off on larger projects where
greater resources are available. One large team where S4 is used to good effect is
Bioconductor. Bioconductor is similar to CRAN: it’s a way of sharing packages
amongst a wider audient. Bioconductor is smaller than CRAN (~1,300 versus ~10,000 packages,
July 2017) and the packages tend to be more tightly integrated because of the
shared domain and because Bioconductor has a stricter review process. Bioconductor
packages are not required to use S4, but most will because the key data structures
(e.g., Summarized Experiment, IRanges, DNAStringSet) are built using S4.”
<span class="citation">(Wickham 2019)</span></p>
</blockquote>
<blockquote>
<p>“The biggest challenge to using S4 is the combination of increased complexity and
absence of a single source of documentation. S4 is a complex system and it can be
challenging to use effectively in practice. This wouldn’t be such a problem if
S4 wasn’t scattered through R documentation, books, and websites. S4 needs a
book length treatment, but that book does not (yet) exist. (The documentation for
S3 is no better, but the lack is less painful because S3 is much simpler.)”
<span class="citation">(Wickham 2019)</span></p>
</blockquote>
</div>
<div id="interfacing-between-object-based-and-tidyverse-workflows" class="section level3">
<h3><span class="header-section-number">3.5.3</span> Interfacing between object-based and tidyverse workflows</h3>
<p>The tidyverse approach in R is based on keeping data in a dataframe structure.
By keeping this common structure, the tidyverse allows for straightforward but
powerful work with your data by chaining together simple, single-purpose
functions. This approach is widely covered in introductory R programming courses
and books. A great starting point is the book <em>R Programming for Data Science</em>,
which is available both in print and freely online at [site]. Many excellent
resources exist for learning this approach, and so we won’t recover that
information here. Instead, we will focus on how to interface between this
approach and the object-based approach that’s more common with Bioconductor
packages. Bioconductor packages often take an object-based approach, and with
good reason because of the complexity and size of many early versions of
biomedical data in the preprocessing process. There are also resources for
learning to use specific Bioconductor packages, as well as some general
resources on Bioconductor, like <em>R Programming for Bioinformatics</em> [ref].
However, there are fewer resources available online that teach how to coordinate
between these two approaches in a pipeline of code, so that you can leverage the
needed power of Bioconductor approaches early in your pipeline, as you
preprocess large and complex data, and then shift to use a tidyverse approach
once your data is amenible to this more straightforward approach to analysis and
visualization.</p>
<p>The heart of making this shift is learning how to convert data, when possible,
from a more complex, class-type data structure (built on the flexible list
data structure) to the simpler, more standardized two-dimensional dataframe
structure that is required for the tidyverse approach. In this subsection, we’ll
cover approaches for converting your data from Bioconductor data structures to
dataframes.</p>
<p>If you are lucky, this might be very straightforward. A pair of packages called
<code>broom</code> and <code>biobroom</code> have been created specifically to facilitate the conversion
of data from more complex structures to dataframes. The <code>broom</code> package was
created first, by David Robinson, to convert the data stored in the objects that
are created by fitting statistical models into tidy dataframes. Many of the functions
in R that run statistical tests or fit statistical models output results in a
more complex, list-based data structure. These structures have nice “print” methods,
so if fitting the model or running the test is the very last step of your pipeline,
you can just read the printed output from R. However, often you want to include
these results in further code—for example, creating plots or tables that show
results from several statistical tests or models. The <code>broom</code> package includes
several functions for pulling out different bits of data that are stored in the
complex data structure created by fitting the model or running the test and convert
those pieces of data into a tidy dataframe. This tidy dataframe can then be
easily used in further code using a tidyverse approach.</p>
<p>The <code>biobroom</code> package was created to meet a similar need with data stored in some
of the complex structures commonly used in Bioconductor packages. [More about
<code>biobroom</code>.]</p>
<p>[How to convert data if there isn’t a <code>biobroom</code> method.] If you are unlucky,
there may not be a <code>broom</code> or <code>biobroom</code> method that you can use for the particular
class-based data structure that your data’s in, or it might be in a more general
list, rather than a specific class with a <code>biobroom</code> method. In this case, you’ll
need to extract the data “by hand” to move it into a dataframe once your data is
simple enough to work with using a tidyverse approach. If you’ve mastered how to
explore data stored in a list (covered in the last subsection), you’ll have a headstart
on how to do this. Once you know where to find each element of the data in the
structure of the list, you can assign these specific pieces to their own R objects
using typical R assignment (e.g., with the <em>gets arrow</em>, <code>&lt;-</code>, or with <code>=</code>, depending
on your preferred R programming style). …</p>
</div>
<div id="extras" class="section level3">
<h3><span class="header-section-number">3.5.4</span> Extras</h3>
<p>[Comparison of complexity of biological systems versus complexity of code and
algorithms for data pre-processing—for the later, nothing is unknowable or even
unknown. Someone somewhere is guaranteed to know exactly how it works, what it’s
doing, and why. By contrast, with biological systems, there are still things
that noone anywhere completely understands. It’s helpful to remember that all
code and algorithms for data pre-processing is knowable, and that the details
are all there if and when you want to dig to figure out what’s going on.]</p>
<p>[There are ways to fully package up and save the computer environment used to
run a pipeline of pre-processing and analysis, including any system settings,
all different software used in analysis steps, and so on. Some of the approaches
that are being explored for this include the use of “containers”, including
Docker containers. This does allow, typically, for full reproducibility of
the workflow. However, this approach isn’t very proactive in emphasizing the
robustness of a workflow or its comprehensibility to others—instead, it
makes the workflow reproducible by putting everything in a black box that must
be carefully unpackaged and explored if someone wants to understand or adapt
the pipeline.]</p>
<blockquote>
<p>“Object-oriented design doesn’t have to be over-complicated design, but we’ve
observed that too often it is. Too many OO designs are spaghetti-like tangles of
is-a and has-a relationships, or feature thick layers of glue in which many of the
objects seem to exist simply to hold places in a steep-sided pyramid of abstractions.
Such designs are the opposite of transparent; they are (notoriously) opaque and
difficult to debug.” <span class="citation">(Raymond 2003)</span></p>
</blockquote>
<blockquote>
<p>“Unix programmers are the original zealots about modularity, but tend to go about it
in a quiter way [that with OOP]. Keeping glue layers thin is part of it; more generally,
our tradition teaches us to build lower, hugging the ground with algorithms and structures
that are designed to be simple and transparent.” <span class="citation">(Raymond 2003)</span></p>
</blockquote>
<blockquote>
<p>“A <em>standard</em> is a precise and detailed description of how some artifact is built
or is supposed to work. Examples of software standards include programming
languages (the definition of syntax and semantics), data formats (how information is
represented), algorithmic processing (the steps necessary to do a computation), and
the like. Some standards, like the Word <code>.doc</code> file format, are <em>de facto</em> standards—they
have no official standing but everyoen uses them. The word ‘standard’ is best reserved for
formal descriptions, often developed and maintained by a quasi-neutral party like a
government or a consortium, that define how something is built or operated. The definition
is sufficiently complete and precise that separate entities can interact or provide independent
implementations. We benefit from hardware standards all the time, though we may not notice
how many there are. If I buy a new television set, I can plug it inot the electrical outlets
in my home thanks to standards for the size and shape of plugs and the voltage they provide.
The set itself will receive signals and display pictures because of standards for broadcast
and cable television. I can plug other devices into it through standard cables and connectors
like HDMI, USB, S-video and so on. But every TV needs its own remote control and every cell
phone needs a different charger because those have not been standardized. Computing has plenty
of standards as well, including character sets like ASCII and Unicode, programming languages
like C and C++, algorithms for encryption and compression, and protocols for exchanging
information over networks.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“Standards are important. They make it possible for independently created things to cooperate,
and they open an area to competition from multiple suppliers, while proprietary systems tend
to lock everyone in. … Standards have disadvantages, too—a standard can impede progress if
it is inferior or outdated yet everyone is forced to use it. But these are modest drawbacks
compared to the advantages.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“A <em>class</em> is a blueprint for constructing a particular package of code and data; each
variable created according to a class’s blueprint is known as an <em>object</em> of that class.
Code outside of a class that creates and uses an object of that class is known as a <em>client</em>
of the class. A <em>class declaration</em> names the class and lists all of the <em>members</em>, or
items inside that class. Each item is either a <em>data member</em>—a variable declared within the
class—or a <em>method</em> (also known as a <em>member function</em>), which is a function declared within
the class. Member functions can include a special type called a <em>constructor</em>, which has the
same name as the class and is invoked implicitly when an object of the class is declared.
In addition to the normal attributes of a variable or function declaration (such as type, and
for functions, the parameter list), each member has an <em>access specifier</em>, which indicates
what functions can access the member. A <em>public member</em> can be accessed by any code using the
object: code inside the class, a client of the class, or code in a <em>subclass</em>, which is a class
that ‘inherits’ all the code and data of an existing class. A <em>private member</em> can be
accessed only by the code inside the class. <em>Protected members</em> … are similar to private
members, except that methods in subclasses can also reference them. Both private and
protected members, though, are inaccessible from client code.” <span class="citation">(Spraul 2012)</span></p>
</blockquote>
<blockquote>
<p>“An object should be a meaningful, closely knit collection of data and code that operates
on the data.” <span class="citation">(Spraul 2012)</span></p>
</blockquote>
<blockquote>
<p>“Recognizing a situation in which a class would be useful is essential to reaching the
higher levels of programming style, but it’s equally important to recognize situations in
which a class is going to make things worse.” <span class="citation">(Spraul 2012)</span></p>
</blockquote>
<blockquote>
<p>“The word <em>encapsulation</em> is a fancy way of saying that classes put multiple pieces of
data and code together in a single package. If you’ve ever seen a gelatin medicine capsule
filled with little spheres, that’s a good analogy: The patient takes one capsule and swallows
all the individual ingredient spheres inside. … From a problem-solving standpoint,
encapsulation allows us to more easily reuse the code from previous problems to solve current
problems. Often, even though we have worked on a problem similar to our current project,
reusing what we learned before still takes a lot of work. A fully encapsulated class can
work like an external USB drive; you just plug it in and it works. FOr this to happen,
though, we must design the class correctly to make sure that the code and data is truly
encapsulated and as independent as possible from anything outside of the class. For example,
a class that references a global variable can’t be copied into a new project without
copying the global variable, as well.” <span class="citation">(Spraul 2012)</span></p>
</blockquote>
<blockquote>
<p>“Beyond reusing classes from one program to the next, classes offer the potential for
a more immediate form of code reuse: inheritance. … Using inheritance, we create parent
classes with methods common to two or more child classes, thereby ‘factoring out’ not
just a few lines of code [as with helper functions in procedural code] but whole methods.”
<span class="citation">(Spraul 2012)</span></p>
</blockquote>
<blockquote>
<p>“One technique we’re returned to again and again is dividing a complex problem into
smaller, more manageable pieces. Classes are great at dividing programs up into functional
units. Encapsulation not only holds data and code together in a reusable package; it also
cordons off that data and code from the rest of the program, allowing us to work on that
class, and everything else separately. The more classes we make in a program, the greater
the problem-dividing effect.” <span class="citation">(Spraul 2012)</span></p>
</blockquote>
<blockquote>
<p>“Some people use the terms <em>information hiding</em> and <em>encapsulation</em> interchangeable, but
we’ll separate the ideas here. As described previously …, encapsulation is
packaging data and code together. Information hiding means separating the interface of a
data structure—the definition of the operations and their parameters—from the implementation
of a data structure, or the code inside the functions. If a class has been written with
information hiding as a goal, then it’s possible to change the implementation of the methods
without requiring any changes in the client code (the code that uses the class). Again, we
have to be clear on the term <em>interface</em>; this means not only the name of the methods and
their parameter list but also the explanation (perhaps expressed in code documentation) of
what the different methods do. When we talk about changing the implementation without
changing the interface, we mean that we change <em>how</em> the class methods work but not
<em>what</em> they do. Some programming authors have referred to this as a kind of implicit contract
between the class and the client: The class agrees never to change the effects of
existing operations, and the client agrees to use the class strictly on the basis of its
interface and to ignore any implementation details.” <span class="citation">(Spraul 2012)</span></p>
</blockquote>
<blockquote>
<p>“So how does information hiding affect problem solving? The principle of information hiding
tells the programmer to put aside the class implementation details when working on the
client code, or more broadly, to be concerned about a particular class’s implementation
only when working inside that class. When you can put implementation details out of your
mind, you can eliminate distracting thoughts and concentrate on solving the problem at hand.”
<span class="citation">(Spraul 2012)</span></p>
</blockquote>
<blockquote>
<p>“A final goal of a well-designed class is expressiveness, or what might be broadly called
writability—the ease with which code can be written. A good class, once written, makes
the rest of the code simpler to write in the way that a good function makes code simpler to
write. Classes effectively extend a language, becoming high-level counterparts to basic
low-level features such as loops, if statements, and so forth. … With classes, programming
actions that previously took many steps can be done in just a few steps or just one.”
<span class="citation">(Spraul 2012)</span></p>
</blockquote>
<blockquote>
<p>“Right now, in labs across the world, machines are sequencing the genomes of the life
on earth. Even with rapidly decreasing costs and huge technological advancements in
genome sequencing, we’re only seeing a glimpse of the biological information contained
in every cell, tissue, organism, and ecosystem. However, the smidgen of total biological
information we’re gathering amounts to mountains of data biologists need to work with. At
no other point in human history has our ability to understand life’s complexities been so
dependent on our skills to work with and analyze data.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Bioinformaticians are concerned with deriving biological understanding from large
amounts of data with specialized skills and tools. Early in biology’s history, the
datasets were small and manageable. Most biologists could analyze their own data after
taking a statistics course, using Microsoft Excel on a personal desktop computer.
However, this is all rapidly changing. Large sequencing datasets are widespread, and will
only become more common in the future. Analyzing this data takes different tools, new skills,
and many computers with large amounts of memory, processing power, and disk space.”
<span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“In a relatively short period of time, sequencing costs dropped drastically, allowing
researchers to utilize sequencing data to help answer important biological questions.
Early sequencing was low-throughput and costly. Whole genome sequencing efforts were
expensive (the human genome cost around $2.7 billion) and only possible through large
collaborative efforts. Since the release of the human genome, sequencing costs have
decreased explonentially until about 2008 … With the introduction of next-generation
sequencing technologies, the cost of sequencing a megabase of DNA dropped even more
rapidly. At this crucial point, a technology that was only affordable to large collaborative
sequencing efforts (or individual researchers with very deep pockets) became affordable
to researchers across all of biology. … What was the consequence of this drop in
sequencing costs due to these new technologies? As you may have guessed, lots and lots
of data. Biological databases have swelled with data after exponential growth. Whereas once
small databases shared between collaborators were sufficient, now petabytes of useful
data are sitting on servers all over the world. Key insights into biological questions are
stored not just in the unanalyzed experimental data sitting on your hard drive, but also
spinning around a disk in a data center thousands of miles away.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“To make matters even more complicated, new tools for analyzing biological data are
continually being created, and their underlying algorithms are advancing. A 2012 review
listed over 70 short-read mappers … Likewise, our approach to genome assembly has
changed considerably in the past five years, as methods to assemble long sequences
(such as overlap-layout-consensus algorithms) were abandoned with the emergence of
short high-throughput sequencing reads. Now, advances in sequencing chemistry are
leading to longer sequencing read lengths and new algorithms are replacing others that
were just a few years old. Unfortunately, this abundance and rapid development of
bioinformatics tools has serious downsides. Often, bioinformatics tools are not adequately
benchmarked, or if they are, they are only benchmarked in one organism. This makes it
difficult for new biologists to find and choose the best tool to analyze their data.
To make matters more difficult, some bioinformatics programs are not actively developed
so that they lose relevance or carry bugs that could negatively affect results. All of
this makes choosing an appropriate bioinformatics program in your own research difficult.
More importantly, it’s imperative to critically assess the output of bioinformatics
programs run on your own data.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“With the nature of biological data changing so rapidly, how are you supposed to
learn bioinformatics? With all of the tools out there and more continually being
created, how is a biologist supposed to know whether a program will work appropriately
on her organism’s data? The solution is to approach bioinformatics as a bioinformatician
does: try stuff, and assess the results. In this way, bioinformatics is just about having
the skills to experiment with data using a computer and understanding your results.
The experimental part is easy: this comes naturally to most scientists. The limiting
factor for most biologists is having the data skills to freely experiment and work with
large data on a computer.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Unfortunately, many of the biologist’s common computational tools can’t scale to the
size and complexity of modern biological data. Complex data formats, interfacing
numerous programs, and assessing software and data make large bioinformatics datasets
difficult to work with.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“In 10 years, bioinformaticians may only be using a few of the bioinformatics
software programs around today. But we most certainly will be using data skills and
experimentation to assess data and methods of the future.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Biology’s increasing use of large sequencing datasets is changing more that the tools
and skills we need: it’s also changing how reproducible and robust our scientific
findings are. As we utilize new tools and skills to analyze genomics data, it’s
necessary to ensure that our approaches are still as reproducible and robust as
any other experimental approaches. Unfortunately, the size of our data and the complexity
of our analysis workflows make these goals especially difficult in genomics.”
<span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“The requisite of reproducibility is that we share our data and methods. In the pre-genomics
era, this was much easier. Papers coule include detailed method summaries and entire
datasets—exactly as Kreitman’s 1986 paper did with a 4,713bp Adh gene flanking sequence
(it was embedded in the middle of the paper). Now papers have long supplementary methods,
code, and data. Sharing data is no longer trivial either, as sequencing projects can include
terabytes of accompanying data. Reference genomes and annotation datasets used in analyses are
constantly updated, which can make reproducibility tricky. Links to supplemental materials,
methods, and data on journal websites break, materials on faculty websites disappear when
faculty members move or update their sites, and software projects become stale when
developers leave and don’t update code. … Additionally, the complexity of bioinformatics
analyses can lead to findings being susceptible to errors and technical confounding.
Even fairly routine genomics projects can use dozens of different programs, complicated
input paramter combinations, and many sample and annotation datasets; in addition, work
may be spread across servers and workstations. All of these computational data-processing
steps create results used in higher-level analyses where we draw our biological conclusions.
The end result is that research findings may rest on a rickety scaffold of numerous
processing steps. To make matters worse, bioinformatics workflows and analyses are usually
only run once to produce results for a publication, and then never run or tested again.
These analyses may rely on very specific versions of all software used, which can make it
difficult to reproduce on a different system. In learning bioinformatics data skills, it’s
necessary to concurrently learn reproducibility and robust best practices.”
<span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“When we are writing code in a programming language, we work most of the time with RAM,
combining and restructuring data values to produce new values in RAM. … The computer memory
in RAM is a series of 0’s and 1’s, just like the computer memory used to store files in mass
storage. In order to work with data values, we need to get those values into RAM in some
format. At the basic level of representing a single number or a single piece of text, the
solution is the same as it was in Chapter 5 [on file formats for mass storage]. Everything
is represented as a pattern of bits, using various numbers of bytes for different sorts of
values. In R, in an English locale, and on a 32-bit operating system, a single character
usually takes up one byte, an integer takes up four bytes, and a real number 8 bytes.
Data values are stored in different ways depending on the <strong>data type</strong>—whether the values
are numbers or texts.” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“ALthough we do not often encounter the details of the memory representation, except
when we need a rough estimate of how much RAM a data set might require, it is
important to keep in mind what sort of data type we are working with because the computer
code that we will produce different results for different data types. For example,
we can only calculate an average if we are dealing with values that have been stored
as text.” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“Another important issue is how <em>collections</em> of values are stored in memory. The
tasks that we will consider will typically involve working with an entire data set,
or an entire variable from a data set, rather than just a single value, so we need
to have a way to represent several related values in memory. This is similar to the
problem of deciding on a storage format for a data set… However, rather than talking
about different file formats, [in this case] we will talk about different <strong>data
structures</strong> for storing a collection of data values in RAM. … It will be important
to always keep close in our minds what data type we are working with and what sort
of data structure we are working with.” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“Every individual data value has a data type that tells us what sort of value it
is. The most common data types are numbers, which R calls <strong>numeric values</strong>, and
text, which R calls <strong>character values</strong>.” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“<strong>Vectors:</strong> A collection of values that all have the same data type. The
<strong>elements</strong> of a vector are all numbers, giving a <strong>number vector</strong>, or all
character values, giving a <strong>character vector.</strong>” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“<strong>Data frames:</strong> A collection of vectors that all have the same length. This is
like a matrix, except that each column can contain a different data type.”
<span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“<strong>Lists:</strong> A collection of data structures. The <strong>components</strong> of a list can
be simply vectors—similar to a data frame, but with each column allowed to have
a different length. However, a list can also be a much more complicated
structure. This is a very flexible data structure. Lists can be used to store
any combination of data values together.” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“Notice the way that lists are displayed. The first component of the list
starts with the component indes, <code>[[1]]</code>, followed by the contents of this
component…The second component of the list starts with the component
index <code>[[2</code>]] followed by the contenets of this component…” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“A list is a very flexible data structure. It can have any number of <strong>components</strong>,
each of which can be any data structure of any length or size. A simple
example is a data-frame-like structure where each column can have a different
length, but much more complex structures are also possible. For example, it is
possible for a component of a list to be another list.” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“Anyone who has worked with a computer should be familiar with the idea of
a list containing another list because a directory or folder of files has this sort
of structure: a folder contains multiple files of different kinds and sizes and
a folder can contain other folders, which can contain more files or even more
folders, and so on. Lists allow for this kind of hierarchical structure.”
<span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“One of the most basic ways that we can manipulate data structures is to
<strong>subset</strong> them—select a smaller portion from a larger data structure. This
is analogous to performing a query on a database. … R has very powerful
mechanisms for subsetting… A subset from a vector may be obtained by
appending an <strong>index</strong> within square brackets to the end of a symbol name. …
The index can be a vector of any length … The index does not have to be
a contiguous sequence, and it can include repetitions… As well as using
integers for indices, we can use logical values… A data frame can also
be indexed using square brackets, though slightly differently because we have
to specify both which rows <em>and</em> which columns we want … When a data structure
has named components, a subset may be selected using those names.”
<span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“Single square bracket subsetting on a data frame is like taking an egg container
that contains a dozen eggs and chopping up the <em>container</em> so that we are left
with a smaller egg container that contains just a few eggs. Double square bracket
subsetting on a data frame is like selecting just one <em>egg</em> from an egg container.”
<span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“We can often get some idea of what sort of data structure we are working with
by simply viewing how the data are displayed on screen. However, a more definitive
answer can be obtained by calling the <code>class()</code> function. … Many R functions
return a data structure that is not one of the basic data structures that we have
already seen [like the ‘xtabs’ and ‘table’ classes]. … We have not seen either
of these data structures before. However, much of what we know about working with
the standard data structures … will work with any new class that we encounter.
For example, it is usually possible to subset any class using the standard
square bracket syntax. … Where appropriate, arithmetic and comparisons will
also generally work… Furthermore, if necessary, we can ofter resort to coercing
a class to something more standard and familiar.” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“Dates are an important example of a special data structure. Representing
dates as just text is convenient for humans to view, but other representations
are better for computers to work with. … Having a special class for dates
means that we can perform tasks with dates, such as arithmetic and comparisons,
in a meaningful way, something we could not do if we stored the date as just
a character value.” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“The Date class stores date values as integer values, representing the number
of days since January 1st 1970, and automatically converts the numbers to
a readable text value to display the dates on the screen.” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“When working with anything but tiny data sets, basic reatures of the data
set cannot be determined by just viewing the data values. [There are] a number of
functions that are useful for obtaining useful summary features from a data structure.
The <code>summary()</code> function produces summary information for a data structure… The
<code>length()</code> function is useful for determining the number of values in a vector or
the number of components in a list. … The <code>str()</code> function (short for ‘structure’)
is useful when dealing with large objects because it only shows a sample of the values
in each part of the object, although the display is very low-level so it may not always
make things clearer. … Another function that is useful for inspecting a large
object is the <code>head()</code> function. This shows just the first few elemeents of an
object, so we can see the basic structure without seeing all of the values.”
<span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“Generic functions … will accept many different data structures as
arguments. … a generic function adapts itself to the data structure it is
given. Generic functions do different things when given different data structures.”
<span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“An example of a generic function is the <code>summary()</code> function. The result of a
call to <code>summary()</code> sill depend on what sort of data structure we provide.”
<span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“Generic functions are another reason why it is easy to work with data in R; a
single function will produce a sensible result no matter what data structure
we provide. However, generic functions are also another reason why it is so
important to be aware of what data structures we are working with. Without
knowing what sort of data we are using, we cannot know what sort of result to
expect from a generic function.” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“R has become very popular and is now being used for projects that require
substantial software engineering as well as its continued widespread use as
an interactive environment for data analysis. This essentially means that
there are two masters—<em>reliability</em> and <em>ease of use</em>. S3 is indeed easy to
use, but can be made unreliable through nothing other than bad luck, or
a poor choice of names, and hence is not a suitable paradigm for constructing
large systems. S4, on the other hand, is better suited for developing large
software projects but has an increased complexity of use.” <span class="citation">(Gentleman 2008)</span></p>
</blockquote>
<blockquote>
<p>“Object-oriented programming has become a widely used and valuable tool for
software engineering. Much of its value derives from the fact that it is often
easier to design, write, and maintain software when there is some clear
separation of the data representation from the operations that are to be
performed on it. In an OOP system, real physical things … are generally
represented by classes, and methods (functions) are written to handle the
different manipulations that need to be performed on the objects.” <span class="citation">(Gentleman 2008)</span></p>
</blockquote>
<blockquote>
<p>“The views that many people have of OOP have been based largely on
exposure to languages like Java, where the system can be described as class-centric.
In a a class-centric system, classes define objects and are repositories for
the methods that act on those objects. In contrast, languages such as … R
separate the class specification from the specification of generic functions,
and could be described as function-centric systems.” <span class="citation">(Gentleman 2008)</span></p>
</blockquote>
<blockquote>
<p>“The genome of every organism is encoded in chromosomes that consist of either
DNA or RNA. High throughput sequencing technology has made it possible to determine
the sequence of the genome for virtually any organism, and there are many that are
currently available. … However, in many cases, either the exact nucleotide at
any location is unknown, or is variable, and the International Union of Pure and
Applied Chemistry (IUPAC) has provided a standard nomenclature suitable for
representing such sequences. The alphabet for dealing with protein sequences
is based on the 20 amino acids. … The basic class used to hold strings [in the
<strong>Biostrings</strong> package] is the <em>BString</em> class, which has been designed to be
efficient in its handling of large character strings. Subclasses include
<em>DNAString</em>, <em>RNAString</em>, and <em>AAString</em> (for holding amino acid sequences).
The <em>BStringViews</em> class holds a set of <em>views</em> on a single <em>BString</em> instance;
each view is essentially a substring of the underlying <em>BString</em> instance. Alignments
are stored using the <em>BStringAlign</em> class.” <span class="citation">(Gentleman 2008)</span> [More on functions that
work with these classes on p. 171]</p>
</blockquote>
<blockquote>
<p>“A number of complete genomes, represented as <em>DNAString</em> objects, are provided
through the Bioconductor project. They rely on the infrastructure in the <strong>BSgenome</strong>
package, and all such packages have names that begin with <code>BSgenome</code>. You can find the
list of available genomes using the <code>available.genomes</code> function.” <span class="citation">(Gentleman 2008)</span></p>
</blockquote>
<blockquote>
<p>“Atomic vectors are the most basic of all data structures. An atomic vector
contains some number of values of the same type; that number could be zero.
Atomic vectors can contain integers, doubles, logicals or character strings.
Both complex numbers and raw (pure bytes) have atomic representations …
Character vectors in the S language are vectors of character strings, not the
vectors of characters. For example, the string ‘super’ would be represented as
a character vector of length one, not of lenth five…” <span class="citation">(Gentleman 2008)</span></p>
</blockquote>
<blockquote>
<p>“Lists can be used to store items that are not all of the same type. …
Lists are also referred to as generic vectors since they share many of the properties
of vectors, but the elements are allowed to have different types.” <span class="citation">(Gentleman 2008)</span></p>
</blockquote>
<blockquote>
<p>“Lists can be of any length, and the elements of a list can be named, or not.
Any R object can be an element of a list, including another list…” <span class="citation">(Gentleman 2008)</span></p>
</blockquote>
<blockquote>
<p>“A <code>data.frame</code> is a special kind of list. Data frames were created to provide a
common structure for storing rectangular data sets and for passing them to
different functions for modeling and visualization. In many cases a data set can be
thought of as a rectangular structure with rows corresponding to cases and columns
corresponding to the different variables that were measured on each of the cases. One
might think that a matrix would be the appropriate representation, but that is only
true if all of the variables are of the same type, and this is seldom the case.”
<span class="citation">(Gentleman 2008)</span></p>
</blockquote>
<blockquote>
<p>“[Data frames] are essentially a list of vectors, with one vector for each variable.
It is an error if the vectors are not all of the same length.” <span class="citation">(Gentleman 2008)</span></p>
</blockquote>
<blockquote>
<p>“Sometimes it will be helpful to find out about an object. Obvious functions to
try are <code>class</code> and <code>typeof</code>. But many find that both <code>str</code> and <code>object.size</code> are
more useful. … The functions <code>head</code> and <code>tail</code> are convenience functions that list
the first few, or the last few, rows of a matrix.” <span class="citation">(Gentleman 2008)</span></p>
</blockquote>
<blockquote>
<p>“The S langauge has its roots in the Algol family of languages and has adopted some
of the general vector subsetting and subscripting techniques that were available in
languages such as APL. This is perhaps one area wehre programmers more familiar with
other languages fail to make appropriate use of the available functionality. …
There are slight differents between subsetting of vectors, arrays, lists, data.frames,
and enviroments that can sometimes catch the unwary. But there are also many
commonalities. … Subsetting can be carried out by three different operators:
the single square bracket <code>[</code>, the double square bracket <code>[[</code>, and the dollar, <code>$</code>.
We note that each of these three operators are actually generic functions and users
can write methods that extend and override them… One way of describing the behavior
of the single bracket operator is that the type of the return value matches the type
of the value it is applied to. Thus, a single bracket subset of a list is a list
itself. … Both <code>[[</code> and <code>$</code> extract a single value. There are some differences
between the two; <code>$</code> does not evaluate its second argument while <code>[[</code> does, and hence
one can use expressions. The <code>$</code> operator uses partial matching when extracting
named elements but <code>[</code> and <code>[[</code> do not.” <span class="citation">(Gentleman 2008)</span></p>
</blockquote>
<blockquote>
<p>“Subsetting plays two roles in the S language. One is an extraction role, where a subset
of a vector is identified by a set of supplied indices and the resulting subset is
returned as a value. Venables and Ripley (2000) refer to this as <em>indexing</em>. The
second purpose is subset assignment, where the goal is to identify a subset of
values that should have their values changed; we call this subset assignment.”
<span class="citation">(Gentleman 2008)</span></p>
</blockquote>
<blockquote>
<p>“There are four basic types of subscript indices: positive integers, negative
integers, logical vectors, and character vectors. These four types cannot be mixed…
For matrix and array subscripting, one can use different types of subscripts for
the different dimensions. Not all vectors, or recursive objects, support all types
of subscripting indices. For example, atomic vectors cannot be subscripted using
<code>$</code>, while environments cannot be subscripted using <code>[</code>.” <span class="citation">(Gentleman 2008)</span></p>
</blockquote>
<blockquote>
<p>“In bioinformatics, the plain-text data we work with is often encoded in <em>ASCII</em>.
ASCII is a character encoding scheme that uses 7 bits to represent 128 different
values, including letters (upper- and lowercase), numbers, and special nonvisible
characters. While ASCII only uses 8 bits, nowadays computers use an 8-bit <em>byte</em>
(a unit representing 8 bits) to store ASCII characters. More information about
ASCII is available in your terminal through <code>man ascii</code>.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Some files will have non-ASCII encoding schemes, and may contain special characters.
The most common character encoding scheme is UTF-8, which is a superset of ASCII
but allows for special characters.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Bioinformatics data is often text—for example, the As, Cs, Ts, and Gs in
sequencing read files or reference genomes, or tab-delimited files fo gene coordinates.
The text data in bioinformatics is often large, too (gigabytes or more that can’t
fit into your computer’s memory at once). This is why Unix’s philosophy of
handling text streams is useful to bioinformatics: text streams allow us to
do processing on a <em>stream</em> of data rather than holding it all in memory.”
<span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Exploratory data analysis plays an integral role throughout an entire bioinformatics
project. Exploratory data analysis skills are just as applicable in analyzing intermediate
bioinformatics data (e.g., are fewer reads from this sequencing lane aligning?) as
they are in making sense of results from statistical analyses (e.g., what’s the distribution
of these p-values, and do they correlate with possible confounders like gene length?).
These exploratory analyses need not be complex or exceedingly detailed (many patterns
are visible with simple analyses and visualization); it’s just about wanting to look
into the data and having the skill set to do so.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Functions like <code>table()</code> are generic—they are designed to work with objects of
all kinds of classes. Generic functions are also designed to do the right thing
depending on the class of the object they’re called on (in programming lingo, we
say that the function is <em>polymorphic</em>).” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“It’s quite common to encounter genomics datasets that are difficult to load into
R because they’re large files. This is either because it takes too long to load the
entire dataset into R, or your machine simply doesn’t have enough memory. In many
cases, the best strategy is to reduce the size of your data somehow: summarizing data
in earlier processing steps, omitting unnecessary columns, splitting your data into
chunks (e.g., working with a chromosome at a tiem), or working on a random subset
of your data. Many bioinformatics analyses do not require working on an entire
genomic dataset at once, so these strategies can work quite well. These approaches
are also the only way to work with data that is truly too large to fit in your
machine’s memory (apart from getting a machine with more memory).” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“If your data is larger than the available memory on your machine, you’ll need to
use a strategy that keeps the bulk of your data out of memory, but still allows for
each access from R. A good solution for moerately large data is to use SQLite and
query out subsets for computation using the R package <code>RSQLite</code>. … Finally …
many Unix data tools have versions that work on gzipped files: <code>zless</code>, <code>zcat</code> (<code>gzcat</code>
on BSD-derived systems like Mac OS X), and others. Likewise, R’s data-reading
functions can also read gzipped files directly—there’s some slight performance
gains in reading in gzipped files, as there are fewer bytes to read off of
(slow) hard disks.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Quite often, data we load in to R will be in the wrong <em>shape</em> for what we want to
do with it. Tabular data can come in two different formats: <em>long</em> and <em>wide</em>. …
In many cases, data is recorded by humans in wide format, but we need data in long
format when working with and plotting statistical modeling functions.”
<span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Exploratory data analysis emphasizes visualization as the best tool to understand
and explore our data—both to learn what the data says and what its limitations are.”
<span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“R vectors require all elements to have the same data type (that is, vectors are
<em>homogeneous</em>). They only support the six data types discussed earlier (integer, double,
character, logical, complex, and raw). In contrast, R’s lists are more versatile: Lists can
contain elements of different types (they are <em>heterogenesou</em>); Elements can be <em>any</em>
object in R (vectors with different types, other lists, environments, dataframes,
matrices, functions, etc.); Because lists can store other lists, they allow for storing
data in a recursive way (in contrast, vectors cannot contain other vectors.”
<span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“The versatility of lists make them indispensable in programming and data analysis with
R.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“As with R’s vectors, we can extract subsets of a list or change values of
specific elements using indexing. However, accessing elements from an R list is
slightly different than with vectors. Because R’s list can contain objects with
different types, a subset containing multiple list elements could contain objects with
different types. Consequently, the only way to return a subset of more than one
list element is with another list. As a result, there are two indexing operators for lists:
one for accessing a subset of multiple elements as a list (the single bracket…) and
one for accessing an element within a list (the double bracket…).”
<span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Because R’s lists can be nested and contain any type of data, list-based data
structures can grow to be quite complex. In some cases, it can be difficult to
understand the overall structure of some lists. The function <code>str()</code> is a convenient
R function for inspecting complex data structures. <code>str()</code> prints a line for each
contained data structure, complete with its type, length (or dimensions), and the
first few elements it contains. … For deeply nested lists, you can simplify <code>str()''s output by specifying the maximum depth of nested structure to return with</code>str()<code>'s  second argument,</code>max.level<code>. By default,</code>max.level<code>is</code>NA`, which returns all
nested structures.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Understanding R’s data structures and how subsetting works are fundamental to
having the freedom in R to explore data any way you like.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Some of Bioconductor’s core packages: <strong>GenomicRanges:</strong> Used to represent and
work with genomic ranges; <strong>GenomicFeatures:</strong> used to represent and work with ranges
that represent gene models and other features of a genome (genes, exons, UTRs,
transcripts, etc.); <strong>Biostrings</strong> and <strong>BSgenome:</strong> Used for manipulating genome
sequence data in R… <strong>rtracklayer:</strong> Used for reading in common bioinformatics formats
like BED, GTF/GFF, and WIG.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“The <code>GenomicRanges</code> package introduces a new class called <code>GRanges</code> for storing
genomic ranges. The <code>GRanges</code> builds off of <code>IRanges</code>. <code>IRanges</code> objects are used
to store ranges of genomic regions on a single sequence, and <code>GRanges</code> objects contain
the two other pieces of information necessary to specify a genomic location:
sequence name (e.g., which chromosome) and strand. <code>GRanges</code> objects also have
<em>metadata columns</em>, which are the data linked to each genomic range.”
<span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“All metadata attached to a <code>GRanges</code> object are stored in a <code>DataFrame</code>, which
behaves identically to R’s base <code>data.frame</code> but supports a wider variety of
column types. For example, <code>DataFrames</code> allow for run-length encoded vectors
to save memory … in practice, we can store any type of data: identifiers
and names (e.g., for genes, transcripts, SNPs, or exons), annotation data
(e.g., conservation scores, GC content, repeat content, etc.), or experimental
data (e.g., if ranges correspond to alignments, data like mapping quality and the
number of gaps). … the union of genomic location with any type of data is what makes
<code>GRanges</code> so pwoerful.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<hr />
</div>
<div id="subsection-2" class="section level3">
<h3><span class="header-section-number">3.5.5</span> Subsection 2</h3>
</div>
<div id="applied-exercise" class="section level3">
<h3><span class="header-section-number">3.5.6</span> Applied exercise</h3>
<ul>
<li>[Example data in a basic list]</li>
<li>[Example data in a Bioconductor list-based class]</li>
<li>[Explore each example dataset. What slots do each have? What are the names of each slot?
What data structures / data types are in each slot?]</li>
<li>[Extract certain elements from each dataset by hand. Assign to its own object name so
you can use it by itself.]</li>
<li>[Use <code>biobroom</code> to extract pieces of data in the Bioconductor dataset as tidy dataframes.
Try using this with further tidyverse code to create a nice table/visualization.]</li>
</ul>

</div>
</div>
<div id="example-converting-from-complex-to-tidy-data-formats" class="section level2">
<h2><span class="header-section-number">3.6</span> Example: Converting from complex to ‘tidy’ data formats</h2>
<p>We will provide a detailed example of a case where data pre-processing in R
results in a complex, ‘untidy’ data format. We will walk through an example of
applying automated gating to flow cytometry data. We will demonstrate the
complex initial format of this pre-processed data and then show trainees how a
‘tidy’ dataset can be extracted and used for further data analysis and
visualization using the popular R ‘tidyverse’ tools. This example will use real
experimental data from one of our Co-Is research on the immunology of
tuberculosis.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Describe how tools like  were used in this real research
example to convert from the complex data format from pre-processing to a format
better for further data analysis and visualization</li>
<li>Understand how these tools would fit in their own research pipelines</li>
</ul>
<div id="subsection-1" class="section level3">
<h3><span class="header-section-number">3.6.1</span> Subsection 1</h3>
</div>
<div id="subsection-2" class="section level3">
<h3><span class="header-section-number">3.6.2</span> Subsection 2</h3>
</div>
<div id="applied-exercise" class="section level3">
<h3><span class="header-section-number">3.6.3</span> Applied exercise</h3>

</div>
</div>
<div id="introduction-to-reproducible-data-pre-processing-protocols" class="section level2">
<h2><span class="header-section-number">3.7</span> Introduction to reproducible data pre-processing protocols</h2>
<p>Reproducibility tools can be used to create reproducible data pre-processing
protocols—documents that combine code and text in a ‘knitted’ document, which
can be re-used to ensure data pre-processing is consistent and reproducible
across research projects. In this module, we will describe how reproducible data
pre-processing protocols can improve reproducibility of pre-processing
experimental data, as well as to ensure transparency, consistency, and
reproducibility across the research projects conducted by a research team.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Define a ‘reproducible data pre-processing protocol’</li>
<li>Explain how such protocols improve reproducibility at the data pre-processing
phase</li>
<li>List other benefits, including improving efficiency and consistency of data
pre-processing</li>
</ul>
<div id="introducing-reproducible-data-pre-processing-protocols" class="section level3">
<h3><span class="header-section-number">3.7.1</span> Introducing reproducible data pre-processing protocols</h3>
<p>When you use scripted code to pre-process biomedical data, you will find that
the same script can often be easily adapted and re-used in later projects that
use the same type of data. You may need to change small elements, like the file
names of files with data you want to use, or some details about the methods
used for certain pre-processing steps. However, often almost all of the pre-processing
steps will repeat over different experiments that you do.</p>
<p>If you have used open-source software tools, like Bioconductor packages, you
are likely familiar with the <em>vignettes</em> that come with the packages. These
provide tutorial guides showing you how to work with the package. They often
leverage example data that you can download so that you can try all the
example code yourself, before you move on to adapting the code to use with
your own data.</p>
<p>You can create your own version of these types of documents. This can use
real data from your research group, and you can create customized instructions
and code examples showing how to use open-source tools to pre-process a
certain type of biomedical data for experiments in your research group.
You can use this document the next time you need to pre-process that type
of data yourself, and you can also share it with others in your research
group. This can help in teaching new laboratory members how to work with
this type of data in your research group. It can also help ensure that
different members of the research group are all using the same steps to
pre-process data, so that there is greater consistency across results from
the group.</p>
<p>You may already create something similar to this, using a general word
processing program like Google Docs or Word. There are two key differences,
however, between how vignettes are created compared to a similar tutorial
created in Word or Google Docs. First, the vignettes are created using
a document compiling program that ensures that any code uses only ASCII
characters. This means that you can copy and paste code from the tutorial
into your R session and it will work. By contrast, programs like Word often
try to “correct” some of the characters when you paste in or type in code.
For example, when you have an apostrophe mark in your code (for example,
when you’re quoting to create a character string), the computer code needs
to have this character as a very basic ASCII version of an apostrophe. Word,
by contrast, will often try to convert the character to use an apostrophe
character that looks smoother—and so is nice for a word processed document
that humans will read—but that R cannot recognize. Hyphens can have similar
problems. [Other examples?]</p>
<p>When you create a reproducible pre-processing protocol using the techniques that
are used to create vignettes—which we’ll teach you how to do in the next few
sections—you will avoid this autocorrection of characters, and so someone
reading the protocol will be able to directly copy and paste example code from
the protocol into their own scripts. This will avoid hard-to-diagnose errors
that come from this character conversion in programs like Word.</p>
<p>The second difference is that the tools that are used to create vignettes
contain code that is not just copied and pasted from a script, but that is
actually, in essence, <em>still in a script</em>. The code, in other words, is
executable and, unless you change the default settings, is re-run every
time you compile the document. This means that you will quickly determine if
there are any typos or other errors in the code, because the document will
not run and render correctly unless the code works. This means that you can
guarantee, when you first create the document, that the code runs, and also
that you can regularly check to see if the example code still works at
later time points. This allows you to, for example, see if changes in the
version of R or of specific packages that you’re using has created problems
with the code running correctly over time.</p>
<p>Finally, these documents can be separated, allowing you to extract solely the
script part of the document, into a classic R script. You can use this directly
to run (or adapt) the pre-processing code for further research.</p>
</div>
<div id="technique-to-create-reproducible-pre-processing-protocols" class="section level3">
<h3><span class="header-section-number">3.7.2</span> Technique to create reproducible pre-processing protocols</h3>
<p>[“Knitted” documents]</p>
</div>
<div id="advantages-of-reproducible-pre-processing-protocols" class="section level3">
<h3><span class="header-section-number">3.7.3</span> Advantages of reproducible pre-processing protocols</h3>
<p>With point-and-click software, even if you are doing the same process from one
experiment to another to pre-process your data, you will still have to go through
each step of preprocessing, re-selecting each choice along the way. For example,
if you are using software to gate flow cytometry data, someone in your research
group must typically go through the gating step-by-step, even if they are trying to
gate the data using the same rules and approach that they’ve applied to gate data
in previous experiments. This approach therefore has two key limitations.</p>
<p>First, it takes a lot of time for someone in the research group to go through
the same series of selection / point-and-click steps over and over each time
research data needs to be pre-processed. If steps do indeed need to be
customized extensively from one experiment to the next, there may be no way to
avoid this time-consuming work. However, if the same choices for pre-processing
apply from one experiment to the next, then there’s not a good reason for
someone in the research group to need to spend a lot of time with this process.</p>
<p>The second issue is that it’s hard to be sure that you’re being completely
consistent from experiment to experiment if you’re going through the
pre-processing “by hand”, going through different steps and selections using
point-and-click software. Even if you’ve written down the choices you plan to
make from time to time, there may be subtle small choices that you forget to
write down. Further, there are some choices that might not be as easy to make
consistent from time to time. For example, when you gate flow cytometry data
using point and click software, you are often visual adjusting a threshold or a
box to select certain data points in the sample to gate. These visual choices
can be subjective from day to day, so you might gate the data slightly different
from one day to the next. Even if the same person does the preprocessing from
time to time, there will likely be subtle variations in the process; these are
likely to expand quite a bit when different people in the research group do the
pre-processing from one experiment to another.</p>
</div>
<div id="subsection-1" class="section level3">
<h3><span class="header-section-number">3.7.4</span> Subsection 1</h3>
<p>[Markup language visualization—a boss dictating to secretary in old movie, including
speaking punctuation and formatting, like new paragraphs or bold.]</p>
<p>[Anecdote—any stories of famous authors trying to retrieve old manuscripts in
outdated file formats?]</p>
<p>[Example of old, plain text emails and the beginnings of Markdown-style notation
there? More on history of Markdown and other markup languages? (Readers minds were the
first renders for this ancestor of Markdown, but now there are software programs
that render the plain-text file into other formats, like Word, PDF, or HTML.)]</p>
</div>
<div id="subsection-2" class="section level3">
<h3><span class="header-section-number">3.7.5</span> Subsection 2</h3>
<blockquote>
<p>“It’s very important to keep a project notebook containing detailed information
about the chronology of your computational work, steps you’ve taken, information
about why you’ve made decisions, and of course all pertinent information to
reproduce your work. Some scientists do this in a handwritten notebook, others in
Microsoft Word documents. As with README files, bioinformaticians usually like keeping
project notebooks in simple plain-text because these can be read, searched, and
edited from the command line and across network connections to servers. Plain text
is also a future-proof format: plain-text files written in the 1960s are still
readable today, whereas files from word processors only 10 years old can be
difficult or impossible to open and edit. Additionally, plain text project notebooks can
also be put under version control … While plain-text is easy to write in your
text editor, it can be inconvenient for collaborators unfamiliar with the command
line to read. A lightweight markup language called <em>Markdown</em> is a plain-text format
that is easy to read and painlessly incorporated into typed notes, and can also be
rendered to HTML or PDF.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Markdown originates from the simple formatting conventions used in plain-text
emails. Long before HTML crept into email, emails were embellished with simple
markup for emphasis, lists, and blocks of text. Over time, this became a defacto
plain-text email formatting scheme. This scheme is very intuitive: underscores or
asterisks that flank text indicate emphasis, and lists are simply lines of text
beginning with dashes.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Markdown is just plain-text, which means that it’s portable and programs to edit
and read it will exist. Anyone who’s written notes or papers in old versions of
word processors is likely familiar with the hassle of trying to share or update
out-of-date proprietary formats. For these reasons, Markdown makes for a simple
and elegant notebook format.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Information, whether data or computer code, should be organized in such a way that
there is only one copy of each important unit of information.” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
</div>
<div id="discussion-questions" class="section level3">
<h3><span class="header-section-number">3.7.6</span> Discussion questions</h3>

</div>
</div>
<div id="rmarkdown-for-creating-reproducible-data-pre-processing-protocols" class="section level2">
<h2><span class="header-section-number">3.8</span> RMarkdown for creating reproducible data pre-processing protocols</h2>
<p>The R extension package RMarkdown can be used to create documents that combine
code and text in a ‘knitted’ document, and it has become a popular tool for
improving the computational reproducibility and efficiency of the data analysis
stage of research. This tool can also be used earlier in the research process,
however, to improve reproducibility of pre-processing steps. In this module, we
will provide detailed instructions on how to use RMarkdown in RStudio to create
documents that combine code and text. We will show how an RMarkdown document
describing a data pre-processing protocol can be used to efficiently apply the
same data pre-processing steps to different sets of raw data.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Define RMarkdown and the documents it can create</li>
<li>Explain how RMarkdown can be used to improve the reproducibility of research
projects at the data pre-processing phase</li>
<li>Create a document in RStudio using</li>
<li>Apply it to several different datasets with the same format</li>
</ul>
<div id="subsection-1" class="section level3">
<h3><span class="header-section-number">3.8.1</span> Subsection 1</h3>
<blockquote>
<p>“WordPerfect was always the best word processor. Because it allowed for insight into
its very structure. You could hit a certain key combination and suddenly the screen
would split and you’d reveal the codes, the bolds and italics, and so forth,
that would define your text when it was printed. It was beloved of legal secretaries
and journalists alike. Because when you work with words, at the practical, everyday
level, the ability to look under the hood is essential. Words are not simple. And
WordPerfect acknowledged that. Microsoft Word did not. Microsoft kept insisting that
what you saw on your screen was the way things <em>were</em>, and if your fonts just kept
sort of randonmly changing, well, you must have wanted it that way. Then along came
HTML, and what I remember most was that sense of being back inside the file. Sure,
HTML was a typographic nightmare, a bunch of unjustified Times New Roman in 12 pt on
screens with chiclet-size pixels, but under the hood you could see all the pieces.
Just like WordPerfect. That transparency was a wonderful thing, and it renewed
computing for me.” <span class="citation">(Ford 2014)</span></p>
</blockquote>
<blockquote>
<p>“TeX was created by Donald E. Knuth, a professor at Stanford University who has
achieved international renown as a mathematician and computer scientist.
Knuth also has an aesthetic sense uncommon in his field, and his work output is
truly phenomenal. TeX is a happy byproduct of Knuth’s mammoth enterprise,
<em>The Art of Computer Programming</em>. This series of reference books, designed
to cover the whole gamut of programming concepts and techniques, is a
<em>sine qua non</em> for all computer scientists.” <span class="citation">(Seroul 2012)</span></p>
</blockquote>
<blockquote>
<p>“Roughly speaking, text processors fall into two categories:
(1) WYSIWYG systems: what you see is what you get. You see on the screen at all
times what the printed document will look like, and what you type has immediate
effect on the appearance of the document. (2) markup systems, where you type your text
interspersed with formatting instructions, but don’t see their effect right away. You must run a program to examine the
resulting image, whether on paper or on the screen. In computer science jargon,
markup systems must compile the source file you type. WYSIWYG systems have the obvious
advantage of immediate feedback, but they
are not very precise: what is acceptable at a resolution of 300 dots per inch, for an
ephemeral publication such as a newsletter or flier, is no longer so for a book that
will be phototypeset at high resolution. The human eye is extraordinarily sensitive:
you can be bothered by the appearance of a text without being able to pinpoint why,
just as you can tell when someone plays the wrong note in an orchestra, without
being able to identify the CUlprit. One quickly leams in typesetting that the beauty,
legibility and comfortable reading of a text depend on minute details: each element
must be placed exactly right, within thousandths of an inch. For this type of work,
the advantage of immediate feedback vanishes: fine details of spacing, alignment,
and so on are much too small to be discernible at the screen’s relatively low
resolution, and even if it such were not the case, it would still be a monumental chore
to find the right place for everything by hand. For this reason it is not surprising that in the world of professional typesetting
markup systems are preferred. They automate the task of finding the right place
for each character with great precision. Naturally, this approach is less attractive for
beginners, since one can’t see the results as one types, and must develop a feeling
for what the system will do. But nowadays, you can have the best of both worlds
by using a markup system with a WYSIWYG <em>front end</em>; we’ll talk about such front
ends for TEX later on. TEX was developed in the late seventies and early eighties,
before WYSIWYG systems were widespread. But were it to be redesigned now, it would
still be a markup
language. To give you an idea of the precision with which TEX operates: the internal
unit it uses for its calculations is about a hundred times smaller than the
wavelength of visible light! (That’s right, a hundred times.) In other words, any
round-off error introduced in the calculations is invisible to the naked eye.”
<span class="citation">(Seroul 2012)</span></p>
</blockquote>
<blockquote>
<p>“You should be sure to understand the difference between a text editor and a text
processor. A text processor is a text editor together with formatting software that
allows you to switch fonts, do double columns, indent, and so on. A text editor
puts your text in a file on disk, and displays a portion of it on the screen. It doesn’t
format your text at all. We insist on the difference because those accustomed to WYSIWYG systems are
often not aware of it: they only know text processors. Where can you find a text
editor? Just about everywhere. Every text processor includes a text editor which
you can use. But if you use your text processor as a text editor, be sure to save your
file using a ‘save ASCII’ or ‘save text only’ option, so that the text processor’s own
formatting commands are stripped off. If you give TEX a file created without this
precaution, you’ll get garbage, because TEX cannot digest your text processor’s
commands.” <span class="citation">(Seroul 2012)</span></p>
</blockquote>
<blockquote>
<p>“TeX enabled authors to encode their precise intent into their manuscripts:
This block of text is a computer program, while this word is a keyword in that
program. The language it used, called TeX markup, formalized the slow,
error-prone communication that is normally carried out with the printer over
repeated galley proofs.” <span class="citation">(Apte, n.d.)</span></p>
</blockquote>
<blockquote>
<p>“The idea of writing markup inside text wasn’t especially novel; it has been
used from 1970’s runoff (the UNIX family of printer-preparation utilities) to
today’s HTML tags. TeX was new in that it captured key concepts necessary for
realistic typesetting and formalized them.” <span class="citation">(Apte, n.d.)</span></p>
</blockquote>
<blockquote>
<p>“With these higher-level commands, the free TeX engine, and the LaTeX book,
the use of TeX exploded. The macro file has since evolved and changed names, but
authors still typically run the program called latex or its variants. Hence,
most people who write TeX manuscripts know the program as LaTeX and the commands
they use as LaTeX commands.” <span class="citation">(Apte, n.d.)</span></p>
</blockquote>
<blockquote>
<p>“The effect of LaTeX on scientific and technical publishing has been profound.
Precise typesetting is critical, particularly for conveying concepts using
chemical and mathematical formulas, algorithms, and similar constructs. The
sheer volume of papers, journals, books, and other publications generated in the
modern world is far beyond the throughput possible via manual typesetting. And
TeX enables automation without losing precision. Thanks to LaTeX, book authors
can generate camera-ready copy on their own. Most academic and journal
publishers accept article manuscripts written in LaTeX, and there’s even an open
archive maintained by Cornell University where authors of papers in physics,
chemistry, and other disciplines can directly submit their LaTeX manuscripts for
open viewing. Over 10,000 manuscripts are submitted to this archive every month
from all over the world.” <span class="citation">(Apte, n.d.)</span></p>
</blockquote>
<blockquote>
<p>“For many users, a practical difficulty with typesetting using TeX is
preparing the manuscripts. When TeX was first developed, technical authors were
accustomed to using plain-text editors like WordStar, vi, or Emacs with a
computer keyboard. The idea of marking up their text with commands and running
the manuscript through a typesetting engine felt natural to them. Today’s
typesetters, particularly desktop publishers, have a different mental model.
They expect to see the output in graphical form and then to visually make edits
with a mouse and keyboard, as they would in any WYSIWYG program. They might not
be too picky about the quality of the output, but they appreciate design
capabilities, such as the ability to flow text around curved outlines. Many
print products are now produced with tools like Microsoft Word for this very
reason. TeX authors cannot do the same work as easily.” <span class="citation">(Apte, n.d.)</span></p>
</blockquote>
<blockquote>
<p>“Poor documentation can lead to irreproducibility and serious errors. There’s
a vast amount of lurking complexity in bioinformatics work: complex workflows,
multiple files, countless program parameters, and different software versions.
The best way to prevent this complexity from causing problems is to document
everything extensively. Documentation also makes your life easier when you need to
go back and rerun an analysis, write detailed methods about your steps for a
paper, or find the origin of some data in a directory.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
</div>
<div id="subsection-2" class="section level3">
<h3><span class="header-section-number">3.8.2</span> Subsection 2</h3>
</div>
<div id="applied-exercise" class="section level3">
<h3><span class="header-section-number">3.8.3</span> Applied exercise</h3>

</div>
</div>
<div id="example-creating-a-reproducible-data-pre-processing-protocol" class="section level2">
<h2><span class="header-section-number">3.9</span> Example: Creating a reproducible data pre-processing protocol</h2>
<p>We will walk through an example of creating a reproducible protocol for the
automated gating of flow cytometry data for a project on the immunology of
tuberculosis lead by one of our Co-Is. This data pre-processing protocol was
created using RMarkdown and allows the efficient, transparent, and reproducible
gating of flow cytometry data for all experiments in the research group. We will
walk the trainees through how we developed the protocol initially, the final
pre-processing protocol, how we apply this protocol to new experimental data.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Explain how a reproducible data pre-processing protocol can be integrated into
a real research project</li>
<li>Understand how to design and implement a data pre-processing protocol to
replace manual or point-and-click data pre-processing tools</li>
</ul>
<div id="subsection-1" class="section level3">
<h3><span class="header-section-number">3.9.1</span> Subsection 1</h3>
</div>
<div id="subsection-2" class="section level3">
<h3><span class="header-section-number">3.9.2</span> Subsection 2</h3>
</div>
<div id="practice-quiz" class="section level3">
<h3><span class="header-section-number">3.9.3</span> Practice quiz</h3>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="experimental-data-recording.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/12-scripted_preprocessing.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["improve_repro.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
