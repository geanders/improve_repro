<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="3.7 Introduction to reproducible data pre-processing protocols | Improving the Reproducibility of Experimental Data Recording and Pre-Processing" />
<meta property="og:type" content="book" />


<meta property="og:description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing." />
<meta name="github-repo" content="rstudio/bookdown-demo" />

<meta name="author" content="Brooke Anderson, Michael Lyons, Mercedes Gonzalez-Juarrero, Marcela Henao-Tamayo, and Gregory Robertson" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing.">

<title>3.7 Introduction to reproducible data pre-processing protocols | Improving the Reproducibility of Experimental Data Recording and Pre-Processing</title>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#overview"><span class="toc-section-number">1</span> Overview</a>
<ul>
<li><a href="1-1-license.html#license"><span class="toc-section-number">1.1</span> License</a></li>
</ul></li>
<li><a href="2-experimental-data-recording.html#experimental-data-recording"><span class="toc-section-number">2</span> Experimental Data Recording</a>
<ul>
<li><a href="2-1-separating-data-recording-and-analysis.html#separating-data-recording-and-analysis"><span class="toc-section-number">2.1</span> Separating data recording and analysis</a></li>
<li><a href="2-2-principles-and-power-of-structured-data-formats.html#principles-and-power-of-structured-data-formats"><span class="toc-section-number">2.2</span> Principles and power of structured data formats</a></li>
<li><a href="2-3-the-tidy-data-format.html#the-tidy-data-format"><span class="toc-section-number">2.3</span> The ‘tidy’ data format</a></li>
<li><a href="2-4-designing-templates-for-tidy-data-collection.html#designing-templates-for-tidy-data-collection"><span class="toc-section-number">2.4</span> Designing templates for “tidy” data collection</a></li>
<li><a href="2-5-example-creating-a-template-for-tidy-data-collection.html#example-creating-a-template-for-tidy-data-collection"><span class="toc-section-number">2.5</span> Example: Creating a template for “tidy” data collection</a></li>
<li><a href="2-6-power-of-using-a-single-structured-project-directory-for-storing-and-tracking-research-project-files.html#power-of-using-a-single-structured-project-directory-for-storing-and-tracking-research-project-files"><span class="toc-section-number">2.6</span> Power of using a single structured ‘Project’ directory for storing and tracking research project files</a></li>
<li><a href="2-7-creating-project-templates.html#creating-project-templates"><span class="toc-section-number">2.7</span> Creating ‘Project’ templates</a></li>
<li><a href="2-8-example-creating-a-project-template.html#example-creating-a-project-template"><span class="toc-section-number">2.8</span> Example: Creating a ‘Project’ template</a></li>
<li><a href="2-9-harnessing-version-control-for-transparent-data-recording.html#harnessing-version-control-for-transparent-data-recording"><span class="toc-section-number">2.9</span> Harnessing version control for transparent data recording</a></li>
<li><a href="2-10-enhance-the-reproducibility-of-collaborative-research-with-version-control-platforms.html#enhance-the-reproducibility-of-collaborative-research-with-version-control-platforms"><span class="toc-section-number">2.10</span> Enhance the reproducibility of collaborative research with version control platforms</a></li>
<li><a href="2-11-using-git-and-gitlab-to-implement-version-control.html#using-git-and-gitlab-to-implement-version-control"><span class="toc-section-number">2.11</span> Using git and GitLab to implement version control</a></li>
</ul></li>
<li><a href="3-experimental-data-preprocessing.html#experimental-data-preprocessing"><span class="toc-section-number">3</span> Experimental Data Preprocessing</a>
<ul>
<li><a href="3-1-principles-and-benefits-of-scripted-pre-processing-of-experimental-data.html#principles-and-benefits-of-scripted-pre-processing-of-experimental-data"><span class="toc-section-number">3.1</span> Principles and benefits of scripted pre-processing of experimental data</a></li>
<li><a href="3-2-introduction-to-scripted-data-pre-processing-in-r.html#introduction-to-scripted-data-pre-processing-in-r"><span class="toc-section-number">3.2</span> Introduction to scripted data pre-processing in R</a></li>
<li><a href="3-3-simplify-scripted-pre-processing-through-rs-tidyverse-tools.html#simplify-scripted-pre-processing-through-rs-tidyverse-tools"><span class="toc-section-number">3.3</span> Simplify scripted pre-processing through R’s ‘tidyverse’ tools</a></li>
<li><a href="3-4-complex-data-types-in-experimental-data-pre-processing.html#complex-data-types-in-experimental-data-pre-processing"><span class="toc-section-number">3.4</span> Complex data types in experimental data pre-processing</a></li>
<li><a href="3-5-complex-data-types-in-r-and-bioconductor.html#complex-data-types-in-r-and-bioconductor"><span class="toc-section-number">3.5</span> Complex data types in R and Bioconductor</a></li>
<li><a href="3-6-example-converting-from-complex-to-tidy-data-formats.html#example-converting-from-complex-to-tidy-data-formats"><span class="toc-section-number">3.6</span> Example: Converting from complex to ‘tidy’ data formats</a></li>
<li><a href="3-7-introduction-to-reproducible-data-pre-processing-protocols.html#introduction-to-reproducible-data-pre-processing-protocols"><span class="toc-section-number">3.7</span> Introduction to reproducible data pre-processing protocols</a></li>
<li><a href="3-8-from-pre-processing-scripts-to-pre-processing-protocols.html#from-pre-processing-scripts-to-pre-processing-protocols"><span class="toc-section-number">3.8</span> From pre-processing scripts to pre-processing protocols</a></li>
<li><a href="3-9-rmarkdown-for-creating-reproducible-data-pre-processing-protocols.html#rmarkdown-for-creating-reproducible-data-pre-processing-protocols"><span class="toc-section-number">3.9</span> RMarkdown for creating reproducible data pre-processing protocols</a></li>
<li><a href="3-10-example-creating-a-reproducible-data-pre-processing-protocol.html#example-creating-a-reproducible-data-pre-processing-protocol"><span class="toc-section-number">3.10</span> Example: Creating a reproducible data pre-processing protocol</a></li>
</ul></li>
<li><a href="4-references.html#references"><span class="toc-section-number">4</span> References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="introduction-to-reproducible-data-pre-processing-protocols" class="section level2" number="3.7">
<h2><span class="header-section-number">3.7</span> Introduction to reproducible data pre-processing protocols</h2>
<p>Reproducibility tools can be used to create reproducible data pre-processing
protocols—documents that combine code and text in a ‘knitted’ document, which
can be re-used to ensure data pre-processing is consistent and reproducible
across research projects. In this module, we will describe how reproducible data
pre-processing protocols can improve reproducibility of pre-processing
experimental data, as well as to ensure transparency, consistency, and
reproducibility across the research projects conducted by a research team.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Define a ‘reproducible data pre-processing protocol’</li>
<li>Explain how such protocols improve reproducibility at the data pre-processing
phase</li>
<li>List other benefits, including improving efficiency and consistency of data
pre-processing</li>
</ul>
<div id="introducing-reproducible-data-pre-processing-protocols" class="section level3" number="3.7.1">
<h3><span class="header-section-number">3.7.1</span> Introducing reproducible data pre-processing protocols</h3>
<p><strong>Data pre-processing</strong></p>
<p>When we take measurements of experimental samples, we do so with the goal of
using the data we collect to gain scientific knowledge. The data are direct
measurement of something, but need to be interpreted to gain knowledge.
Sometimes direct measurements line up very closely with a research
question—for example if you are conducting a study that investigates the
mortality status of each test subject then whether or not each subject to dies
is a data point that is directly related to the research question you are aiming
to answer. In this case these data may go directly into a statistical analysis
model without extensive pre-processing. However, there are often cases where we
collect data that are not as immediately linked to the scientific question.
Instead, these data may require pre-processing before they can be used to test
meaningful scientific hypotheses. This is often the case for data extracted
using complex equipment. Equipment like mass spectrometers and flow cytometers
leverage physics, chemistry, and biology in clever ways to help us derive more
information from samples, but one tradeoff is that the data from such equipment
often require a bit of work to move into a format that is useful for answering
scientific questions.</p>
<p>One example if the data collected through liquid chromatography-mass
spectrometry (LC-MS). This is a powerful and useful technique for chemical
analysis, including analysis of biochemical molecules like metabolites and
proteins. However, when using this technique, the raw data require extensive
pre-processing before they can be used to answer scientific questions.</p>
<p>First, the data that are output by the mass spectrometer are often stored in a
specialized file format, like a netCDF or mzML file format. While these file
formats are standardized, they are likely formats you don’t regularly use in
other contexts, and so you may need to find special tools to read the data into
programs to analyze it. In some cases, the data are very large, and so it may be
necessary to use analysis tools that allow most of the data to stay “on disk”
while you analyze it, bringing only small parts into your analysis software at a
time.</p>
<p>Once the data are read in, they must be pre-processed in a number of ways. For
example, these data can be translated into features that are linked to the
chemical composition of the sample, with each feature showing up as a “peak” in
the data that are output from the mass spectrometer. A peak can be linked to a
specific metabolite feature based on its mass-to-charge ratio (m/z) and its
retention time. However, the exact retention time for a metabolite feature may
vary a bit from sample to sample. Pre-processing is required both to identify
peaks in the data and also to align the peaks from the same metabolite feature
across all samples from your experiment. There may also be technical bias across
samples, resulting in differences in the typical expression levels of all peaks
from one sample to the next. For example it may be the case that all intensities
measured for one sample tend to be higher than for another sample because of
technical bias in terms of the settings used for the equipment when the two
samples were run. These biases must also be corrected through pre-processing
before you can use the data within statistical tests or models to explore scientific hypotheses.</p>
<p>[Image of identifying and aligning peaks in LC-MS data]</p>
<p>In the research process, these pre-processing steps should be done before
the data are used for further analysis. There are the first step in working
with the data after they are collected by the equipment (or by laboratory
personal, in the case of data from simpler process, like plating samples
and counting colony-forming units). After the data are appropriately
pre-processed, you can use them for statistical tests—for example, to
determine if metabolite profiles are different between experimental groups—and
also combine them with other data collected from the experiment—for example,
to see whether certain metabolite levels are correlated with the bacterial
load in a sample.</p>
<p><strong>Approaches for pre-processing data.</strong></p>
<p>There are two main approaches for pre-processing experimental data in this
way. First, when data are the output of complex laboratory equipment, there
will often be proprietary software that is available for this pre-processing.
This software may be created by the same company that made the equipment, or
it may be created and sold by other companies. The interface will typically
be a graphical-user interface (GUI), where you will use pull-down menus and
point-and-click interfaces to work through the pre-processing steps. You
often will be able to export a pre-processed version of the data in a
common file format, like a delimited file or an Excel file, and that version
of the data can then be read into more general data analysis software, like
Excel or R.</p>
<p>[Include a screenshot of this type of software in action.]</p>
<p>The second approach is to conduct the pre-processing directly within general
data analysis software like R or Python. These programs are both open-source,
and include extensions that were created and shared by users around the world.
Through these extensions, there are often powerful tools that you can use to
pre-process complex experimental data. In fact, the algorithms used in
proprietary software are sometimes extended from algorithms first shared through
R or Python. With this approach, you will read the data into the program (R,
for example) directly from the file output from the equipment. You can
record all the code that you use to read in and pre-process the data in a
code script, allowing you to reproduce this pre-processing work. You can
also go a step further, and incorporate your code into a pre-processing
protocol, which combines nicely formatted text with executable code, and
which we’ll describe in much more detail later in this module and in the
following two modules.</p>
<p>There are advantages to taking the second approach—using scripted code in an
open-source program—rather than the first—using proprietary software with a
GUI interface. The use of codes scripts ensures that the steps of pre-processing
are reproducible. This means both that you will be able to re-do all the steps
yourself in the future, if you need to, but that also that other researchers can
explore and replicate what you do. You may want to share your process with
others in your laboratory group, for example, so they can understand the choices
you made and steps you took in pre-processing the data. You may also want to
share the process with readers of the articles you publish, and this may in fact
be required by the journal. Further, the use of a code script encourages you to
document this code and this process, even moreso when you move beyond a script
and include the code in a reproducible pre-processing protocol. Well-documented
code makes it much easier to write up the method section later in manuscripts
that leveraged the data collected in the experiment.</p>
<p>Also, when you use scripted code to pre-process biomedical data, you will find
that the same script can often be easily adapted and re-used in later projects
that use the same type of data. You may need to change small elements, like the
file names of files with data you want to use, or some details about the methods
used for certain pre-processing steps. However, often almost all of the
pre-processing steps will repeat over different experiments that you do. By
extending to write a pre-processing protocol, you can further support the
ease of adapting and re-using the pre-processing steps you take with one
experiment when you run later experiments that are similar.</p>
</div>
</div>
<p style="text-align: center;">
<a href="3-6-example-converting-from-complex-to-tidy-data-formats.html"><button class="btn btn-default">Previous</button></a>
<a href="3-8-from-pre-processing-scripts-to-pre-processing-protocols.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
