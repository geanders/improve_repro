<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Module 2 Experimental Data Recording | Improving the Reproducibility of Experimental Data Recording and Pre-Processing</title>
  <meta name="description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing." />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="Module 2 Experimental Data Recording | Improving the Reproducibility of Experimental Data Recording and Pre-Processing" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Module 2 Experimental Data Recording | Improving the Reproducibility of Experimental Data Recording and Pre-Processing" />
  
  <meta name="twitter:description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing." />
  

<meta name="author" content="Brooke Anderson, Michael Lyons, Mercedes Gonzalez-Juarrero, Marcela Henao-Tamayo, and Gregory Robertson" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="experimental-data-pre-processing.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Visualization in R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Overview of these modules</a>
<ul>
<li class="chapter" data-level="1.0.1" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i><b>1.0.1</b> License</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html"><i class="fa fa-check"></i><b>2</b> Experimental Data Recording</a>
<ul>
<li class="chapter" data-level="2.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module1"><i class="fa fa-check"></i><b>2.1</b> Separating data recording and analysis</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#popularity-of-spreadsheets"><i class="fa fa-check"></i><b>2.1.1</b> Popularity of spreadsheets</a></li>
<li class="chapter" data-level="2.1.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#hazards-of-combining-recording-and-analysis"><i class="fa fa-check"></i><b>2.1.2</b> Hazards of combining recording and analysis</a></li>
<li class="chapter" data-level="2.1.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#approaches-to-separate-recording-and-analysis"><i class="fa fa-check"></i><b>2.1.3</b> Approaches to separate recording and analysis</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module2"><i class="fa fa-check"></i><b>2.2</b> Principles and power of structured data formats</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#data-recording-standards"><i class="fa fa-check"></i><b>2.2.1</b> Data recording standards</a></li>
<li class="chapter" data-level="2.2.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#elements-of-a-data-recording-standard"><i class="fa fa-check"></i><b>2.2.2</b> Elements of a data recording standard</a></li>
<li class="chapter" data-level="2.2.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#defining-data-recording-standards-for-data-recorded-by-hand"><i class="fa fa-check"></i><b>2.2.3</b> Defining data recording standards for data recorded “by hand”</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module3"><i class="fa fa-check"></i><b>2.3</b> The “tidy” data format</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#keeping-things-tidy"><i class="fa fa-check"></i><b>2.3.1</b> Keeping things tidy</a></li>
<li class="chapter" data-level="2.3.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#what-makes-data-tidy"><i class="fa fa-check"></i><b>2.3.2</b> What makes data “tidy”?</a></li>
<li class="chapter" data-level="2.3.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#why-make-your-data-tidy"><i class="fa fa-check"></i><b>2.3.3</b> Why make your data tidy?</a></li>
<li class="chapter" data-level="2.3.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#using-tidyverse-tools-with-data-in-the-tidy-data-format"><i class="fa fa-check"></i><b>2.3.4</b> Using tidyverse tools with data in the tidy data format</a></li>
<li class="chapter" data-level="2.3.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#discussion-questions"><i class="fa fa-check"></i><b>2.3.5</b> Discussion questions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module4"><i class="fa fa-check"></i><b>2.4</b> Designing templates for “tidy” data collection</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#exampledata-on-rate-of-bacterial-growth"><i class="fa fa-check"></i><b>2.4.1</b> Example—Data on rate of bacterial growth</a></li>
<li class="chapter" data-level="2.4.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#features-that-make-data-collection-templates-untidy"><i class="fa fa-check"></i><b>2.4.2</b> Features that make data collection templates untidy</a></li>
<li class="chapter" data-level="2.4.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#converting-to-a-tidier-format-for-data-collection-templates"><i class="fa fa-check"></i><b>2.4.3</b> Converting to a “tidier” format for data collection templates</a></li>
<li class="chapter" data-level="2.4.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#learning-more-about-tidy-data-collection-in-the-laboratory"><i class="fa fa-check"></i><b>2.4.4</b> Learning more about tidy data collection in the laboratory</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module5"><i class="fa fa-check"></i><b>2.5</b> Example: Creating a template for “tidy” data collection</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#example-datadata-on-rate-of-bacterial-growth"><i class="fa fa-check"></i><b>2.5.1</b> Example data—Data on rate of bacterial growth</a></li>
<li class="chapter" data-level="2.5.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#limiting-the-template-to-the-collection-of-data"><i class="fa fa-check"></i><b>2.5.2</b> Limiting the template to the collection of data</a></li>
<li class="chapter" data-level="2.5.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#making-sensible-choices-about-rows-and-columns"><i class="fa fa-check"></i><b>2.5.3</b> Making sensible choices about rows and columns</a></li>
<li class="chapter" data-level="2.5.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#avoiding-problematic-characters-or-formatting"><i class="fa fa-check"></i><b>2.5.4</b> Avoiding problematic characters or formatting</a></li>
<li class="chapter" data-level="2.5.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#separating-data-analysis-from-data-collection"><i class="fa fa-check"></i><b>2.5.5</b> Separating data analysis from data collection</a></li>
<li class="chapter" data-level="2.5.6" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#applied-exercise"><i class="fa fa-check"></i><b>2.5.6</b> Applied exercise</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module6"><i class="fa fa-check"></i><b>2.6</b> Organizing project files</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#advantages-of-organizing-project-files"><i class="fa fa-check"></i><b>2.6.1</b> Advantages of organizing project files</a></li>
<li class="chapter" data-level="2.6.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#how-to-organize-project-files"><i class="fa fa-check"></i><b>2.6.2</b> How to organize project files</a></li>
<li class="chapter" data-level="2.6.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#what-is-a-project-directory-template"><i class="fa fa-check"></i><b>2.6.3</b> What is a project directory template?</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module7"><i class="fa fa-check"></i><b>2.7</b> Creating project directory templates</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#goals-in-designing-a-project-template"><i class="fa fa-check"></i><b>2.7.1</b> Goals in designing a project template</a></li>
<li class="chapter" data-level="2.7.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#steps-in-designing-the-conceptual-blueprint-for-a-project-directory-template"><i class="fa fa-check"></i><b>2.7.2</b> Steps in designing the conceptual blueprint for a project directory template</a></li>
<li class="chapter" data-level="2.7.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#creating-and-using-a-project-template"><i class="fa fa-check"></i><b>2.7.3</b> Creating and using a project template</a></li>
<li class="chapter" data-level="2.7.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#project-directories-as-rstudio-projects"><i class="fa fa-check"></i><b>2.7.4</b> Project directories as RStudio Projects</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module8"><i class="fa fa-check"></i><b>2.8</b> Example: Creating a project template</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#description-of-the-example-set-of-studies"><i class="fa fa-check"></i><b>2.8.1</b> Description of the example set of studies</a></li>
<li class="chapter" data-level="2.8.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#step-1-survey-of-data-collected-for-the-projects"><i class="fa fa-check"></i><b>2.8.2</b> Step 1: Survey of data collected for the projects</a></li>
<li class="chapter" data-level="2.8.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#step-2-organizing-a-project-directory"><i class="fa fa-check"></i><b>2.8.3</b> Step 2: Organizing a project directory</a></li>
<li class="chapter" data-level="2.8.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#step-3-establishing-file-name-conventions"><i class="fa fa-check"></i><b>2.8.4</b> Step 3: Establishing file name conventions</a></li>
<li class="chapter" data-level="2.8.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#step-4-designing-data-collection-templates"><i class="fa fa-check"></i><b>2.8.5</b> Step 4: Designing data collection templates</a></li>
<li class="chapter" data-level="2.8.6" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#step-5-designing-a-report-template"><i class="fa fa-check"></i><b>2.8.6</b> Step 5: Designing a report template</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module9"><i class="fa fa-check"></i><b>2.9</b> Harnessing version control for transparent data recording</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#challenges-of-collaborating-on-evolving-research-materials"><i class="fa fa-check"></i><b>2.9.1</b> Challenges of collaborating on evolving research materials</a></li>
<li class="chapter" data-level="2.9.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#recording-data-in-the-laboratoryfrom-paper-to-computers"><i class="fa fa-check"></i><b>2.9.2</b> Recording data in the laboratory—from paper to computers</a></li>
<li class="chapter" data-level="2.9.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#defining-version-and-version-control"><i class="fa fa-check"></i><b>2.9.3</b> Defining “version” and “version control”</a></li>
<li class="chapter" data-level="2.9.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#what-are-the-key-elements-of-version-control"><i class="fa fa-check"></i><b>2.9.4</b> What are the key elements of version control?</a></li>
<li class="chapter" data-level="2.9.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#comparing-git-to-other-tools"><i class="fa fa-check"></i><b>2.9.5</b> Comparing Git to other tools</a></li>
<li class="chapter" data-level="2.9.6" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#discussion-questions-1"><i class="fa fa-check"></i><b>2.9.6</b> Discussion questions</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module10"><i class="fa fa-check"></i><b>2.10</b> Enhance the reproducibility of collaborative research with version control platforms</a>
<ul>
<li class="chapter" data-level="2.10.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#what-are-version-control-platforms"><i class="fa fa-check"></i><b>2.10.1</b> What are version control platforms?</a></li>
<li class="chapter" data-level="2.10.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#why-use-version-control-platforms"><i class="fa fa-check"></i><b>2.10.2</b> Why use version control platforms?</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module11"><i class="fa fa-check"></i><b>2.11</b> Using Git and GitHub to implement version control</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#leveraging-git-and-github-as-a-non-coder"><i class="fa fa-check"></i><b>2.11.1</b> Leveraging Git and GitHub as a non-coder</a></li>
<li class="chapter" data-level="2.11.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#leveraging-git-and-github-as-a-scientist-who-programs"><i class="fa fa-check"></i><b>2.11.2</b> Leveraging Git and GitHub as a scientist who programs</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html"><i class="fa fa-check"></i><b>3</b> Experimental Data Pre-processing</a>
<ul>
<li class="chapter" data-level="3.1" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#module12"><i class="fa fa-check"></i><b>3.1</b> Principles of pre-processing experimental data</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#what-is-data-pre-processing"><i class="fa fa-check"></i><b>3.1.1</b> What is data pre-processing?</a></li>
<li class="chapter" data-level="3.1.2" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#common-themes-and-processes-in-data-pre-processing"><i class="fa fa-check"></i><b>3.1.2</b> Common themes and processes in data pre-processing</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#module12a"><i class="fa fa-check"></i><b>3.2</b> Selecting software options for pre-processing</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#gui-based-software-versus-script-based-software"><i class="fa fa-check"></i><b>3.2.1</b> GUI-based software versus script-based software</a></li>
<li class="chapter" data-level="3.2.2" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#open-source-versus-proprietary-software"><i class="fa fa-check"></i><b>3.2.2</b> Open-source versus proprietary software</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#module13"><i class="fa fa-check"></i><b>3.3</b> Introduction to scripted data pre-processing in R</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#what-is-a-code-script"><i class="fa fa-check"></i><b>3.3.1</b> What is a code script?</a></li>
<li class="chapter" data-level="3.3.2" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#how-code-scripts-improve-reproducibility-of-pre-processing"><i class="fa fa-check"></i><b>3.3.2</b> How code scripts improve reproducibility of pre-processing</a></li>
<li class="chapter" data-level="3.3.3" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#how-to-write-an-r-code-script"><i class="fa fa-check"></i><b>3.3.3</b> How to write an R code script</a></li>
<li class="chapter" data-level="3.3.4" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#how-to-run-code-in-an-r-script"><i class="fa fa-check"></i><b>3.3.4</b> How to run code in an R script</a></li>
<li class="chapter" data-level="3.3.5" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#exercise"><i class="fa fa-check"></i><b>3.3.5</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#module13a"><i class="fa fa-check"></i><b>3.4</b> Tips for improving reproducibility when writing R scripts</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#write-code-for-computers-but-edit-it-for-humans"><i class="fa fa-check"></i><b>3.4.1</b> Write code for computers, but edit it for humans</a></li>
<li class="chapter" data-level="3.4.2" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#modify-rather-than-start-from-scratch"><i class="fa fa-check"></i><b>3.4.2</b> Modify rather than start from scratch</a></li>
<li class="chapter" data-level="3.4.3" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#do-not-repeat-yourself"><i class="fa fa-check"></i><b>3.4.3</b> Do not repeat yourself</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#module14"><i class="fa fa-check"></i><b>3.5</b> Simplify scripted pre-processing through R’s ‘tidyverse’ tools</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#tools-for-data-input"><i class="fa fa-check"></i><b>3.5.1</b> Tools for data input</a></li>
<li class="chapter" data-level="3.5.2" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#tools-for-changing-or-creating-columns"><i class="fa fa-check"></i><b>3.5.2</b> Tools for changing or creating columns</a></li>
<li class="chapter" data-level="3.5.3" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#tools-for-working-with-character-strings"><i class="fa fa-check"></i><b>3.5.3</b> Tools for working with character strings</a></li>
<li class="chapter" data-level="3.5.4" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#tools-for-working-with-dates-and-times"><i class="fa fa-check"></i><b>3.5.4</b> Tools for working with dates and times</a></li>
<li class="chapter" data-level="3.5.5" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#tools-for-statistical-modeling"><i class="fa fa-check"></i><b>3.5.5</b> Tools for statistical modeling</a></li>
<li class="chapter" data-level="3.5.6" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#resources-to-learn-more-on-tidyverse-tools"><i class="fa fa-check"></i><b>3.5.6</b> Resources to learn more on tidyverse tools</a></li>
<li class="chapter" data-level="3.5.7" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#practice-quiz"><i class="fa fa-check"></i><b>3.5.7</b> Practice quiz</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#module15"><i class="fa fa-check"></i><b>3.6</b> Complex data types in experimental data pre-processing</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#how-the-bioconductor-and-tidyverse-approaches-differ"><i class="fa fa-check"></i><b>3.6.1</b> How the Bioconductor and tidyverse approaches differ</a></li>
<li class="chapter" data-level="3.6.2" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#why-is-the-bioconductor-approach-designed-as-it-is"><i class="fa fa-check"></i><b>3.6.2</b> Why is the Bioconductor approach designed as it is?</a></li>
<li class="chapter" data-level="3.6.3" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#why-is-it-sometimes-necessary-to-use-a-bioconductor-approach-with-biomedical-data"><i class="fa fa-check"></i><b>3.6.3</b> Why is it sometimes necessary to use a Bioconductor approach with biomedical data</a></li>
<li class="chapter" data-level="3.6.4" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#navigating-bioconductor-packages-and-data-structures"><i class="fa fa-check"></i><b>3.6.4</b> Navigating Bioconductor packages and data structures</a></li>
<li class="chapter" data-level="3.6.5" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#combining-bioconductor-and-tidyverse-approaches-in-a-workflow"><i class="fa fa-check"></i><b>3.6.5</b> Combining Bioconductor and tidyverse approaches in a workflow</a></li>
<li class="chapter" data-level="3.6.6" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#outlook-for-a-tidyverse-approach-to-biomedical-data"><i class="fa fa-check"></i><b>3.6.6</b> Outlook for a tidyverse approach to biomedical data</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#module18"><i class="fa fa-check"></i><b>3.7</b> Introduction to reproducible data pre-processing protocols</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#introducing-reproducible-data-pre-processing-protocols"><i class="fa fa-check"></i><b>3.7.1</b> Introducing reproducible data pre-processing protocols</a></li>
<li class="chapter" data-level="3.7.2" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#using-knitted-documents-for-protocols"><i class="fa fa-check"></i><b>3.7.2</b> Using knitted documents for protocols</a></li>
<li class="chapter" data-level="3.7.3" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#advantages-of-using-knitted-documents-for-data-focused-protocols"><i class="fa fa-check"></i><b>3.7.3</b> Advantages of using knitted documents for data-focused protocols</a></li>
<li class="chapter" data-level="3.7.4" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#how-knitted-documents-work"><i class="fa fa-check"></i><b>3.7.4</b> How knitted documents work</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#module19"><i class="fa fa-check"></i><b>3.8</b> RMarkdown for creating reproducible data pre-processing protocols</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#creating-knitted-documents-in-r"><i class="fa fa-check"></i><b>3.8.1</b> Creating knitted documents in R</a></li>
<li class="chapter" data-level="3.8.2" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#formatting-text-with-markdown-in-rmarkdown"><i class="fa fa-check"></i><b>3.8.2</b> Formatting text with Markdown in Rmarkdown</a></li>
<li class="chapter" data-level="3.8.3" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#preambles-in-rmarkdown-documents"><i class="fa fa-check"></i><b>3.8.3</b> Preambles in Rmarkdown documents</a></li>
<li class="chapter" data-level="3.8.4" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#executable-code-in-rmarkdown-files"><i class="fa fa-check"></i><b>3.8.4</b> Executable code in Rmarkdown files</a></li>
<li class="chapter" data-level="3.8.5" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#more-advanced-rmarkdown-functionality"><i class="fa fa-check"></i><b>3.8.5</b> More advanced Rmarkdown functionality</a></li>
<li class="chapter" data-level="3.8.6" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#learning-more-about-rmarkdown."><i class="fa fa-check"></i><b>3.8.6</b> Learning more about Rmarkdown.</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#module20"><i class="fa fa-check"></i><b>3.9</b> Example: Creating a reproducible data pre-processing protocol</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#introduction-and-example-data"><i class="fa fa-check"></i><b>3.9.1</b> Introduction and example data</a></li>
<li class="chapter" data-level="3.9.2" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#advice-on-designing-a-pre-processing-protocol"><i class="fa fa-check"></i><b>3.9.2</b> Advice on designing a pre-processing protocol</a></li>
<li class="chapter" data-level="3.9.3" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#writing-data-pre-processing-protocols"><i class="fa fa-check"></i><b>3.9.3</b> Writing data pre-processing protocols</a></li>
<li class="chapter" data-level="3.9.4" data-path="experimental-data-pre-processing.html"><a href="experimental-data-pre-processing.html#applied-exercise-1"><i class="fa fa-check"></i><b>3.9.4</b> Applied exercise</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Improving the Reproducibility of Experimental Data Recording and Pre-Processing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="experimental-data-recording" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Module 2</span> Experimental Data Recording<a href="experimental-data-recording.html#experimental-data-recording" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This section includes modules on:</p>
<ul>
<li><a href="experimental-data-recording.html#module1">Module 2.1: Separating data recording and analysis</a></li>
<li><a href="experimental-data-recording.html#module2">Module 2.2: Principles and power of structured data formats</a></li>
<li><a href="experimental-data-recording.html#module3">Module 2.3: The “tidy” data format</a></li>
<li><a href="experimental-data-recording.html#module4">Module 2.4: Designing templates for “tidy” data collection</a></li>
<li><a href="experimental-data-recording.html#module5">Module 2.5: Example: Creating a template for “tidy” data collection</a></li>
<li><a href="experimental-data-recording.html#module6">Module 2.6: Organizing project files</a></li>
<li><a href="experimental-data-recording.html#module7">Module 2.7: Creating project directory templates</a></li>
<li><a href="experimental-data-recording.html#module8">Module 2.8: Example: Creating a project template</a></li>
<li><a href="experimental-data-recording.html#module9">Module 2.9: Harnessing version control for transparent data recording</a></li>
<li><a href="experimental-data-recording.html#module10">Module 2.10: Enhance the reproducibility of collaborative research with version control platforms</a></li>
<li><a href="experimental-data-recording.html#module11">Module 2.11: Using Git and GitHub to implement version control</a></li>
</ul>
<div id="module1" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Separating data recording and analysis<a href="experimental-data-recording.html#module1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Do you use spreadsheets within your scientific work? If so, you’re not alone. In
fact, studies have surveyed scientists about their work practices, and they’ve
found that spreadsheets are a common tool. Examples include surveys of over 250
biomedical researchers at the University of Washington <span class="citation">(Anderson et al. 2007)</span>,
and of neuroscience researchers at the University of Newcastle. In both these
studies, most respondents reported that they used spreadsheets and other
general-purpose software in their research <span class="citation">(AlTarawneh and Thorne 2017)</span>. A working
group on bioinformatics and data-intensive science similarly found spreadsheets
were the most common tool used across attendees <span class="citation">(Barga et al. 2011)</span>.</p>
<p>These software tools, such as Microsoft Excel or Google Sheets, provide for
manual or automated entry of data into rows and columns of cells. Standard or
custom formulas and other operations can be applied to the cells, and are
commonly used to reformat or clean the data, calculate various statistics, and
to generate simple plots; all of which are embedded as additional data entries
and programming elements within the spreadsheet. While these tools greatly
improved the paper worksheets on which they were originally based
<span class="citation">(Campbell-Kelly 2007)</span>, this all-in-one practice impedes the transparency and
reproducibility of both recording and analysis of the large and complex data
sets that are routinely generated in life science experiments.</p>
<p>To improve the computational reproducibility of a research project, it is
critical for biomedical researchers to learn the importance of maintaining
recorded experimental data as “read-only” files, separating data recording from
any data pre-processing or data analysis steps <span class="citation">(Broman and Woo 2018; Marwick, Boettiger, and Mullen 2018)</span>.</p>
<p>In this module, we’ll talk about why spreadsheets are so popular, as well
as some of their features that are beneficial for researchers. However, there
are also many problems they can introduce, particularly when spreadsheets are
used in a way that combines data collection with data pre-processing and analysis.
We’ll walk through some of these problems, and in later modules we’ll walk you
through alternatives, where spreadsheets are limited to recording data (if they’re
used at all), while steps of pre-processing and analysis are done with other
tools.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Explain why spreadsheets are a popular tool among scientists</li>
<li>Explain the difference between data recording and data analysis</li>
<li>Understand why collecting data on spreadsheets with embedded formulas impedes
reproducibility</li>
<li>Compare ways that spreadsheets can be used as a solid research tool
versus other ways their use can be problematic</li>
<li>Discuss downsides to using spreadsheets for data analysis in scientific
research</li>
</ul>
<div id="popularity-of-spreadsheets" class="section level3 hasAnchor" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Popularity of spreadsheets<a href="experimental-data-recording.html#popularity-of-spreadsheets" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>All of us authors are old enough to remember when home computers were a novelty.
When you first got a computer in your home, it opened up all kinds of new powers.</p>
<p>For one of us, one particularly exciting piece of software was something called
<em>The Print Shop</em>. This software let you be an amateur graphic designer. You could
design things like signs and invitations. Because the printer paper at the time
was connected from one sheet to the next, with perforations between, you could
even make long banners. “Happy Birthday” banners, “Congratulations” banners,
“Welcome Home” banners: you could do it all. For someone who’d never had these
tools before, it was thrilling.</p>
<p>This was evidently how early spreadsheet software made business executives feel.
Before these programs, if an executive wanted to crunch some numbers, they’d
have to send a request to their accounting department. The initial spreadsheet
program (VisiCalc) disrupted this process. It allowed one person to quickly
apply and test different models or calculations on recorded data
<span class="citation">(Levy 1984)</span>. These spreadsheet programs allowed non-programmers to
engage with data, including data processing and analysis tasks, in a way that
previously required programming expertise <span class="citation">(Levy 1984)</span>. With
spreadsheet programs an executive could just play with the numbers themselves.</p>
<p>Because an early target for spreadsheet programs was these business
executives, the programs were designed to be very simple and easy to use—just
one step up in complexity from crunching numbers on the back of an envelope
<span class="citation">(Campbell-Kelly 2007)</span>.Spreadsheet programs in fact became so popular within
businesses that many attribute these programs with driving the uptake of
personal computers <span class="citation">(Campbell-Kelly 2007)</span>.</p>
<p>Spreadsheets have become so popular in part because so many people know how to
use them, at least in basic ways, and so many people have the software on their
computers that files can be shared with almost a guarantee that everyone will be
able to open the file on their own computer <span class="citation">(Hermans et al. 2016)</span>.
Spreadsheets use the visual metaphore of a traditional gridded ledger sheet
<span class="citation">(Levy 1984)</span>, providing an interface that is easy for users to
immediately understand and for which they can easily create a mental map
<span class="citation">(Birch, Lyford-Smith, and Guo 2018; Barga et al. 2011)</span>. This visually clear interface also means that
spreadsheets can be printed or incorporated into other documents “as-is”, as a
workable and understandable table of data values. In fact, some of the most
popular plug-in software packages for the early spreadsheet program Lotus 1-2-3
were programs for printing and publishing spreadsheets <span class="citation">(Campbell-Kelly 2007)</span>.
This “What You See Is What You Get” interface was a huge advance from previous
methods of data analysis for the first spreadsheet program, VisiCalc, providing
a “window to the data” that was accessible to business executives and others
without programming expertise <span class="citation">(Creeth 1985)</span>. Several surveys of
researchers have found that spreadsheets were popular because of their
simplicity and ease-of-use <span class="citation">(Anderson et al. 2007; AlTarawneh and Thorne 2017; Barga et al. 2011)</span>. By contrast, databases and scripted programming
languages can be perceived as requiring a cognitive load and lengthy training
that is not worth the investment when an easier tool is available
<span class="citation">(Hermans et al. 2016; Anderson et al. 2007; Myneni and Patel 2010; Barga et al. 2011; Topaloglou et al. 2004)</span>.</p>
<p>Software tools like The Print Shop and spreadsheet programs were perfectly
designed for amateurs to begin to do some of the things that otherwise required
outsourcing to a professional. They are fantastic tools for amateur exploration.
They make it fun to test out ideas.</p>
<p>However, these types of software tools are so easy and convenient to use that it
can be tempting to let them replace more solid, production-level tools. It’s
easy, in other words, to make them the <em>only</em> tool used to tackle a problem,
rather than just the <em>first</em> tool to use to explore a solution. These tools are
also often the cheapest option, either in monetary cost or in the time
investment to learn them. However, they often fail when they’re used as a
replacement for more solid options. This can be the case with spreadsheet
programs in biomedical research, where spreadsheets are often used not only as a
straightforward way to record data (for which they can be a very solid tool),
but also to develop complex pipelines that process and analyze the data once its
been collected.</p>
</div>
<div id="hazards-of-combining-recording-and-analysis" class="section level3 hasAnchor" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Hazards of combining recording and analysis<a href="experimental-data-recording.html#hazards-of-combining-recording-and-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In some cases, researchers use spreadsheets solely to record data, as a simple
type of database <span class="citation">(Birch, Lyford-Smith, and Guo 2018)</span>. However, biomedical researchers often use
spreadsheets to both record and analyze experimental data <span class="citation">(Anderson et al. 2007; Broman and Woo 2018)</span>. In this case, data processing and analysis is implemented
through the use of formulas and macros embedded within the spreadsheet.</p>
<p>When a spreadsheet has formulas or macros within it, the spreadsheet program
creates an internal record of how cells are connected through these formulas.
For example, say the value in a specific cell is converted from Fahrenheit to
Celsius to fill a second cell, and then that value is combined with other values
in a column to calculate the mean temperature across several observations. In
this case, the spreadsheet program has internally saved how the later cells
depend on the earlier ones. When you change the value recorded in a cell of a
spreadsheet, the spreadsheet program queries this record and only recalculates
the cells that depend on that cell. This process allows the program to quickly
“react” to any change in cell inputs, immediately providing an update to all
downstream calculations and analyses <span class="citation">(Levy 1984)</span>. Since early in
their development, spreadsheet programs have also included <em>macros</em>, “a single
computer instruction that stands for a sequence of operations”
<span class="citation">(Creeth 1985)</span>.</p>
<p>There are some important downsides to using these tools within scientific
research to create spreadsheets that combine data recording with data analysis.
Those include:</p>
<ul>
<li>Raw data are often lost</li>
<li>Analysis steps are often opaque</li>
<li>There is a higher potential for errors in the analysis</li>
<li>There are better software tools available for data analysis</li>
<li>It will make it more difficult to collaborate with statisticians</li>
</ul>
<p>Let’s take a look at each of these.</p>
<p><strong>Raw data often lost</strong></p>
<p>One of the key tenets of ensuring that research is computationally reproducible
is to always keep a copy of all raw data, as well as the steps taken to get from
the raw data to a cleaned version of the data through to the results of data
analysis. However, maintaining an easily accessible copy of all original raw data
for a project is a common problem among biomedical researchers
<span class="citation">(Goodman et al. 2014)</span>, especially as team members move on from a laboratory group
<span class="citation">(Myneni and Patel 2010)</span>. In fact, one study of
operational spreadsheets found:</p>
<blockquote>
<p>“The data used in most spreadsheets is undocumented and there is no practical
way to check it. Even the original developer would have difficulty checking the
data.” <span class="citation">(S. G. Powell, Baker, and Lawson 2009)</span></p>
</blockquote>
<p>One thing that can contribute to this problem is the use of spreadsheets to
jointly record and analyze data. First, data in a spreadsheet is typically not
saved as “read-only”, so it is possible for it to be accidentally overwritten:
in situations where spreadsheets are shared among multiple users, original cell
values can easily be accidentally written over, and it may not be clear who last
changed a value, when it was changed, or why <span class="citation">(AlTarawneh and Thorne 2017)</span>. Further,
raw and processed data are combined in a spreadsheet, which makes it hard to
identify which data points within the spreadsheet make up the raw data and which
are the result of processing that raw data.</p>
<p>Another issue is that many spreadsheets use a proprietary format. In the
development of spreadsheet programs, this use of proprietary binary file formats
helped a software program keep users, increasing barriers for a user to switch
to a new program (since the new program wouldn’t be able to read their old
files) <span class="citation">(Campbell-Kelly 2007)</span>. However, this file format may be hard to open in
the future, as software changes and evolves <span class="citation">(Michener 2015)</span>; by comparison,
plain text files should be widely accessible through general purpose tools—a
text editor is a type of software available on all computers, for
example—regardless of changes to proprietary software like Microsoft Excel.</p>
<p><strong>Analysis steps are often opaque</strong></p>
<p>To keep analysis steps clear—whether the calculation is being done in scripted
code or in spreadsheets or in pen-and-paper calculations—it is important to
document what is being done at each step and why <span class="citation">(Goodman et al. 2014)</span>. Scripted
languages allow for code comments, which are written directly into the script
but not evaluated by the computer, and so can be used to document steps within
the code without changing the operation of the code. Further, the program file
itself often presents a linear, step-by-step view of the pipeline, stored
separated from the data <span class="citation">(Creeth 1985)</span>. Calculations done
with pen-and-paper (e.g., in a laboratory notebook) can be annotated with text
to document the steps. Spreadsheets, on the other hand, are often poorly
documented, or documented in ways that are hard to keep track of.</p>
<p>Within spreadsheets, the logic and methods behind the pipeline of data
processing and analysis is often not documented, or only documented with cell
comments (hard to see as a whole) or in emails, not the spreadsheet file.
One study that investigated a large collection of spreadsheets found that most
do not include documentation explaining the logic or implementation of data
processing and analysis implemented within the spreadsheet
<span class="citation">(Hermans et al. 2016)</span>. A survey of neuroscience researchers at a UK
institute found that about a third of respondents included no documentation
for spreadsheets used in their research laboratories <span class="citation">(AlTarawneh and Thorne 2017)</span>.</p>
<p>When spreadsheet pipelines are documented, it is often through methods that are
hard to find and interpret later. One study of scientific researchers found
that, when research spreadsheets were documented, it was often through “cell
comments” added to specific cells in the spreadsheet, which can be hard to
interpret inclusively to understand the flow and logic of a spreadsheet as a
whole <span class="citation">(AlTarawneh and Thorne 2017)</span>.</p>
<p>In some cases, teams use email chains, rather than the document itself, to
discuss and document functionality and changes in spreadsheets. They pass
versions of the spreadsheet file as attachments of emails discuss the
spreadsheet in the email body.</p>
<p>One research team investigated over 700,000 emails from employees of Enron that
were released during legal proceedings <span class="citation">(Hermans and Murphy-Hill 2015)</span>. They specifically
investigated the spreadsheets attached to these emails (over 15,000
spreadsheets) and how teams discussed the spreadsheets within the emails
themselves . They found that the logic and methods of calculations within the
spreadsheets were often documented within the bodies of emails. This means that,
if someone needs to figure out why a step was taken or identify when an error
was introduced into a spreadsheet, they must dig through the chain of old emails
documenting that spreadsheet, rather than having the relevant documentation
within the spreadsheet’s own file.</p>
<p>Adding to this issue is that data processing and analysis pipelines for
spreadsheets are not carefully designed; instead, it’s more typically for
spreadsheet user to start by directly entering data and formulas without a clear
overall plan <span class="citation">(AlTarawneh and Thorne 2017)</span>. As a result, research spreadsheets are
often not designed to follow a common structure for the research field or for
the laboratory group <span class="citation">(Anderson et al. 2007)</span>.</p>
<p>Another problem comes up because there may only be one person on the team who
fully understands the spreadsheet: the person who created the spreadsheet
<span class="citation">(Myneni and Patel 2010)</span>. This is particularly common if the spreadsheet
includes complex macros or a complicated structure in the analysis pipeline
<span class="citation">(Creeth 1985)</span>. This practice creates a heavy dependence on the
person who created that spreadsheet anytime the data or results in that
spreadsheet need to be interpreted. This is particularly problematic in projects
where the spreadsheet will be shared for collaboration or adapted to be used in
a future project, as is often done in scientific research groups. In this case,
it can be hard to “onboard” new people to use the file, and much of the work and
knowledge about the spreadsheet can be lost when that person moves on from the
business or laboratory group <span class="citation">(Creeth 1985; Myneni and Patel 2010)</span>.</p>
<p>If you share a spreadsheet with numerous and complex macros and formulas
included to clean and analyze the data, it can take an extensive amount of time,
and in some cases may be impossible, for the researcher you share it with to
decipher what is being done to get from the original data input in some cells to
the final results shown in others and in graphs. Further, if others can’t figure
out the steps being done through macros and formulas in a spreadsheet, they will
not be able to check it for problems in the logic of the overall analysis
pipeline or for errors in the specific formulas used within that pipeline. They
also will struggle to extend and adapt the spreadsheet to be used for other
projects. These problems come up not only when sharing with a collaborator, but
also when reviewing spreadsheets that you have previously created and used (as
many have noted, your most frequent collaborator will likely be “future you”).
In fact, one survey of biomedical researchers at the University of Washington
noted that,</p>
<blockquote>
<p>“The profusion of individually created spreadsheets containing overlapping and
inconsistently updated data created a great deal of confusion within some labs.
There was little consideration to future data exchange of submission
requirements at the time of publication.”
<span class="citation">(Anderson et al. 2007)</span></p>
</blockquote>
<p><strong>Potential for errors</strong></p>
<p>Because spreadsheets often do a poor job of making the analysis steps
transparent, they can be prone to bugs in analysis. As one early article on
the history of spreadsheet programs notes:</p>
<blockquote>
<p>“People tend to forget that even the most elegantly crafted spreadsheet is a
house of cards, ready to collapse at the first erroneous assumption. The
spreadsheet that looks good but turns out to be tragically wrong is becoming
a familiar phenomenon.” <span class="citation">(Levy 1984)</span></p>
</blockquote>
<p>Indeed, previous studies have found that errors are very common within
spreadsheets <span class="citation">(Hermans et al. 2016)</span>. For example, one study of 50
operational spreadsheets found that about 90% contained at least one error
<span class="citation">(S. G. Powell, Baker, and Lawson 2009)</span>.</p>
<p>In part, it is easier to make errors in spreadsheets and harder to catch errors
in later work with a spreadsheet because the formulas and connections between
cells aren’t visible when you look at the spreadsheet—they’re behind the
scenes <span class="citation">(Birch, Lyford-Smith, and Guo 2018)</span>. This makes it hard to get a clear and complete
view of the pipeline of analytic steps in data processing and analysis within a
spreadsheet, or to discern how cells are connected within and across sheets of
the spreadsheet.</p>
<p>Some characteristics of spreadsheets may heighten chances for errors. These
include high conditional complexity, which can result from lots of branching of
data flow through if / else structures, as well as formulas that depend on a
large number of cells or that incorporate many functions
<span class="citation">(Hermans et al. 2016)</span>. Following the logical chain of spreadsheet formulas
can be particularly difficult when several calculations are chained in a row
<span class="citation">(Hermans and Murphy-Hill 2015)</span>. In some cases, if you are trying to figure out very long
chains of dependent formulas across spreadsheet cells, you may even have to
sketch out by hand the flow of information through the spreadsheet to understand
what’s going on <span class="citation">(Nardi and Miller 1990)</span>. When a spreadsheet uses macros, it can
also make it particularly hard to figure out the steps of an analysis and to
diagnose and fix any bugs in those steps <span class="citation">(Nash 2006; Creeth 1985)</span>. One study investigated how spreadsheets are used in
practice and noted that, “Many spreadsheets are so chaotically designed that
auditing (especially of a few formulas) is extremely difficult or impossible.”
<span class="citation">(S. G. Powell, Baker, and Lawson 2009)</span></p>
<p>In some cases, formula dependencies might span across different sheets of a
spreadsheet file. These cross-sheet dependencies can make the analysis steps
even more opaque <span class="citation">(Hermans et al. 2016)</span>, as a change in the cell value of
one sheet might not be immediately visible as a change in another cell on that
sheet (the same is true for spreadsheets so large that all the cells in a sheet
are not concurrently visible on the screen). Other common sources of errors
included incorrect references to cells inside formulas and incorrect use of
formulas <span class="citation">(S. G. Powell, Baker, and Lawson 2009)</span> or errors introduced through the common practice of
copying and pasting when developing spreadsheets <span class="citation">(Hermans et al. 2016)</span>.</p>
<p>There are methods that have been brought from more traditional programming work
into spreadsheet programming to try to help limit errors, including a tool
called <em>assertions</em> that allows users to validate data or test logic within their
spreadsheets <span class="citation">(Hermans et al. 2016)</span>. However, these are often not
implemented, in part perhaps because many spreadsheet users see themselves as
“end-users”, creating spreadsheets for their own personal use rather than as
something robust to future use by others, and so don’t seek out strategies
adopted by programmers when creating stable tools for others to use
<span class="citation">(Hermans et al. 2016)</span>. In practice, though, a spreadsheet is often used
much longer, and by more people, than originally intended. From early in the
history of spreadsheet programs, users have shared spreadsheet files with
interesting functionality with other users <span class="citation">(Levy 1984)</span>, and the
lifespan of a spreadsheet can extend and extend—a spreadsheet created by one
user for their own personal use can end up being used and modified by that
person or others for years <span class="citation">(Hermans et al. 2016)</span>.</p>
<p><strong>Better software tools are available</strong></p>
<p>While spreadsheets serve as a widely-used tool for data recording and analysis,
in many cases spreadsheets programs are poorly suited to pre-process and analyze
scientific data compared to other programs. As tools and interfaces continue to
develop that make other software more user-friendly to those new to programming,
scientists may want to reevaluate the costs and benefits, in terms of both time
required for training and aptness of tools, of spreadsheet programs compared to
scripted programming languages like R and Python.</p>
<p>Several problems have been identified with spreadsheet programs in the context
of recording and, especially, analyzing scientific data. First, some statistical
methods may be inferior to those available in other statistical programming
language. Many statistical operations require computations that cannot be
perfectly achieved with a computer, since the computer must ultimately solve
many mathematical problems using numerical approximations (e.g., calculus). The
choice of the algorithms used for these approximations heavily influence how
closely a result approximates the true answer. Since the most popular
spreadsheet program (Excel) is closed-source, it is hard to identify and
diagnose such problems, and there is likely less of an incentive for problems in
statistical methodology to be fixed (rather than using development time and
funds to increase easier-to-see functionality in the program).</p>
<p>A series of papers examined the quality of statistical methods in several
statistical software programs, including Excel, starting in the 1990s
<span class="citation">(Bruce D. McCullough and Wilson 1999, 2005; Bruce D. McCullough 1999; B. D. McCullough and Wilson 2002; Bruce D. McCullough and Heiser 2008; Mélard 2014)</span>. In the
earliest studies, they found some concerns across all programs considered
<span class="citation">(Bruce D. McCullough and Wilson 1999; Bruce D. McCullough 1999)</span>. One of the biggest
concerns, however, was that there was little evidence over the years that the
identified problems in Excel were resolved, or at least improved, over time
<span class="citation">(B. McCullough 2001; Bruce D. McCullough and Heiser 2008)</span>. The authors note that there may
be little incentive for checking and fixing problems with algorithms for
statistical approximation in closed-source software like Excel, where sales
might depend more on the more immediately evident functionality in the software,
while problems with statistical algorithms might be less evident to potential
users <span class="citation">(B. McCullough 2001)</span>.</p>
<p>Open-source software, on the other hand, offers pathways for identifying and fixing
any problems in the software, including for statistical algorithms and methods
implemented in the software’s code. Since the full source code is available, researchers
can closely inspect the algorithms being used and compare them to the latest
knowledge in statistical computing methodology. Further, if an inferior algorithm is in
use, most open-source software licenses allow a user to adapt and extend the software,
including to implement better statistical algorithms.</p>
<p>Another problem is that spreadsheet programs can include automated functionality
that’s meant to make something easier for most users, but that might invisibly
create some problems. A critical problem, for example, has been identified when
using Excel for genomics data. When Excel encounters a cell value in a format
that seems like it could be a date (e.g., “Mar-3-06”), it will try to convert
that cell to a “date” class. Many software programs save date as this special
“date” format, where it is printed and visually appears in a format like
“3-Mar-06” but is saved internally by the program as a number (for Microsoft
Excel, the number of days since January 1, 1900 <span class="citation">(Willekens 2013)</span>).
By doing this, the software can more easily undertake calculations with dates,
like calculating the number of days between two dates or which of two dates is
earlier. Bioinformatics researchers at the National Institutes of Health found
that Excel was doing this type of automatic and irreversible date conversion for
30 gene names, including “MAR3” and “APR-4”, resulting in these gene names being
lost for further analysis <span class="citation">(Zeeberg et al. 2004)</span>.</p>
<p>Avoiding this automatic date conversion required specifying that columns
susceptible to these problems, including columns of gene names, should
be retained in a “text” class in Excel’s file import process. While this
problem was originally identified and published in 2004 <span class="citation">(Zeeberg et al. 2004)</span>,
along with tips to identify and avoid the problem, a study in 2016 found that
approximately a fifth of genomics papers investigated in a large-scale review
had gene name errors resulting from Excel automatic conversion, with the rate of
errors actually increasing over time <span class="citation">(Ziemann, Eren, and El-Osta 2016)</span>.</p>
<p>Other automatic conversion problems caused the lost of clone identifiers that
were composed of digits and the letter “E” <span class="citation">(Zeeberg et al. 2004; Welsh et al. 2017)</span>. These were assumed to be expressing a number using scientific
notation and so automatically and irreversibly converted to a numeric class.
Further automatic conversion problems can be caused by cells that start with an
operator (e.g., “+ control”) or with leading zeros in a numeric identifier
(e.g., “007”) <span class="citation">(Welsh et al. 2017)</span>.</p>
<p>Finally, spreadsheet programs can be limited as analysis needs become more
complex or large <span class="citation">(Topaloglou et al. 2004)</span>. For example, spreadsheets can be
problematic when integrating or merging large, separate datasets
<span class="citation">(Birch, Lyford-Smith, and Guo 2018)</span>. Further, while spreadsheet programs continue to expand in
their capacity for data, for very large datasets they continue to face limits
that may be reached in practical applications <span class="citation">(Birch, Lyford-Smith, and Guo 2018)</span>—until
recently, for example, Excel could not handle more than one million rows of data
per spreadsheet. Even when spreadsheets can handle larger data, their efficiency
in running data processing and analysis pipelines across large datasets can be
slow compared to code implemented with other programming languages.</p>
<p><strong>Difficulty collaborating with statisticians</strong></p>
<p>Modern biomedical researchers requires large teams, with statisticians and
bioinformaticians often included, to enable sophisticated processing and
analysis of experimental data. However, the process of combining data recording
and analysis, especially through the use of spreadsheet programs, can create
barriers in working across disciplines. One group defined these issues as “data
friction” and “science friction”—the extra steps and work required at each
interface where data passes, for example, from a machine to analysis or from a
collaborator in one discipline to one in a separate discipline
<span class="citation">(Edwards et al. 2011)</span>.</p>
<p>When collaborating with statisticians or bioinformaticians, one of the key
sources of this “data friction” can result from the use of spreadsheets to
jointly record and analyze experiemental data. First, spreadsheets are easy to
print or copy into another format (e.g., PowerPoint presentation, Word
document), and so researchers often design spreadsheets to be immediately
visually appealing to viewers. For example, a spreadsheet might be designed to
include hierarchically organized headers (e.g., heading and subheading, some
within a cell merged across several columns), or to show the result of a
calculation at the bottom of a column of observations (e.g., “Total” in the last
cell of the column) <span class="citation">(Teixeira and Amaral 2016)</span>. Multiple separate small tables
might be included in the same sheet, with empty cells used for visual
separation, or use a “horizontal single entry” design , where the headers are in
the leftmost column rather than the top row <span class="citation">(Teixeira and Amaral 2016)</span>.</p>
<p>These spreadsheet design choices make it much more difficult for the contents of
the spreadsheet to be read into other statistical programs. These types of data
require several extra steps in coding, in some cases fairly complex coding, with
regular expressions or logical rules needed to parse out the data and convert it
to the needed shape, before the statistical work can be done for the dataset.
This is a poor use of time for a collaborating statistician, especially if it
can be avoided through the design of the data recording template. Further, it
introduces many more chances for errors in cleaning the data.</p>
<p>Further, information embedded in formulas, macros, and extra formatting like
color or text boxes is lost when the spreadsheet file is input into other
programs. Spreadsheets allow users to use highlighting to represent information
(e.g., measurements for control animals shown in red, those for experiment
animals in blue) and to include information or documentation in text boxes. For
example, one survey study of biomedical researchers at the University of
Washington included this quote from a respondent: “I have one spreadsheet that
has all of my chromosomes … and then I’ve gone through and color coded it for
homozygosity and linkage.” <span class="citation">(Anderson et al. 2007)</span> All the information encoded in
this sheet through color will be lost when the data from the spreadsheet is read
into another statistical program.</p>
</div>
<div id="approaches-to-separate-recording-and-analysis" class="section level3 hasAnchor" number="2.1.3">
<h3><span class="header-section-number">2.1.3</span> Approaches to separate recording and analysis<a href="experimental-data-recording.html#approaches-to-separate-recording-and-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the remaining modules in this section, we will present and describe
techniques that can be used to limit or remove these problems. First, in the
next few modules, we will walk through techniques to design data recording
formats so that data is saved in a consistent format across experiments within a
laboratory group, and in a way that removes “data friction” for collaboration
with statisticians or later use in scripted code. These techniques can be
immediately used to design a better spreadsheet to be used solely for data
collection.</p>
<p>In later modules, we will discuss the use of project directories to coordinate
data recording and analysis steps within a directory, while using separate files
for data recording versus data processing and analysis. These more advanced
formats will enable the use of quality assurance / control measures like testing
of data entry and analysis functionality, better documentation of data analysis
pipelines, and easy use of version control to track projects and collaborate
transparently and with a recorded history.</p>

</div>
</div>
<div id="module2" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Principles and power of structured data formats<a href="experimental-data-recording.html#module2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Guru Madhavan, Senior Director of Programs at the National Academy of
Engineering, wrote a book in 2015 called <em>Applied Minds: How Engineers Think</em>.
In this book, he described a powerful tool for engineers—standards:</p>
<blockquote>
<p>“Standards are for products what grammar is for language. People sometimes
criticize standards for making life a matter of routine rather than inspiration.
Some argue that standards hinder creativity and keep us slaves to the past.
But try imagining a world without standards. From tenderloin beef cuts to
the geometric design of highways, standards may diminish variety and
authenticity, but they improve efficiency. From street signs to nutrition
labels, standards provide a common language of reason. From Internet
protocols to MP3 audio formats, standards enable systems to work together.
From paper sizes … to George Laurer’s Universal Product Code, standards
offer the convenience of comparability.” <span class="citation">(Madhavan 2015)</span></p>
</blockquote>
<p>Standards can be a powerful tool for biomedical researchers, as well, including
when it comes to recording data.
The format in which experimental data is recorded can have a large influence on
how easy and likely it is to implement reproducibility tools in later stages of
the research workflow. Recording data in a “structured” format brings many
benefits. In this module, we will explain what makes a dataset “structured” and
why this format is a powerful tool for reproducible research.</p>
<p>Every extra step of data cleaning is another chance to introduce errors in
experimental biomedical data, and yet laboratory-based researchers often share
experimental data with collaborators in a format that requires extensive
additional cleaning before it can be input into data analysis <span class="citation">(Broman and Woo 2018)</span>.
Recording data in a “structured” format brings many benefits for later stages of
the research process, especially in terms of improving reproducibility and
reducing the probability of errors in analysis <span class="citation">(Ellis and Leek 2018)</span>. Data that is
in a structured, tabular, two-dimensional format is substantially easier for
collaborators to understand and work with, without additional data formatting
<span class="citation">(Broman and Woo 2018)</span>. Further, by using a consistent structured format across many
or all data in a research project, it becomes much easier to create solid,
well-tested code scripts for data pre-processing and analysis and to apply those
scripts consistently and reproducibly across datasets from multiple experiments
<span class="citation">(Broman and Woo 2018)</span>. However, many biomedical researchers are unaware of this
simple yet powerful strategy in data recording and how it can improve the
efficiency and effectiveness of collaborations <span class="citation">(Ellis and Leek 2018)</span>. In this
module, we’ll walk through several types of standards that can be used when
recording biomedical data.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Define ontology, minimum information, and file format</li>
<li>List the elements of a structured data format</li>
<li>Explain how standards can improve scientific data recording</li>
<li>Find existing ontologies for biological and biomedical research</li>
</ul>
<div id="data-recording-standards" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Data recording standards<a href="experimental-data-recording.html#data-recording-standards" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Many people and organizations (including funders) are excited about the idea of
developing and using data standards. Good standards—ones that are widely
adapted by researchers—can help in making sure that data submitted to data
repositories are used widely and that software can be developed that is
interoperable with data from many research groups.</p>
<p>For a simple example, think about recording dates. The minimum
information standard for a date might always be the same—a recorded value
must include the day of the month, month, and year. However, this information
can be structured in a variety of ways. Often in scientific data, it’s common
to record this information going from the largest to smallest units, so March
12, 2006, would be recorded “2006-03-12”. Another convention (especially in the
US) is to record the month first (e.g., “3/12/06”), while another (more common
in Europe) is to record the day of the month first (e.g., “12/3/06”).</p>
<p>If you are trying to combine data from different datasets with dates, and all
use a different structure, it’s easy to see how mistakes could be introduced
unless the data is very carefully reformatted. For example, March 12 (“3-12”
with month-first, “12-3” with day-first) could be easily mistaken to be December
3, and vice versa. Even if errors are avoided, combining data in different
structures will take more time than combining data in the same structure,
because of the extra needs for reformatting to get all data in a common
structure.</p>
<p>Standards can operate both at the level of individual research groups and at the
level of the scientific community as a whole. The potential advantages of
community-level standards are big: they offer the chance to develop
common-purpose tools and code scripts for data analysis, as well as make it
easier to re-use and combine experimental data from previous research that is
posted in open data repositories. If a software tool can be reused, then more
time can be spent in developing and testing it, and as more people use it, bugs
and shortcomings can be identified and corrected. Community-wide standards can
lead to databases with data from different experiments, and from different
laboratory groups, structured in a way that makes it easy for other researchers
to understand each dataset, find pieces of data of interest within datasets, and
integrate different datasets <span class="citation">(Lynch 2008)</span>. Similarly, with community-wide
standards, it can become much easier for different research groups to
collaborate with each other or for a research group to use data generated by
equipment from different manufacturers <span class="citation">(Schadt et al. 2010)</span>. As
an article on interoperable bioscience data notes,</p>
<blockquote>
<p>“Without community-level harmonization and interoperability, many community
projects risk becoming data silos.” <span class="citation">(Sansone et al. 2012)</span></p>
</blockquote>
<p>However, there are important limitations to community-wide standards, as well.
It can be very difficult to impose such standards top-down and community-wide,
particularly for low-throughput data collection (e.g., laboratory bench
measurements), where research groups have long been in the habit of recording
data in spreadsheets in a format defined by individual researchers or research
groups. One paper highlights this point:</p>
<blockquote>
<p>“The data exchange formats PSI-MI and MAGE-ML have helped to get many of the
high-throughput data sets into the public domain. Nevertheless, from a bench
biologist’s point of view benefits from adopting standards are not yet
overwhelming. Most standardization efforts are still mainly an investment for
biologists.” <span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span></p>
</blockquote>
<p>Further, in some fields, community-wide standards have struggled to remain
stable, which can frustrate community members, as scripts and software must be
revamped to handle shifting formats <span class="citation">(Buffalo 2015; Barga et al. 2011)</span>. In some cases, a useful compromise is to follow a
general data recording format, rather than one that is very prescriptive. For
example, committing to recording data in a format that is “tidy” (which we
discuss extensively in the next module) may be much more flexible—and able to
meet the needs of a large range of experimental designs—than the use of a
common spreadsheet template or a more prescriptive standardized data format.</p>
</div>
<div id="elements-of-a-data-recording-standard" class="section level3 hasAnchor" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Elements of a data recording standard<a href="experimental-data-recording.html#elements-of-a-data-recording-standard" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Standards can clarify several elements: the vocabulary used within data, the
content that should be included in a dataset, and the format in which that
content is stored. One article names these three facets of a data standard as
<em>ontologies</em>, <em>minimum information</em>, and <em>file formats</em> <span class="citation">(Ghosh et al. 2011)</span>.</p>
<p><strong>Ontology standards.</strong></p>
<p>The first facet of a data standard is called an <em>ontology</em> (sometimes called a
<em>terminology</em> <span class="citation">(Sansone et al. 2012)</span>). An ontology helps define a vocabulary that
is controlled and consistent. It helps researchers, when they want to talk about
an idea or thing, to use one word, and just one word, and to ensure that it will
be the same word used by other researchers when they refer to that idea or
thing. Ontologies also help to define the relationships between ideas or
concrete things in a research area <span class="citation">(Ghosh et al. 2011)</span>, but here we’ll focus on
their use in provided a consistent vocabulary to use when recording data.</p>
<p>Let’s start with a very simple example to give you an idea of what an ontology
is. What do you call a small mammal that is often kept as a pet and that has
four legs and whiskers and purrs? If you are recording data that includes this
animal, do you record this as “cat” or “feline” or maybe, depending on the
animal, even “tabby” or “tom” or “kitten”? Similarly, do you record tuberculosis
as “tuberculosis” or “TB” or maybe even “consumption”? If you do not use the
same word consistently in a dataset to record an idea, then while a human might
be able to understand that two words should be considered equivalent, a computer
will not be able to immediately tell.</p>
<p>At a larger scale, if a research community can adapt an ontology—one they
agree to use throughout their studies—it will make it easier to understand and
integrate datasets produced by different research laboratories. If every
research group uses the term “cat” in the example above, then code can easily be
written to extract and combine all data recorded for cats across a large
repository of experimental data. On the other hand, if different terms are used,
then it might be necessary to first create a list of all terms used in datasets
in the respository, then pick through that list to find any terms that are
exchangeable with “cat”, then write script to pull data with any of those terms.</p>
<p>Several onotologies already exist or are being created for biological and other
biomedical research <span class="citation">(Ghosh et al. 2011)</span>. For biomedical science, practice, and
research, the BioPortal website (<a href="http://bioportal.bioontology.org/" class="uri">http://bioportal.bioontology.org/</a>) provides
access to over 1,000 ontologies, including several versions of the International
Classification of Diseases, the Medical Subject Headings (MESH), the National
Cancer Institute Thesaurus, the Orphanet Rare Disease Ontology and the National
Center for Biotechnology Information (NCBI) Organismal Classification. For each
ontology in the BioPortal website, the website provides a link for downloading
the ontology in several formats.</p>
<p>Try downloading one of the ontologies using a plaintext file format (the “CSV”
choice in the download options at the BioPortal link). Once you do, you can open
it in your favorite spreadsheet program and explore how it defines specific
terms to use for each idea or thing you might need to discuss within that topic
area, as well as synonyms for some of the terms.</p>
<p>To use an ontology when recording your own data, just make sure you use the
ontology’s suggested terms in your data. For example, if you’d like to use the
Ontology for Biomedical Investigations
(<a href="http://bioportal.bioontology.org/ontologies/OBI" class="uri">http://bioportal.bioontology.org/ontologies/OBI</a>) and you are recording how many
children a woman has had who were born alive, you should name that column of the
data “number of live births”, not “# live births” or “live births (N)” or
anything else. Other collections of ontologies exist for fields of scientific
research, including the Open Biological and Biomedical Ontology (OBO) Foundry
(<a href="http://www.obofoundry.org/" class="uri">http://www.obofoundry.org/</a>).</p>
<p>If there are community-wide ontologies in your field, it is worthwhile to use
them in recording experimental data in your research group. Even better is to
not only consistently use the defined terms, but also to follow any conventions
with capitalization. While most statistical programs provide tools to change
capitalization (for example, to change all letters in a character string to
lower case), this process does require an extra step of data cleaning and an
extra chance for confusion or for errors to be introduced into data.</p>
<p><strong>Minimum information standards</strong></p>
<p>Another part of a data standard is <em>minimum information</em>. Within a data
recording standard, minimum information (sometimes also called <em>minimum
reporting guidelines</em> <span class="citation">(Sansone et al. 2012)</span> or <em>reporting requirements</em>
<span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span>) specify what should be included in a dataset
<span class="citation">(Ghosh et al. 2011)</span>. Using minimum information standards help ensure that data
within a laboratory, or data posted to a repository, contain a number of
required elements. This makes it easier to re-use the data, either to compare it
to data that a lab has newly generated, or to combine several posted datasets to
aggregate them for a new, integrated analysis, considerations that are growing
in importance with the increasing prevalence of research repositories and
research consortia in many fields of biomedical science <span class="citation">(Keller et al. 2017)</span>.</p>
<p>One article that discusses software for systems biology provides a definition
as well as examples of minimum information within this field:</p>
<blockquote>
<p>“Minimum information is a checklist of required supporting information for
datasets from different experiments. Examples include: Minimum Information About
a Microarray Experiment (MIAME), Minimum Information About a Proteomic
Experiment (MIAPE), and the Minimum Information for Biological and Biomedical
Investigations (MIBBI) project.” <span class="citation">(Ghosh et al. 2011)</span></p>
</blockquote>
<p><strong>Standardized file formats</strong></p>
<p>While using a standard ontology and a standard for minimum information is a
helpful start, it just means that each dataset has the required elements
<em>somewhere</em>, and using a consistent vocabulary—it doesn’t specify where those
elements are in the data or that they’ll be in the same place in every dataset
that meets those standards. As a result, datasets that all meet a common
standard can still be very hard to combine, or to create common data analysis
scripts and tools for, since each dataset will require a different process to
pull out a given element.</p>
<p>Computer files serve as a way to organize data, whether that’s recorded
datapoints or written documents or computer programs <span class="citation">(Kernighan and Pike 1984)</span>. A
<em>file format</em> defines the rules for how the bytes in the chunk of memory that
makes up a certain file should be parsed and interpreted anytime you want to
meaningfully access and use the data within that file
<span class="citation">(Murrell 2009)</span>. There are many file formats you may be familiar
with—a file that ends in “.pdf” must be opened with a Portable Document Format
(PDF) Reader like Adobe Acrobat, or it won’t make much sense (you can try this
out by trying to open a “.pdf” file with a text editor, like TextEdit or
Notepad). The PDF Reader software has been programmed to interpret the data in a
“.pdf” file based on rules defining what data is stored where in the section of
computer memory for that file. Because most “.pdf” files conform to the same
<em>file format</em> rules, powerful software can be built that works with any file in
that format.</p>
<p>For certain types of biomedical data, the challenge of standardizing a format
has similarly been addressed through the use of well-defined rules for not only
the content of data, but also the way that content is structured. This can be
standardized through <em>standardized file formats</em> (sometimes also called <em>data
exchange formats</em> <span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span>) and often defines not only the
upper-level file format (e.g., use of a comma-separated plain text, or “.csv”,
file format), but also how data within that file type should be organized. If data
from different research groups and experiments is recorded using the same file
format, researchers can develop software tools that can be repeatedly used to
interpret and visualize that data. On the other hand, if different experiments
record data using different formats, bespoke analysis scripts must be written
for each separate dataset.</p>
<p>This is a blow not only to the efficiency of data analysis, but also a
threat to the accuracy of that analysis. If a set of tools can be developed that
will work over and over, more time can be devoted to refining those tools and
testing them for potential errors and bugs, while one-shot scripts often can’t
be curated with similar care. One paper highlights the problems that come with
working with files that don’t follow a defined format:</p>
<blockquote>
<p>“Vast swathes of bioscience data remain locked in esoteric formats, are
described using nonstandard terminology, lack sufficient contextual information,
or simply are never shared due to the perceived cost or futility of the
exercise.” <span class="citation">(Sansone et al. 2012)</span></p>
</blockquote>
<p>Some biomedical data file formats have been created to help smooth over the
transfer of data that’s captured by complex equipment into software that can
analyze that data. For example, many immunological studies need to measure
immune cell populations in experiments, and to do so they use piece of equipment
called a flow cytometer that probes cells in a sample with lasers and measures
resulting intensities to determine characteristics of that cell. The data
created by this equipment are large (often measurements from several lasers are
taken for a million or more cells in a single run). The data also are complex, as
they need to record not only the intensity measurements from each laser, but
also some metadata about the equipment and characteristics of the run.</p>
<p>If every model of flow cytometer used a different file format for
saving the resulting data, then a different set of analysis software would need
to be developed to accompany each piece of equipment. For example, a laboratory
at a university with flow cytometers from two different companies would need
licenses for two different software programs to work with data recorded by flow
cytometers, and they would need to learn how to use each software package
separately. There is a chance that software could be developed that used shared
code for data analysis, but only if it also included separate sets of code to
read in data from all types of equipment and to reformat them to a common
format.</p>
<p>This isn’t the case, however. Instead, there is a commonly agreed on file format
that flow cytometers should use to record the data they collect, called the the
FCS file format. This format has been defined through a series of papers
(e.g., <span class="citation">Spidlen et al. (2021)</span>), with several separate versions as the file format
has evolved. It provides clear specifications regarding where to save each relevant
piece of information in the block of memory devoted to the data recorded by the
flow cytometer. As a result, people have been able
to create software, both proprietary and open-source, that can be used with any
data recorded by a flow cytometer, regardless of which company manufacturer the
piece of equipment that was used to generate the data.</p>
<p>Other types of biomedical data also have some standardized file formats,
including the FASTQ file format for sequencing data and the mzML file format for
metabolomics data. In some cases these were defined by an organization, society,
or initiative (e.g., the Metabolomics Standards Initiative)
<span class="citation">(Ghosh et al. 2011)</span>, while in some cases the file format developed by a
specific equipment manufacturer has become popular enough that it’s established
itself as the standard for recording a type of data <span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span>.</p>
</div>
<div id="defining-data-recording-standards-for-data-recorded-by-hand" class="section level3 hasAnchor" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Defining data recording standards for data recorded “by hand”<a href="experimental-data-recording.html#defining-data-recording-standards-for-data-recorded-by-hand" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If some of the data you record from your experiments comes from complex
equipment, like flow cytometers or mass spectrometers, you may be recording much
of that data in a standardized format without any extra effort, because that
format is the default output format for the equipment. However, you may have
more control over other data recorded from your experiments, including smaller,
less complex data that you record directly into a laboratory notebook or
spreadsheet. You can derive a number of benefits from defining and using a
standard for collecting these data, which one paper describes as
the output of “traditional, low-throughput bench science” <span class="citation">(Wilkinson et al. 2016)</span>.</p>
<p>When recording this type of data, the data may be written down in an <em>ad hoc</em>
way—however the particular researcher doing the experiment thinks makes
sense—and that format might change with each experiment, even if many
experiments collect similar data. As a result, it becomes harder to create
standardized data processing and analysis scripts that work with this data or to
integrate the data with other data collected through the experiment. Further, if
everyone in a laboratory sets up their spreadsheets for data recording in their
own way, it is much harder for one person in the group to look at data another
person recorded and immediately find what they need within the spreadsheet.</p>
<p>As a step in a better direction, the head of a research group may designate some
common formats (e.g., a spreadsheet template) that all researchers in the group
will use when recording the data from a specific type of experiments. One key
advantage to using standardized data formats even for recording simple,
“low-throughput” data is that everyone in the research group will be able to
understand and work with data recorded by anyone else in the group—data will
not become impenetrable once the person who recorded it leaves the group. Also,
once a group member is used to the format, the process of setting up to record
data from a new experiment will be quicker, as it won’t require the effort of
deciding and setting up a <em>de novo</em> format for a spreadsheet or other recording
file. Instead, a template file can be created that can be copied as a starting
point for any new data recording.</p>
<p>It also allows your team to create tools or scripts that read in and
analyze the data and that can be re-used across multiple experiments with minor
or no changes. This helps improve the efficiency and reproducibility of data
analysis, visualization, and reporting steps of the research project.</p>
<p>Developing these kinds of standards does require some extra time commitment
<span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span>. First, time is needed to design the format, and it does
take a while to develop a format that is inclusive enough that it has a
place to put all data you might want to record for a certain type of experiment.
Second, it will take some time to teach each laboratory member what the format
is and some oversight to make sure they comply with it when they record data.</p>
<p>On the flip side, the longer-term advantages of using a defined, structured
format will outweigh the short-term time investments for many laboratory groups
for frequently used data types. By creating and using a consistent structure to
record data of a certain type, members of a laboratory group can increase their
efficiency (since they do not need to re-design a data recording structure
repeatedly). They can also make it easier for downstream collaborators, like
biostatisticians and bioinformaticians, to work with their output, as those
collaborators can create tools and scripts that can be recycled across
experiments and research projects if they know the data will always come to them
in the same format. One paper suggests that the balance can be found, in terms of deciding whether
the benefits of developing a standard outweigh the costs, by considering how
often data of a certain type is generated and used:</p>
<blockquote>
<p>“To develop and deploy a standard creates an overhead, which can be expensive.
Standards will help only if a particular type of information has to be
exchanged often enough to pay off the development, implementation, and usage
of the standard during its lifespan.” <span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span></p>
</blockquote>
<p>These benefits are even more dramatic if data format standards
are created and used by a whole research field (e.g., if a standard data
recording format is always used for researchers conducting a certain type of
drug development experiment). In that case, the tools built at one institution
can be used at other insitutions. However, this level of field-wide coordination
can be hard to achieve, and so a more realistic immediate goal might be
formalizing data recording structures within your research group or department,
while keeping an eye out for formats that are gaining popularity as standards in
your field to adopt within your group.</p>
<p>Once you commit to creating a defined, structured format, you’ll need to decide
what that structure should be. There are many options here, and it’s very
tempting to use a format that is easy on human eyes
<span class="citation">(Buffalo 2015)</span>. For example, it may seem appealing to create a
format that could easily be copied and pasted into presentations and Word
documents and that will look nice in those presentation formats. To facilitate
this use, a laboratory might set up a recording format based on a spreadsheet
template that includes multiple tables of different data types on the same
sheet, or multi-level column headings.</p>
<p>Unfortunately, many of these characteristics—which make a format attractive to
human eyes—will make it harder for a computer to make sense of. For example,
if you include two tables in the same spreadsheet, it might make it easier for a
person to get a look at two small data tables without having to toggle to
different parts of the spreadsheet. However, if you want to read that data into
a statistical program (or work with a collaborator who would), it will likely
take some complex code to try to tell the computer how to find the second table
in the spreadsheet. The same applies if you include some blank lines at the top
of the spreadsheet, or use multi-level headers, or use “summary” rows at the
bottom of a table. Further, any information you’ve included with colors or with
text boxes in the spreadsheet will be lost when the data’s read into a
statistical program. These design elements make it much harder to read the data
embedded in a spreadsheet into other computer programs, including programs for
more complex data analysis and visualization, like R and Python.</p>
<p>As one article notes:</p>
<blockquote>
<p>“Data should be formatted in a way that facilitates computer readability. All
too often, we as humans record data in a way that maximizes its readability to
us, but takes a considerable amount of cleaning and tidying before it can be
processed by a computer. The more data (and metadata) that is computer readable,
the more we can leverage our computers to work with this data.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<p>One of the easiest format for a computer to read is a two-dimensional
“box” of data, where the first row of the spreadsheet gives the column names,
and where each row contains an equal number of entries. This type of
two-dimensional tabular structure forms the basis for several popular
“delimited” file formats that serve as a <em>lingua franca</em> across many simple
computer programs, like the comma-separated values (CSV) format, the
tab-delimited values (TSV) format, and the more general delimiter-separated
values (DSV) format, which are a common format for data exchange across
databases, spreadsheet programs, and statistical programs <span class="citation">(Janssens 2014; E. S. Raymond 2003; Buffalo 2015)</span>.</p>
<p>Any deviations from this two-dimensional “box” shape can crate problems when a
computer program tries to parse the data. For anything in a data format that
requires extra coding when reading data into another program, you are
introducing a new opportunity for errors at the interface between data recording
and data analysis. If there are strong reasons to use a format that requires
these extra steps, it will still be possible to create code to read in and parse
the data in statistical programs, and if the same format is consistently used,
then scripts can be developed and thoroughly tested to allow this. However, keep
in mind that this will be an extra burden on any data analysis collaborators who
are using a program besides a spreadsheet program. The extra time this will
require could be large, since this code should be vetted and tested thoroughly
to ensure that the data cleaning process is not introducing errors. By contrast,
if the data is recorded in a two-dimensional format with a single row of column
names as the first row, data analysts can likely read it quickly and cleanly
into other programs, with low risks of errors in the transfer of data from the
spreadsheet. In the next module, we’ll go into detail about a more refined
format for these two-dimensional data called the tidy data format.</p>
<!-- ### Applied exercise -->

</div>
</div>
<div id="module3" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> The “tidy” data format<a href="experimental-data-recording.html#module3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the previous module, we explained the benefits of saving data in a structured
format, and in particular one that follows standards for your discipline. In
this section, we’ll talk about the “tidy” data format. The tidy data format is
one implementation of a tabular, two-dimensional structured data format that has
quickly gained popularity among statisticians and data scientists since it was
defined in a 2014 paper <span class="citation">(Wickham 2014)</span>.</p>
<p>These principles cover some basic rules for ordering the data, and even if you
haven’t heard the term <em>tidy data</em>, you may already be implementing many of its
standards in your own datasets. Datasets in this format tend to be very easily
to work with, including to further clean, model, and visualize the data, as well
as to integrate the data with other datasets. In particular, this data format is
compatible with a collection of open-source tools on the R platform called the
<em>tidyverse</em>. These characteristics mean that, if you are planning to use a
standardized data format for recording experimental data in your research group,
you may want to consider creating one that adheres to the tidy data format.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>List characteristics defining the “tidy” structured data format</li>
<li>Understand how to reformat a dataset to make it follow the “tidy” format</li>
<li>Explain the difference between the a structured data format (general concept)
and the “tidy’ data format (one popular implementation)</li>
<li>Understand benefits of recording data in a “tidy” format</li>
</ul>
<div id="keeping-things-tidy" class="section level3 hasAnchor" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Keeping things tidy<a href="experimental-data-recording.html#keeping-things-tidy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Adam Savage has built a career out of making things. He became famous as the
host of the TV show <em>Mythbusters</em>, where a crew builds contraptions to test
urban myths. For many years before that, he created models and special effects
for movies. He has thought a lot about how to effectively work in teams to make
things, and in 2019 he published a book about his life as a maker called <em>Every
Tool is a Hammer</em> <span class="citation">(Savage 2020)</span>.</p>
<p>Among many insights, Savage focuses on the importance
of tidying up as part of the creation process, saying “It’s time, when taken,
that you might feel is slowing you down in the moment, but in fact is saving you
time in the long run.” <span class="citation">(Savage 2020)</span> He introduces a new word for the
process of straightening up tools and materials—“knolling”. He borrowed the
term from an artist, Tom Sachs, whose rules for his own workshop include,
“Always Be Knolling”.</p>
<p>The idea of “knolling” includes a few key principles. First, only have what you
need out. Put everything else somewhere else. Removing any extras makes it
faster to find what you need when you need it. Second, for things you need, make
sure they’re out and available. “Drawers are where things go to die,” Savage
says, highlighting inefficiency when you have to look
for things that are hidden from site as you work. Finally, organize the things
that you have out. Put like things together, and arrange everything neatly,
aligning things in parallel or perpendicular patterns, rather than piling it
haphazardly.</p>
<p>Just as organizing tools and materials improves efficiency in a workshop,
organizing your data can dramatically improve the efficiency of data
pre-processing, analysis, and visualization. Indeed, “tidying up” your data
can give such dramatic improvements that a number of researchers have
developed systems and written papers that describe good organization schemes
to use to tidy up data (e.g., <span class="citation">(Wickham 2014)</span>).</p>
<p>The principles for tidying up data follow some of the principles for knolling.
For example, you want to make sure that you’re saving data in a file or
spreadsheet that only includes the data, removing any of the extras. Lab groups
will sometimes design spreadsheets for data collection that include a space for
recording data, but also space for notes, embedded calculations, and plots.
These extra elements can make it hard to extract and use the data itself. One
way to tidy up a dataset is to remove any of these extra elements. While you can
do this after you’ve collected your data, it’s more efficient to design a way to
record your data in the first place without extra elements in the file or
spreadsheet.</p>
<p>You can further tidy up your data format by reformatting it to
follow the rules of a data format called the “tidy data” format. Just as
Adam Savage’s “knolling” helps you find things when you need them, using
a tidy data format puts elements of your data in the “right” place to be
found by a powerful collection of tools called the <em>tidyverse</em>.</p>
<p>We’ll start this module by describing rules a dataset format must follow for it
to be “tidy” and clarifying how you can set up your data recording to follow
these rules. In later parts of this module, we’ll talk more about why it’s
helpful to use a tidy data format, as well as a bit about the tidyverse tools
that you can use with data in this format.</p>
</div>
<div id="what-makes-data-tidy" class="section level3 hasAnchor" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> What makes data “tidy”?<a href="experimental-data-recording.html#what-makes-data-tidy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The “tidy” data format describes one way to structure tabular data. The name
follows from the focus of this data format and its associated set of tools—the
“tidyverse”—on preparing and cleaning (“tidying”) data, in contrast to sets of
tools more focused on other steps, like data analysis <span class="citation">(Wickham 2014)</span>. The
word “tidy” is not meant to apply that other formats are “dirty”, or that they
include data that is incorrect or subpar. In fact, the same set of datapoints
could be saved in a file in a way that is either “tidy” (in the sense of
<span class="citation">(Wickham 2014)</span>) or untidy, depending only on how the data are organized
across columns and rows.</p>
<p>Wickham notes in his article, where he first describes the tidy data format,
that his ideas about this format evolved from seeing many examples of
different ways that data could be organized within a two-dimensional structure.
He notes:</p>
<blockquote>
<p>“The development of tidy data has been driven by my experience from working
with real-world datasets. With few, if any, constraints on their organization,
such datasets are often constructed in bizarre ways. I have spent countless
hours struggling to get such datasets organized in a way that makes data
analysis possible, let alone easy.” <span class="citation">(Wickham 2014)</span></p>
</blockquote>
<p>To help you understand the tidy data format that Wickham developed, let’s start
with a checklist of rules that make a dataset tidy. Some of these are drawn
directly from the journal article that originally defined the data format
<span class="citation">(Wickham 2014)</span>. Other rules are based on common untidy patterns that show
up in data recording templates for laboratory research. The checklist is:</p>
<ul>
<li>Data are recorded in a tabular, two-dimensional format</li>
<li>The data collection file or spreadsheet avoids extra elements
like plots or embedded equations in the file</li>
<li>Each observation forms a row</li>
<li>Column headers are variable names, not values</li>
<li>Each type of observational unit forms its own table</li>
<li>Each variable forms a column</li>
<li>A single variable is in a single column, not spread across multiple columns</li>
<li>A column contains only one variable; multiple variables are not stored in one column</li>
<li>Data types are consistent within a column</li>
</ul>
<p>In module 2.1, we discussed the first two principles, highlighting how
important it is to separate data collection from further steps of data
processing and analysis. To start this module, we’ll go through other
items in this checklist, to help you understand what makes a dataset follow the
tidy data format. We aim to help you be able to set up your data recording template
to follow this format, as well as be able to tell when you work with data that
others collect if it is in this format, and restructure it if not.</p>
<p>Tidy data, first, must be in a tabular format—that is, two-dimensional, with
columns and rows, and with all rows and columns of the same length. If it’s in a
spreadsheet, it should be stored without any “extras”, like embedded plots and
calculations. If you record data in a spreadsheet using a very basic strategy
of saving a single table per spreadsheet, with the first row giving the column
names, then your data will be in a tabular format. In general, if your recorded
data looks “boxy”, it’s probably in a two-dimensional tabular format.</p>
<p>There are some additional criteria for the tidy data format, though, and so
not every structured, tabular dataset is in a tidy format. As Wickham notes
in his paper defining the format,</p>
<blockquote>
<p>“Most statistical datasets are rectangular tables made up of rows and columns
… [but] there are many ways to structure the same underlying data. …
Real datasets can, and often do, violate the three precepts of tidy data in
almost every way imaginable.” <span class="citation">(Wickham 2014)</span></p>
</blockquote>
<p>First, each row of a tidy dataset records the values for a single observation
<span class="citation">(Wickham 2014)</span>. To figure out if your data format follows this rule, it’s
important to determine the <em>unit of observation</em> of that data, which is the unit
at which you take measurements <span class="citation">(Sedgwick 2014)</span>. This idea is different than
the <em>unit of analysis</em>, which is the unit that you’re focusing on in your study
hypotheses and conclusions (this is sometimes also called the “sampling unit” or
“unit of investigation”) <span class="citation">(Altman and Bland 1997)</span>. In some cases, these two might be
equivalent (the same unit is both the unit of observation and the unit of
measurement), but often they are not <span class="citation">(Sedgwick 2014)</span>. Sedgwick notes:</p>
<blockquote>
<p>“The unit of observation and unit of analysis are often confused.
The unit of observation, sometimes referred to as the unit of
measurement, is defined statistically as the ‘who’ or ‘what’
for which data are measured or collected. The unit of analysis
is defined statistically as the ‘who’ or ‘what’ for which
information is analysed and conclusions are made.” <span class="citation">(Sedgwick 2014)</span></p>
</blockquote>
<p>As an example, say you are testing how the immune system of mice responds to a
certain drug over time. In this case, the unit of analysis might be the drug,
or a combination of drug and dose—ultimately, you may want to test something
like if one drug is more effective than another. To answer this research
question, you likely have several replicates of mice in each treatment group. If
a separate mouse (replicate) is used to collect each observation, and a mouse is
never measured twice (i.e., at different time points, or for a different
infection status), then the unit of measurement—the level at which each data
point is collected—is the mouse. This is because each mouse is providing a
single observation to help answer the larger research question.</p>
<p>As another example, say you conducted a trial on human subjects, to see how
a certain treatment affects the speed of recovery, where each study
subject was measured at different time points. In this case, the unit of
observation is the combination of study subject and time point (while the unit
of analysis is the treatment). That means that Subject 1’s measurement at Time 1 would be one
observation, and the same person’s measurement at Time 2 would be a separate
observation. For a dataset to comply with the tidy data format, these two
observations would need to be recorded on separate lines in the data. If the
data instead had different columns to record each study subject’s measurements
at different time points, then the data would still be tabular, but it would not
be tidy.</p>
<p>[No values in column names]</p>
<p>In the example of human subjects measured at repeated time points, you may
initially find the tidy format unappealing, because it seems like it would
lead to a lot of repeated data. For example, if you wanted to record each study
subject’s sex, it seems like the tidy format would require you to repeat that
information in each separate line of data that’s used to record the measurements
for that subject for different time points. This isn’t the case—instead, with
a tidy data format, different “levels” of data observations should be recorded
in separate tables <span class="citation">(Wickham 2014)</span>. In other words, you should design a
separate table for each unit of observation if you have data at several of these
units for your experiment. For example, if you have some data on each study
subject that does not change across the time points of the study—like the
subject’s ID, sex, and age at enrollment—those form a separate dataset, one
where the unit of observation is the study subject, so there should be just one
row of data per study subject in that data table, while the measurements for
each time point should be recorded in a separate data table. A unique
identifier, like a subject ID, should be recorded in each data table so it can
be used to link the data in the two tables. If you are using a spreadsheet to
record data, this would mean that the data for these separate levels of
observation should be recorded in separate sheets, and not on the same sheet of
a spreadsheet file. Once you read the data into a scripting language like R or
Python, it will be easy to link the larger and smaller tidy datasets as needed
for analysis, visualizations, and reports.</p>
<p>Next, for a dataset to be tidy, each column should be used
to measure a separate characteristic or measurement (a <em>variable</em>) for each
measurement <span class="citation">(Wickham 2014)</span>. A column could either give characteristics of
the data that were pre-defined by the study design—for example, the treatment
assigned to a mouse (a type of variable called a <em>fixed variable</em>, since its
value was fixed before the start of the experiment) or observed measurements,
like the level of infection measured in an animal (a type of variable called a
<em>measured variable</em>, since its value is determined through the experiment)
<span class="citation">(Wickham 2014)</span>.</p>
<p>[One and only one variable in a column]</p>
<p>[Consistent data type in column (e.g., don’t combine numberic with character
comments)]</p>
</div>
<div id="why-make-your-data-tidy" class="section level3 hasAnchor" number="2.3.3">
<h3><span class="header-section-number">2.3.3</span> Why make your data tidy?<a href="experimental-data-recording.html#why-make-your-data-tidy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This may all seem like a lot of extra work to make a dataset tidy, and why
bother if you already have it in a structured, tabular format? It turns out
that, once you get the hang of what gives data a tidy format, it’s pretty
simple to design recording formats that comply with these rules. What’s more,
when data are in a tidy format, they can be directly input into a collection
of tools in R that belong to something called the <em>tidyverse</em>.</p>
<p>R’s <em>tidyverse</em> framework enables powerful and user-friendly data management,
processing, and analysis by combining simple tools to solve complex, multi-step
problems <span class="citation">(Ross, Wickham, and Robinson 2017; Silge and Robinson 2016; Wickham 2016; Wickham and Grolemund 2016)</span>. Since the tidyverse tools are simple and share a common
interface, they are easier to learn, use, and combine than tools created in the
traditional base R framework <span class="citation">(Ross, Wickham, and Robinson 2017; Lowndes et al. 2017; McNamara 2016)</span>. This tidyverse framework is quickly becoming the standard
taught in introductory R courses and books <span class="citation">(Hicks and Irizarry 2017; B. Baumer 2015; Kaplan 2018; Stander and Dalla Valle 2017; McNamara 2016)</span>, ensuring ample training resources for researchers new to
programming, including books (e.g., <span class="citation">(B. S. Baumer, Kaplan, and Horton 2017; Irizarry and Love 2016; Wickham and Grolemund 2016)</span>), massive open online courses (MOOCs), on-site university courses
<span class="citation">(B. Baumer 2015; Kaplan 2018; Stander and Dalla Valle 2017)</span>, and Software
Carpentry workshops <span class="citation">(Wilson 2014; Pawlik et al. 2017)</span>. Further, tools
that extend the tidyverse have been created to enable high-quality data
analysis and visualization in several domains, including text mining
<span class="citation">(Silge and Robinson 2017)</span>, microbiome studies <span class="citation">(McMurdie and Holmes 2013)</span>, natural language
processing <span class="citation">(Arnold 2017)</span>, network analysis <span class="citation">(Tyner, Briatte, and Hofmann 2017)</span>, ecology
<span class="citation">(Hsieh, Ma, and Chao 2016)</span>, and genomics <span class="citation">(Yin, Cook, and Lawrence 2012)</span>.</p>
<p>The tidyverse is a collection of tools united by a common philosophy: very complex
things can be done simply and efficiently with small, sharp tools that share a
common interface. Zev Ross, in an article about tidy tools and how they can
declutter a workflow, notes:</p>
<blockquote>
<p>“The philosophy of the tidyverse is similar to
and inspired by the “unix philosophy”, a set of loose principles that ensure
most command line tools play well together. … Each function should solve one
small and well-defined class of problems. To solve more complex problems, you
combine simple pieces in a standard way.” <span class="citation">(Ross, Wickham, and Robinson 2017)</span></p>
</blockquote>
<p>The tidyverse isn’t the only popular system that follows this
philosophy—one other favorite is Legos. Legos are small, plastic bricks, with
small studs on top and tubes for the studs to fit into on the bottom. The studs
all have the same, standardized size and are all spaced the same distance apart.
Therefore, the bricks can be joined together in any combination, since each
brick uses the same <em>input format</em> (studs of the standard size and spaced at the
standard distance fit into the tubes on the bottom of the brick) and the same
<em>output format</em> (again, studs of the standard size and spaced at the standard
distance at the top of the brick). Because of this design, bricks can be joined
regardless of whether the bricks are different colors or different heights or
different widths or depths. With Legos, even though each “tool” (brick) is very
simple, the tools can be combined in infinite variations to create very complex
structures.</p>
<p>The tools in the tidyverse operate on a similar principle. They all input a
tidy dataset (or a column from a tidy dataset) and they (almost) all output data
in the same format they input it. For most of the tools, their required format
for input and output is the tidy data format <span class="citation">(Wickham 2014)</span>, called a tidy
<em>dataframe</em> in R—this is a dataframe that follows the rules detailed earlier
in this section.</p>
<p>This common input / output interface, and the use of small tools that follow
this interface and can be combined in various ways, is what makes the tidyverse
tools so powerful. However, there are other good things about the tidyverse that
make it so popular. One is that it’s fairly easy to learn to use the tools, in
comparison to learning how to write code for other R tools <span class="citation">(D. Robinson 2017; R. Peng 2018)</span>. This is because the developers who have created the
tidyverse tools have taken a lot of effort to try to make sure that they have a
clear and consistent <em>user interface</em> <span class="citation">(Wickham 2017; Bryan and Wickham 2017)</span>.</p>
<p>To help understand a user interface, and how having a consistent user interface
across tools is useful, let’s think about a different example—cars. When you
drive a car, you get the car to do what you want through the steering wheel, the
gas pedal, the break pedal, and different knobs and buttons on the dashboard.
When the car needs to give you feedback, it uses different gauges on the
dashboard, like the speedometer, as well as warning lights and sounds.
Collectively, these ways of interacting with your car make up the car’s user
interface. In the same way, each function in a programming language has a
collection of parameters you can set, which let you customize the way the
function runs, as well as a way of providing you output once the function has
finished running and the way to provide any messages or warnings about the
function’s run. For functions, the software developer can usually choose design
elements for the function’s user interface, including which parameters to
include for the function, what to name those parameters, and how to provide
feedback to the user through messages, warnings, and the final output.</p>
<p>If tools are similar in their user interfaces, it will make it
easier for users to learn and use any of the tools once
they’ve learned how to use one. For cars, this explains how the rental car
business is able to succeed. Even though different car models are very different
in many characteristics—their engines, their colors, their software—they are
very consistent in their user interfaces. Once you’ve learned how to drive one
car, when you get in a new car, the gas pedal, brake, and steering wheel are
almost guaranteed to be in about the same place and to operate about the same
way as in the car you learned to drive in. The exceptions are rare enough to be
memorable—think how many movies have a laughline from a character trying to
drive a car with the driver side on the opposite side of what they’re used to.</p>
<p>The tidyverse tools are similarly designed so that they all have a very similar
user interface. For example, many of the tidyverse functions use a parameter
named “.data” to refer to the input data. Similarly, parameters
named “.vars” and “.funs” are repeatedly used over tidyverse functions, with the
same meaning in each case. What’s more, the tidyverse functions are typically given names
that very clearly describe the action that the function does, like <code>filter</code>,
<code>summarize</code>, <code>mutate</code>, and <code>group</code>. As a result, the final code is very clear
and can almost be “read” as a natural language, rather than code. As Jenny
Bryan notes, in an article on data science:</p>
<blockquote>
<p>“The Tidyverse
philosophy is to rigorously (and ruthlessly) identify and obey common
conventions. This applies to the objects passed from one function to another
and to the user interface each function presents. Taken in isolation, each
instance of this seems small and unimportant. But collectively, it creates
a cohesive system: having learned one component you are more likely to be
able to guess how another different component works.”
<span class="citation">(Bryan and Wickham 2017)</span></p>
</blockquote>
<p>Many people who teach
R programming now focus on first teaching the tidyverse, given these
characteristics <span class="citation">(D. Robinson 2017; R. Peng 2018)</span>, and it’s often a
first focus for online courses and workshops on R programming. Since its main
data structure is the tidy data structure, it’s often well worth recording
data in this format so that all these tools can easily be used to explore and
model the data.</p>
</div>
<div id="using-tidyverse-tools-with-data-in-the-tidy-data-format" class="section level3 hasAnchor" number="2.3.4">
<h3><span class="header-section-number">2.3.4</span> Using tidyverse tools with data in the tidy data format<a href="experimental-data-recording.html#using-tidyverse-tools-with-data-in-the-tidy-data-format" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When you download R, you get what’s called <em>base R</em>. This includes the main code
that drives anything you do in R, as well as functions for doing many core
tasks. However, the power of R is that, in addition to base R, you can also add
onto R through what are called <em>packages</em> (sometimes also referred to as
<em>extensions</em> or <em>libraries</em>). These are kind of like “booster packs” that add on
new functions for R. They can be created and contributed by anyone, and many are
collected through a few key repositories like CRAN and Bioconductor.</p>
<p>All the tidyverse tools are included in R extension packages, rather than base
R, so once you download R, you’ll need to download these packages as well to use
the tidyverse tools. The core tidyverse functions include functions to read in
data (the <code>readr</code> package for reading in plain text, delimited files, <code>readxl</code>
to read in data from Excel spreadsheets), clean or summarize the data (the
<code>dplyr</code> package, which includes functions to merge different datasets, make
new columns as functions of old ones, and summarize columns in the data, either
as a whole or by group), and reformat the data if needed to get it in a tidy
format (the <code>tidyr</code> package). The tidyverse also includes more precise tools,
including tools to parse dates and times (<code>lubridate</code>) and tools to work with
character strings, including using regular expressions as a powerful way to find
and use certain patterns in strings (<code>stringr</code>). Finally, the tidyverse
includes powerful functions for visualizing data, based around the <code>ggplot2</code>
package, which implements a “grammar of graphics” within R. We cover some
tidyverse tools you may find helpful for pre-processing biomedical data in
module 3.5.</p>
<p>You can install and load any of these tidyverse packages one-by-one using the
<code>install.packages</code> and <code>library</code> functions with the package name from within R.
If you are planning on using many of the tidyverse packages, you can also
install and load many of the tidyverse functions by installing a package called
tidyverse, which serves as an umbrella for many of the tidyverse packages.</p>
<p>In addition to the original tools in the tidyverse, many people have developed
<em>tidyverse extensions</em>—R packages that build off the tools and principles in
the tidyverse. These often bring the tidyverse conventions into tools for
specific areas of science. For example, the <code>tidytext</code> package provides tools to
analyze large datasets of text, including books or collections of tweets, using
the tidy data format and tidyverse-style tools. Similar tidyverse extensions
exist for working with network data (<code>tidygraph</code>) or geospatial data (<code>sf</code>).
Extensions also exist for the visualization branch of the tidyverse
specifically. These include <em>ggplot extensions</em> that allow users to create
things like calendar plots (<code>sugrrants</code>), gene arrow maps (<code>gggene</code>), network
plots (<code>igraph</code>), phytogenetic trees (<code>ggtree</code>) and anatogram images
(<code>gganatogram</code>). These extensions all allow users to work with data that’s in a
tidy data format, and they all provide similar user interfaces, making it
easier to learn a large set of tools to do a range of data analysis and
visualization, compared to if the set of tools lacked this coherence.</p>
<!-- ### Practice quiz -->
<!-- Question 1: What is a fundamental characteristic of tidy data?  -->
<!-- a) All data are stored in a single column  -->
<!-- b) Each variable is stored in a separate column [Correct answer]  -->
<!-- c) Observations are mixed within a single column  -->
<!-- d) Redundant information is encouraged for completeness  -->
<!-- Question 2: Which of the following is the only statement that does not describe one of the advantages of using a tidy data format?  -->
<!-- a) This data format is commonly taught in beginning data analysis courses, and so there are numerous resources to help learn to use it.  -->
<!-- b) This format will always make the data easy to read and interpret when printed out in a paper document [Correct answer]  -->
<!-- c) Many functions and packages in the R programming environment are available to work with data stored in this format  -->
<!-- d) This format is appropriate for chaining together simple commands to create a more complex pipeline to analyze and process data  -->
<!-- Question 3:  -->
<!-- In a data collection template, which of the following are considered extra elements and should be removed to help make the template meet the standards for “tidy” data? Select all that apply.  -->
<!-- a) Embedded formulas that make calculations between cells [Should be selected]  -->
<!-- b) An area to record the observed data from the experiment  -->
<!-- c) Plots [Should be selected]  -->
<!-- Question 4: Which is not one of the guidelines for the tidy data format?  -->
<!-- a) A single variable is not spread across multiple columns  -->
<!-- b) Each observation forms a row  -->
<!-- c) All data from an experiment are included in the same table, even if data are collected at different units of observation [Correct answer]  -->
<!-- d) Each variable forms a column  -->
<!-- Question 5: What is a tabular data format?  -->
<!-- a) One in which the data are hierarchical, with different numbers of observations for each study subject  -->
<!-- b) One in which the data form a matrix, with the same number of rows and columns and the same type of data in each table cell  -->
<!-- c) One in which the data are arranged in a rectangular, two-dimensional format, with rows and columns [Correct answer]  -->
<!-- d) One in which the data adhere to all rules of the tidy data format  -->
<!-- Question 6: You are conducting a study that compares a drug to a placebo. You take measurements on human subjects over time, recording measurements at five timepoints for each study subject. Which of the following is true of this dataset?  -->
<!-- a) The unit of observation is the combination of study subject and timepoint, while the unit of analysis is the treatment (drug or placebo) [Correct answer]  -->
<!-- b) The unit of observation is the study subject, while the unit of analysis is the treatment (drug or placebo)  -->
<!-- c) The unit of observation is the timepoint, while the unit of analysis is the human subject  -->
<!-- d) The unit of observation and the unit of analysis are both the treatment (drug or placebo)  -->
</div>
<div id="discussion-questions" class="section level3 hasAnchor" number="2.3.5">
<h3><span class="header-section-number">2.3.5</span> Discussion questions<a href="experimental-data-recording.html#discussion-questions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>What are your main considerations when you decide how to record your data?</p>
<p>Based on the reading, can you define the tidy data format? Were you familiar with this format before preparing for this discussion? Do you use some of these principles when recording your own data?</p>
<p>Describe advantages, as well as potential limitations, of storing data in a tidy data format</p>
<p>In data that you have collected, can you think of examples when the data collection format included extra elements, beyond simply space for recording the data? Examples might include plots, calculations, notes, and highlighting. What were some of the advantages of having these extra elements in the template? Based on the reading or your own experience, what are some disadvantages to including these extra elements in a data collection template?</p>
<p>In research collaborations, have you experienced a case where the data format for one researcher created difficulties for the other?</p>

</div>
</div>
<div id="module4" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Designing templates for “tidy” data collection<a href="experimental-data-recording.html#module4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this module, we will use a real example of data collected in a biomedical
laboratory. We’ll use this example to show how data is often collected in a way
that is not “tidy” (module 2.3), focusing on the features of data collection
that make it “untidy”. We’ll then describe some general principles for why and
how to instead create and use tidy (or at least tidier) templates to collect
data in the laboratory. We’ll also show how this can be the first step in a
pipeline to creating useful, attractive, and reproducible reports that describe
the data you collected. This module will focus on the principles of templates
for tidy data collection, while in the next module we’ll dig deeper into the
details of making this conversion for the example dataset that we use as a
demonstration in this module.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Detect features in a data collection template that keep a dataset from being
“tidy”</li>
<li>Discuss how “untidy” data collection can affect the rigor and reproducibility
of data recording</li>
<li>Distinguish “untidy” features of a data collection templates into those that
can affect rigor and reproducibility versus those that can easily be addressed
later in the code pipeline</li>
<li>Discuss three principles for designing a data collection template for “tidy”
data collection</li>
<li>List “extra” elements that are sometimes included in a data collection
spreadsheet but that can impede reproducibility</li>
<li>Compare “wide” versus “long” data formats</li>
<li>Explain why certain characters and formatting in a data collection template
may cause problems with later analysis</li>
</ul>
<div id="exampledata-on-rate-of-bacterial-growth" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Example—Data on rate of bacterial growth<a href="experimental-data-recording.html#exampledata-on-rate-of-bacterial-growth" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Throughout this module, we’ll use a real dataset to illustrate principles of
data collection in a biomedical laboratory. First, let’s start by looking at the
original data collection template, and use this to walk through some details of
this dataset. Figure <a href="experimental-data-recording.html#fig:growthexcel1">2.1</a> provides an annotated view of the
data set, showing the format used when the data were originally collected.</p>
<div class="figure"><span style="display:block;" id="fig:growthexcel1"></span>
<img src="figures/growth_curve_example.png" alt="Example of an Excel spreadsheet used to record and analyze data for a laboratory experiment. Annotations highlight where data is entered by hand, where calculations are done by hand, and where embedded Excel formulas are used. The figures are created automatically using values in a specified column." width="\textwidth" />
<p class="caption">
Figure 2.1: Example of an Excel spreadsheet used to record and analyze data for a laboratory experiment. Annotations highlight where data is entered by hand, where calculations are done by hand, and where embedded Excel formulas are used. The figures are created automatically using values in a specified column.
</p>
</div>
<p>These data were collected to measure the compare growth yield and doubling time
of <em>Mycobacterium tuberculosis</em> (the bacteria that causes tuberculosis in
humans) under two conditions—high oxygen and low oxygen. In humans, <em>M.
tuberculosis</em> can persist for years or decades in granulomas, and the centers of
these granulomas are often hypoxic (low in oxygen). Therefore, it’s important to
understand how these bacteria grow in hypoxic conditions.</p>
<p>To conduct this experiment, the researchers used test tubes that were capped
with sealed caps to prevent and air exchange between the contents of the tube
and the environment. Inside the tubes, the amount of oxygen was controlled
by shifting the ratio of the volume of the culture (the liquid with nutrients
in which the M. tuberculosis will grow) versus the volume of air.
In the high oxygen condition, a lower volume of culture was used, which leaves
room for a lot of air in the top of the tube. In the low oxygen condition,
the tube was filled almost to the top with culture, which left very little air
at the top of the tube.</p>
<p>Once the tubes were filled and capped, they were left to grow for about a week.
During this time, the researchers took several measurements to determine the
growth of the bacteria in each tube. To do this, they used a spectrophotometer
to track optical density over time. This method gives a measurement that is
directly proportional to the cell mass in each tube, and so provides a measure
of how much the bacteria has grown since the start of the experiment.</p>
<p>To record data from this experiment, researchers used the spreadsheet shown in
Figure <a href="experimental-data-recording.html#fig:growthexcel1">2.1</a>. This spreadsheet is an example of a data
collection template—it was created not only for this experiment, but also for
other experiments that this research group conducts to measure bacterial growth
under different conditions. It was designed to allow a researcher working in the
laboratory to record measurements over the course of the experiment.</p>
<p>Let’s take a closer look at some of the features of this spreadsheet. First, it
has a section on the top right that focuses on data collection during the
experiment, with one row for each time when the tubes were measured for the cell
mass. This section of the spreadsheet starts with several
columns related to the time of each measurement, including the clock time at
measurement (column A), the difference in time (hours) between each time point
in which data were collected (column B), the date on which data were gathered
(column C), and the time in hours for each data point from the start of the
study for graphing purposes (column D). The columns for clock time (A) and date
(C) were recorded by hand, while the columns for time since the start of the
experiment (B and D) were calculated or converted by hand from these values and
then entered in the column. The remaining columns (E–I) provide data on the
optical density (absorbance at 600 nm), which is directly proportional to cell
mass in the tube. There is one column per test tub, and each of these column
labels includes a test tube ID (A1, A3, L1, L2, L3). If a tube ID starts with
“A”, it was grown in high oxygen conditions, and if it starts with “L”, it was
grown in low oxygen conditions.</p>
<p>Next, the spreadsheet has areas that provide summaries of the data, calculated
using embedded formulas or through the spreadsheet’s plotting functions. For
example, rows 17–18 provide calculations of the doubling time of the bacteria
in each tube for two periods (early and late in the experiment), while two
growth curves are plotted at the bottom of the spreadsheet.</p>
<p>Finally, the spreadsheet includes a couple of other features, including some
written notes about one of the hand calculations and a macro in the top right
that can be used by the researcher to calculate the amount of the initial
inoculum to add to each tube at the start of the experiment.</p>
<p>What the researchers found appealing about the format of this spreadsheet was
the ease with which the researcher collecting data in the laboratory could
accomplish the study goals. The data being graphed in real time, and the
inclusion of a simple macro to calculate doubling time, allowed the research in
the laboratory to see tangible differences between the two assay conditions as
data were collected over the one-week experiment. They also cited the and ease
with which additional sampling data points could be added.</p>
<p>However, many of these features can have undesired consequences. They can increase
the chance of errors in recording the data and in calculating summaries based on the
data. They also make it hard to move the data into a reproducible pipeline, and
so limit opportunities for more sophisticated analysis and visualization. In the
next section of this module, we’ll highlight features of data collection templates
like this one that can make data collection untidy. In the next module (module 2.5),
we’ll discuss how you could create a new data collection template for this example
data that would be tidier, and use this to open a more general discussion of
principles of tidy data collection templates.</p>
</div>
<div id="features-that-make-data-collection-templates-untidy" class="section level3 hasAnchor" number="2.4.2">
<h3><span class="header-section-number">2.4.2</span> Features that make data collection templates untidy<a href="experimental-data-recording.html#features-that-make-data-collection-templates-untidy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are several features of the data collection template shown in Figure
<a href="experimental-data-recording.html#fig:growthexcel1">2.1</a> that make it untidy. These will make it difficult read
the data into a statistical program like R or Python to conduct data analysis
and visualization. There are also some features that make it prone to errors in
data collection and analysis.</p>
<p>First, these data will be hard to read into a statistical program because the
raw data form only part of the spreadsheet (Figure <a href="experimental-data-recording.html#fig:extractraw">2.2</a>, area
highlighted by the blue box). The “extra” elements on the spreadsheet, which
include the output from calculations, plots, macros, and notes, make it harder
to isolate the raw data from the file when using a statistical program.</p>
<div class="figure"><span style="display:block;" id="fig:extractraw"></span>
<img src="figures/growth_curve_raw_data.png" alt="Isolating raw data collected in a template from extra elements. The box in this figure highlights the area of the spreadsheet where data are collected. All other elements of the spreadsheet focus on other aims (e.g., summarizing these data, adding notes, macros for experimental design). Those other elements make it difficult to extract the raw data for more advanced analysis and visualization through a statistical program like R, Python, or Perl." width="\textwidth" />
<p class="caption">
Figure 2.2: Isolating raw data collected in a template from extra elements. The box in this figure highlights the area of the spreadsheet where data are collected. All other elements of the spreadsheet focus on other aims (e.g., summarizing these data, adding notes, macros for experimental design). Those other elements make it difficult to extract the raw data for more advanced analysis and visualization through a statistical program like R, Python, or Perl.
</p>
</div>
<p>While these extra elements make it hard to extract the raw data, it isn’t
impossible. Programming languages like R include functions to read data in from
a spreadsheet, and these functions often provide options to specify the sheet of
the file to read in, as well as the rows and columns to read from a specific
sheet. In the example spreadsheet in Figure <a href="experimental-data-recording.html#fig:extractraw">2.2</a>, for example,
you could specify to read in only rows 1–15 of columns A–I, to focus on the
raw data.</p>
<p>However, one goal of reproducible research is to create tools and pipelines that
are robust—that is, ones that still work as desired when the raw data is
changed in small ways, or even across different raw data files. Therefore, while
we could customize code to read in data from a specific part of a complex
spreadsheet, like that shown in Figure <a href="experimental-data-recording.html#fig:extractraw">2.2</a>, this customization
would make the code less robust. If we asked the statistical program to read in
rows 1–15 of columns A–I, for example, the code would perform incorrectly if
we later added one more time point to the experiment, or if we tried to use the
same template for an experiment that used more test tubes. If we instead use a
template that only records the raw data, without additional elements, then we
can create more robust tools, since we can write code to read in whatever is in
a spreadsheet, rather than restricting to certain rows and columns.</p>
<p>Next, the example template helps demonstrate how specific ways of recording data
can make the template less tidy. First, let’s look at how the template records
the time of each measurement. It does this using four separate columns
(Figure <a href="experimental-data-recording.html#fig:extractraw">2.2</a>). In column C, the researcher records the date a
measurement was taken, and in Column A he or she records the clock time of the
measurement. The experiment was started, for example, at 12:00 PM (“12:00” in column A)
on July 9 (“9-Jul” in column C). These values are entered by hand by the researcher.
Next, these values are used to calculate, for each measurement, how long it had been
since the start of the experiment. This value is recorded in two separate ways—as
hours and minutes in column B and converted into hours and percents of hours (using
decimals) in column D. For example, the second measurement was taken at 4:05 PM
on July 9 (“16:05” in column A and “9-Jul” in column C), which is 4 hours and 5 minutes
after the start of the experiment (“4hr 5min” in column B) or, since 5 minutes is about
8% of an hour, 4.08 hours after the start of the experiment (“4.08” in column D).</p>
<div class="figure"><span style="display:block;" id="fig:timemeasures"></span>
<img src="figures/growth_curve_time_measures.png" alt="Measurements of time in the example data collection template. The four highlighted columns (columns A, B, C, and D) are all used in this spreadsheet to record time. The methods of recording time in this template, however, may make it more likely to create errors in data recording and collection and will make it harder to use the data in a reproducible pipeline." width="\textwidth" />
<p class="caption">
Figure 2.3: Measurements of time in the example data collection template. The four highlighted columns (columns A, B, C, and D) are all used in this spreadsheet to record time. The methods of recording time in this template, however, may make it more likely to create errors in data recording and collection and will make it harder to use the data in a reproducible pipeline.
</p>
</div>
<p>There are a few things that could be changed about how the time data are
recorded here that could make this data collection template tidier. First, it
would be better to focus only on recording the raw data, rather than adding
calculations based on that data. Columns B and D in Figure <a href="experimental-data-recording.html#fig:extractraw">2.2</a>
are both the output from calculations. Anytime a spreadsheet includes a
calculation, it creates the room for mistakes in data collection and analysis.
Often, calculations in a spreadsheet will be done using embedded formulas. These
can cause problems when new columns or rows are added to the data, as that
can shift the cells meant to be used in the calculation. Further, these formulas
are embedded in the spreadsheet, where they can’t be seen and checked very
easily, which makes it easy to miss a typo or other error in the formula.</p>
<p>In the example in Figure <a href="experimental-data-recording.html#fig:extractraw">2.2</a>, columns B and D aren’t
calculated by embedded formulas, but rather calculated by the researcher by hand
and then entered. This creates room for user error with each calculation
and data entry. Later, we’ll see how we can tidy this data collection
template by removing columns that calculate time (columns B and D) and instead
doing that calculation once the raw data are read into a statistical program.</p>
<p>The second thing that could be changed is how the template records the date and
time of the measurement. Currently, it uses two columns (A and C) to record this
information. However, each piece of information is useless without the
other—instead, they must be known jointly to do things like calculate the time
since the start of the experiment. It would therefore be tidier to record this
information in a single column. For example, instead of recording the starting
time of the experiment as “12:00” in column A and “9-Jul” in column C, you could
record it as “July 9, 2019 12:00” in a single date-time column. In this example,
adding the year (“2019”) to the date will also make this data point easier to
work with in a programming language, as languages like R and Python often have
special functions to work with data in date-time classes, but all elements of
the date and/or time must be included to convert data points into these useful
format.</p>
<p>Next, let’s look at how the template collects data related to cell growth in
each tube (columns E–I, Figure <a href="experimental-data-recording.html#fig:growthmeasures">2.4</a>). These data are
recorded in a format that will work pretty well. Strictly speaking, they aren’t
fully tidy (module 2.3), since the column headers include information that we
might want to use as variables in analysis and visualization. Specifically, each
test tube’s ID is incorporated in the column name where measurements for that
tube are recorded, since each test tube is recorded using a separate column.</p>
<div class="figure"><span style="display:block;" id="fig:growthmeasures"></span>
<img src="figures/growth_curve_growth_measures.png" alt="Measurements of bacterial growth in the example data collection template. The five highlighted columns (columns E--I) are all used in this spreadsheet to record optical density in each test tube at each measurement time." width="\textwidth" />
<p class="caption">
Figure 2.4: Measurements of bacterial growth in the example data collection template. The five highlighted columns (columns E–I) are all used in this spreadsheet to record optical density in each test tube at each measurement time.
</p>
</div>
<p>If we want to run analysis where we estimate values for each test tube, or
create plots where each test tube’s measurements are shown with a separate line,
then once we read the data into another program, we’ll need to convert the
format of the data a bit. However, that’s quite easy to do in more statistical
programming languages now, and so it’s reasonable to compromise on this element
of “tidiness” in the data collection format. As we’ll show in the next module,
changing this layout in the original data collection would require the
researcher to re-type the measurement date and time several times and would
result in the spreadsheet being longer, and so harder to see at once when
recording data.</p>
<p>There is a final element we’d like to highlight on this example template that
could make the data hard to integrate into a reproducible pipeline. There are
cases in the example template where either column names or cell values are
formatted in a way that would be hard to work with when the data is read into a
program like R or Python (Figure <a href="experimental-data-recording.html#fig:growthformatting">2.5</a>). For
example, the column names include spaces and parentheses (e.g., “Time (clock)”).
If left as-is, when the data are read into another program, the column names
will need to be cleaned up to take these characters out, so that the column
names are composed only of alphabetical characters, numbers, or underscores.
While this can be done in code like R or Python, it will add to the data
cleaning process and could be avoided by using simpler column names in the
original data collection template.</p>
<div class="figure"><span style="display:block;" id="fig:growthformatting"></span>
<img src="figures/growth_curve_formatting.png" alt="Examples of special characters and formatting in the example template that could cause problems later in a data analysis pipeline." width="\textwidth" />
<p class="caption">
Figure 2.5: Examples of special characters and formatting in the example template that could cause problems later in a data analysis pipeline.
</p>
</div>
</div>
<div id="converting-to-a-tidier-format-for-data-collection-templates" class="section level3 hasAnchor" number="2.4.3">
<h3><span class="header-section-number">2.4.3</span> Converting to a “tidier” format for data collection templates<a href="experimental-data-recording.html#converting-to-a-tidier-format-for-data-collection-templates" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now that we’ve looked at characteristics that can make a data collection
template untidy, let’s go through some principles for creating tidy templates to
record the same data. In this module, we’ll focus on higher-level strategies,
using the example data collection template to highlight out points. In the
next module (module 2.5), we’ll provide a detailed walk-through of how the
example template can be modified to use a tidier format.</p>
<p>There are three basic principles for designing tidy templates that will go a
long way to creating ways to collect data in a research group that can be easily
used within a reproducible analysis pipeline. The first principle in designing a
tidier template for collecting laboratory data is to <strong>limit the template to the
collection of data</strong>. The key here is the word “collection”. A tidy template
will avoid any calculations done on the original data and instead focus only on
the initial data that the researcher records for the experiment. This means that
you should exclude from the template any element that provides a calculation,
summary, or plot based on the initial recorded element. You should also exclude
any special formatting that you are using to encode information. For example,
say that you are collecting data, and in some cases you get a warning that the
reading may be below the instrument’s detection limit. It may be tempting to
highlight the cells with measurements where this warning was displayed as you
record the data. However, you should avoid doing this, as any color or other
formatting information will be lost when you read the data in the file into a
statistical program. Instead, you could add a second column to indicate if the
measurement included a warning.</p>
<p>The second principle is to <strong>make sensible choices when dividing data collection
into rows and columns</strong>. There are many different ways that you could spread the
data collection into rows and columns. One decision is how (and whether) to
divide recorded information across columns. Figure <a href="experimental-data-recording.html#fig:arrangingcolumns">2.6</a>,
for example, shows several ways that you could divide data on a date and time
into one or more columns. In this example, it typically makes the most sense to
use a single column to record all the date and time elements (the top example in
Figure <a href="experimental-data-recording.html#fig:arrangingcolumns">2.6</a>). As mentioned earlier, most statistical
programs have powerful functions for parsing dates and times, after which they
store these data in special classes that allow time-related operations (for
example, calculating the time difference between two date-time measurements). It
will be most efficient to record all date and time elements in a single column.</p>
<div class="figure"><span style="display:block;" id="fig:arrangingcolumns"></span>
<img src="figures/arranging_columns.png" alt="Examples of special characters and formatting in the example template that could cause problems later in a data analysis pipeline." width="\textwidth" />
<p class="caption">
Figure 2.6: Examples of special characters and formatting in the example template that could cause problems later in a data analysis pipeline.
</p>
</div>
<p>Conversely if you have complex data with different elements (for example, height
in components of inches and feet), it may make sense to use separate columns for
each of the components. For example, rather than using one column to record
<code>5'7"</code>, you could divide the information into one column with the component that
is in feet (<code>5</code>) and one with the component in inches (<code>7</code>). In the first case
(recording <code>5'7"</code>), when you read the data into a program like R you would need
to use complex code to split the value into its parts to be able to use it. In
the second case, you could easy work with the values in the two separate columns
to calculate a value to use in further work (e.g., use a formula like <code>height_ft * 12 + height_in</code> to calculate the full height in inches).</p>
<p>Another decision at this stage is how “long” versus “wide” you make your
template. A “wide” design will include more columns, while a “long” design will
include more rows. Often, you can create different designs that allow you to
collect the same values but with different designs along this wide-versus-long
spectrum. Figure <a href="experimental-data-recording.html#fig:longversuswide">2.7</a> gives two examples of templates that
collect the same data, but one is using a wider design and the other is using a
longer design.</p>
<div class="figure"><span style="display:block;" id="fig:longversuswide"></span>
<img src="figures/growth_curve_long_vs_wide.png" alt="Examples of two ways arranging the same data in a data recording template. The format on the left records the optical density measurements for each test tube in a separate column, and the column header identifies the test tube. This is an example of a 'wider' format. The format on the right records the optical density for all test tupbes in a single column, using a separate column to record which test tube the measurement represents. This is an example of a 'longer' format." width="\textwidth" />
<p class="caption">
Figure 2.7: Examples of two ways arranging the same data in a data recording template. The format on the left records the optical density measurements for each test tube in a separate column, and the column header identifies the test tube. This is an example of a ‘wider’ format. The format on the right records the optical density for all test tupbes in a single column, using a separate column to record which test tube the measurement represents. This is an example of a ‘longer’ format.
</p>
</div>
<p>In module 2.3, we described the rules for the tidy format for dataframes.
If you record data directly into a tidy format, it will be very easy to
read into a programming language to analyze and visualize. However,
this tidy format can sometimes result in datasets that are very long. It
may be more convenient to record data in a wider format, especially if you
are recording the data in a laboratory setting where it is inconvenient to
scroll up and down within a longer-format spreadsheet as you record.</p>
<p>Fortunately, there are some convenient tools in programs like R and Python that
can be used to take data that are collected in a wider format and reformat them
to the tidy format as soon as they are read into the software program. While
this will require some extra code, it is usually code that is fairly simple and
straightforward. Therefore, when you design your data collection template, you
can balance any practical advantages of using a wider data collection format
against the advantages of a fully tidy format that apply once your input the
data into a statistical program for analysis and visualization. Often, the wider
format might win out in this balance, and that’s fine.</p>
<p>The third principle is to <strong>avoid characters or formatting that will make it
hard for a computer program to process the data</strong>. This principle is
particularly important for the column names for each column. When you read data
into a statistical program like R, these names will automatically be used as the
column names in the R data frame object, and the code will regularly use these
column names to refer to parts of the data when analyzing and visualizing it.
You will find it easiest to use the data in a reproducible pipeline if you
follow a couple rules for the column names. The reason that these rules will help
is that they replicate the rules for naming objects in programming languages,
and so will help in seamlessly transitioning between the stages of data
collection and data analysis. First, always start a column name with a letter.
Second, only use letters, numbers, or the underscore character (“_“) for
the rest of the characters in the column name.</p>
<p>Based on these rules, then, you should avoid putting spaces in your column names
when you design a data collection template. It is tempting to include spaces to
make the names clearer for humans to read, and this is understandable. Often,
an underscore serves the same function, allowing for easy human comprehension
while still avoiding characters that are difficult for statistical programs.
For example, if you have a column named “Optical density”, you can change it
to “Optical_density” without making it much more difficult for a person to
understand. As with other choices in designing a data collection template,
these choices about column names can be a balance between making the template
easy for researchers to use in the laboratory and easy for the statistical
program to parse later in the pipeline. For example, statistical programs like
R have functions for working with character strings that can be used to
replace all the spaces in column names with another character. However, if
it isn’t unreasonable to follow the recommended rules in writing column names
for the data collection template, you can keep code later in the pipeline
much simpler, so it’s worth considering.</p>
<p>Beyond spaces, there are a number of other special characters that you might
be tempted to include in column names. These could include parentheses, dollar signs,
percent signs, hash marks (“#”), and so on. Any of these will require extra code
in later steps of an analysis pipeline, and some can cause more severe problems
because they have special meanings in the programming language. For example,
hash marks are used in the R programming language to add comments within code, while
dollar signs are used for subsetting elements of a list or data frame object.
It is worth the effort to avoid all these characters in column names in a data
collection template.</p>
<p>There are also considerations you can make in terms of how you record data within
cells of the data collection template, and these can make a big difference in terms
of how hard or easy it is to work with the data within a statistical program. While
statistical programs like R are very powerful in terms of being able to handle even
very “messy” input data, they require a lot of code to leverage this power. By being
thoughtful when you design the template to record the data, you can avoid having to
use a lot of code to input and clean the data in later stages of the pipeline.</p>
<p>Figure <a href="experimental-data-recording.html#fig:recordingtime">2.8</a> gives an example of a choice that you could make
in the format you use to record data. This figure shows two columns from the
original data collection template from the example experiment for this module.
This template includes two columns that record the time since the start of the
experiment, and they use different formats for doing this. In column B, time is
recorded in hours and minutes, with the characters “hr” and “min” used to
separate the two time components. In column D, the same information is recorded,
but in decimals of hours (e.g., 4.08 hours for 4 hours and 5 minutes). While the
format in column B is more similar to how humans think of time, it will take
more code to parse in a statistical program. When reading this data into a
program like R, you would need to use regular expressions to split apart the
different elements and then recombine them into a format that the program
understands. By contrast, the values recorded in column D could be easily read
in by a statistical program, with minimal code needed before they could be used
in analysis and visualizations.</p>
<div class="figure"><span style="display:block;" id="fig:recordingtime"></span>
<img src="figures/growth_curve_recording_time.png" alt="Examples of two ways of recording time in the original template from the example experiment. Column B uses hours and minutes, with characters embedded to separate hours from minutes, while column D uses hours in decimal degrees. The format in column D will be much easier to integrate into a larger data analysis pipeline." width="\textwidth" />
<p class="caption">
Figure 2.8: Examples of two ways of recording time in the original template from the example experiment. Column B uses hours and minutes, with characters embedded to separate hours from minutes, while column D uses hours in decimal degrees. The format in column D will be much easier to integrate into a larger data analysis pipeline.
</p>
</div>
<p>These three principles are an excellent starting point for designing a tidy
template for collecting data. By using these, you will be well on your way to
collecting data in a way that is easy to integrate in a longer reproducible data
analysis pipeline.</p>
<p>When you convert data collection templates to “tidier” formats, they will
typically look much simpler than the templates that your research group may have
been using. In the example experiment that we described earlier in this module,
this process of tidying the template results in a template like that shown in
Figure <a href="experimental-data-recording.html#fig:growthsimple1">2.9</a> (in the next module, we’ll walk through all the
steps to create this tidier template, using this principles we’ve covered in
this module). By comparison, the starting template for data collection for this
experiment is shown in Figure <a href="experimental-data-recording.html#fig:growthexcel1">2.1</a>.</p>
<div class="figure"><span style="display:block;" id="fig:growthsimple1"></span>
<img src="figures/growth_curve_simple.png" alt="Example of an simpler format that can be used to record and analyze data for the same laboratory experiment as the previous figure. Annotations highlight where data is entered by hand. No calculations are conducted or figures created---these are all done later, using a code script." width="\textwidth" />
<p class="caption">
Figure 2.9: Example of an simpler format that can be used to record and analyze data for the same laboratory experiment as the previous figure. Annotations highlight where data is entered by hand. No calculations are conducted or figures created—these are all done later, using a code script.
</p>
</div>
<p>By comparing these two templates, you can see that the simpler template does
not, by itself, provide immediate, real-time summaries of the collected data. The
simpler template has removed elements like plots and values calculated by embedded
formulas. At first glance, this might seem like a disadvantage of using a tidier
template to collect data. However, by combining other tools in a pipeline, it is
easy to connect the tidier raw data file to reporting tools. In this way, you can
quickly create real-time summaries of the data that are similar to those shown in
Figure <a href="experimental-data-recording.html#fig:growthexcel1">2.1</a>, but that are created and reported outside the file
used to originally record the data.</p>
<p>Figure <a href="experimental-data-recording.html#fig:growthreport1">2.10</a> shows an example of a simple report that could
be created for the example experiment. This report is generated using a
statistical program, R, which inputs the data from the simple template shown in
Figure <a href="experimental-data-recording.html#fig:growthsimple1">2.9</a>. The report then uses R code to generate a PDF
or Word file with the output shown below. The file for this report is created in
a way that the output can be quickly regenerated with a single button click, and
so it can be applied to other data saved using the same template. In fact, you can
create templates for reports that coordinate with each data collection template
that you create. In the next module, we’ll walk through how you could create
the generating file for this report, and in later modules (3.7–3.9), we provide
a thorough overview of creating these types of “knitted” documents.</p>
<div class="figure"><span style="display:block;" id="fig:growthreport1"></span>
<img src="figures/growth_curve_report.png" alt="Examples of an automated report that can be created to quickly generate summaries and estimates of the data collected in the simplified data collection template for the example experiment." width="\textwidth" />
<p class="caption">
Figure 2.10: Examples of an automated report that can be created to quickly generate summaries and estimates of the data collected in the simplified data collection template for the example experiment.
</p>
</div>
<p>The report shown in Figure <a href="experimental-data-recording.html#fig:growthreport1">2.10</a> repeats some of the same summaries
that were shown in the more complex original data collection template
(Figure <a href="experimental-data-recording.html#fig:growthexcel1">2.1</a>). There are a number of advantages, however, to using
separate steps and files for the processes of collecting versus analyzing the data.
The separate report (Figure <a href="experimental-data-recording.html#fig:growthreport1">2.10</a>) provides a starting point that can
be easily adapted to make more complex figures and analysis, as well as to integrate
the collected data with data measured in other ways for the experiment.</p>
</div>
<div id="learning-more-about-tidy-data-collection-in-the-laboratory" class="section level3 hasAnchor" number="2.4.4">
<h3><span class="header-section-number">2.4.4</span> Learning more about tidy data collection in the laboratory<a href="experimental-data-recording.html#learning-more-about-tidy-data-collection-in-the-laboratory" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>It may take some iteration to develop the data collection templates that are both
convenient and appropriate to input to more complex programs for pre-processing,
analysis, and visualization. This module and the next module provide guidance and
examples, but it can be helpful to see more examples. Two excellent resources on this
topic are articles by <span class="citation">Ellis and Leek (2018)</span> and <span class="citation">Broman and Woo (2018)</span>.</p>
<!-- ### Applied exercise -->

</div>
</div>
<div id="module5" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Example: Creating a template for “tidy” data collection<a href="experimental-data-recording.html#module5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will walk through an example of creating a template to collect data in a
“tidy” format for a laboratory-based research project, based on a research
project on drug efficacy in murine tuberculosis models. It is important to note
that there’s no reason that you can’t continue to use a spreadsheet program like
Excel or Google Sheets to collect data. The spreadsheet program itself can
easily be used to create a simple template to use as you collect data. In fact,
we’ll continue using a spreadsheet format in this module as we show how to
redesign the data collection for the example experiment that we introduced in
the last module. It is important, however, to think through how you will arrange
that template spreadsheet to make it most useful in the larger context of
reproducible research.</p>
<p>As we show how to redesign the data collection template, we’ll focus on the
three principles for designing tidy templates for data collection in a
biomedical laboratory that we introduced in module 2.4. As a reminder, those
three principles are:</p>
<ol style="list-style-type: decimal">
<li>Limit the template to the collection of data.</li>
<li>Make sensible choices when dividing data collection into rows and columns.</li>
<li>Avoid characters or formatting that will make it hard for a computer program
to process the data.</li>
</ol>
<p>In this module, we’ll show you how to apply those principles to create a tidier
template for the example dataset from the last module. Finally, we will show how
the data can then easily be analyzed and visualized using reproducible tools.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Understand how the principles of “tidy” data can be applied for a real, complex research project</li>
<li>List advantages of the “tidy” data format for the example project</li>
<li>Apply steps that follow three principles for creating a template for “tidy”
data collection</li>
<li>Construct a template for “tidy” data collection</li>
<li>Explain how a report template can help replace visualization tools in a
spreadsheet when collecting data</li>
</ul>
<div id="example-datadata-on-rate-of-bacterial-growth" class="section level3 hasAnchor" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> Example data—Data on rate of bacterial growth<a href="experimental-data-recording.html#example-datadata-on-rate-of-bacterial-growth" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here, we’ll walk through an example using real data collected in a laboratory
experiment. We described these data in detail in the previous module. As a
reminder, they were collected to measure the growth rate of <em>Mycobacteria
tuberculosis</em> under two conditions—high oxygen and low oxygen. They were
collected from five test tubes that were measured regularly over one week for
bacteria growth using a measure of optical density. Figure
<a href="experimental-data-recording.html#fig:growthexcel2">2.11</a> shows the original template that the research group used
to record these data.</p>
<div class="figure"><span style="display:block;" id="fig:growthexcel2"></span>
<img src="figures/growth_curve_example.png" alt="Example of an Excel spreadsheet used to record and analyze data for a laboratory experiment. Annotations highlight where data is entered by hand, where calculations are done by hand, and where embedded Excel formulas are used. The figures are created automatically using values in a specified column." width="\textwidth" />
<p class="caption">
Figure 2.11: Example of an Excel spreadsheet used to record and analyze data for a laboratory experiment. Annotations highlight where data is entered by hand, where calculations are done by hand, and where embedded Excel formulas are used. The figures are created automatically using values in a specified column.
</p>
</div>
<p>In the previous module, we described features that make this template “untidy”
and potentially problematic to include in a larger pipeline of reproducible
research. In the next few sections of this module, we’ll walk step-by-step
through changes that you could make to make this template tidier. We’ll finish
the module by showing how you could then design a further step of the
analysis pipeline to visualize and analyze the collected data, so that the
advantages of real-time plotting from the more complex spreadsheet are not
missed when moving to a tidier template.</p>
</div>
<div id="limiting-the-template-to-the-collection-of-data" class="section level3 hasAnchor" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> Limiting the template to the collection of data<a href="experimental-data-recording.html#limiting-the-template-to-the-collection-of-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The first principle in designing a template for tidy data collection is to
<strong>limit the template to the collection of data.</strong> The example template (Figure
<a href="experimental-data-recording.html#fig:growthexcel2">2.11</a>), however, includes a number of “extra” elements beyond
simple data collection—all the elements outside rows 1–15 of columns A–I.
Outside this area, there are a number of extra elements, including plots that
visualize the data, summaries generated based on the data (rows 16–18, for
example), notes about the data, and even a macro (top right) that wasn’t
involved in data collection but instead was used by the researcher to calculate
the initial volume of inoculum to include in each test tube. None of these
“extras” can be easily read into a statistical program like R or Python—at
best, they will be ignored by the program. They can even complicate reading in
the cells with measurements (rows 1–15 of columns A–I), as most statistical
programs will try to read in all the non-empty cells of a spreadsheet unless
directed otherwise.</p>
<p>A good starting point, then, would be to start designing a tidy data collection
template for this experiment by extracting only the content from the box in
Figure <a href="experimental-data-recording.html#fig:extractraw">2.2</a>. This would result in a template that looks like
Figure <a href="experimental-data-recording.html#fig:step1">2.12</a>. Notice that, as we’ve done this, we’ve also removed any
of the color formatting from the spreadsheet. It is fine to keep color in the
spreadsheet if it will help the research to find the right spot to record data
while working in the laboratory, but you should make sure that you’re not using
it to encode information about the data—all color formatting will be ignored
when the data are read by a statistical program like R.</p>
<div class="figure"><span style="display:block;" id="fig:step1"></span>
<img src="figures/growth_curve_step1.png" alt="First step in designing a tidy data collection template for the example project. A template has been created that focuses only on the raw data, removing all extra elements like plots, notes, macros, and summaries." width="\textwidth" />
<p class="caption">
Figure 2.12: First step in designing a tidy data collection template for the example project. A template has been created that focuses only on the raw data, removing all extra elements like plots, notes, macros, and summaries.
</p>
</div>
<p>Other “extras” in spreadsheets are embedded elements like formulas and macros.
To make your data collection tidy, you should remove any computation steps from
the file that you use to record the data. While the template shown in Figure
<a href="experimental-data-recording.html#fig:step1">2.12</a> has removed a lot of the calculated values from the original
template, it has not removed all of them. Two of the columns are still values
that were determined by calculation after the original data were collected.
Column B and column D both provide measures of the length of time since the
start of the experiment, and both are calculated by comparing a measurement time
to the time at the start of the experiment.</p>
<p>The time since the start of the experiment can easily be calculated later in the
analysis pipeline, once you read the data into a statistical program like R. By
delaying this step, you can both simplify the data collection template
(requiring fewer columns for the research in the laboratory to fill out) and
also avoid the chance for mistakes, which could occur both in the hand
calculations of these values and in data entry, when the researcher enters the
results of the calculations in the spreadsheet cell. Figure <a href="experimental-data-recording.html#fig:step2">2.13</a>
shows a new version of the template, where these calculated columns have been
removed. This template is now restricted to only data points originally
collected in the course of the experiment. It has removed all elements that are
based on calculations or other derivatives of those original, raw data points.</p>
<div class="figure"><span style="display:block;" id="fig:step2"></span>
<img src="figures/growth_curve_step2.png" alt="Second step in designing a tidy data collection template for the example project. This template started from the previous one, but removed columns that were hand-calculated and then entered by the researcher in the previous template. This version has removed all calculated values on the template, limiting it to only the original recorded values required for the experiment." width="\textwidth" />
<p class="caption">
Figure 2.13: Second step in designing a tidy data collection template for the example project. This template started from the previous one, but removed columns that were hand-calculated and then entered by the researcher in the previous template. This version has removed all calculated values on the template, limiting it to only the original recorded values required for the experiment.
</p>
</div>
</div>
<div id="making-sensible-choices-about-rows-and-columns" class="section level3 hasAnchor" number="2.5.3">
<h3><span class="header-section-number">2.5.3</span> Making sensible choices about rows and columns<a href="experimental-data-recording.html#making-sensible-choices-about-rows-and-columns" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The second principle in designing a template for tidy data collection is to
<strong>make sensible choices when dividing data collection into rows and columns</strong>.
There are many different ways that you could spread the data collection into
rows and columns, and in this step, you can consider which method would meet a
reasonable balance between making the template easy for the researcher in the
laboratory to use to record data and also making the resulting data file easy to
incorporate in a reproducible data analysis pipeline.</p>
<p>For the example experiment, Figure <a href="experimental-data-recording.html#fig:extractraw">2.2</a> shows three possibilities
that we can consider to arrange data collection across rows and columns.
All three build on the changes we made in the earlier step of “tidying” the template,
which resulted in the template shown in Figure <a href="experimental-data-recording.html#fig:step2">2.13</a>.</p>
<div class="figure"><span style="display:block;" id="fig:columnoptions"></span>
<img src="figures/growth_curve_column_options.png" alt="Examples of ways that data collection could be divided into rows and columns in the example template. Panel A shows an example where date and time are recorded in different columns. Panel B is similar to Panel A, but in this case, date and time are recorded in a single column. Panel C shows a classically 'tidy' data format, where each measurement date-time is repeated for each of the five test tubes, and columns give the test tube ID and absorbance measurement at that time for that tube (only part of the data is shown for this format, while remaining rows are off the page). While Panel C provides the 'tidiest' format, it may have some practical constraints when used in a laboratory setting. For example, it would require more data entry during data collection (since date-time is entered five times at each measurement time), and its long format prevent it all from being seen at once without scrolling on a computer screen." width="\textwidth" />
<p class="caption">
Figure 2.14: Examples of ways that data collection could be divided into rows and columns in the example template. Panel A shows an example where date and time are recorded in different columns. Panel B is similar to Panel A, but in this case, date and time are recorded in a single column. Panel C shows a classically ‘tidy’ data format, where each measurement date-time is repeated for each of the five test tubes, and columns give the test tube ID and absorbance measurement at that time for that tube (only part of the data is shown for this format, while remaining rows are off the page). While Panel C provides the ‘tidiest’ format, it may have some practical constraints when used in a laboratory setting. For example, it would require more data entry during data collection (since date-time is entered five times at each measurement time), and its long format prevent it all from being seen at once without scrolling on a computer screen.
</p>
</div>
<p>Panel A (an exact repeat of the template shown in Figure <a href="experimental-data-recording.html#fig:step2">2.13</a>) shows
an example where date and time are recorded in different columns. Panel B is
similar to Panel A, but in this case, date and time are recorded in a single
column. Panel C shows a classically “tidy” data format, where each measurement’s
date-time is repeated for each of the five test tubes, and columns give the test
tube ID and absorbance measurement at that time for that tube (only part of the
data is shown for this format, while remaining rows are off the page).</p>
<p>In this example, the template that may be the most reasonable is the one shown
in Panel B. While Panel C provides the “tidiest” format (module 2.3), it has
some practical constraints when used in a laboratory setting. For example, it
would require more data entry during data collection (since date-time is entered
five times at each measurement time), and its long format prevent it all from
being seen at once without scrolling on a computer screen.</p>
<p>When comparing Panels A and B, the template in Panel B has an advantage. The
information on date and time are useful together, but not individually. For
example, to calculate the time since the start of the experiment, you cannot
just calculate the difference in dates or just the difference in times, but
instead must consider both the date and time of the measurement in comparison to
the date and time of the start of the experiment. As a result, at some point in
the data analysis pipeline, you’ll need to combine information about the date
and the time to make use of the two elements.</p>
<p>While this combination of two columns can be easily done within a statistical
program like R, it can also be directly designed into the original template for
collecting the data. Therefore, unless there is a practical reason why it would
be easier for the researcher to enter date and time separately, the template
shown in Panel B is preferable to that shown in Panel A in terms of allowing for
the “tidy” collection of research data into a file that is easy to include in a
reproducible pipeline.</p>
<p>Figure <a href="experimental-data-recording.html#fig:step3">2.15</a> shows the template design at this stage in the process
of tidying it, highlighting the column that combines date and time elements in a
single column. In this version of the template, we’ve also been careful about
how date and time are recorded, a consideration that we’ll discuss more in the
next section.</p>
<div class="figure"><span style="display:block;" id="fig:step3"></span>
<img src="figures/growth_curve_step3.png" alt="Third step in designing a tidy data collection template for the example project. This template started from the previous one, but combined collection of the date and time of the measurement into a single column and revised the format to include all date elements and to prevent automatic conversion by the spreadsheet program." width="\textwidth" />
<p class="caption">
Figure 2.15: Third step in designing a tidy data collection template for the example project. This template started from the previous one, but combined collection of the date and time of the measurement into a single column and revised the format to include all date elements and to prevent automatic conversion by the spreadsheet program.
</p>
</div>
</div>
<div id="avoiding-problematic-characters-or-formatting" class="section level3 hasAnchor" number="2.5.4">
<h3><span class="header-section-number">2.5.4</span> Avoiding problematic characters or formatting<a href="experimental-data-recording.html#avoiding-problematic-characters-or-formatting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The third principle in designing templates for tidy data collection is to
<strong>avoid characters or formatting that will make it hard for a computer program
to process the data</strong>. There are a number of special characters and formatting
conventions that can be hard for a statistical program to handle. In the example
template shown in Figure <a href="experimental-data-recording.html#fig:step3">2.15</a>, for example, the column names include
spaces (for example, in “Date and time”), as well as parentheses (for example,
in “VA 001 (A1)”). While most statistical programs have tools that allow you to
handle and convert these characters once the data are read in, it’s even simpler
to choose column names that avoid these problems in the original data collection
template. This will save some extra coding further along in the analysis
pipeline. Two general rules for creating easy-to-use column names in a data
collection template are: (1) start each column name with a letter and (2) for
the rest of the column name, use only letters, numbers, or the underscore
character (“_“). For example,”aerated1” would work well, but “1–aerated” of
“aerated–1” would not.</p>
<p>Within the cell values below the column names, there is more flexibility. For
example, if you have a column that gives the IDs of different samples, it would
be fine to include spaces and other characters in those IDs. There are a few
exceptions, however. A big one is with values that record dates or date-time
combinations.</p>
<p>First, it is important to include all elements of the date (or date and time, if
both are recorded). The year should be included in the recorded
date, even if the experiment only took a few days. This is because statistical
programs have excellent functions for working with data that are dates or
date-times, but to take advantage of these, the data must be converted into a
special class in the program, and conversion to that class requires specific
elements (for a date, it must include the year, month, and day of month).</p>
<p>Second, it is useful to avoid recording dates and date-times in a way that
results in a spreadsheet program automatically converting them. Surrounding the
information about a date in quotation marks when entering it (as shown in Figure
<a href="experimental-data-recording.html#fig:step3">2.15</a>) can avoid this.</p>
<p>Finally, consider using a format to record the date that is unambiguous and so
less likely to have recording errors. Dates are sometimes recorded using only
numbers—for example, the first date of “July 9, 2019” in the example data
could be recorded as “7/9/2019” or “7/9/19”, to be even more concise. However,
this format has ambiguity. It can be unclear if this refers to July 9 or to
September 7, both of which could be written as “7/9”. For the version that uses
two digits for the year, it can be unclear if the date is for 2019 or 1919 (or
any other century). Using the format “July 9, 2019”, as done in the latest
version of the sample template, avoids this potential ambiguity.</p>
<p>Figure <a href="experimental-data-recording.html#fig:growthsimple2">2.16</a> shows the template for the example experiment after the
column names have been revised to avoid any problematic characters. This template is now in
a very useful format for a reproducible research pipeline—the data collected using this
template can be very easily read into and processed using further statistical programs like
R or Python.</p>
<div class="figure"><span style="display:block;" id="fig:growthsimple2"></span>
<img src="figures/growth_curve_simple.png" alt="Example of an simpler format that can be used to record and analyze data for the same laboratory experiment as the previous figure. Annotations highlight where data is entered by hand. No calculations are conducted or figures created---these are all done later, using a code script." width="\textwidth" />
<p class="caption">
Figure 2.16: Example of an simpler format that can be used to record and analyze data for the same laboratory experiment as the previous figure. Annotations highlight where data is entered by hand. No calculations are conducted or figures created—these are all done later, using a code script.
</p>
</div>
</div>
<div id="separating-data-analysis-from-data-collection" class="section level3 hasAnchor" number="2.5.5">
<h3><span class="header-section-number">2.5.5</span> Separating data analysis from data collection<a href="experimental-data-recording.html#separating-data-analysis-from-data-collection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Once you have created a “tidy” template for collecting your data in the
laboratory, you can create a report template that will input that data and then
provide summaries and visualizations. This allows you to separate the steps (and
files) for collecting data from those for analyzing data. Figure
<a href="experimental-data-recording.html#fig:growthreport2">2.17</a> shows an example of the output of a report template
that could be created to pair with the data collection template shown in Figure
<a href="experimental-data-recording.html#fig:growthsimple2">2.16</a>.</p>
<div class="figure"><span style="display:block;" id="fig:growthreport2"></span>
<img src="figures/growth_curve_report.png" alt="Examples of an automated report that can be created to quickly generate summaries and estimates of the data collected in the simplified data collection template for the example experiment." width="\textwidth" />
<p class="caption">
Figure 2.17: Examples of an automated report that can be created to quickly generate summaries and estimates of the data collected in the simplified data collection template for the example experiment.
</p>
</div>
<p>To create a report template like this, you can use tools for reproducible
reports from statistical programs like R and Python. In this section, we will
give an overview of how you could create the report template shown in Figure
<a href="experimental-data-recording.html#fig:growthreport2">2.17</a>.</p>
<p>This report is written using a framework called RMarkdown, which allows you to
include executable code inside a nicely-formatted document, resulting in a
document in Word, PDF, or HTML that is easy for humans to read while also
generating results based on R code. We will cover this format in details in
modules 3.7–3.9. The code used to generate the results in Figure
<a href="experimental-data-recording.html#fig:growthreport2">2.17</a> is all in the programming language R. Module 3.3
provides guidance on getting started with R, if you have not used it before.</p>
<p>A programming language can seem, at first glance, much more difficult to learn
and use than using a spreadsheet program like Excel to set up formulae and
macros. However, languages like R have evolved substantially in recent years to
allow for much more straightforward coding than you may have seen in the past,
and the barrier to learning to use them for straightforward data management and
analysis is not much higher than the effort required to become proficient in
using a spreadsheet program. To demonstrate this, let’s look through a few of
the tasks required to generate the results shown in Figure
<a href="experimental-data-recording.html#fig:growthreport2">2.17</a>. We won’t cover all the code, just highlight some of
the key steps. If you’d like to look in details over the code and the output
document, you can download those files and explore them: you can access the file
for the <a href="https://github.com/geanders/improve_repro/blob/master/data/growth_curve_data_in_excel%20(1)/Example_report.Rmd">Rmarkdown
file</a>,
and you can download the <a href="https://github.com/geanders/improve_repro/raw/master/data/growth_curve_data_in_excel%20(1)/Example_report.pdf">output
PDF</a>.
If you’d like to try out the code in the Rmarkdown file, you’ll also need the
example data, which you can download by clicking
<a href="https://github.com/geanders/improve_repro/raw/master/data/growth_curve_data_in_excel%20(1)/growth%20curve%20data_GR.xls">here</a>.</p>
<p>One key step is to read the collected data into R. When you use a spreadsheet
for both data collection and analysis, you don’t need to read the data to start
working with them, since everything is saved in the same file. Once you separate
the steps of data collection and data analysis, however, you do need to take an
extra step to read the data file into another program for analysis. Fortunately,
this is very simple in R. The data in this example are recorded using an Excel
spreadsheet, and there is a simple function in R that lets you read data in from
this type of spreadsheet (Figure <a href="experimental-data-recording.html#fig:growthreadexcel">2.18</a>). After this step of
code, you will have an object in R called <code>growth_data</code>, which contains the data
in a two-dimensional form very similar to how it is recorded in the spreadsheet
(this type of object in R is called a dataframe).</p>
<div class="figure"><span style="display:block;" id="fig:growthreadexcel"></span>
<img src="figures/growth_curve_readxl.png" alt="Code to read data from the data collection template into R for cleaning, analysis, and visualization. The data were recorded in the tidy data collection template described earlier in this module. Here, those data are read into R (code shown at top). The resulting data in R are stored in a format that is very similar to the design of a spreadsheet, with rows for observations and columns for the values recorded for each observation (bottom)." width="\textwidth" />
<p class="caption">
Figure 2.18: Code to read data from the data collection template into R for cleaning, analysis, and visualization. The data were recorded in the tidy data collection template described earlier in this module. Here, those data are read into R (code shown at top). The resulting data in R are stored in a format that is very similar to the design of a spreadsheet, with rows for observations and columns for the values recorded for each observation (bottom).
</p>
</div>
<p>Another key step is to calculate, for each observation, the time since the start
of the experiment. In the original data collection template shown in Figure
<a href="experimental-data-recording.html#fig:growthexcel2">2.11</a>, this calculation was done by hand by the researcher and
entered into the spreadsheet. When we converted the spreadsheet to a tidier
version, we took out all steps that involved calculations with the data, and
instead limited the data collection to only raw, observed values. This helps us
avoid errors and typos—instead of having the researcher calculate the
difference in time as they are running the experiment, they can just record the
time, and we can write code in the analysis document that handles the further
calculations, using well-designed and well-tested tools to do this calculation.</p>
<p>Figure <a href="experimental-data-recording.html#fig:growthadddifftime">2.19</a> shows code that can be used for this
calculation. At the start of this code, the data are stored in an object named
<code>growth_data</code>. The <code>mutate</code> function adds a column to the data, named
<code>sampling_delta_time</code>, that will give the difference between the time of an
observation and the start of the experiment. Within the <code>mutate</code> call, a special
function named <code>difftime</code> calculates the difference in two time points. This
function lets us specify the time units we’d like to use, and here we can pick
<code>"hours"</code> for the units. The <code>first</code> function lets us pull out the first value
in the data for a recorded time—in other words, the time when the experiment
started. This lets us compare each observation time to the time of the start of
the experiment. The result of this code is a new version of the <code>growth_data</code>
dataframe, with a new column giving time since the start of the experiment:</p>
<div class="figure"><span style="display:block;" id="fig:growthadddifftime"></span>
<img src="figures/growth_curve_adddifftime.png" alt="Code to add a column to the data that gives the time since the start of the experiment. This code (top) uses the time recorded for each experiment and compares it to the first recorded time, at the start of the experiment. This determines the time since the start of the experiment for each observation, given in a new column in the data (bottom)." width="\textwidth" />
<p class="caption">
Figure 2.19: Code to add a column to the data that gives the time since the start of the experiment. This code (top) uses the time recorded for each experiment and compares it to the first recorded time, at the start of the experiment. This determines the time since the start of the experiment for each observation, given in a new column in the data (bottom).
</p>
</div>
<p>Another key step is to plot results from the data. In R, there is a package
called <code>ggplot2</code> that provides tools for visualization. The tools in this
package work by building a plot using “layers”, adding on small elements
line by line through simple functions that each do one simple thing.
While the resulting code can be long, each step is simple, and so it
becomes simple to learn these different “layers” and learn how to combine
them to create complex plots.</p>
<p>Figure <a href="experimental-data-recording.html#fig:growthplotcode">2.20</a> walks through the code for one of the
visualizations in the report. At this point in the report code, the data have
been reformatted into an object called <code>growth_data_tidy</code>, which has columns for
each observation on the time since the start of the experiment
(<code>sampling_delta_time</code>), the measured optical density (<code>optical_density</code>),
whether the tube was aerated or low oxygen (<code>growth_conditions</code>), and a short ID
for the test tube (<code>short_tube_id</code>). The code starts by creating a plot object,
specifying that in this plot the color will show the growth conditions, the
position on the x-axis will show the time since the start of the experiment, and
the y-axis will show the optical density. Layers are then added to this plot
object that add points and lines to the plot based on these mappings, and for
the lines, it’s further specified that the type of line should show the test
tube ID (for example, one tube will be shown with a dotted line, another with a
dashed line). Further layers are added to customize the scale labels with
<code>labs</code>, including the labels for the x-axis and y-axis and the legends of the
color and linetype scales. Another layer is used to customize the appearance of
the plot—things like the background color and the font used—and another
layer is added to use a log-10 scale for the x-axis.</p>
<div class="figure"><span style="display:block;" id="fig:growthplotcode"></span>
<img src="figures/growth_curve_plot_code.png" alt="Code to plot growth curves from the data. When the plotting code is run, the data have been transformed into a 'tidy' format (top), with columns that include the time since the start of the experiment, a test tube ID, the growth condition for the test tube, and the optical density measured in that test tube. The code (middle) add layers to implement each element of the plot based on this input data. The final plot is shown at the bottom." width="\textwidth" />
<p class="caption">
Figure 2.20: Code to plot growth curves from the data. When the plotting code is run, the data have been transformed into a ‘tidy’ format (top), with columns that include the time since the start of the experiment, a test tube ID, the growth condition for the test tube, and the optical density measured in that test tube. The code (middle) add layers to implement each element of the plot based on this input data. The final plot is shown at the bottom.
</p>
</div>
<p>While this looks like a lot of code, the process isn’t any longer than it would
be to customize elements of a plot in a spreadsheet program. The advantages of
the coded approach are that you maintain a full record of all the steps you took
to customize the plot. This is something that you can use to reproduce your plot
later, or even to use as a starting point for creating a similar plot with new
data.</p>
<p>The next key step that we’d like to point out is how you can write and use small
functions to do customized tasks for the experimental data. As one example, for
the data in this example, we want to estimate doubling times based on the
observed data. The principal investigator has decided that we should do this
based on comparing bacteria levels at two times points—the measured time that
is closest to 65 hours after the start of the experiment, and the time that is
closest to 24 hours after the start of the experiment.</p>
<p>In the original data collection template—where the data were both recorded
and analyzed in a spreadsheet—this step was done by hand by the researcher,
looking through the data and selecting the cell closest to each of these
times, and then connecting that cell to a spreadsheet formula calculation
to calculate the doubling time. We can make this process more rigorous and
less prone to error by writing a small function that does the same thing,
then using that function to automate the process of identifying the
relevant observations to use in calculating the doubling rate.</p>
<p>Figure <a href="experimental-data-recording.html#fig:growthfunction">2.21</a> shows how you can write and then use a small
function in R. This function will input your <code>growth_data</code> dataset, as well as a
time that you are aiming for, and will output the sampling time in the data that
is closest to—while not larger than—that time. It does that in a few steps
within the body of the function. First, the code in the function filters to only
observations earlier than the target time. Then it measures the difference
between each of the times for these observations and the target time, and uses
this to identify the observation with the closest time to the target. It pulls
out the time of this observation and returns it.</p>
<div class="figure"><span style="display:block;" id="fig:growthfunction"></span>
<img src="figures/growth_curve_function.png" alt="Code to create and apply a small function. The code at the top can be used to create a function that can input your dataframe and determine the observation time in that data that is closest to (without being larger than) a target time. The function does this through a series of small steps. This function can then be applied to find the observation time in the data that is closest to specific target times, like 24 hours and 64 hours (bottom)." width="\textwidth" />
<p class="caption">
Figure 2.21: Code to create and apply a small function. The code at the top can be used to create a function that can input your dataframe and determine the observation time in that data that is closest to (without being larger than) a target time. The function does this through a series of small steps. This function can then be applied to find the observation time in the data that is closest to specific target times, like 24 hours and 64 hours (bottom).
</p>
</div>
<p>Small functions like this can easily be reused in other code for your research
group. By writing the logic of the step out as a function—rather than redoing
the steps by hand or step-by-step each time you need to do it—you can save
time later, and in return, you have extra time that you can spend in writing
the original function and carefully checking to make sure that it works
correctly.</p>
<p>Finally, many of these steps require extensions to base R. When you download R,
you are getting a base set of tools. Many people have developed helpful
extensions that build on this base. These are stored and shared in what are
called <em>R packages</em>. You can install these extra packages for free, and you
use the <code>library</code> function in R to load a package you’ve installed, giving you
access to the extra functions that it provides. Figure
<a href="experimental-data-recording.html#fig:growthloadpackage">2.22</a> shows the spot in the Rmarkdown code where we
loaded packages we needed for this report. These include packages with functions
to read data in R from Excel (the <code>readxl</code>) package, as well as a suite of
packages with tools for cleaning and visualizing data (the <code>tidyverse</code> package).
In later modules, we’ll talk some more about R coding tools that you might find
useful for working with biomedical data, including the tools in the powerful and
popular <code>tidyverse</code> suite of packages.</p>
<div class="figure"><span style="display:block;" id="fig:growthloadpackage"></span>
<img src="figures/growth_curve_load_packages.png" alt="Code to load packages with additional functionality. These provide functions that are not offered in base R, but that are useful in working with the example data. They include packages with functions for reading in data from an Excel file, as well as packages with functions for cleaning and visualizing data." width="\textwidth" />
<p class="caption">
Figure 2.22: Code to load packages with additional functionality. These provide functions that are not offered in base R, but that are useful in working with the example data. They include packages with functions for reading in data from an Excel file, as well as packages with functions for cleaning and visualizing data.
</p>
</div>
<p>Overall, you can see that the code in this document provides a step-by-step
recipe that documents all the calculations and cleaning that we do with the
data, as well as how we create the plots. This code runs every time we create
the report shown in Figure <a href="experimental-data-recording.html#fig:growthreport2">2.17</a>, and it gives us a good
starting point if we run additional experiments that generate similar data.</p>
</div>
<div id="applied-exercise" class="section level3 hasAnchor" number="2.5.6">
<h3><span class="header-section-number">2.5.6</span> Applied exercise<a href="experimental-data-recording.html#applied-exercise" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Rmarkdown document includes a number of other steps, and you might find
it interesting to download the document and the example data and walk through
them to get a feel for the process. All the steps are documented in the
Rmarkdown document with extensive code comments, to explain what’s happening
along the way.</p>
<!-- ### Discussion questions -->

</div>
</div>
<div id="module6" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> Organizing project files<a href="experimental-data-recording.html#module6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In earlier modules, we discussed how to separate data collection from data
analysis. By separating data collection and analysis into separate files, we can
make the file for each step simpler. Further, by separating steps into different
files, we can save the files in plain text, which makes it easier to track them
using version control software (discussed in later modules). This helps create a
record of changes made to the data or analysis code during the research process.</p>
<p>While this process helps in reproducibility, it results in more files being
collected for an experiment. Instead of data and its analysis collected within a
single spreadsheet file, you may end up with multiple files of data collected
from the experiment, as well as separate files with scripts for processing,
analyzing, and visualizing the data. With more complex experiments, there may be
different data files containing the data collected from different assays. For example,
you may run an experiment where you collect data from each research animal on
bacterial load, as well as flow cytometry data, as well as a measure of antibody
levels through ELISA. As a result, you may have one raw data file from each
assay and, for some assays, even one file per study subject (e.g., flow
cytometry). The files for a research project will also include files with
writing and presentations (posters and slides) associated with the project, as
well as code scripts for pre-processing data, for conducting data analysis, and
for creating and sharing final figures and tables.</p>
<p>In this and the next few modules, we’ll discuss how you can organize the files
for an experiment using a single directory that is designed to follow a similar
format across all your projects. The modules will discuss the advantages of
well-designed project directories, tips for arranging files within a project
directory, and how to create a directory template that allows you to use
consistent file organization across many experiments.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Explain how poor file organization can impede reproducibility</li>
<li>List benefits of good file organization</li>
<li>List several principles for organizing research project files</li>
<li>Define the design concept of “discoverability”</li>
<li>Apply the idea of discoverability in organizing project files</li>
<li>Explain how a project directory template works</li>
</ul>
<div id="advantages-of-organizing-project-files" class="section level3 hasAnchor" number="2.6.1">
<h3><span class="header-section-number">2.6.1</span> Advantages of organizing project files<a href="experimental-data-recording.html#advantages-of-organizing-project-files" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As the files for a project accumulate, do you have a clear plan for keeping them
organized? Based on one analysis, many biomedical researchers do not. One study,
for example, surveyed over 250 biomedical researchers at the University of
Washington. They noted that, “Some researchers admitted to having no
organizational methodology at all, while others used whatever method best suited
their individual needs” <span class="citation">(Anderson et al. 2007)</span>. One respondent answered, “They’re
not organized in any way—they’re just thrown into files under different
projects,” while another said “I grab them when I need them, they’re not
organized in any decent way,” and another, “It’s not even organized—a file on
a central computer of protocols that we use, common lab protocols but those are
just individual Word files within a folder so it’s not searchable <em>per se</em>”
<span class="citation">(Anderson et al. 2007)</span>.</p>
<p>This lack of organization can make scientists reluctant to share their research
files, impeding reproducibility. In an article on organizing project files for
research, Marwick notes:</p>
<blockquote>
<p>“Virtually all researchers use computers as a central tool in their
workflow. However, our formal education rarely includes any training in
how to organise our computer files to make it easy to reproduce results
and share our analysis pipeline with others. Without clear instructions,
many researchers struggle to avoid chaos in their file structures, and so
are understandable reluctant to expose their workflow for others to see.
This may be one of the reasons that so many requests for details about
method, including requests for data and code, are turned down or go
unanswered.” <span class="citation">(Marwick, Boettiger, and Mullen 2018)</span></p>
</blockquote>
<p>Sharing data and code is crucial to research reproducibility, especially for
projects that include extensive proprocessing and complex analysis of data, as
many biomedical research projects now do. As a further bonus, when research articles
include data, they tend to be more impactful, as measured by
citations that the paper receives <span class="citation">(Marwick, Boettiger, and Mullen 2018)</span>.</p>
<p>In an earlier module, we introduced Adam Savage’s idea of “knolling” to keep a
workspace tidy (Module 2.3). He was talking about a physical workspace. When
you are working with data, computer files and directories are your workspace.
For any type of work, the design of the workspace plays a critical role in
how the workers approach tasks and solve problems. Rod Judkins, who is a
lecturer at St Martin’s College of Art, highlights this in a book on
creative thinking:</p>
<blockquote>
<p>“Your working environment, whether it’s a supermarket, office, studio, or
building site, persuades you to work and think in certain ways. The more aware
you are of that, and the more you understand your medium, the more you can use
it to your advantage.” <span class="citation">(Judkins 2016)</span></p>
</blockquote>
<p>Adam Savage describes how important this is in another type of work: gourmet
cooking. He describes how this idea of an organized workspace is captured by the
technique of <em>mise en place</em>—of laying out all the elements needed for the
work ahead of time and in an organized way—introduced by the famous French
chef August Escoffier:</p>
<blockquote>
<p>“Kitchens are pressure cookers in which wasted movement and hasty technique
can ruin a dish, slice an artery, burn a hand, land you in the weeds, and
ultimately kill a restaurant. <em>Mise en place</em> is the only way to reliably create
a perfect dish, to exact specifications, over and over again, night after night,
for paying customers who demand nothing less.” <span class="citation">(Savage 2020)</span></p>
</blockquote>
<p>Good organization of your files can similarly encourage clear thinking, and it
can help you in reasoning through how to analyze data. One article notes that
“mundane issues such as organizing files and directories and documenting
progress … are important because poor organizational choices can lead to
significantly slower research progress.” <span class="citation">(Noble 2009)</span> In fact, if files are
organized in a consistent way across multiple projects, this can even allow you
to start automating some necessary tasks through code that is built to work with
that consistent structure <span class="citation">(Buffalo 2015)</span>.</p>
<p>Organization also helps you in finding things, and finding them quickly. You can
even find things quickly when you come back to a project after a while away from
it (for example, while the paper was out for review). You can teach others how
to find things quickly and consistently across your multiple projects, as well
as where to put things they’re contributing.</p>
<p>Good file organization will also help you find information you need when it’s
time to write up your results. As one article notes, with good organization,
“methods and data sections in papers practically write themselves, with no time
wasted in frenzied hunting for missing information.” <span class="citation">(Baker 2016)</span></p>
<p>Finally, good file organization can improve your efficiency. An article on
organizing computational biology projects highlights this:</p>
<blockquote>
<p>“Everything you do, you will probably have to do over again. Inevitably, you
will discover some flaw in your initial preparation of the data being analyzed,
or you will get access to new data, or you will decide that your
parameterization of a particular model was not broad enough. This means that the
experiment you did last week, or even the set of experiments you’ve been working
on over the past month, will probably need to be redone. If you have organized
and documented your work clearly, then repeating the experiment with the new
data or the new parameterization will be much, much easier.” <span class="citation">(Noble 2009)</span></p>
</blockquote>
</div>
<div id="how-to-organize-project-files" class="section level3 hasAnchor" number="2.6.2">
<h3><span class="header-section-number">2.6.2</span> How to organize project files<a href="experimental-data-recording.html#how-to-organize-project-files" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now that we’ve explained <em>why</em> to organize project files, let’s talk about
<em>how</em> you can do that. We’ll cover higher-level principles in this
module. In the next few modules, we’ll move into more details and examples.</p>
<p>First, and at a minimum, you should get in the habit of storing all of the files
for an experiment in the same place. Specifically, project files should all be
in a single directory within the file system of a computer <span class="citation">(Noble 2009; Buffalo 2015)</span>. While this can be an individual’s computer, it may
also be on a dedicated server or through an online, cloud-based program.</p>
<p>There are a number of advantages to keeping all of a project’s files inside a
dedicated file directory. First, it provides a clear and obvious place to search
for all project files as you work on the project, including after lulls (like
waiting for reviews from a paper submission).</p>
<p>One article about the reproducibility of scientific papers talks about how helpful
this organization can be, describing the experience for a project that involved
a large research group:</p>
<blockquote>
<p>“Instead of squirrelling away data in individual folders and lab books,
researchers now archive all published data in a designated central drive, so
that the information is accessible for the long haul. Initially, people thought
the process was just extra bureaucratic work, or that it had been invented so I
could police their data. Now, it has become the norm, and researchers tell me
they save time and worry by having their data organized and archived.”
<span class="citation">(Winchester 2018)</span></p>
</blockquote>
<p>By keeping all project files within a single directory, you also make it
easier to share those files as a unit. There are several reasons you might
want to share these files. An obvious one is that you to share
the project files across members in your research team, so they can collaborate
on the project. However, there are also other reasons you’d need to
share files, and one that is growing in importance is that you may be asked to
share files (data, code scripts, etc.) when you publish a paper describing your
results.</p>
<p>When files are all stored in one directory, the directory can be compressed and
shared as an email attachment (if the file size is small enough) or through a
file sharing platform like Google Drive. When all the materials for a project
are stored in a single directory, it also makes it easier to share the set of
files through version control and online version control platforms
<span class="citation">(Vuorre and Crump 2021)</span>. In later modules in this book (modules 3.9–3.11), we will
introduce Git version control software and the GitHub platform for sharing files
under this type of version control—this is one example of this more dynamic
way of sharing files, but requires them to be stored in a single directory.</p>
<p>To gain the advantages of directory-based project file organization, all the
files need to be within a single directory, but they don’t all have to be within
the same “level” in that directory. Instead, you can use subdirectories to
structure and organize these files, while still retaining all the advantages of
directory-based file organization. Computer file systems are well-structured to
use a hierarchical design, with subdirectories nested inside directories. You
can leverage this structure to manage the complexity and breadth of files for your
project.</p>
<p>This will help limit the number of files in each “level” of the directory, so
none becomes an overwhelming collection of files of different types. It can help
you navigate the files in the directory, and also help someone else quickly
figure out what’s in the directory and where everything is. However,
to leverage these gains, you need to be thoughtful about exactly how you
organize the files into subdirectories.</p>
<p>As you decide how to organize files, keep in mind a concept called
<em>discoverability</em>. In the classic design book <em>The Design of Everyday
Things</em>, Don Norman presents discoverability as a key principle of good design,
explaining it as the ability for a user to be able to figure out, from the design
of something, how to use that thing quickly, easily, and correctly.</p>
<p>He illustrates this with an example of discoverability in the design of doors.
For a door, the location of a pull handle and a push bar immediately shows
someone how to use the door: pull on the side of the door where you see a pull
handle and push where you see a push bar. If the door is lacking these, it makes
it harder for a user to “discover” how to use it at first glance, and they might
try to push when they need to pull or vice-versa.</p>
<p>The same idea applies when you design an organizational system for project
files. You want to make sure that a new user (or you in the future) will be able
to easily navigate through the directory to find what they need. One article on
organizing research project files notes that, when it comes to deciding how to
organize your files, “The core guiding principle is simple: Someone unfamiliar
with your project should be able to look at your computer files and understand
in detail what you did and why.” <span class="citation">(Noble 2009)</span> Another notes, “The key
principle is to organize the [project directory] so that another person can know
what to expect from the plain meaning of the file and directory names.”
<span class="citation">(Marwick, Boettiger, and Mullen 2018)</span></p>
<p>Another way to improve discoverability is to name your files and subdirectories
in meaningful ways. The computer will give you wide flexibility in setting
names for files and subdirectories, but a human will find it much easier to
navigate a directory when the names are clear labels that describe the contents.
For example, if you have data from different assays, you might organize them
all into a directory named “raw_data” that is then divided into subdirectories
named with the type of assay.</p>
<p>As you develop names that are discoverable, keep in mind that your users may
include some people outside your field, for whom some shorthand common in the
field might be unclear. For example, in some studies of infectious bacterial
disease, the bacterial load is measured in an assay that counts colony forming
units. Among bench scientists in this field, the assay is often called “CFUs”.
If you are collaborating with a statistician, however, they may find the
files more discoverable if you named the subdirectory with these files something
like “bacterial_load” rather than “cfus”, as they may not be familiar with that
shorthand.</p>
<p>One way to improve discoverability is to follow any standards that exist for
organizing project files <span class="citation">(Marwick, Boettiger, and Mullen 2018)</span>. The use of standards or
conventions tend to make it easier for users to navigate (“discover”) new
instances of a certain type of thing. In module 2.2, we discussed this role of
standards when it comes to the format you use to record your data. When it comes
to project file organization, standards will come in the form of the
subdirectories that are included, how they’re organized hierarchically, and how
subdirectories and files are named.</p>
<p>These standards could exist as several levels: at a top level for your
discipline, but also just for your lab group, or even for you as an individual.
It is very helpful when standards exist at a discipline-wide level, as following
this type of high-level standard will immediately make your work discoverable
(in the design sense) to a wide group of people. As one article notes,
“Using widely held conventions… will help other people to understand how your
files relate to each other without having to ask you.” <span class="citation">(Marwick, Boettiger, and Mullen 2018)</span></p>
<p>As an example of this, when people develop R packages, the package consists of a
set of files, and there is a very clear and highly enforced standard for how
these files are arranged in a directory and how the subdirectories are named. By
enforcing this standard, many different people can create packages and have them
work in a similar way.</p>
<p>On the opposite end of the spectrum, if there are not clear
standards at the level of your discipline, you could create a clear standard
that you plan to follow either for your lab group or even for your individual
work. If you’re consistent in organizing your files using that standard, it will
make it easier to navigate files as you move from one project to another.</p>
<p>As an added bonus, subdirectory organization can also be used in clever ways
within code scripts applied to files in the directory. For example, there are
functions in all scripting languages that will list all the files in a specified
subdirectory. If you keep all your raw data files of a certain type (for
example, all output from flow cytometry for the project) within a single
subdirectory, you can use this type of function with code scripts to list all
the files in that directory and then apply code that you’ve developed to
preprocess or visualize the data across all those files. This code would
continue to work as you added files to that directory, since it starts by
looking in that subdirectory each time it runs and working with all files there
as of that moment.</p>
<p>This type of automation can be a huge efficiency boost for your project.
One article describes how this type of automation can increase efficiency
with a comparison to a simpler task in working with computer files:</p>
<blockquote>
<p>“Organizing data files into a single directory with consistent filenames
prepares us to iterate over <em>all</em> of our data, whether it’s the four example
files used in this example, or 40,000 files in a real project. Think of it this
way: remember when you discovered you could select many files with your mouse
cursor? With this trick, you could move 60 files as easily as six files. You
could also select certain file types (e.g., photos) and attach them all to an
email with one movement. By using consistent file naming and directory
organization, you can do the same programatically using the Unix shell and other
programming languages.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<p>A final way to improve your directory organization is to make sure the directory
is not cluttered with unnecessary files. Unnecessary files can include old versions
of project files, which have been superseded by newer versions. In later modules
(modules 2.9–2.11), we’ll describe how version control can help avoid this clutter
from old versions of files while retaining information from older versions as
files evolve.</p>
</div>
<div id="what-is-a-project-directory-template" class="section level3 hasAnchor" number="2.6.3">
<h3><span class="header-section-number">2.6.3</span> What is a project directory template?<a href="experimental-data-recording.html#what-is-a-project-directory-template" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Louis Pastuer famously said that “Luck favors the prepared mind.” In file
organization, as with so much else, time spent preparing can pay off
exponentially later. In this case, the next step is to not only use a structured
directory for each project or experiment, but to start using the same,
standardized structure for every one of your projects and experiments—in other
words, to create a standard for file organziation and to use it consistently.</p>
<p>In other modules, we talk about how templates can be used to improve the rigor
and reproducibility of collecting and reporting on data. Just as it’s possible
to create templates for data collection and for reports, it’s also possible to
create a template for how you organize file directories for your scientific
projects, creating and applying standards for things like which subdirectories
are included and how files are named. This takes more work—to design a
structure that can be used across many projects, rather than to set something
up <em>ad hoc</em> as you start each new experiment. However, the gains in terms of
organization and efficiency can be extraordinary.</p>
<p>This involves first designing a common template for the directory structure for
your projects. Once you have decided on a structure for this template, you can
create a version of it on your computer—a file directory with all the
subdirectories included, but without any files (or only template files you’d
want to use as a starting point in each project, like templates for data
collection and reports as presented in Modules 2.4 and 2.5). When you start a
new project, you can then just copy this template directory, rename it, and
start using it for your new research project. If you are using R and begin to
use R Projects (described in the next section), you can also create an R Studio
Project template to serve as this kind of starting point each time you start a
new project.</p>
<p>In other areas of science and engineering, this idea of standardized directory
structures has allowed the development of powerful techniques for open-source
software developers to work together. For example, anyone may create their own
extensions to the R programming language and share these with others through
GitHub or several large repositories. As mentioned earlier in this
module, this is coordinated by enforcing a common directory structure on these
extension “packages”—to create a new package, you must put certain types of
files in certain subdirectories within a project directory. With these
standardized rules of directory structure and content, each of these packages
can interact with the base version of R, since there are functions that can tap
into any of these new packages by assuming where each type of file will be
within the package’s directory of files.</p>
<p>In a similar way, if you impose a common directory structure across all the
project directories in your research lab, your collaborators will quickly be
able to learn where to find each element, even in projects they are new to, and
you will all be able to write code that can be easily applied across all project
directories, allowing you to improve reproducibility and comparability across
all projects by assuring that you are conducting the same pre-processing and
analysis across all projects (or, if you are conducting things differently for
different projects, that you are deliberate and aware that you are doing so).
Creating a project template that you copy and rename as you start a new
project is one way to facilitate this.</p>
<p>As you use a template for a project, you can customize it as you need. For
example, if you had included a subdirectory for flow cytometry data, but are not
running that assay in this experiment, you can remove that subdirectory.
Similarly, you can customize the report as you go to help it work well for this
specific experiment. However, you will aim to keep to the standard format as
much as possible, since it’s the standardization across projects that provides
many of the advantages.</p>
<p>In the next module, we will walk through the steps of designing a project
template that you can use across experiments for your laboratory group. In
module 2.8, we’ll walk through an example of creating and using this kind of
project template for an example set of studies.</p>
<!-- ### Practice quiz -->

</div>
</div>
<div id="module7" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> Creating project directory templates<a href="experimental-data-recording.html#module7" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<p>The last module described the advantages of organizing all the files for a
research project within a single directory, and the added advantages of using a
consistent directory structure for all of the experiments or projects in your
research group. In this module, we’ll walk through the steps required to design
and create a template for your project directories. Creating and using a common
template for your directory structure for projects will help create consistency
across projects in the directory structure, which can facilitate the use and
re-use of automated tools like code scripts across different experiments.</p>
<ul>
<li>Be able to designed a structured project directory template for
research projects</li>
<li>Understand how project directories can be turned into RStudio “Projects”</li>
</ul>
<div id="goals-in-designing-a-project-template" class="section level3 hasAnchor" number="2.7.1">
<h3><span class="header-section-number">2.7.1</span> Goals in designing a project template<a href="experimental-data-recording.html#goals-in-designing-a-project-template" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Designing a project template will include two parts—first, designing a
<em>conceptual</em> template for your file organization and, second, creating a
<em>physical</em> implementation of that concept. The conceptual template will
develop a structure and rules for how you’ll organize and name files within a
project directory. The physical template will use these ideas to develop a file
directory that follows that organization, which you can then copy, paste, and
adapt each time you start a new project.</p>
<p>In other words, before you open your computer to make a “physical” template, you
should design it. This involves deciding what types of data will go into a
project directory, how those files will be organized within the directory and
the naming conventions for files. In other words, you should create a blueprint
for your template before you create a physical template.</p>
<p>The hardest part of this is the conceptual part—deciding on the structure and
rules you will consistently use. This is a process of designing, and so you can
make this process a bit easier by following principles that facilitate design.
For example, as you design, it’s useful to start by defining the problem
<span class="citation">(Osann, Mayer, and Wiele 2020)</span>. What are you aiming to achieve with your file organization
system?</p>
<p>Based on our own experiences and the advice of others <span class="citation">(Marwick, Boettiger, and Mullen 2018; Bertin and Baumer 2021)</span>, key goals to consider for a research project
directory template are that the system:</p>
<ul>
<li>Keeps all files for a research project within a single directory, using
subdirectories to organize files into a hierarchical structure</li>
<li>Keeps data collection and analysis separate (see module 2.1)</li>
<li>Avoids or removes unnecessary files</li>
<li>Uses meaningful names for files and subdirectories, allowing easy navigation
and discoverability (module 2.6) by a new user</li>
<li>Facilitates creation of reports and analysis that incorporate data from
different assays for an experiment</li>
<li>Makes it easy to share all project files across the team, as well as
publicly, once a paper is published</li>
<li>Makes it easy to implement version control for a project (modules 2.9–2.11)</li>
<li>Incorporates enough flexibility to be used with minimal changes across many
research projects</li>
</ul>
</div>
<div id="steps-in-designing-the-conceptual-blueprint-for-a-project-directory-template" class="section level3 hasAnchor" number="2.7.2">
<h3><span class="header-section-number">2.7.2</span> Steps in designing the conceptual blueprint for a project directory template<a href="experimental-data-recording.html#steps-in-designing-the-conceptual-blueprint-for-a-project-directory-template" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As you design a conceptual framework for a project directory template, you
can break the process into a few key steps:</p>
<ol style="list-style-type: decimal">
<li>Observe your current research project practices</li>
<li>Determine which subdirectories you’ll include and how you’ll name them</li>
<li>Decide on file name conventions</li>
</ol>
<p>In this section, we’ll go into detail about each of these tasks.</p>
<p><strong>Observe your current research project practices</strong></p>
<p>As you work on this blueprint, you will want to prioritize how it will fit the
needs of the user—your research group. One way you can do this is to follow a
key early step in the design process: observe <span class="citation">(Osann, Mayer, and Wiele 2020)</span>. One of the
best ways to get an idea of what your research group needs within a project
directory is to take a survey of past research projects from your group. Make a
list of what types of data were collected and what types of pre-processing and
analysis were done using those data. For each type of data, it’s helpful to make
a note its typical file type and typical size. How are data for a specific assay
divided across files? Are the data for all animals and all timepoints included
in a single spreadsheet file? If so, are they saved in the same sheet, or
divided across sheets? Conversely, are different files used for the data from
different animals or different time points?</p>
<p>Doing this kind of survey will help you create a standard structure of
subdirectories that you can use consistently across the directories for all the
projects in your research program. Of course, some projects may not include
certain files, and some might have a new or unusual type of file. You can
customize the directory structure to some degree for these types of cases, but
it is still a big advantage to include as many common elements as possible
across all your projects. The best way to determine what these common elements
might be in future projects is to look at your past projects.</p>
<p>It can also be helpful to have an example of each file type, to help capture
the typical size, structure, and contents of each type of file. For
data that you will record yourself in the lab, these can be the templates that you
developed to collect the data in a tidy format (modules 2.3 through 2.5).
For data from equipment, these can be one or more example files from the
equipment that you have collected for a past project. Having these example files
will help you to develop a template project report that can input the type of
data that you typically collect for this type of project.</p>
<p>This is also a good stage to diagnose if there are data collection files that
are not successful in separating data collection from data pre-processing and
analysis (module 2.1). As you progress, you may also want to add templates that
serve as a starting point for data collection files within this project. This
idea of creating data collection templates is described in detail in modules 2.4
and 2.5.</p>
<p><strong>Determine which subdirectories you’ll include and how you’ll name them</strong></p>
<p>Once you have examined past projects to determine the types of files that you’ll
normally include in a project, you can decide how to organize them into
subdirectories. This subdirectory structure will create the core framework of
your project directory template.</p>
<p>In general, as you design the structure of subdirectories, keep in mind that a
key aim is to create a structure that is general enough that you can use it
consistently for many projects, but also clear enough that you can quickly find
things within the directory. As one paper notes, you want a directory setup that
is “flexible and configurable” <span class="citation">(Blischak, Carbonetto, and Stephens 2019)</span>.</p>
<p>A number of researchers have put a lot of thought into how to organize project
directories for scientific research <span class="citation">(Vuorre and Crump 2021; Johnston 2022; Blischak, Carbonetto, and Stephens 2019; Marwick, Boettiger, and Mullen 2018; Noble 2009)</span>. A common theme
across these papers is to include subdirectories to store files in four main
areas:</p>
<ul>
<li>data</li>
<li>code</li>
<li>reports</li>
<li>meta-documentation</li>
</ul>
<p>We’ll go through each of these to discuss what might be included in each, as
well as how it might make sense to name subdirectories in each of the areas.</p>
<p><em>Data subdirectories</em></p>
<p>Data should be saved in an area that is separate from any code for analysis. See
module 2.1 for a deeper discussion on the benefits of separating data from
analysis to improve reproducibility. The raw data should also be treated as
“read-only”—in other words, the raw data should never be edited or changed. To
work with the data, including any necessary quality control, pre-processing, or
analysis, these raw data should be read into a separate program for analysis.
That way, you can work with the data (and even create and save intermediary,
“processed” versions of the data), while maintaining the original raw files
without alteration.</p>
<p>There are different recommendations on how to name and organize subdirectories
for data. Several papers recommend having separate subdirectories for the raw
data versus intermediate processed data. Some researchers have suggested naming
the subdirectory for raw data as “data-raw” and the one for intermediate data as
“data” <span class="citation">(Vuorre and Crump 2021; Johnston 2022)</span>. Others have suggested naming the raw
data subdirectory as “data” and the one for intermediate data “outputs”
<span class="citation">(Blischak, Carbonetto, and Stephens 2019)</span>. Either or these choices—or a reasonable
alternative—is fine, as long as you use your naming scheme consistently every
time you set up a project directory. In some cases, you may also decide to use
the raw data directory keep the code scripts that you used to create
intermediate processed data from those raw data <span class="citation">(Johnston 2022)</span>.</p>
<p>One thing that can be challenging is working with raw data files that are
extremely large, as in this case you may not have room on your personal computer
to store the full set of raw data. One article suggested a solution: store a
smaller example dataset in your project directory that can be used to test or
demonstrate the analysis code, while storing the full set of raw data files on a
computer with adequate storage capacity <span class="citation">(Marwick, Boettiger, and Mullen 2018)</span>. The article
notes:</p>
<blockquote>
<p>“If your data are very large, or streaming, an alternative is to include a
small-sample dataset so that people can try out the techniques without having to
run very expensive computations.” <span class="citation">(Marwick, Boettiger, and Mullen 2018)</span></p>
</blockquote>
<p><em>Code subdirectories</em></p>
<p>Next, you’ll want to include one or more subdirectories for code. Again, this
structure helps in separating data collection from data analysis (module 2.1).
This code may include data for cleaning and pre-processing the data, although
some researchers choose to put code for these steps in the “raw-data”
subdirectory, as separate files from the raw data files but within the same
section of the project directory. This code will also include code to analyze
and visualize the data. In some cases, it might include code for functions that
you plan to reuse within different code scripts in the project or even across
projects.</p>
<p>One article recommended having a single code subdirectory, named
“code” <span class="citation">(Blischak, Carbonetto, and Stephens 2019)</span>. This subdirectory can
store any code scripts (outside of any code running as part of a report
RMarkdown file; see modules 3.7–3.9). Another recommends that, if you
have both compiled code (like C code) and code scripts (for a language
like R), you may want to have separate subdirectories for source code (“src”)
versus compiled code or scripts (“bin”) <span class="citation">(Noble 2009)</span>.</p>
<p>Other researchers have recommended having an “R” subdirectory that is only used
for code that you write for reusable R functions, ones that you plan to use
several times across other code scripts in your project <span class="citation">(Vuorre and Crump 2021; Marwick, Boettiger, and Mullen 2018)</span>. For the code that runs data analysis, they recommend a
separate subdirectory named “model” <span class="citation">(Vuorre and Crump 2021)</span> or “analysis”
<span class="citation">(Marwick, Boettiger, and Mullen 2018)</span>.</p>
<p><em>Report subdirectories</em></p>
<p>You can leverage the standard structure you’ve created for your directory to
create a report. This can be designed to generate some exploratory analysis and
visualizations that you find you typically want to generate from your data. You
can create this using tools for reproducible reports—in R, a key tool for this
is RMarkdown. Here, we’ll cover using this tool for creating a report, and there
are many more details in modules 3.7 through 3.9. Briefly, RMarkdown allows you
to include both code and text meant for humans within a single, plain text
document. This document can then be rendered, a process that executes the code
and formats the text meant for humans, producing a document in an easy-to-read
format like Word or PDF.</p>
<p>Whether you use these tools or not, though, you should have a space in your
project directory to keep the documents you create to report your findings.
These will include initial reports, but they can also include documents like
paper articles, conference abstracts, posters, and presentations.</p>
<p>You could use a single subdirectory for these report files, named something like
“doc” <span class="citation">(Johnston 2022; Noble 2009)</span>. Alternatively, if you are using RMarkdown
files, you could keep these files (which are the ones you should work on as you
edit reports) in one subdirectory and have another subdirectory to store the
output of those RMarkdown files (the generated reports in a format like PDF or
Word, which you should treat as read-only if they were generated from an
RMarkdown file) <span class="citation">(Blischak, Carbonetto, and Stephens 2019)</span>. These two subdirectories could be named
“analysis” and “output”, respectively <span class="citation">(Blischak, Carbonetto, and Stephens 2019)</span>. Another article
recommends using separate subdirectories for different types of report outputs,
for example “posters”, “manuscript”, and “slides” <span class="citation">(Vuorre and Crump 2021)</span>.</p>
<p><em>Metadata subdirectories or files</em></p>
<p>The final major area to cover in your project directory are files for metadata.
These files contain information that describes your project as a whole. In some
cases you might store this information in subdirectories, but in many cases,
this information might alternatively go in a single file at the main level of
the project directory.</p>
<p>There are a number of pieces of information that you may want to include in this
metadata. You could include, for example, information about the experiment, like
which model animal you were using or which treatment you were testing. You could
also include information related to the code analysis. One piece of information
that’s very important, for example, is a list of the dependencies and versions
of software. For example, if you used R for analysis, which version of R did you
use, and which packages did you use to supplement the base R distribution?</p>
<p>The metadata can also provide some information on who was involved
in the project, what role each person had, and the conditions for reusing
elements of the project, like code and data. If the project directory will
be shared once you complete the information, these details on reuse will be
particularly helpful. This might include information, for example, about the
license under which you are sharing any code within the project.</p>
<p>Several articles suggest sharing this metadocumentation through a type of file
called a “README” file <span class="citation">(Marwick, Boettiger, and Mullen 2018; Bertin and Baumer 2021; Johnston 2022)</span>.
The idea of a README file comes from the tradition of software engineering.
The code that builds a software system can be large and complex, with many
source files that must be combined and compiled to “build” the software.
Since it can be hard to navigate the directory with all these files, one
long-standing solution is to include a README file. This README file is
put in the top level of the directory’s hierarchy. This way, when someone opens
the directory, they’ll see this file right away, and it has a very discoverable
filename, since “README” tells you exactly what you should do with it.</p>
<p>This file serves as a spot where you can help someone navigate the rest of the
files in the directory. You can also use it to record metadata for the project:
things like who was involved in the research and a citation to a resulting
paper. You can write this file in plain text, but if you’re sharing the project
directory through a version control platform, you might want to explore
writing it in the mark-up language Markdown (see module 2.11 for more on
using Markdown for a README file that will be shared through a version control
platform).</p>
<p><strong>Decide on file name conventions</strong></p>
<p>The final step in designing a conceptual framework is to create some rules for
how you’ll name files in the project. When you create rules for how you name
files, the first thing to keep in mind is this: use names that balance
<em>generalizability</em> with <em>discoverability</em>.</p>
<p>In terms of generalizability, you want to use file names that generalize to all
of your projects. In other words, don’t make file names so specific that they
won’t work the next time you do a project. You may, for example, want to include
the name of the grant or experiment in your filename for your metadata. This
instinct is good—it can be helpful to include information about your
experiment somewhere in the filenames. But try to put this type of information
as high in the directory structure as possible: specifically, put that information in
the name of the project directory itself. Then, within the filenames in that
directory, use names that can be used across many projects.</p>
<p>This is because if a type of file always has the same name in all your project
directories, you and your team will find it easy to find that file and use
that file as they move from one project to the next. It even will allow you
to write code that leverages the fact that certain files always have the
same name across projects.</p>
<p>You don’t, however, want to make file names so general that they aren’t
<em>discoverable</em> (see module 2.6 for more on the idea of <em>discoverability</em>).
A filename, in other words, shouldn’t be so generic that its name doesn’t
give you an good idea of what it contains.</p>
<p>Say, for example, that you used the name “file_a” for the metadata file in your
project directory. This filename is generic and would work across many projects,
unlike a filename that includes something like the name of the experiment. It’s
so generic, though, that it would be hard for someone to figure out what the
file contains just by looking at its name. A better name would be something like
“experiment_metadata”—generic enough to work across many projects, but
detailed enough to be discoverable.</p>
<p>Another thing to consider, as you select file naming conventions, is to avoid
special characters in filenames. We discussed this idea in module 2.4, in
the context of avoiding special characters in the column names and cell entries
in a data collection spreadsheet. Similar considerations apply to filenames.
While many operating systems allow you to include things like spaces in
filenames, these special characters can make it harder to write code that works
with the file. Try to write filenames that have only alphanumeric characters and
underscores.</p>
<p>One of the biggest culprits here are spaces. It is appealing to include spaces
in a filename: it’s easier to read the words in the filename if they’re
separated by spaces. You should, though, get out of this habit. Once you move to
coding with the file, the spaces will be a pain. Often, when a computer parses
code, it thinks it’s gotten to the end of something when it gets to a space.
When it gets to a space in a filename, for example, it can think it’s gotten to
the end of that filename in some contexts. So, if you put a space in the middle
of a filename, it can confuse the computer.</p>
<p>There are ways to help the computer out—ways to “escape” special characters,
to the computer will treat them literally rather than attributing special
meaning to these special characters. However, it’s no fun to have to do over
and over again as you use a set of files. It’s much simpler to enforce a rule to
use underscores instead of spaces in your filenames: “experiment_metadata.Md”,
for example, rather than “experiment metadata.Md”. The underscores serve the
same purpose of legibility that the spaces do, by separating words within the
filename. They won’t confuse the computer in the same way, though.</p>
<p>Another consideration is that it is good practice to write code using relative pathnames that start from the
top-level of the project directory <span class="citation">(Bertin and Baumer 2021)</span>. In other words, tell
the computer where to find the files starting from the top level of the project
directory. This is because these relative pathnames will work equally well on
someone else’s computer, whereas if you use file pathnames that are absolute
(i.e., giving directions to the file from the root directory on your computer),
then when someone else tries on run the code on their own computer, it won’t
work and they’ll need to change the filepaths in the code, since everyone’s
computer has its files organized differently. For example, if you, on your
personal computer, have the project directory stored in your “Documents” folder,
while a colleague has stored the project directory in his or her “Desktop”
directory, then the absolute filepaths for each file in the directory will be
different for each of you. The relative pathnames, starting from the top level
of the project directory, will be the same for both of you, though, regardless
of where you each stored the project directory on your computer.</p>
<p>When it comes to code scripts in your project, there’s also one other think you
may want to consider in naming conventions. Often, you will have divided key
tasks (like data entry, pre-processing, and analysis) into separate scripts. The
scripts will need to follow a specific order when they are run to recreate the
results for the project. In this case, you may want to consider starting each
script’s filename with a number, where the numbers indicate the order that the
scripts should be run <span class="citation">(Marwick, Boettiger, and Mullen 2018; Bertin and Baumer 2021)</span>. For example, your script files might look like:
“01_reading_data.R”, “02_preprocessing_data.R”, “03_exploratory_analysis.R”, and
so on.</p>
<p>There are other cases where you’ll have more than one of a certain file type.
For example, within your raw data files, you may have one file per sample for
an assay like flow cytometry, or one file per timepoint if you’re recording
data for multiple timepoints.</p>
<p>In this case, you’ll need to develop rules for how you name these files,
chosing a system that allows different filenames for each file. As you do,
there are two things you can keep in mind: first, adhering to standards when
possible, and second designing filenames in a way that you can leverage
something called <em>regular expressions</em>.</p>
<p>In an earlier module (2.2), we talked about standards in terms of recording
data. We emphasized how powerful standards can be if they are regularly
followed in practice. Just as standards are a powerful tool when recording
data, they are also a powerful tool when creating filenames.
If there are conventions in your discipline for how certain files are named,
follow these.</p>
<p>One example is that there may be a standard way that a piece of laboratory
equipment names a file. For example, it may always include some elements like
the sample name and the date that the sample was run through the equipment.
In this case, you don’t want to change these filenames. You want to keep them
in the standard format, as people may have already built tools to work with
that standard. If you change from the standard, those tools wouldn’t be available.
Standards also tend to help with discoverability, so if you change the filenames
for the standard, it may make it harder for someone else to navigate your files.</p>
<p>If standards don’t exist for naming a certain type of file, you can create your
own standards. As you do, you can think about how to create filenames that
leverage <em>regular expressions</em>. These are coding tools that can search for
patterns that you specify in character strings, including filenames.</p>
<p>As an example, say you have files that record separate timepoints of your
experiment. You could pick a naming convention that always includes the
timepoint in the filename, recorded using the same conventions and always in
the same place in the filename. If you collect at timepoints that you call
“day 7”, “day 14”, and “day 21”, you might incorporate these within the filenames
using “D” for “day” and the two digits for the number. This would result in
files that include something like “D07”, “D14”, or “D21” in the name. It would
be straightforward to pull this information back out of the filenames if you
always put it in the same spot in the filename. For example, if you’re collecting
bacterial loads by measuring colony-forming units, you might name the files,
“cfu_D07.xlsx”, “cfu_D14.xlsx”, and “cfu_D21.xlsx”. Because you have
always put the varying information (the timepoint) in the same spot, it
will be easy to extract this with code using regular expressions.</p>
</div>
<div id="creating-and-using-a-project-template" class="section level3 hasAnchor" number="2.7.3">
<h3><span class="header-section-number">2.7.3</span> Creating and using a project template<a href="experimental-data-recording.html#creating-and-using-a-project-template" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Once you have a blueprint for a template for a project directory, you can create
this as a “physical template” directory on your computer.
This process is, once you have designed the template, very easy. It involves no
fancy tools—in fact, it’s so straightforward that at first it might seem too
simple to be useful. For this basic approach, you will create an example file
directory that captures your desired project directory structure. If you have
created any templates, either for data collection (module 2.4 and 2.5) or for
reports (modules 3.7–3.9), you can include those within this structure.</p>
<p>In other words, you will create a basic file directory with the desired template
files and file directory structure. When you are ready to start a new project,
you will copy this template, rename the copy to be specific to the new project,
and then use this directory to store and work with the data you collect for the
project. Figure <a href="experimental-data-recording.html#fig:templatedirectory">2.23</a> gives an example of what the final
resulting template directory might look like, as well as how it can be copied,
renamed, and used as you start new projects.</p>
<div class="figure"><span style="display:block;" id="fig:templatedirectory"></span>
<img src="figures/project_template_directory.png" alt="A research group can create a file directory that will serve as a template for all the experiments of a certain type in your laboratory. The template can include templates of files for data recording and for generating reports. To start recording data for a new experiment, a researcher can copy and rename this template directory." width="\textwidth" />
<p class="caption">
Figure 2.23: A research group can create a file directory that will serve as a template for all the experiments of a certain type in your laboratory. The template can include templates of files for data recording and for generating reports. To start recording data for a new experiment, a researcher can copy and rename this template directory.
</p>
</div>
<p>This template is not restrictive—it serves as a starting point, but it can
be adapted for each specific project. For example, if you are collecting
data from an assay that you have not used in past experiments, you can add
a new data subdirectory to your project directory to use for storing that new
type of data. Figure <a href="experimental-data-recording.html#fig:projecttemplatecomplex">2.24</a> shows an example of
how you could customize the basic template shown in Figure <a href="experimental-data-recording.html#fig:templatedirectory">2.23</a>.</p>
<div class="figure"><span style="display:block;" id="fig:projecttemplatecomplex"></span>
<img src="figures/project_template_morecomplex.png" alt="Example of a more complex project directory structure that could be created, with directories added to store data collected through flow cytometry and single cell RNA sequencing." width="\textwidth" />
<p class="caption">
Figure 2.24: Example of a more complex project directory structure that could be created, with directories added to store data collected through flow cytometry and single cell RNA sequencing.
</p>
</div>
<p>Keep in mind, though, that you do want to keep a balance, where you avoid
unneeded changes to the project template within each specific project’s
directory. This is because many of the benefits of standardizing (e.g.,
knowing where things are, building tools that leverage the standardized
directory structure) are lost as the directories for specific projects grow
to be more and more different from each other.</p>
<p>Figure <a href="experimental-data-recording.html#fig:basicprojecttemplateuse">2.25</a> gives a basic walk-through of the
simple steps you’ll use to start a new project directory once you’ve created
this type of template (we will cover this example in much more detail in the
next module, where we walk through a full example of designing and using a
project template).</p>
<div class="figure"><span style="display:block;" id="fig:basicprojecttemplateuse"></span>
<img src="figures/project_template_basic_use.png" alt="Steps in using a basic project directory template that you have created for a type of study or experiment." width="\textwidth" />
<p class="caption">
Figure 2.25: Steps in using a basic project directory template that you have created for a type of study or experiment.
</p>
</div>
</div>
<div id="project-directories-as-rstudio-projects" class="section level3 hasAnchor" number="2.7.4">
<h3><span class="header-section-number">2.7.4</span> Project directories as RStudio Projects<a href="experimental-data-recording.html#project-directories-as-rstudio-projects" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If you are using the R programming language for data pre-processing, analysis,
and visualization—as well as RMarkdown for writing reports and
presentations—then you can use RStudio’s “Project” functionality to make it
even more convenient to work with files within a research project’s directory.
You can make any file directory a “Project” in RStudio by chosing “File” -&gt;
“New Project” in RStudio’s menu. This gives you the option to create a
project from scratch or to make an existing directory and RStudio Project.</p>
<p>When you make a file directory an RStudio Project, it doesn’t change much in
the directory itself except adding a “.RProj” file. This file keeps track of
some things about the file directory for RStudio, including preferred settings
for RStudio to use when working in that project.</p>
<p>When you are working in an RStudio Project, RStudio will automatically move your
working directory to be the top-level directory of the Project directory. This
makes it easy to write code that uses this directory as the presumed working
directory, using relative file paths to identify and files within the directory.
We discussed the value of using relative pathnames earlier in this module, when
we discussed how to design file naming conventions for your project directory.
In particular, if you share the project directory with someone else, they can
similarly open the RStudio Project in their own version of RStudio, and all the
relative pathnames to files should work on their system without any problems.
This feature helps make code in an RStudio Project directory reproducible across
different people’s computers.</p>
<p>There are some other advantages, as well, to turning each of your research
project directories into RStudio Projects. One is that it is very easy to
connect each of these Projects with GitHub, which facilitates collaborative work
on the project across multiple team members while tracking all changes under
version control. If you are tracking the project directory under the Git version
control system, then when you open the RStudio Project, there will be a special
tab in one of the panes to help in using Git with the project. This tab provides
a visual interface for you to commit changes you’ve made, so they are tracked
and can be reversed if needed, and also so you can easily push and pull these
committed changes to and from a remote repository, like a GitHub repository, if
you are collaborating with others. This functionality is described in modules
2.9 through 2.11.</p>
<p>Having your project directories set up as R Projects also makes it easy to
navigate among different projects. When you close RStudio and reopen it, it will
automatically open in the last Project you had open. There is a small tab in the
top right hand corner of the RStudio window that lists the project you are
currently in. To move to a different Project, you can click on the down arrow
beside this project name. There will be a list of your most recent projects, as
well as options to open any Project on your computer. If you want to work in
RStudio, but not in any of the Projects, you can choose to “Close Project”.</p>
<!-- ### Discussion questions -->

</div>
</div>
<div id="module8" class="section level2 hasAnchor" number="2.8">
<h2><span class="header-section-number">2.8</span> Example: Creating a project template<a href="experimental-data-recording.html#module8" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For this module, we’ll show an example of creating a project directory template
for a lab group. We will walk through the process of creating a project
directory template that could be used to manage and analyze data from any of the
specific studies for this group. We’ll start by discussing the steps of the
conceptual design—figuring out how a blueprint for the standard subdirectories
and the file naming conventions. We’ll then show how this blueprint can be
developed into physical implementation: a file directory that can be copied and
renamed to initiate a new project. The full directory of files for this example
can be found at <a href="https://github.com/geanders/example_for_improve_repro" class="uri">https://github.com/geanders/example_for_improve_repro</a>, where you
can download them or explore them online.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Examine files from previous projects as a step in developing a project
directory template</li>
<li>Develop a structure of subdirectories for a project directory template</li>
<li>Create a project directory template to initialize consistently-formatted
directories for a lab group’s experiments</li>
<li>Explain how a report template can be incorporated within a project directory
template</li>
</ul>
<div id="description-of-the-example-set-of-studies" class="section level3 hasAnchor" number="2.8.1">
<h3><span class="header-section-number">2.8.1</span> Description of the example set of studies<a href="experimental-data-recording.html#description-of-the-example-set-of-studies" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this module, we’ll use an example based on a set of real immunology
experiments. This example highlights how a research laboratory will often
conduct a similar type of experiment many times, so it lets us demonstrate how a
project directory template can be reused across similar experiments. It will
allow us to show you how you can move from designing a file directory for a
single experiment to designing one that can be used repeatedly, and then how you
can take advantage of consistency in the directory structure across projects to
make templates for data collection that can be reused.</p>
<p>This example covers a group of studies that explored novel
treatments for tuberculosis. While treatments exist for tuberculosis, the
current treatment regime is lengthy and involves a combination of multiple
drugs. If the treatment is not completed, it can result in the development and
spread of drug-resistant tuberculosis strains, and so the treatment sometimes
must be done under observation <span class="citation">(Barry and Cheung 2009)</span>. If the patient has a
strain of tuberculosis that is resistant to first-line drugs, they
need to be treated with second-line drugs, which can have serious side effects
<span class="citation">(Barry and Cheung 2009)</span>. There is a critical need to develop more candidate drugs
against this disease, given all the limitations and struggles of the current
treatment regime.</p>
<p>Each study investigates how mice challenged with tuberculosis respond
to different treatments, both in terms of how well they handle the treatment
(assessed by checking if their weight decreases notably while on treatment) and
also how well the treatment manages to limit the growth of tuberculosis in the
mouse’s lungs.</p>
<p>These example studies were conducted with similar designs and similar
goals—all aimed to test candidate treatments for tuberculosis. Most studies in
this set tested one or more treatments as well as one or more controls. The
controls could include negative controls, like saline solution, or positive
controls, like a drug already in use to treat the disease (e.g., isoniazid). A
few of the studies tested only controls, to develop baseline expectations for
things like the bacterial load in different mouse strains. The set of studies
tested some treatments that were monotherapies (only one drug given to the
animal) as well as some that were combinations of two or three drugs.
For many of the drugs that were tested, they were tested at different doses and,
in some cases, different methods of delivery or different mouse models.</p>
<p>Each of the treatments were given to several mice that had been infected with
<em>Mycobacterium tuberculosis</em>. During the treatment, the mice were weighed
regularly. This weight measurement helps to determine if a particular treatment
is well-tolerated by the animals—if not, it may show through the treated mice
losing weight during treatment. For convenience, the mice were not weighed
individually. Instead, mice with the same treatment were kept in a single cage,
and the entire cage was weighed, the weight of the cage itself factored out, and
the average weight of mice determined by dividing by the number of mice in the
cage. After a period of time, the mice were sacrificed and one lobe from their
lungs was used to determine each mouse’s bacterial load, through plating the
material from the lobe and counting the colony forming units (CFUs). One aim of
the data analysis is to compare the bacterial load of mice under various
treatments to the bacterial load of mice in the control group.</p>
<p>The full set of studies included 19 studies. These were conducted at
different times, but the data for all of the studies can be collected using a
common format, and we’ll talk about how both data collection templates and a
project directory template could be designed to accomodate these experiments.</p>
</div>
<div id="step-1-survey-of-data-collected-for-the-projects" class="section level3 hasAnchor" number="2.8.2">
<h3><span class="header-section-number">2.8.2</span> Step 1: Survey of data collected for the projects<a href="experimental-data-recording.html#step-1-survey-of-data-collected-for-the-projects" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The first step in developing a project template is to survey the typical types
of files that are included in your research projects. To give an example of this
part of the design process, let’s walk through the types of data that were
collected for the example studies.</p>
<p>First, there were metadata recorded for each study. Figure
<a href="experimental-data-recording.html#fig:metadata">2.26</a> gives an example. This includes information about the strain
of mouse that was used in the study, treatment details (including the method of
giving the drug or drugs, how often they were given each week, and for how many
weeks), how much bacteria the animals were exposed to (measured both in terms of
the inoculum they were given and their bacterial load one day after they were
given that inoculum, which was based on sacrificing one animal the day after
challenging all the animals with the bacteria), and, if the study included a
novel drug as part of the tested treatment, the batch number of that drug.</p>
<div class="figure"><span style="display:block;" id="fig:metadata"></span>
<img src="figures/project_metadata.png" alt="Example of recording metadata for a study in the set of example studies for this module." width="\textwidth" />
<p class="caption">
Figure 2.26: Example of recording metadata for a study in the set of example studies for this module.
</p>
</div>
<p>Next, the researchers recorded some information about each treatment group
within the experiment. This typically included at least one negative control. In
some cases, there was also a positive control, in which the animals were treated
with a drug that’s in standard use against tuberculosis already (e.g.,
isoniazid). Most studies would also test one or more treatments, which could
include monotherapies or combined therapies. Figure <a href="experimental-data-recording.html#fig:treatmentdetails">2.27</a>
shows an example of the data that were recorded on each treatment in the study.
These data include the names and doses of up to three drugs in each treatment,
as well as a column where the researcher can provide detailed specifications of
the treatment.</p>
<div class="figure"><span style="display:block;" id="fig:treatmentdetails"></span>
<img src="figures/project_treatment_details.png" alt="Example of recording treatment details for a study in the set of example studies for this module." width="\textwidth" />
<p class="caption">
Figure 2.27: Example of recording treatment details for a study in the set of example studies for this module.
</p>
</div>
<p>Once the animals were challenged with the bacteria, treatment began, and two
main types of data were measured and recorded. First, the mice were weighed once
a week. For convenience, the mice were not weighed
individually. Instead, mice with the same treatment were kept in a single cage,
and the entire cage was weighed, the weight of the cage itself factored out, and
the average weight of mice for that treatment determined by dividing the weight
of all mice in the cage by the number of mice in the cage. These weights were converted to a measure of the percent change in
weight since the start of treatment. If the animals’ weights decrease during the
treatment, it is a marker that the treatment is not well-tolerated by the
animals. Figure <a href="experimental-data-recording.html#fig:mouseweight">2.28</a> shows an example of how these data could be
recorded. All animals within a treatment group were kept in the same cage, and
this cage was measured once a week. By dividing the weight of all animals in the
cage by the number of animals, the researchers could estimate the average weight
of animals in that treatment group, which is recorded as shown in Figure
<a href="experimental-data-recording.html#fig:mouseweight">2.28</a>.</p>
<div class="figure"><span style="display:block;" id="fig:mouseweight"></span>
<img src="figures/project_mouse_weights.png" alt="Example of recording weekly weights of mice in each treatment group for the example set of studies." width="\textwidth" />
<p class="caption">
Figure 2.28: Example of recording weekly weights of mice in each treatment group for the example set of studies.
</p>
</div>
<p>Finally, after the treatment period, the mice were sacrificed and a portion of
each mouse’s lung was used to estimate the bacterial load in that mouse. Figure
<a href="experimental-data-recording.html#fig:bacterialload">2.29</a> shows an example of how the data on the bacterial load
in each mouse can be recorded.</p>
<div class="figure"><span style="display:block;" id="fig:bacterialload"></span>
<img src="figures/project_bacterial_load.png" alt="Example of recording the bacterial load in the lungs of each mouse at the end of treatment for the example set of studies." width="\textwidth" />
<p class="caption">
Figure 2.29: Example of recording the bacterial load in the lungs of each mouse at the end of treatment for the example set of studies.
</p>
</div>
<p>These examples are all data that the researchers record by entering them on
spreadsheets. It is helpful at this stage to ensure this type of data is
recorded in a way that separates data recording and analysis (module 2.1).
The example files we’ve shown here do—there are no extra elements in these
spreadsheets that do calculations or create graphs. Later in this module, we’ll
talk a bit more about how these templates can be designed as part of the
process of designing the full project directory template. Earlier modules
(modules 2.4 and 2.5) provide more focused details on designing data collection
templates like these.</p>
<p>Another type of files that the group’s studies typically generate are ones
that are generated directly by laboratory equipment. For example, their
experiments may include flow cytometry assays, with files output in a
specialized format directly from the flow cytometer. Some experiments might
also collect data through single-cell RNA sequencing. We’ll want to keep
these files in mind as we design the structure of the project directory
template.</p>
</div>
<div id="step-2-organizing-a-project-directory" class="section level3 hasAnchor" number="2.8.3">
<h3><span class="header-section-number">2.8.3</span> Step 2: Organizing a project directory<a href="experimental-data-recording.html#step-2-organizing-a-project-directory" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Once you’ve determined the types of files that you’ll normally include in your
project, you then need to decide how to organize them into subdirectories in the
project file directory. In this case, we’ve organized the project directory
template to include just a few things at the top level (Figure
<a href="experimental-data-recording.html#fig:templatedirectorymod8">2.30</a>, also shown in module 2.7):</p>
<ul>
<li>A spreadsheet that stores meta-data about the experiment</li>
<li>A subdirectory named “raw_data”, where we’ll store original raw files of
data, before it is pre-processed, focusing on data generated by laboratory
equipment</li>
<li>A subdirectory named “data”, which will store experimental data once it
has been pre-processed, as well as recorded data that do not require pre-processing</li>
<li>A subdirectory named “R”, which will store code for pre-processing and analysis</li>
<li>A subdirectory named “reports”, which will store the files to generate
reports, as well as any reports that are ultimately generated</li>
</ul>
<div class="figure"><span style="display:block;" id="fig:templatedirectorymod8"></span>
<img src="figures/project_template_directory.png" alt="A research group can create a file directory that will serve as a template for all the experiments of a certain type in your laboratory. The template can include templates of files for data recording and for generating reports. To start recording data for a new experiment, a researcher can copy and rename this template directory." width="\textwidth" />
<p class="caption">
Figure 2.30: A research group can create a file directory that will serve as a template for all the experiments of a certain type in your laboratory. The template can include templates of files for data recording and for generating reports. To start recording data for a new experiment, a researcher can copy and rename this template directory.
</p>
</div>
<p>You may have noticed that this structure captures each of the main elements
that we discussed including in a project template in the last module:
data, code, reports, and metadata.</p>
<p>In this structure, we’ve selected subdirectory names that are generic enough
(e.g., “data”, “reports”) that they can be reused across many of our projects
without modification. These names should also be clear to any researcher that
explores this directory in the future, since the names are clear and
unambiguous. However, you might make different choices—for example, if
some of your team aren’t familiar with R as a programming language, you may
want to use the subdirectory name “code” rather than “R”.</p>
<p>Within some of the subdirectories, we can include more subdirectories to further
organize files (Figure<a href="experimental-data-recording.html#fig:projecttemplatecomplexmod8">2.31</a>, repeated from
module 2.7). For example, within the “data” subdirectory, we can have
subdirectories for different types of data:</p>
<ul>
<li>A subdirectory named “flow_data” for data from flow cytometry</li>
<li>A subdirectory named “recorded_data”, for data that are recorded “by hand”
in the laboratory (for example, the weights of animals)</li>
<li>A subdirectory named “sc_rna_seq_data” for data from single-cell RNA
sequencing</li>
</ul>
<div class="figure"><span style="display:block;" id="fig:projecttemplatecomplexmod8"></span>
<img src="figures/project_template_morecomplex.png" alt="Example of a more complex project directory structure that could be created, with directories added to store data collected through flow cytometry and single cell RNA sequencing." width="\textwidth" />
<p class="caption">
Figure 2.31: Example of a more complex project directory structure that could be created, with directories added to store data collected through flow cytometry and single cell RNA sequencing.
</p>
</div>
<p>Again, these subdirectories are named in a way that will generalize to many
different experiments and yet also clearly labels the contents. Similar
subdirectory diversions could also be used within the “raw_data” subdirectory,
which would include files for data that need to be pre-processed before
they’re used in statistical analysis (modules 3.1–3.3). For example, the
raw flow cytometry data will need to be gated—a process that will quantify
immune cell phenotypes in each sample—before it’s used in statistical
analysis.</p>
<p>The exact combination of subdirectories within the “data” subdirectory might
change from experiment to experiment. For example, some experiments might
include single-cell RNA sequencing assays, while some may not. When we use the
template, it will be easy to delete any “data” subdirectories for assays we are
not conducting, but by including them in the template, we can insure that we use
a consistent name for each subdirectory when we do include it. If you’re
trying to be consistent, it’s easier to start with everything you might need
and delete some elements to customize for a particular project rather than
starting with a minimal framework and adding.</p>
</div>
<div id="step-3-establishing-file-name-conventions" class="section level3 hasAnchor" number="2.8.4">
<h3><span class="header-section-number">2.8.4</span> Step 3: Establishing file name conventions<a href="experimental-data-recording.html#step-3-establishing-file-name-conventions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Next, we decided on how we’d name files in the directory. First, there are some
files that will be included in every project. These include a file to record
metadata about the experiment, as well as a file to record mouse weights over
the course of the experiment and bacterial load at the end of the experiment.</p>
<p>For both of these files, we selected filenames that would balance
generalizability with discoverability (see module 2.7 for more on setting rules
for filenames based on these principles). The file with experimental data, for
example, is named “experiment_metadata.xlsx”. This name is generic enough that
it will work for each of the studies, but it is also clear enough that people
will have a good idea of what’s in the file when they explore the project
directory. Similarly, the file for recording weights and bacterial load is named
“recorded_lab_data.xlsx”, which again balances considerations generalizability
with discoverability.</p>
<p>You may have noticed that both these names avoid any special characters,
including spaces. Instead, the filenames use underscores to help distinguish
different words in the filenames and make them easy to read.</p>
<p>These experiments may have read-outs from laboratory equipment. Examples include
data from assays like flow cytometry and single-cell RNA sequencing. In these
cases, we’ll keep the filenames that are generated by the laboratory equipment,
so that we’ll maintain those standard formats.</p>
</div>
<div id="step-4-designing-data-collection-templates" class="section level3 hasAnchor" number="2.8.5">
<h3><span class="header-section-number">2.8.5</span> Step 4: Designing data collection templates<a href="experimental-data-recording.html#step-4-designing-data-collection-templates" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The next step is to create any necessary data collection templates. We’ll create
a separate spreadsheet for each type of data, but we can group them into files
if we’d like (e.g., one spreadsheet file with several separate sheets). In our
example, we created two files to store this type of data, one for the metadata
that are recorded at the start of the experiment (overall experiment details and
the details of each tested treatment) and one for the data that are collected
over the course of the experiment (mouse weights and bacterial loads). Within
each file, we’ve used separate sheets to record the different types of data.
This allows us to keep similar types of data together in the same file, while
having a tidy collection format for each specific type of data (Figure
<a href="experimental-data-recording.html#fig:projectdatacollection">2.32</a>).</p>
<div class="figure"><span style="display:block;" id="fig:projectdatacollection"></span>
<img src="figures/project_data_collection.png" alt="Data collection templates for the example project directory template. These templates were created in two files, one for metadata, which is saved in the main directory of the project, and one for data collected in the laboratory during the experiment, which is saved in the 'data' subdirectory. Each file is saved as a spreadsheet file, with two sheets in each file to store different types of data." width="\textwidth" />
<p class="caption">
Figure 2.32: Data collection templates for the example project directory template. These templates were created in two files, one for metadata, which is saved in the main directory of the project, and one for data collected in the laboratory during the experiment, which is saved in the ‘data’ subdirectory. Each file is saved as a spreadsheet file, with two sheets in each file to store different types of data.
</p>
</div>
<p>All of these data collection files are designed using the principles of tidy
data collection. In modules 2.4 and 2.5, we showed how you can create tidy data
collection templates to use, and how these can be paired with
reproducible reporting tools to separate the steps of data collection and
reporting (modules 3.7 through 3.9 go into much more depth on these reproducible
reporting tools). Once you have decided on the types of data that you will
usually collect for the type of study that this template is for, you can use
that process to create tidy data collection templates for each type of data.</p>
<p>When we created the template for each type of data, we added placeholder data
(formatted in red to indicate that it is placeholder, rather than final
data). This is so the researcher can see an example of how to enter data in
the template when they start a new project.</p>
<p>Figure <a href="experimental-data-recording.html#fig:replacingplaceholdermetadata">2.33</a> gives an example of this process.
One of the files that is included in the example template directory shown
earlier is a spreadsheet to record metadata on the experiment. This spreadsheet
file has two sheets, one that records overall metadata on the study (for
example, the weeks of treatment given and the strain of mouse used) and one that
records details on each of the treatments that was tested. In the file in the
template directory, these spreadsheet pages include placeholder data. These are
formatted in red, so that they visually can be identified as placeholders. By
including these placeholder data, the researcher can see an example of the
format that you expect to be used in recording data in this file. Once the
project template is copied, the researcher will replace these data with the real
data, and then change the font color to black to indicate that the placeholder
data have been replaced (Figure <a href="experimental-data-recording.html#fig:replacingplaceholdermetadata">2.33</a>).</p>
<div class="figure"><span style="display:block;" id="fig:replacingplaceholdermetadata"></span>
<img src="figures/project_replace_placeholder_metadata.png" alt="The template includes a file with experiment metadata, with a sheet for recording the overall details of the experiment. A user can open this file and replace the placeholder values (in red) with real values for the experiment. By changing the text color to black, the user can have a visual confirmation that the placeholder data have been replaced with real study data." width="\textwidth" />
<p class="caption">
Figure 2.33: The template includes a file with experiment metadata, with a sheet for recording the overall details of the experiment. A user can open this file and replace the placeholder values (in red) with real values for the experiment. By changing the text color to black, the user can have a visual confirmation that the placeholder data have been replaced with real study data.
</p>
</div>
<p>Another sheet of this spreadsheet allows the researcher to record the details of
each of the treatments that were tested in the experiment. Again, placeholder
data are included in the template in a red font to help show the researcher how
to record the data, and these are meant to be replaced with real data from the
specific experiment (Figure <a href="experimental-data-recording.html#fig:replacingplaceholdertreatment2">2.34</a>). A
similar format is used in the template file to record data from the experiment,
including the weights of each animal over each week of treatment and the final
bacterial load in each animal at the end of treatment. Again, there are
placeholder values in the template file, which the researcher will replace with
real data after copying the project template for a new experiment.</p>
<div class="figure"><span style="display:block;" id="fig:replacingplaceholdertreatment2"></span>
<img src="figures/project_replacing_placeholder_treatment_data.png" alt="The template includes a file with experiment metadata, with a sheet for recording the details of each treatment. A user can open this file and replace the placeholder values (in red) with real values for the treatments in the experiment. By changing the text color to black, the user can have a visual confirmation that the placeholder data have been replaced with real study data." width="\textwidth" />
<p class="caption">
Figure 2.34: The template includes a file with experiment metadata, with a sheet for recording the details of each treatment. A user can open this file and replace the placeholder values (in red) with real values for the treatments in the experiment. By changing the text color to black, the user can have a visual confirmation that the placeholder data have been replaced with real study data.
</p>
</div>
</div>
<div id="step-5-designing-a-report-template" class="section level3 hasAnchor" number="2.8.6">
<h3><span class="header-section-number">2.8.6</span> Step 5: Designing a report template<a href="experimental-data-recording.html#step-5-designing-a-report-template" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A final and optional step is to create one or more template reports. You can
create report templates using tools for reproducible reports—in R, a key tool
for this is RMarkdown. Here, we’ll cover using this tool for creating a report
briefly, but there are many more details in modules 3.7 through 3.9. Having
example files will help you to develop a template project report that can input
the type of data that you typically collect for this type of project.</p>
<p>We created an Rmarkdown file that does this analysis and visualization and
included it in the project template directory. This means that the report file
will be copied and available each time someone copies the project template
directory at the start of a new project. However, you are not obligated to keep
the report identical to the template. Instead, the template report serves as a
starting point, and you can add to it or adapt it as you work on a study.</p>
<p>This file is created using the RMarkdown format,
which combines text with executable code. You can create this template so that it
inputs the experimental data from the file formats created for the data recording
files in the project template. By doing this, the researcher should be able to “knit”
this report for a new experiment, and it should recreate the report based on the
data recorded for that experiment (Figure <a href="experimental-data-recording.html#fig:makingareport">2.35</a>). By knitting
this template report, you can create a nicely formatted version of the report for
the experimental data (Figure <a href="experimental-data-recording.html#fig:examplereport1">2.36</a>).</p>
<div class="figure"><span style="display:block;" id="fig:makingareport"></span>
<img src="figures/project_opening_and_running_report.png" alt="Example of how a user can create a report from the template. The template includes an example report, which is written using RMarkdown. The user can open this template report file and use the 'Knit' button in RStudio to render the file. As long as the experimental data are recorded using the data template files, the code for this report can process the data to generate a report from the data. The user can also make changes and additions to the template report." width="\textwidth" />
<p class="caption">
Figure 2.35: Example of how a user can create a report from the template. The template includes an example report, which is written using RMarkdown. The user can open this template report file and use the ‘Knit’ button in RStudio to render the file. As long as the experimental data are recorded using the data template files, the code for this report can process the data to generate a report from the data. The user can also make changes and additions to the template report.
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:examplereport1"></span>
<img src="figures/project_example_report_study001.png" alt="Example of the output from 'knitting' a report from the project template" width="\textwidth" />
<p class="caption">
Figure 2.36: Example of the output from ‘knitting’ a report from the project template
</p>
</div>
<p>Specifically, for this set of studies a preliminary report was designed, with an
example shown in Figure <a href="experimental-data-recording.html#fig:prelimreport">2.37</a>. This report uses the first page
to provide a nicely format version of the metadata for the study, including a
table with overall details and a table with details for each specific treatment
that was tested. The second page provides a graph that shows the percent weight
change for mice in each treatment group compared to the weight of that group at
the start of treatment. The third page provides a graph that shows the bacterial
loads in each mouse, grouped by treatment, as well as the results of running a
statistical test, for each treatment group, of the hypothesis that the mean
of a transformed version of the measure of bacterial load (log-10) for the group
was the same as for the untreated control group.</p>
<div class="figure"><span style="display:block;" id="fig:prelimreport"></span>
<img src="figures/project_prelim_report.png" alt="Example of the preliminary report generated for each study in the set of example studies for this module. The first page includes metadata on the study, as well as details on each treatment that was tested. The second page shows how mouse weights in each treatment group changed over the course of treatment, to help identify if a treatment was well-tolerated. The third page graphs the bacterial load in each mouse, grouped by treatment, and gives the result of a statistical analysis to test which treatment groups had outcomes that were significantly different from the untreated control group." width="\textwidth" />
<p class="caption">
Figure 2.37: Example of the preliminary report generated for each study in the set of example studies for this module. The first page includes metadata on the study, as well as details on each treatment that was tested. The second page shows how mouse weights in each treatment group changed over the course of treatment, to help identify if a treatment was well-tolerated. The third page graphs the bacterial load in each mouse, grouped by treatment, and gives the result of a statistical analysis to test which treatment groups had outcomes that were significantly different from the untreated control group.
</p>
</div>
<p>Let’s take a closer look at a few of these elements. For example, Figure
<a href="experimental-data-recording.html#fig:studytable">2.38</a> shows the tables from the first page of the report shown
in Figure <a href="experimental-data-recording.html#fig:prelimreport">2.37</a>. If you look back to the data collection for
this study (e.g., Figures <a href="experimental-data-recording.html#fig:metadata">2.26</a> and <a href="experimental-data-recording.html#fig:treatmentdetails">2.27</a>),
you can see that all of the information in these tables was pulled from data
recorded at the start of the study.</p>
<div class="figure"><span style="display:block;" id="fig:studytable"></span>
<img src="figures/project_study_info_table.png" alt="Example of one element of the preliminary report generated for each study in the set of example studies for this module. The first page provides tables with metadata about the study and details about each treatment that was tested." width="\textwidth" />
<p class="caption">
Figure 2.38: Example of one element of the preliminary report generated for each study in the set of example studies for this module. The first page provides tables with metadata about the study and details about each treatment that was tested.
</p>
</div>
<p>Figure <a href="experimental-data-recording.html#fig:mouseweightsplot">2.39</a> shows the second page of the report. This
figure has taken the mouse weights—which were recorded in one of the data
collection templates for the project (Figure <a href="experimental-data-recording.html#fig:mouseweight">2.28</a>)—and used
them to generate a plot of how average mouse weight in each treatment group
changed over the course of the treatment.</p>
<div class="figure"><span style="display:block;" id="fig:mouseweightsplot"></span>
<img src="figures/project_mouse_weights_graph.png" alt="Example of one element of the preliminary report generated for each study in the set of example studies for this module. The second page provides a plot of how the weights of mice in each treatment changed over the course of treatment." width="\textwidth" />
<p class="caption">
Figure 2.39: Example of one element of the preliminary report generated for each study in the set of example studies for this module. The second page provides a plot of how the weights of mice in each treatment changed over the course of treatment.
</p>
</div>
<p>Figure <a href="experimental-data-recording.html#fig:bactcompare">2.40</a> shows the last page of the report. This page
starts with a figure that shows the bacterial load in the lungs of each mouse in
the study at the end of the treatment period. In this figure, the measurement
for each mouse is shown with a point, and these points are grouped by the
treatment group of the mouse. Boxplots are added to show the distribution across
the mice in each group. The color is used to show whether the treatment was a
negative control, a positive control, a monotherapy, or a combined therapy. The
second part of the page gives a table with the results from running a
statistical analysis to compare the bacterial load for mice in each treatment
group to the bacterial load in the mice in the untreated control group. Color is
added to the table to highlight treatments that had a large difference in
bacterial load from the untreated control, as well as treatments for which the
difference from the untreated control was estimated to be statistically
significant. All the data for these results, including the labels for the plot,
are from the data collected in the data collection templates shown earlier.</p>
<div class="figure"><span style="display:block;" id="fig:bactcompare"></span>
<img src="figures/project_bact_compare_plot.png" alt="Example of one element of the preliminary report generated for each study in the set of example studies for this module. The third page provides results on how bacterial load in the lungs compares among treatments at the end of the treatment period." width="\textwidth" />
<p class="caption">
Figure 2.40: Example of one element of the preliminary report generated for each study in the set of example studies for this module. The third page provides results on how bacterial load in the lungs compares among treatments at the end of the treatment period.
</p>
</div>
<p>We wrote the code in the report in a way that it will still run if there are
more or fewer observations in any of the data collection files, so the report
template has some flexibility in terms of how each study in the set of studies
might vary. For example, in the example set of studies, some of the experiments
were run using only a control group of mice, while others were run to test
several different treatment groups. The report template can accommodate
these differences across studies in the set of studies.</p>
<!-- ### Applied exercise -->

</div>
</div>
<div id="module9" class="section level2 hasAnchor" number="2.9">
<h2><span class="header-section-number">2.9</span> Harnessing version control for transparent data recording<a href="experimental-data-recording.html#module9" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As a research project progresses, researchers will often end up with many files
(e.g., ‘draft1.doc’, ‘draft2.doc’). This can result in an explosion of files,
and it becomes hard to track which files represent the “current” state of a
project. Version control allows researchers to edit and change research project
files more cleanly, while including messages to explain changes and maintaining
the power to backtrack to previous versions.</p>
<p>In this module, we will explain what version control is and how it can be used
in research projects to improve the transparency and reproducibility of
research, particularly for data recording. We’ll introduce you
to the basic idea of version control, using the Git software program as an
example. In later modules, we’ll explain version control platforms like GitHub,
as well as give some tips on how to use both within your research projects.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Define “version”, “version control”, “version control software”,
“repository”, “commit”, and “commit message”</li>
<li>Discuss challenges in coordinating changes in project files when working
in teams</li>
<li>List some downsides of physical laboratory notebooks</li>
<li>Distinguish between “version control” and “version control software”</li>
<li>Identify examples of versioning in a digital context (data, code, files)</li>
<li>Discuss how version control principles can improve collaboration in
scientific projects</li>
</ul>
<div id="challenges-of-collaborating-on-evolving-research-materials" class="section level3 hasAnchor" number="2.9.1">
<h3><span class="header-section-number">2.9.1</span> Challenges of collaborating on evolving research materials<a href="experimental-data-recording.html#challenges-of-collaborating-on-evolving-research-materials" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When research groups—or any other professional teams—collaborate on
publications and research, the process can be a bit haphazard. Teams often use
emails and email attachments to share updates on the project, and they sometimes
use email attachments to pass around the latest version of a document for others
to review and edit.</p>
<p>One fascinating example comes from the business world. After the implosion of
Enron, a trove of emails from within the company were released. This set of
emails has become known as the Enron Corpus and has been used for a variety of
research studies. One group of researchers investigated emails from this corpus
that involved people who were doing work with spreadsheets <span class="citation">(Hermans and Murphy-Hill 2015)</span>.
They found that passing Excel files through email attachments was a common
practice, and that messages within emails suggested that spreadsheets were
stored locally, rather than in a location that was accessible to all team
members <span class="citation">(Hermans and Murphy-Hill 2015)</span>. This meant that team members might often be
working on different versions of the same spreadsheet file. They note that “the
practice of emailing spreadsheets is known to result in serious problems in
terms of accountability and errors, as people do not have access to the latest
version of a spreadsheet, but need to be updated of changes via email.”
<span class="citation">(Hermans and Murphy-Hill 2015)</span> The same process for collaboration is often used in
scientific research: one study found, “Team members regularly pass data files
back and forth by hand, by email, and by using shared lab or project servers,
websites, and databases.” <span class="citation">(Edwards et al. 2011)</span></p>
<p>These practices make it very difficult to keep track of all project files, and
in particular, to track which version of each file is the most current. Further,
this process constrains patterns of collaboration—it requires each team member
to take turns in editing each file, or for one team member to attempt to merge
in changes that were made by separate team members at the same time when all
versions are collected.</p>
<p>This process also makes it difficult to keep track of why changes were made, and
often requires one team member to approve the changes of other team members.
While “Track changes” and “Comment” features in software like Microsoft Word can
help the team communicate with each other, these features often lead to a very
messy document at stages in the editing, where it is hard to pick out the
current versus suggested wording, and once a change is accepted or a comment
deleted, these conversations can be lost forever. Finally, word processing tools
are poorly suited to track changes or add suggestions directly to data or code,
as both data and code are usually saved in formats that aren’t native to word
processing programs, and copying them into a format like Word can introduce
problematic hidden formatting that can cause the data or code to malfunction.</p>
</div>
<div id="recording-data-in-the-laboratoryfrom-paper-to-computers" class="section level3 hasAnchor" number="2.9.2">
<h3><span class="header-section-number">2.9.2</span> Recording data in the laboratory—from paper to computers<a href="experimental-data-recording.html#recording-data-in-the-laboratoryfrom-paper-to-computers" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>More and more scientific researchers are tackling these challenges in their own
projects using something called <em>version control</em>. But how does version
control—traditionally a tool of software engineers—relate to collaborating
to collect and analyze scientific research data? Traditionally, experimental
data collected in a laboratory was recorded in a paper laboratory notebook.
These laboratory notebooks played a role not only as the initial recording of
data, but also keep a legal record of the data recorded in the lab
<span class="citation">(Mascarelli 2014)</span>. They were also a resource for collaborating across a
team and for passing on a research project from one lab member to another
<span class="citation">(Butler 2005)</span>.</p>
<p>However, paper laboratory notebooks have a number of limitations. First, they
can be very inefficient. In a time when almost all data analyses—even simple
calculations—are done on a computer, recording research data on paper rather
than directly entering it into a computer is inefficient. Also, any stage of
copying data from one format to another, especially when done by a human rather
than a machine, introduces the chance to copying errors. Handwritten laboratory
notebooks can be hard to read <span class="citation">(Butler 2005; J. M. Perkel 2011)</span>, and
they may lack adequate flexibility to handle the complex experiments often
conducted. Further, electronic alternatives can also be easier to search,
allowing for deeper and more comprehensive investigations of the data collected
across multiple experiments <span class="citation">(Giles 2012; Butler 2005; J. M. Perkel 2011)</span>. As one article notes, physical lab notebooks are “usually
chaotic and always unsearchable” <span class="citation">(J. M. Perkel 2011)</span>.</p>
<p>Given a widespread recognition of the limitations of paper laboratory notebooks,
in the past couple of decades, there have been a number of efforts, both formal
and informal, to move from paper laboratory notebooks to electronic
alternatives. In some fields that rely heavily on computational analysis, there
are very few research labs (if any) that use paper laboratory notebooks
<span class="citation">(Butler 2005)</span>. In other fields, where researchers have traditionally
used paper lab notebooks, companies have been working for a while to develop
electronic laboratory notebooks specifically tailored to scientific research
<span class="citation">(Giles 2012)</span>. Some early adapters were pharmaceutical industrial
labs, where companies had the budgets to get customized versions and the
authority to require their use. In academic laboratories, electronic lab
notebooks have taken longer to be adapted <span class="citation">(Giles 2012; Butler 2005)</span>. Indeed, a widely adopted platform for electronic laboratory
notebooks has yet to be taken up by the scientific community <span class="citation">(Kwok 2018)</span>,
despite clear advantages of recording data directly into a computer rather than
first using a paper notebook. As Kwok notes in a 2018 commentary,</p>
<blockquote>
<p>“Since at least the 1990s, articles on technology have predicted the imminent,
widespread adoption of electronic laboratory notebooks (ELNs) by researchers. It has
yet to happen” <span class="citation">(Kwok 2018)</span></p>
</blockquote>
<p>Instead of using customized electronic laboratory notebook software, some
academics are moving their data recording online, but are using more generalized
electronic alternatives, like Dropbox, Google applications, OneNote, and
Evernote <span class="citation">(J. M. Perkel 2011; Kwok 2018; Giles 2012; K. Powell 2012)</span>.
Some scientists have started using version control software, especially the
combination of Git and GitHub, as a way to improve laboratory data recording,
and in particular to improve transparency and reproducibility standards.
These pieces of software share the same pattern as Google applications or
Dropbox—they are generalized tools that have been honed and optimized for ease
of use through their role outside of scientific research, but can be harnessed
as a powerful tool in a scientific laboratory, as well. They are also free—at
least, for GitHub, at the entry and academic levels—and, even better, one
(Git) is open-source.</p>
</div>
<div id="defining-version-and-version-control" class="section level3 hasAnchor" number="2.9.3">
<h3><span class="header-section-number">2.9.3</span> Defining “version” and “version control”<a href="experimental-data-recording.html#defining-version-and-version-control" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Most scientific research today involves collaboration across a team of
researchers, rather than an individual scientist working alone. Collaboration
drives interdisciplinary science, but it also creates challenges. One
challenge comes with coordinating versions of research materials. These
materials can include data collection files, but can also include other
documents like study protocols, as well as physical materials like cell lines,
antibodies, and model organisms.</p>
<p>A <em>version</em> is one iteration of a research material that is evolving. For
example, a draft of a research paper is one version of that paper. Research data
that you collect may also go through several versions. For example, if you
identify a typo in data after you record it, you may need to correct the typo
and add a note or signature to explain that update. Further, if you are
collecting data at multiple timepoints, you may have new versions of a data file
as you complete each timepoint.</p>
<p>As materials evolve across versions, it introduces challenges in maintaining a
research process that is smooth, efficient, and error-free. One challenge is to
make sure it is always clear which version is the most current, as well as which
version should be used for specific purposes. For example, if several coauthors
are editing a paper draft, it is important to ensure they are all working on the
most recent version.</p>
<p>Another challenge is to coordinate the changes that different people make when
they work on the material at the same time. Scientific collaboration often does
not operate as an assembly line, where one person finishes their work on a
document or material and then hands it off to the next person. Instead, there
will often be several copies of a version in different peoples’ hands, with all
of them working on it at once. One example is a paper draft—often coauthors
all edit the latest draft at the same time, rather than one-by-one.
This creates the challenge of taking the contributions of each person and
coordinating their changes and additions into one primary copy.</p>
<p>A third challenge is to keep track of the changes that are made at each step, as
the document moves from version to version. This record can help in auditing for
errors or bugs that might be introduced as the document evolves. Ideally, the
record also will include some information about why changes were made at each
step.</p>
<p>These challenges can be addressed through a process called <em>version control</em>.
While the term is most commonly used in reference to software development, the
idea of version control is widely relevant. Any process that creates evolving
versions of a document or material can benefit from the idea of version control,
which aims to record and document changes to the material over time, coordinate
the contributions of different members of a team, and revert back to older
versions if needed. In this module, we’ll focus on version control as it applies
to research materials that are electronic (files and directories), but you may
also find it useful to think about how the principles and elements of version
control can be applied to other research materials, like cell lines and
antibodies.</p>
</div>
<div id="what-are-the-key-elements-of-version-control" class="section level3 hasAnchor" number="2.9.4">
<h3><span class="header-section-number">2.9.4</span> What are the key elements of version control?<a href="experimental-data-recording.html#what-are-the-key-elements-of-version-control" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The term <em>version</em> in <em>version control</em> refers to one iteration or state of a
document or set of documents, for example the current version of a data file.
The word <em>control</em> captures the idea of allowing for safe changes and updates to
the version, especially when more than one person is working on it. Part of this
“control” will also include recording the changes made from one version to the
next and annotating reasons for those changes.</p>
<p>The general term <em>version control</em> can refer to any method of syncing
contributions from several people to a file or set of files. Version control of
computer files can be done “by hand”, with a person manually logging each change,
and originally was <span class="citation">(Irving 2011)</span>. However, it’s much more efficient
to use a computer program to handle this tracking and to coordinate
contributions from multiple people. As Eric Raymond notes in <em>The Art of Unix
Programming</em>, “tracking all that detail is just the sort of thing computers are
good at and humans are not” <span class="citation">(E. S. Raymond 2003)</span>. He goes on to describe version
control as “a suite of programs that automates away most of the drudgery
involved in keeping an annotated history of your project and avoiding
modification conflicts” <span class="citation">(E. S. Raymond 2003)</span>.</p>
<p>Software for this purpose—<em>version control software</em>—was first developed for
software programming projects. Some popular version control software today
comes from these roots. In this section, we’ll introduce the key features of
version control, and to do so we’ll use examples and terminology from a common
version control software program called Git. While these terms are derived
from this particular software program, they represent ideas that are important
in any implementation of version control. Later, we’ll touch on how some of these
ideas are incorporated in other software, like Google Docs.</p>
<p>The software available for version control tracks electronic files. While the
very earliest version control software systems tracked single files, these
systems quickly moved to tracking sets of files, called <em>repositories</em>. A
repository is almost identical to a file directory (which you may also know as a
file folder), and indeed a repository starts from a file directory. The only
difference is the repository is enhanced with some additional overhead
<span class="citation">(Klemens 2014)</span>. This overhead is added to record how the files in the
directory have changed over time. You can compare this to how you might track
document changes if the documents were paper rather than electronic—you could
store the documents in a paper folder and add a piece of paper where you record
a log of each change you make to the documents in the folder. The extra overhead
that changes a regular file directory to a repository is very similar to the log
in this example. A repository, in other words, is a directory that is under
version control.</p>
<p>In a repository of files that is under version control, the version control
software takes snapshots of how the files look during your work on them. Each
snapshot is called a <em>commit</em>, and it provides a record of which lines in each
file changed from one snapshot to another, as well as exactly how they changed.
The idea behind these commits—recording the differences, line-by-line, between
an older and newer version of each file derives from a longstanding Unix command
line tool called <em>diff</em>. This tool, developed early in the history of Unix at
AT&amp;T’s Bell Labs <span class="citation">(E. S. Raymond 2003)</span>, is a solid and well-tested tool that does
the simple but important job of generating a list of all the differences between
two plain text files. Each commit in a repository includes the same type of
information about the differences introduced in the files at the time of that
commit.</p>
<p>When you are working with a directory under version control, you explain your
changes as you make them—in other words, version control allows for annotation
of the developing and editing process <span class="citation">(E. Raymond 2009)</span>. Each commit
requires you to enter a <em>commit message</em> describing why the changes in that
commit were made. The commit messages can serve as a powerful tool for
explaining changes to other team members or for reminding yourself in the future
about why certain changes were made. A repository under version control, then,
can include not only a complete history of how files in a project directory have
changed over the course of the project, but also why. If this feature is used
thoughtfully, then the commit history of the project provides a well-documented
description of the project’s full evolution. If you’re working on a manuscript,
for example, when it’s time to edit, you can cut whole paragraphs, and if you
ever need to get them back, they’ll be right there in the commit history for
your project, with their own commit message about why they were cut. If you make
the commit message clear, it will make it easy to find that commit if you ever
need those paragraphs again.</p>
<p>Further, each of the commits is given its own ID tag (in the Git software,
this is done through something called a unique SHA-1 hash <span class="citation">(Klemens 2014)</span>),
and version control systems have a number of commands that let you “roll back”
to earlier versions. This provides <em>reversability</em> within the project files,
allowing you to go back to the version as it was when a certain commit was made
<span class="citation">(E. Raymond 2009)</span>.</p>
<p>It turns out that this functionality—of being able to roll back to earlier
versions—has a wonderful side benefit when it comes to working on a large
project. It means that you don’t need to save earlier versions of each file. You
can maintain one and only one version of each project file in the project’s
directory, with the confidence that you never “lose” old versions of the file
<span class="citation">(J. Perkel 2018; Blischak, Davenport, and Wilson 2016)</span>. This allows you to maintain a clean and
simple version of the project files, with only one copy of each, ensuring it’s
always clear which version of a file is the “current” one (since there’s only
one version) <span class="citation">(Klemens 2014)</span>. This also provides the reassurance that you can
try new directions in a project, and always roll back to the old version if that
direction doesn’t work well.</p>
<p>In a 2011 commentary in <em>Nature Methods</em>, Perkel tells a story about how this
functionality helped one researcher keep his project directories simpler:</p>
<blockquote>
<p>“Early in his graduate career, John Blischak found himself creating figures
for his advisor’s grant application. Blischak was using the programming language
R to generate the figures, and as he iterated and optimized his code, he ran
into a familiar problem: Determined not to lose his work, he gave each new
version a different filename—analysis_1, analysis_2, and so on, for
instance—but failed to document how they had evolved. ‘I had no idea what had
changed between them,’ says Blischak… Using Git, Blischak says, he no longer
needed to maintain multiple copies of his files. ‘I just keep overwriting it and
changing it and saving the snapshots. And if the professor comes back and says,
’oh, you sent me an email back in March with this figure’, I can say, ‘okay,
well, I’ll just bo back to the March version of my code and I can recreate
it’.” <span class="citation">(J. Perkel 2018)</span></p>
</blockquote>
<p>A key strength, then, of using version control is its ability to track every
change made to files in the project, why the change was made, and who made it.
Version control creates a full history of the evolution of each file in the
project. When a change is committed, the history records the exact change made,
including the previous version of the file. No change is ever fully lost,
therefore, unless a great deal of extra work is taken to erase something from
the project’s commit history.</p>
<p>It’s also helpful to understand how version control programs handle
collaboration. In earlier types of version control programs, under what is
called a <em>centralized</em> framework, there was one central repository for the file
or set of files the team was working on <span class="citation">(E. Raymond 2009; Target 2018; Irving 2011)</span>. A team member who wanted to make
a change would “check out” the file he or she wanted to work on, make changes,
and then check it back in as the newest main version <span class="citation">(E. S. Raymond 2003)</span>. While
one team member had this file checked out, other members would be locked out of
making any changes to that file—they could look at it, but couldn’t make any
edits <span class="citation">(E. Raymond 2009; Target 2018)</span>. This meant that there was no
chance of two people trying to change the same part of a file at the same time.
In spirit, this early system is pretty similar to the idea of sending a file
around the team by email, with the understanding that only one person works on
it at a time. A slightly more modern analogy is the idea of having a single
version of a file in Dropbox or Google Docs, and avoiding working on the file
when you see that another team member is working on it.</p>
<p>This assembly-line approach is pretty clunky, though. In particular, it usually
increases the amount of time that it takes the team to finish the project,
because only one person can work on a file at a time. Later types of version
control programs moved toward a different style, allowing for <em>distributed</em>
rather than centralized collaborative work on a file or a set of files
<span class="citation">(E. Raymond 2009; Irving 2011)</span>. Under the distributed model,
all team members can have their own version of all the files, work on them and
make records of changes they make to the files, and then occassionally sync with
everyone else to share your changes with them and bring their changes into your
copy of the files. This functionality is called <em>concurrency</em>, since it allows
team members to concurrently work on the same set of files
<span class="citation">(E. Raymond 2009)</span>.</p>
<p>This idea allowed for the development of other useful features and styles of
working, including <em>branching</em> and <em>forking</em>. Branching allows you to try out
new ideas that you’re not sure you’ll ultimately want to go with. Forking is a
key tool used in open-source software development, and, among other things,
facilitates someone who isn’t part of the original team getting a copy of the
files they can work with and suggesting some changes that might be helpful. So,
this is the basic idea of modern version control—for a project that involves a
set of computer files, everyone on the team has their own copy of the directory
on their own computer, makes changes at the time and in the spots in the files
that they want, and then regularly re-syncs their local directory with everyone
else’s to share changes and updates.</p>
<p>This distributed model also means there is a copy of the full repository on
every team member’s computer, which has the side benefit of providing additional
backup of the project files. Remote repositories—which may be on a server in a
different location—can be added with another copy of the project, which can
similarly be synced regularly to update with any changes made to project files.</p>
</div>
<div id="comparing-git-to-other-tools" class="section level3 hasAnchor" number="2.9.5">
<h3><span class="header-section-number">2.9.5</span> Comparing Git to other tools<a href="experimental-data-recording.html#comparing-git-to-other-tools" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>While there are a number of software systems for version control, one of the
most common currently used for scientific projects is Git. This program was
created by Linus Torvalds, who also created the Linux operating system, in 2005
as a way to facilitate the team working on Linux development. This program for
version control thrives in large collaborative projects, for example open-source
software development projects that include numerous contributors, both regular
and occasional <span class="citation">(Brown 2018)</span>. As Target notes in a 2018 article about
version control:</p>
<blockquote>
<p>“While people sometimes grouse about its steep learning curve or unintuitive
interface, Git has become everyone’s go-to for version control.”
<span class="citation">(Target 2018)</span></p>
</blockquote>
<p>In recent years, some complementary tools have been developed that make the
process of collaborating together using version control software easier. Other
tools, such as bug trackers or issue trackers, facilitate corroborative
file-based projects to allow the team to keep a running “to-do” list of what
needs to be done to complete the project. These tools—which are discussed in
modules 2.10 and 2.11—can be used to improve collaboration on scientific
projects done by teams. GitHub is one a very popular version control platform
with these additional tools. It was created in 2008 as a web-based platform to
facilitate collaborating on projects running under Git version control. It can
provide an easier entry to using Git for version control than trying to learn to
use Git from the command line <span class="citation">(Perez-Riverol et al. 2016)</span>. It also interfaces well with RStudio,
making it easy to integrate a collaborative workflow through GitHub from the
same RStudio window on your computer where you are otherwise doing your analysis
<span class="citation">(Perez-Riverol et al. 2016)</span>.</p>
<p>While Git version control software is one of the best established ways
of implementing version control, there are growing efforts to enable some level
of version control through other platforms. For example, Google Docs enables a
level of version control through its Version History feature. This feature
allows you name different versions of a document as they are is saved in Google
Docs. It also allows you to restore a document to earlier versions, as well as
see which changes have been made to a document and who made each change.</p>
<p>While some generalized tools like Google tools and Dropbox might be simpler to
initially learn, more powerful version control tools like Git offer some key
advantages for recording scientific data and are worth the effort to adopt. A
key advantage is their ability to track the full history of files as they
evolve, including not only the history of changes to each file, but also a
record of why each change was made.</p>
<p>Git excels in tracking changes made to plain text files. For these files,
whether they record code, data, or text, Git can show line-by-line differences
between two versions of the file. This makes it very easy to go through the
history of “commits” to a plain text file in a Git-tracked repository and see
what change was made at each time point, and then read through the commit
messages associated with those commits to see why a change was made. For
example, if a value was entered in the wrong row of a plain text file or
spreadsheet, and the researcher then made a commit to correct that data entry
mistake, the researcher could explain the problem and its resolution in the
commit message for that change.</p>
<p>There are, of course, some limitations to using version control tools when
recording experimental data. First, while ideally laboratory data is recorded in
a plain text format, some data may be recorded in a binary file format. Some
version control tools, including Git, can be used to track changes in binary
files. However, Git does not take to these types of files naturally. In
particular, Git typically will not be able to show a useful comparison of
the differences between two versions of a binary file.</p>
<p>More problems can arise if the binary file is
very large <span class="citation">(Perez-Riverol et al. 2016; Blischak, Davenport, and Wilson 2016)</span>, as some experimental research
data files are (e.g., if they are high-throughput output of laboratory equipment
like a mass spectrometer). However, there are emerging tools and strategies for
improving the ability to include and track large binary files when using Git and
GitHub <span class="citation">(Blischak, Davenport, and Wilson 2016)</span>.</p>
<p>Finally, as with other tools and techniques described in this book, there is an
investment required to learn how to use Git <span class="citation">(Perez-Riverol et al. 2016)</span>, as well
as some extra overhead when using version control tools in a project
<span class="citation">(E. S. Raymond 2003)</span>. However, Git can bring dramatic gains to
efficiency, transparency, and organization of research projects, even if you
only use a small subset of its basic functionality <span class="citation">(Perez-Riverol et al. 2016)</span>. In module
2.11, we provide guidance on getting started with using Git and GitHub to track
a scientific research project.</p>
<p>Third, the combination of Git and GitHub can help as a way to backup study data
<span class="citation">(Blischak, Davenport, and Wilson 2016; Perez-Riverol et al. 2016; J. Perkel 2018)</span>. Together, Git and GitHub
provide a structure where the project directory (repository) is copied on
multiple computers, both the users’ laptop or desktop computers and on a remote
server hosted by GitHub or a similar organization. This set-up makes it easy to
bring all the project files onto a new computer—all you have to do is clone
the project repository. It also ensures that there are copies of the full
project directory, including all its files, in multiple places
<span class="citation">(Blischak, Davenport, and Wilson 2016)</span>. Further, not only is the data backed up across multiple
computers, but so is the full history of all changes made to that data and the
recorded messages explaining those changes, through the repositories commit
messages <span class="citation">(Perez-Riverol et al. 2016)</span>.</p>
</div>
<div id="discussion-questions-1" class="section level3 hasAnchor" number="2.9.6">
<h3><span class="header-section-number">2.9.6</span> Discussion questions<a href="experimental-data-recording.html#discussion-questions-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>In your own research, do you collect data in paper laboratory notebooks, electronically, or a mixture of the two? What have you found to be advantages and disadvantages of the method you typically use? Are there ever cases where you have no choice and must either record on paper or electronically (examples might include when working behind a secure barrier or when data are recorded directly by equipment into a digital format)?<br />
</li>
<li>Have you used any of the following tools for recording, sharing, and versioning data or other research files (e.g., drafts of research papers, code):
<ul>
<li>Electronic laboratory notebooks</li>
<li>Dropbox</li>
<li>Google Docs / Google Drive</li>
<li>Microsoft Teams</li>
<li>Local server or drive run by your institution</li>
<li>GitHub / GitLab</li>
</ul></li>
<li>Describe how any of these tools have helped in version control, including tracking changes to the file and helping to coordinate several people working on a file at once. Are there aspects where the tools you’ve used have been limited in this capacity?</li>
<li>Can you think of any examples of times when you’ve experienced a failure of version control? Examples might include a case where some team members worked on the wrong version of a file, or when you lost track of the changes that had been made to a file. What did you learn from the experience? Have you developed methods to avoid similar problems in the future? How might a version control problem like this result in problems with the rigor and reproducibility of scientific research?</li>
<li>How does the idea of version control relate to physical research materials, like model organisms, antibodies, or cell lines? Do you have any examples you can share of issues that have come up in research related to the version of these types of physical research materials?</li>
<li>What steps do you think you could take in your research to improve version control? Do you see this as a higher or lower priority change to take compared to other steps that might improve rigor and reproducibility in your research? Discuss your reasoning.</li>
</ul>
<!-- ### Practice Quiz -->
<!-- Question 1: Which of the following research materials can have different versions and therefore is a material for which you can consider principles of version control?  -->
<!-- a) Electronically-recorded research data   -->
<!-- b) Study protocols  -->
<!-- c) Cell lines  -->
<!-- d) All of the above [Correct answer]  -->
<!-- Question 2: What is one challenge that is created when several people make changes to the same document concurrently?  -->
<!-- a) It is difficult to share copies of the document with everyone at once  -->
<!-- b) Science researchers tend to work alone on most projects, so it is difficult to convince them to collaborate  -->
<!-- c) It is difficult to resolve and coordinate the changes each person made to incorporate all the changes into a new version [Correct answer]  -->
<!-- d) Most researchers prefer to work one after another on a document, rather than all working at the same time  -->
<!-- Question 3:  -->
<!-- TRUE / FALSE: The idea of version control is only relevant in the context of developing software.  -->
<!-- [Correct answer: FALSE]  -->
<!-- Question 4: In the term version control, what does “control” mean?  -->
<!-- a) Allowing only one person to work on a document at a time while freezing it to changes from any other user  -->
<!-- b) Ensuring that none of the updates to a document introduce bugs or errors  -->
<!-- c) Allowing for safe changes and updates to a version of a document when more than one person is working on it [Correct answer]  -->
<!-- d) Giving priority to one primary user to overrule changes made to the document by other users  -->
<!-- Question 5: What is the difference between a normal file directory and a repository?  -->
<!-- a) There is no difference; these words are synonyms  -->
<!-- b) A file directory is under version control, while a repository may not be  -->
<!-- c) A repository is one version of a file directory   -->
<!-- d) A repository is under version control, while a file directory may not be [Correct answer]  -->
<!-- Question 6: Why is reversibility appealing in a version control system?  -->
<!-- a) It means that you can roll back to an earlier version of the document if you want to [Correct answer]  -->
<!-- b) It means that you have a well-documented history of all changes made to the document  -->
<!-- c) It means that multiple people can make changes to a document at once  -->
<!-- d) It means that new files can be added to a repository  -->
<!-- Question 7: In the git version control system, what is the term for a snapshot of a version of the files in a repository at a certain point in its history?  -->
<!-- a) Merge  -->
<!-- b) History  -->
<!-- c) Commit message  -->
<!-- d) Commit [Correct answer]  -->
<!-- Question 8: How can the combination of version control software and a version control platform (like GitHub) help provide backup for research documents?  -->
<!-- a) To enable this combination, you must also keep a copy of the research files on a dedicated server  -->
<!-- b) With this combination, each contributor has a full copy of the repository of research files on their local computer, which they can sync with other versions through the platform [Correct answer]  -->
<!-- c) This combination coordinates with Google Drive to archive files  -->
<!-- d) This combination provides a full history of all versions of the files in the repository, while using the version control software without the platform would not  -->

</div>
</div>
<div id="module10" class="section level2 hasAnchor" number="2.10">
<h2><span class="header-section-number">2.10</span> Enhance the reproducibility of collaborative research with version control platforms<a href="experimental-data-recording.html#module10" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Once a researcher has learned to use Git on their own computer for local version
control, they can begin using version control platforms (e.g., GitLab, GitHub)
to collaborate with others under version control. In this module, we will
describe how a research team can benefit from using a version control platform
to work collaboratively. In the next module, we’ll give detailed examples
of how you can use a version control platform even if you’re not familiar with
coding.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>List benefits of using a version control platform to collaborate
on research projects, particularly for reproducibility</li>
<li>Describe the difference between version control (e.g., Git) and
a version control platform (e.g., GitHub)</li>
<li>Explain how version control software and version control platforms can
help coordinate contributions from different team members</li>
<li>Define “merging”, “merge conflicts”, “issue trackers”</li>
<li>Explain how commit messages can improve project management</li>
<li>Explain how to-do lists can help project management</li>
<li>Describe how a version control platform provides additional back-up for
study files</li>
</ul>
<div id="what-are-version-control-platforms" class="section level3 hasAnchor" number="2.10.1">
<h3><span class="header-section-number">2.10.1</span> What are version control platforms?<a href="experimental-data-recording.html#what-are-version-control-platforms" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The last module introduced the idea of version control, including the popular
software version control program Git. In this module, we’ll go a
step further, telling you about how you can expand the idea of version control
to leverage it when collaborating across your research team, using <em>version
control platforms</em>. Version control platforms build on the functionality of
version control software, and they can provide you and your team tools
for sharing, tools for visualization, and tools for project management.</p>
<p>Version control platforms offer a number of advantages when collaborating on a
research project that can help to improve your efficiency, rigor, and
reproducibility. Further, as their use has become more popular, there are more
and more resources to help you learn how to use these platforms effectively.
Some of the key advantages of using a version control platform like GitHub to
collaborate on research projects include that the platform:</p>
<ul>
<li>Can track and merge changes that different collaborators made to the
document</li>
<li>Allows you to create alternative versions of project files (<em>branches</em>), and merge them into the main project as desired</li>
<li>Includes tools for project management, including Issue Trackers</li>
<li>Provides additional backup of project files</li>
<li>Allows you to share project information online, including through hosting websites related to the project or supplemental files related to a manuscript</li>
</ul>
<p>A number of version control platforms are available. Two that are currently very
popular for scientific research are GitHub (<a href="https://github.com/" class="uri">https://github.com/</a>) and GitLab
(<a href="https://about.gitlab.com/" class="uri">https://about.gitlab.com/</a>). Both provide free options for scientific
researchers, including the capabilities for using both public and private
repositories in collaboration with other researchers.</p>
<p>Version control platforms are always used in conjunction with version control
software, like the Git software described in the last module. A version control
platform adds attractive visual interfaces for working with the project, free or
low-cost online hosting of project files, and team management tools for each
project. In this sense, you can think of Git as the engine and the version
control platform as the driver’s seat, with dashboard, steering wheel, and gears
to leverage the power of the underlying Git software. One scientist, in an
article about Git and GitHub for scientists, highlighted that resources like
GitHub are “essential for collaborative software projects because they enable
the organization and sharing of programming tasks between different remote
contributors.” <span class="citation">(Perez-Riverol et al. 2016)</span></p>
<p>A version control platform therefore combines the strengths of a “Track changes”
feature with those of a file sharing platform like Dropbox. To some extent,
Google Docs or Google Drive also combine these features, and some spreadsheet
programs are moving toward rudimentary functionality for version control
<span class="citation">(Birch, Lyford-Smith, and Guo 2018)</span>. However, there are added advantages of version control
platforms. For example, there are version control platforms that are
open-source. GitLab is one example. Since these can be set up on a server that
you own, they can be used to collaborate on projects with sensitive data, and
also can operate directly on the server you’re using to store large
project datasets or to run computationally-intensive pre-processing or analysis.
Also, most version control platforms include tools that help you manage and
track the project. These include “Issue Trackers”, tools for exploring the
history of each file and each change, and features to assign project tasks to
specific team members. The next section will describe the features of version
control platforms that make them helpful as a tool for collaborating on
scientific research. These systems are being leveraged by some scientists both
to manage research projects and to collaborate on writing scientific manuscripts
and grant proposals <span class="citation">(Perez-Riverol et al. 2016)</span>.</p>
</div>
<div id="why-use-version-control-platforms" class="section level3 hasAnchor" number="2.10.2">
<h3><span class="header-section-number">2.10.2</span> Why use version control platforms?<a href="experimental-data-recording.html#why-use-version-control-platforms" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s look in detail at some of the advantages of using a version control
program. The first is that they can provide an easy-to-use interface to the
power of Git. Years ago, the use of version control required users to be
familiar with the command line, and to send arcane commands to track the project
files through that interface. However, version control platforms will typically
allow team members to explore and work with the functions from Git in an easier
way than if they try to use the barebones version control software. With the
rising popularity of version control platforms, version control for project
management can be taught relatively quickly to students with a few months—or
even weeks—of coding experience. In fact, version control is beginning to be
used as a method of turning in and grading homework in beginning programming
classes, with students learning these techniques in the first few weeks of class
<span class="citation">(Beckman et al. 2021)</span>. This would be practically unimaginable without the
user-friendly interface of a version control platform as a wrapper for the power
of the version control software itself.</p>
<p>A second advantage is that a version control platform helps in tracking and
managing contributions from team members. As the proverb about too many cooks in
the kitchen captures, any time you have multiple people working on a project, it
introduces the chance for conflicts—cases where contributions from different
people disagree. While higher-level conflicts, like about what you want the
final product to look like or who should do which jobs, can’t be easily managed
by a computer program, now the complications of integrating everyone’s
contributions—and letting people work in their own space and then bring
together their individual work into one final project—can be. While
programs for version control were originally created to help with programmers
developing code, they can be used now to coordinate group work on numerous types
of file-based projects, including scientific manuscripts, books, and websites
<span class="citation">(E. Raymond 2009)</span>. Although they can work with projects that include
files saved in binary (Word documents, for example), they thrive in projects
with a heavier concentration of text-based files, and so they fit in nicely in a
scientific research / data analysis workflow that is based on data stored in
plain text formats and data analysis scripts written in plain text files, tools
we discuss in other modules.</p>
<p>There is one key feature of modern version control that’s critical to making
this work—resolving changes in files that started the same but were edited in
different ways by different people and now need to be put back together. This
step is called <em>merging</em>. While this is a feature driven by the Git software
itself, you typically won’t use it until you’re collaborating on a project
through a version control platform like GitHub.</p>
<p>You can think of this as merging the changes that two people have made as
they edited a single file, a file where they both started out with identical
copies. Without version control, this process can be time-consuming and
frustrating. As one scientist notes:</p>
<blockquote>
<p>“You will likely share your code with multiple lab mates or collaborators,
and they may have suggestions on how to improve it. If you email the code
to multiple people, you will have to manually incorporate all the changes
each of them sends.” <span class="citation">(Blischak, Davenport, and Wilson 2016)</span></p>
</blockquote>
<p>The version control software can handle this for you. Think of the file broken
up into each of its separate lines. There will be some lines that neither person
changed. Those are easy to handle in the “merge”—they stay the same as in the
original copy of the file. Next, there will be some lines that one person
changed, but that the other person didn’t. It turns out that these are pretty
easy to handle, too. If only one person changed the line, then you use their
version—it’s the most up-to-date, since if both people started out with the
same version, it means that the other person didn’t make any changes to that
part of the file. Finally, there may be a few lines that both people changed at
about the same time. These are called <em>merge conflicts</em>. They’re places in the
file where there’s not a clear, easy-to-automate way that the computer can know
which version to put into the integrated, latest version of the file. Different
version control programs handle these merge conflicts in different ways.</p>
<p>For the most common version control program used today, Git, these spots in
the file are flagged with a special set of symbols when you try to integrate the
two updated versions of the file. Along with the special symbols to denote a
conflict, there will also be both versions of the conflicting lines of the
file. Whoever is integrating the files must go in and pick the version of those
lines to use in the integrated version of the file, or write in some compromise
version of those lines that brings in elements from both people’s changes, and
then delete all the symbols denoting that was a conflict and save this latest
version of the file.</p>
<p>Another advantage of a version control platform is that, when you collaborate
using a version control platform, the commit messages provide a way to
communicate across the team members. For example, if one person is the key
person working on a certain file, but has run into a problem with one spot and
asks another team member to take a go, then the second team member isn’t limited
to just looking at the file and then emailing some suggestions. Instead, the
second person can make sure he or she has the latest version of that file, make
the changes they think will help,
<em>commit</em> those changes with a message (a <em>commit message</em>) about why they think
this change will fix the problem, and then push that latest version of the file
back to the first person. If there are several places where it would help to
change the file, then these can be fixed through several separate commits, each
with their own message. The first person, who originally asked for help, can
read through the updates in the file (most platforms for using version control
will now highlight where all these changes are in the file) and read the second
person’s message or messages about why each change might help. Even better, days
or months later, when team members are trying to figure out why a certain change
was made in that part of the file, can go back and read these messages to get an
explanation.</p>
<p>Even better, platforms for using Git often include nice tools for visualizing
differences between two files, providing a more visual way to look at the
differences between files across time points in the project. For example, GitHub
automatically shows changes using colors to highlight additions and subtractions
of plain text for one file compared to another version of it when you look
through a repository’s commit history. Similarly, RStudio provides a new
“Commit” window that can be used to compare differences between the original and
revised version of plain text files at a particular stage in the commit history.
In the next module, we’ll walk you through examples of navigating these features.</p>
<p>Another advantage of a version control platform is that they often include extra
tools for project management. These include <em>issue trackers</em>, which allow the
team to keep a running “to-do” list of what needs to be done to complete the
project <span class="citation">(Perez-Riverol et al. 2016)</span>. Sometimes the best tools also happen to be those that
are cheap and easy. In this case, the tool might be so obvious that you don’t
even think of formalizing it as a tool. The “to-do” list is an excellent
example.</p>
<p>A to-do list allows you to take a big task and break it into specific steps
that need to be done to complete that task. It helps with something very key
to solving big problems: being able to zoom between the big picture—big
but vague descriptions of major steps to solve the problem—and the fine
details of how you will tackle each of those steps. As Adam Savage of
the TV show <em>Mythbusters</em> notes:</p>
<blockquote>
<p>“The value of a list is that it frees you up to think more creatively, by
defining a project’s scope and scale for you on the page, so your brain doesn’t
have to hold on to so much information. The beauty of the checkbox is that it
does the same thing with regard to progress, allowing you to monitor the status
of your project, without having to mentally keep track of everything.” <span class="citation">(Savage 2020)</span></p>
</blockquote>
<p>The Issues section of a GitHub repository works as this type of to-do list. By
looking at the home Issues page, you see an overview of the tasks you need to
complete to finish the project. For each of these tasks, you can zoom in on
the details by clicking on its Issue. This will take you to a page where your
team can discuss the details of the task, honing in on how you will solve it.</p>
<p>It’s tempting to use emails to discuss progress on a task and talk about how
to solve it. Don’t. Use an Issue instead. This will keep the discussion in one
place, and so you won’t have to go back through emails to find your old
discussion on how you solved it. Also, the Issues section of GitHub doesn’t
delete an Issue once you’ve complete that task. Instead, it allows you to “close”
the Issue. This moves the Issue into a section with all your other closed
issues—it takes it out of your to-do list, but saves the full discussion
somewhere that you’ll be able to find easily in the future if you ever want to
revisit how you solved that problem.</p>
<p>Another advantage of version control platforms is that, if a project uses a
version control platform, it is very easy to share data recorded for the project
publicly. On GitHub, you can set the access to a project to be either public or
private, a setting that can be converted easily from one form to the other over
the course of the project <span class="citation">(Metz 2015)</span>. A private project can be viewed
only by fellow team members, while a public project can be viewed by anyone.
This can be used to share the project data online once an associated manuscript
is published, an increasingly common request or requirement from journals and
funding agencies <span class="citation">(Blischak, Davenport, and Wilson 2016)</span>. Sharing data allows a more complete
assessment of the research by reviewers and readers and makes it easier for
other researchers to build off the published results in their own work,
extending and adapting the code to explore their own datasets or ask their own
research questions <span class="citation">(Perez-Riverol et al. 2016)</span>.</p>
<p>Further, because Git tracks the full history of changes to
these documents, it includes functionality that lets you tag the code and data
at a specific point (for example, the date when a paper was submitted) so that
viewers can look at that specific version of the repository files, even while
the project team continues to move forward in improving files in the directory.
At the more advanced end of functionality, there are even ways to assign a
persistent digital identifier (e.g., a DOI, like those assigned to published articles)
to a specific version of a GitHub repository <span class="citation">(Perez-Riverol et al. 2016)</span>.</p>
<p>Version control platforms also help in providing a way to backup study data
<span class="citation">(Blischak, Davenport, and Wilson 2016; Perez-Riverol et al. 2016; J. Perkel 2018)</span>. Together, Git and GitHub
provide a structure where the project directory (repository) is copied on
multiple computers. Under a distributed model, each collaborator will
have their own copy of all project files on their local computer. All project
files are also stored on the remote repository to which you all push and pull
commits. If you are using the GitHub platform, this will be GitHub’s servers; if
you use GitLab, you can set up the system on your own server. Each time you push
or pull from the remote copy of the project repository, you are syncing your
copy of the project files with those on other computers.</p>
<p>This set-up makes it easy to bring all the project files onto a new
computer—all you have to do is clone the project repository. It also ensures
that there are copies of the full project directory, including all its files, in
multiple places <span class="citation">(Blischak, Davenport, and Wilson 2016)</span>. Further, not only is the data backed up
across multiple computers, but so is the full history of all changes made to
that data and the recorded messages explaining those changes, through the
repositories commit messages <span class="citation">(Perez-Riverol et al. 2016)</span>.</p>
<p>Leips highlights the importance of backup for research data and code:</p>
<blockquote>
<p>“Backup, backup, backup—this is the main action you can take to care for your
computers and your data. Many PIs assume that backup systems are inherently
permanent and foolproof, and it often takes a loss to remind one that
materials break, systems fail, and humans make mistakes. Even if your data
are backed up at work, have at least one other backup system. Keep at least
one backup off site, in case of a diaster in the lab (yes, fires and floods
do happen). It doesn’t make much sense to have two separate backup systems stored
next to each other in a drawer.” <span class="citation">(LEIPS 2010)</span></p>
</blockquote>
<p>Finally, version control platforms like GitHub can be used for a number
of supplementary tasks for your research project. These include publishing
webpages or other web resources linked to the project and otherwise improving
public engagement with the project, including by allowing other researchers
to copy and adapt your project through a process called <em>forking</em>. Version
control platforms also provide a supplemental backup to project files.</p>
<p>First, GitHub can be used to collaborate on, host, and publish websites and
other online content <span class="citation">(Perez-Riverol et al. 2016)</span>. Version control systems have been used by
some for a long time to help in writing longform materials like books (e.g.,
<span class="citation">E. S. Raymond (2003)</span>); new tools are making the process even easier. The GitHub Pages
functionality, for example, is now being used to host a number of books created
in R using the <code>bookdown</code> package, including the online version of this book
<span class="citation">(<strong>bookdown?</strong>)</span>. The <code>blogdown</code> package similarly can be used to create websites,
either for individual researchers, for research labs, or for specific projects
or collaborations <span class="citation">(<strong>blogdown?</strong>)</span>. Further, if a project includes the creation of
scientific software, a version control platform can be used to share that
software—as well as associated documentation—in a format that is easy for
others to work with.</p>
<p>The platform can also be used to share supplemental material for a manuscript,
including the code used for pre-processing and analyzing data. Perez highlights
this functionality:</p>
<blockquote>
<p>“The traditional way to promote scientific software is by publishing an
associated paper in the peer-reviewed scientific literature, though, as pointed
out by Buckheir and Donoho, this is just advertising. Additional steps can boost
the visibility of an organization. For example, GitHub Pages are simple websites
freely hosted by GitHub. Users can create and host blog websites, help pages,
manuals, tutorials, and websites related to specific projects.” <span class="citation">(Perez-Riverol et al. 2016)</span></p>
</blockquote>
<p>With GitHub, while only collaborators on a public project can directly change
the code, anyone else can <em>suggest</em> changes through a process of copying a
version of the project (<em>forking</em> it). This allows someone to make the changes
they would like to suggest directly to a copy of the code, and then ask the
project’s owners to consider integrating the changes back into the main version
of the project through a <em>pull request</em>. GitHub therefore creates a platform
where people can explore, adapt, and add to other people’s coding projects,
enabling a community of coders <span class="citation">(Perez-Riverol et al. 2016)</span>. Because of this
functionality, GitHub has been described as “a social network for software
development” <span class="citation">(J. Perkel 2018)</span> and as “a kind of bazaar that offers just about
any piece of code you might want—and so much of it free.” <span class="citation">(Metz 2015)</span>.
This same process can be leveraged for others to copy and adapt code—this is
particularly helpful in ensuring that a software or research project won’t be
“orphaned” if its main developer is unavailable (e.g., retires, dies), but
instead can be picked up and continued by other interested researchers.
Copyright statements and licenses within code projects help to clarify
attribution and rights in these cases.</p>
<p>In the next module, we describe practical ways to leverage these resources
within your research group. We include instructions both for team leaders—who
may not code but may want to use GitHub within projects to help manage the
projects—as well as researchers who work directly with data and code for the
research team. There are also a number of excellent resources that are now
available that walk users through how to set up and use a version control
platform. The process is particularly straightforward when the research project
files are collected in an RStudio Project format, as described in earlier
modules.</p>
<!-- ### Discussion questions -->

</div>
</div>
<div id="module11" class="section level2 hasAnchor" number="2.11">
<h2><span class="header-section-number">2.11</span> Using Git and GitHub to implement version control<a href="experimental-data-recording.html#module11" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this chapter, we will give you an overview of how to use Git and GitHub
for your laboratory research projects. If you prefer an open-source version
control platform, GitLab has similar functionality and can be installed on
a server you own.</p>
<p>We’ll address two separate groups, in separate sections. As the main focus,
we’ll overview how you can leverage and use these tools as the director or
manager of a project, without knowing how to code in a language like R. We are
focusing on this audience in this module, as we see this as an area where there
aren’t a lot of available resources to provide guidance. GitHub provides a
number of useful tools that can be used by anyone, providing a common space for
managing the data recording, analysis and reporting for a scientific research
project. In this case, there would need to be at least one member of your team
who is comfortable with a programming language, to set up and maintain the
GitHub repository, but all team members can participate in many features of the
GitHub repository regardless of programming skill.</p>
<p>The other audience for information on using Git and GitHub are researchers who
are comfortable coding. Fortunately, there are many good resources available for
this audience. We’ll end the module by providing advice to this audience to
point them to resources where they can go to learn more and fully develop these
skills.</p>
<p>As examples, we’ll show different elements from two real GitHub repository, used
for scientific projects and papers. The first repository is available at
<a href="https://github.com/aef1004/cyto-feature_engineering" class="uri">https://github.com/aef1004/cyto-feature_engineering</a>. It provides example data
and code to accompany a published article on a pipeline for flow cytometry
analysis <span class="citation">(Fox et al. 2020)</span>. The second repository is available at <a href="https://github.com/PodellLab/Granuloma_RSratio_ISH" class="uri">https://github.com/PodellLab/Granuloma_RSratio_ISH</a>.
It provides example data and code to accompany a published article on immune
cell composition and replication status of <em>Mycobacterium tuberculosis</em> at
the level of granulomas <span class="citation">(Cooper et al. 2024)</span>.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Apply tools within a version control platform to manage a research project,
even if some of the members are not coders</li>
<li>Utilize visualization tools on a version control platform to explore the
evolution of project files</li>
<li>Utilize the Issues tracker on a version control platform to break a project
into tasks and discuss details of each task with your team</li>
<li>Describe roles and ownership on a repository on a version control platform</li>
<li>Explain how a repository on GitHub can be switched between public and private
states</li>
</ul>
<div id="leveraging-git-and-github-as-a-non-coder" class="section level3 hasAnchor" number="2.11.1">
<h3><span class="header-section-number">2.11.1</span> Leveraging Git and GitHub as a non-coder<a href="experimental-data-recording.html#leveraging-git-and-github-as-a-non-coder" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Because Git has its history in software development, and because most
introductions to it quickly present arcane-looking code commands, you may have
hesitations about whether it would be useful in your scientific research group.
This is particularly likely to be the case if you, and many in your research
group, do not have experience programming.</p>
<p>This is not at all the case. While Git itself traditionally has been used with a
command-line interface (think of the black and green computer screens shown when
movies portray hackers), GitHub has wrapped Git’s functionality with an
attractive graphical user interface that is easy to understand. This is how you
will interact with a project repository if you are online and logged into
GitHub, rather than exploring it on your own computer (although there are also
graphical user interfaces you can use to more easily explore Git repositories
locally, on your computer).</p>
<p>In fact, the combination of Git and GitHub can become a secret weapon for your
research group if you are willing to encourage those in your group who do know
some programming (or are willing to learn a bit) to take the time to learn to
set up a project in this environment for project management. Once a project has
been set up in GitHub, there are a number of features that can be used by all
team members, whether they code or not. These features facilitate collaboration
between coders and non-coders as the data and code evolve. The major features
and advantages of Git and GitHub are described in modules 2.9 and 2.10.</p>
<p>As mentioned in the previous two modules, repositories that are tracked with
Git and shared through GitHub provide a number of tools that are useful in
managing a project, both in terms of keeping track of what’s been done in the
project and also for planning what needs to be done next, breaking those goals
into discrete tasks, assigning those tasks to team members, and maintaining a
discussion as you tackle those tasks.</p>
<p>You can go a long way by just starting with a subset of the tools that Git and
GitHub offer. In this module, we’ll focus on:</p>
<ul>
<li>Exploring commits and commit history</li>
<li>Tracking and making progress on issues</li>
<li>Managing repository access and ownership</li>
<li>Providing project documentation that will help others navigate the project
files</li>
</ul>
<p>At the end of this module, there is a video demonstration that walks you through
the elements we’ve highlighted.</p>
<p>GitHub is free to join; while there are paid plans, the free plan is adequate
for getting started. To create an account, visit <a href="https://github.com/" class="uri">https://github.com/</a>. If you
find you need more than the free plan provides, academic researchers can request
free use of some of the more extensive versions if needed, or you can explore an
open-source alternative, GitLab.</p>
<p>Even if you are not coding, you will need to be logged in to your GitHub account
to contribute to a repository. For some actions, you need to be a collaborator
on a project to take the action; in the later sections of this module, we
describe how people can be added as collaborators in a GitLab repository.</p>
<p><strong>Exploring commits and the commit history</strong></p>
<p>A version control platform like GitHub can help with managing your projects
by providing tools to visually explore how the project has evolved.
Each time a team member makes a change to files in a GitHub repository, the
change is recorded as a <em>commit</em>, and the team member must include a short
<em>commit message</em> describing the change. Each file in the project will have its
own page on GitHub (Figure <a href="experimental-data-recording.html#fig:githubcommits1">2.41</a> shows an example). You can
see the history of changes to that files by clicking the “History” link on that
page.</p>
<div class="figure"><span style="display:block;" id="fig:githubcommits1"></span>
<img src="figures/github_commits1.png" alt="Example of a file page within a GitHub repository. Each file in a repository has its own page. On this page, you can see the history of changes made to the file by looking at 'History'. You can also make a commit an edit directly in GitHub by clicking on the 'Edit' icon." width="\textwidth" />
<p class="caption">
Figure 2.41: Example of a file page within a GitHub repository. Each file in a repository has its own page. On this page, you can see the history of changes made to the file by looking at ‘History’. You can also make a commit an edit directly in GitHub by clicking on the ‘Edit’ icon.
</p>
</div>
<p>Figure <a href="experimental-data-recording.html#fig:githubcommithistory">2.42</a> gives an example of how you can see the
full history of changes that have been made to each file in the project. Each
change is tracked through a commit, which includes markers of who made the
change and a message describing the change. This allows you to quickly pinpoint
changes in a file in your research project. Near the commit message are listings
of which team member made the commit and when it was made. This also helps you
see how team members have contributed as the file evolves.</p>
<div class="figure"><span style="display:block;" id="fig:githubcommithistory"></span>
<img src="figures/github_commit_history.png" alt="Commit history in GitHub. Each file in a repository has a 'History' page, where you can explore each change commited for the file. Each commit has a unique identifier and commit message describing the change. You can click on the entry for any of these commits to see the changes made to the file with the commit (see next figure)." width="\textwidth" />
<p class="caption">
Figure 2.42: Commit history in GitHub. Each file in a repository has a ‘History’ page, where you can explore each change commited for the file. Each commit has a unique identifier and commit message describing the change. You can click on the entry for any of these commits to see the changes made to the file with the commit (see next figure).
</p>
</div>
<p>If you click on one of the commits listed on a file’s History page (Figure
<a href="experimental-data-recording.html#fig:githubcommithistory">2.42</a> points to one example of where you would click),
it will take you to a page providing information on the changes made with that
commit (Figure <a href="experimental-data-recording.html#fig:githubcommithistory2">2.43</a>). This page provides a
line-by-line view of each change that was made to project files with that
commit, as well as the commit message for that commit. If the person
committing the change included a longer description or commentary,
this information will also be included.</p>
<p>Within the body of the page, you can see the changes made with the commit. Added
lines will be highlighted in green while deleted lines are highlighted in red.
If only part of a line was changed, it will be shown twice, once in red as its
version before the commit, and once in green showing its version following the
commit. You can visually compare the two versions of the line to see how it was
changed with the commit.</p>
<div class="figure"><span style="display:block;" id="fig:githubcommithistory2"></span>
<img src="figures/github_commit_history2.png" alt="Commit history in GitHub. Each commit has its own page, where you can explore what changes were made with the commit, who made them, and when they were committed." width="\textwidth" />
<p class="caption">
Figure 2.43: Commit history in GitHub. Each commit has its own page, where you can explore what changes were made with the commit, who made them, and when they were committed.
</p>
</div>
<p>The page shown in Figure <a href="experimental-data-recording.html#fig:githubcommits1">2.41</a> also allows you to make your
own edits to the file and commit them. For team members who are working a lot on
coding, they will usually make changes to a file locally, on the repository copy
on their own computers and then push their latest changes to the GitHub version.
This workflow will allow them to test the code locally before they update the
GitHub version.</p>
<p>However, it is also possible to make a commit directly on GitHub, and this may
be a useful option for team members who are not coding and would like to make
small changes to the writing files. On the file’s page on GitHub, there is an
“Edit” icon (Figure <a href="experimental-data-recording.html#fig:githubcommits1">2.41</a>). By clicking on this, you will
get to a page where you can directly edit the file (Figure
<a href="experimental-data-recording.html#fig:githubcommits2">2.44</a> shows an example of what this page looks like). Once
you have made your edits, you will need to commit them, along with a short
description of the commit, the “commit message”. If you would like to include a
longer explanation of your changes, there is space for that, as well, when you
make the commit (Figure <a href="experimental-data-recording.html#fig:githubcommits2">2.44</a>). These commits will show up
in the repository’s history, attributed to you and with your commit message
attached to the change.</p>
<div class="figure"><span style="display:block;" id="fig:githubcommits2"></span>
<img src="figures/github_commits2.png" alt="Committing changes directly in GitHub. When you click on the 'Edit' button in a file's GitHub page (see previous figure), it will take you to a page where you can edit the file directly. You save the changes with a 'commit', including a commit message describing why you made the change. The change will be tagged with the message and your name." width="\textwidth" />
<p class="caption">
Figure 2.44: Committing changes directly in GitHub. When you click on the ‘Edit’ button in a file’s GitHub page (see previous figure), it will take you to a page where you can edit the file directly. You save the changes with a ‘commit’, including a commit message describing why you made the change. The change will be tagged with the message and your name.
</p>
</div>
<p><strong>Tracking and making progress on issues</strong></p>
<p>Another way that a version control platform like GitHub can help you manage a
project is through the “Issues” tracker. As we described in module 2.10,
this Issues page can serve as a “to-do” list for the project as a whole.
It lets you keep track of the tasks that need to be done, as well as
have detailed conversations with your team about each task.</p>
<p>Each repository includes this type of tracker, and it can be easily used by all
team members, whether they are comfortable coding or not. Figure
<a href="experimental-data-recording.html#fig:githubissues1">2.45</a> gives an example of <a href="https://github.com/aef1004/cyto-feature_engineering/issues">the Issues tracker
page</a> for the
repository we are using as an example. There will be a main Issues page, like
one shown in this figure, as well as separate pages for each Issue.</p>
<div class="figure"><span style="display:block;" id="fig:githubissues1"></span>
<img src="figures/github_issues.png" alt="Issues tracker page for an example GitHub repository. Arrows highlight the tab to click to get to the Issues tracker page in a repository, as well as where to go to find open and closed Issues for the repository." width="\textwidth" />
<p class="caption">
Figure 2.45: Issues tracker page for an example GitHub repository. Arrows highlight the tab to click to get to the Issues tracker page in a repository, as well as where to go to find open and closed Issues for the repository.
</p>
</div>
<p>The main Issues tracker page provides clickable links to all open issues for
the repository. You can open a new issue using the “New Issue” on this main
page or on the specific page of any of the repository’s issues. See Figure
<a href="experimental-data-recording.html#fig:githubissues2">2.46</a> for an example of this button.</p>
<div class="figure"><span style="display:block;" id="fig:githubissues2"></span>
<img src="figures/github_issues2.png" alt="Conversation about an Issue on Issues tracker page of an example GitHub repository. In this example, you can see how GitHub Issues trackers allow you to discuss how to resolve an issue across your team. From this page, you can read the current conversation about Issue \#1 of the repository and add your own comments. Once the Issue is resolved, you can 'Close' the Issue, which moves it off the list of active issues, but allows you to still re-read the conversation and, if necessary, re-open the issue later. You can also open a new issue from this page, using the button highlighted at the top right." width="\textwidth" />
<p class="caption">
Figure 2.46: Conversation about an Issue on Issues tracker page of an example GitHub repository. In this example, you can see how GitHub Issues trackers allow you to discuss how to resolve an issue across your team. From this page, you can read the current conversation about Issue #1 of the repository and add your own comments. Once the Issue is resolved, you can ‘Close’ the Issue, which moves it off the list of active issues, but allows you to still re-read the conversation and, if necessary, re-open the issue later. You can also open a new issue from this page, using the button highlighted at the top right.
</p>
</div>
<p>On the page for a specific issue (e.g., Figure <a href="experimental-data-recording.html#fig:githubissues2">2.46</a>), you
can have a conversation with your team to determine how to resolve the issue.
This conversation can include web links, figures, and even lists with check boxes, to
help you discuss and plan how to resolve the issue. Each issue is numbered,
which allows you to track each individually as you work on the project.</p>
<p>Once you have resolved an issue, you will close it, using a “Close” button on the
Issue’s page (see Figure <a href="experimental-data-recording.html#fig:githubissues2">2.46</a> for an example). This moves the issue
from the active list into a “Closed” list. Each closed issue still has its
own page, where you can read through the conversation describing how it
was resolved. If you need to, you can re-open a closed issue later, if you
determine that it was not fully resolved. Figure <a href="experimental-data-recording.html#fig:githubissues1">2.45</a> shows
where to go to see a list of closed Issues for a project.</p>
<div class="figure"><span style="display:block;" id="fig:githubissues3"></span>
<img src="figures/github_issues3.png" alt="Labeling and assigning Issues. The GitHub Issues tracker allows you to assign each issue to one or more team members, clarifying that they will take the lead in resolving the issue. It also allows you to tag each issue with one or more labels, so you can easily navigate to issues of a specific type or identify the category of a specific issue." width="\textwidth" />
<p class="caption">
Figure 2.47: Labeling and assigning Issues. The GitHub Issues tracker allows you to assign each issue to one or more team members, clarifying that they will take the lead in resolving the issue. It also allows you to tag each issue with one or more labels, so you can easily navigate to issues of a specific type or identify the category of a specific issue.
</p>
</div>
<p>The Issues tracker page includes more advanced functionality, as well
(Figure <a href="experimental-data-recording.html#fig:githubissues3">2.47</a>). For example, you can assign an issue to one
of more team members, indicating that they are responsible for resolving that
issue. You can also tag each issue with one of more labels, allowing you to
group issues into common categories. For example, you could tag all issues that
cover questions about pre-processing the data using a “pre-processing” label,
and all that are related to creating figures for the final manuscript with a
“figures” label.</p>
<p><strong>Managing repository access and ownership</strong></p>
<p>Repositories include functionality for inviting team members, assigning
roles, and otherwise managing access to the repository. First, a repository
can be either public or private. For a public repository, anyone will be
able to see the full contents of the repository through GitHub. You can
also set a repository to be private. In this case, the repository can only
be seen by those who have been invited to collaborate on the repository, and
only when they are logged in to their GitHub accounts. The private / public
status of a repository can be changed at any time, so if you want you can
maintain a repository for a project as private until you publish the results,
and then switch it to be public, to allow others to explore the code and data
that are linked to your published results.</p>
<p>You can invite team members to collaborate on a repository, as long as they have
GitHub accounts. While public repositories can be seen by anyone, the only
people who can add to or change the contents of the repository are people who
have been invited to collaborate on the repository. The person who creates the
repository (the repository “owner”) can invite other collaborators through the
“Settings” tab of the repository, which will have a “Manage access” function for
the repositories maintainer. Only the owner of the repository will have access
to this tab for the repo. On this page, you can invite other collaborators by
searching using their GitHub “handle” (the short name they chose to be
identified by in GitHub). You can also change access rights, for example,
allowing some team members to be able to make major changes to the
repository—like deleting it—while others can make only smaller
modifications.</p>
<p>[Add: Roles on a repository]</p>
<p><strong>Providing project documentation</strong></p>
<p>[Add: README with Markdown]</p>
<p>If you are planning to use GitHub as a way to share the project directory, you
will find it useful to create the README file using a file format called
“Markdown”. [Automatically renders in a nice format when you put it on GitHub]</p>
<ul>
<li>Module 2.7: metadata, README</li>
<li>Markdown renders nicely when posted on GitHub</li>
<li>Show example from Amy’s project</li>
</ul>
</div>
<div id="leveraging-git-and-github-as-a-scientist-who-programs" class="section level3 hasAnchor" number="2.11.2">
<h3><span class="header-section-number">2.11.2</span> Leveraging Git and GitHub as a scientist who programs<a href="experimental-data-recording.html#leveraging-git-and-github-as-a-scientist-who-programs" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To be able to leverage GitHub to manage projects and share data, you will need
to have at least one person in the research group who can set up the initial
repository. GitHub repositories can be created very easily starting from an
RStudio Project, a format for organizing project files that was described in
module 3.7.</p>
<p>There are many excellent resources that provide instructions on this topic
meant for researchers who are comfortable with using R and RStudio. An
excellent place to start is with an online book written by Jenny Bryan
named <em>Happy Git and GitHub for the useR</em>. This book is available for free
at <a href="https://happygitwithr.com/" class="uri">https://happygitwithr.com/</a>. It provides a gentle yet thorough introduction
to using Git, connecting it to RStudio Projects, and connecting everything
with an online version control platform like GitHub. It also includes a
helpful section that covers what a daily workflow will look like when you
are using Git and GitHub in conjunction with projects that include R code.</p>
<p>Once you’ve explored that resource, some others you might find useful are:</p>
<ul>
<li>Software Carpentry’s introduction to version control with Git, available at
<a href="https://swcarpentry.github.io/git-novice/" class="uri">https://swcarpentry.github.io/git-novice/</a></li>
<li>Article on <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004668"><em>A Quick Introduction to Version Control with Git and GitHub</em></a>
<span class="citation">(Blischak, Davenport, and Wilson 2016)</span></li>
<li>Article on <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004947">Ten Simple Rules for Taking Advantage of Git and GitHub</a>
<span class="citation">(Perez-Riverol et al. 2016)</span></li>
</ul>
<!-- ### Applied exercise -->

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="experimental-data-pre-processing.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/01-separating.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["improve_repro.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
