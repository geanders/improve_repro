<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Module 2 Experimental Data Recording | Improving the Reproducibility of Experimental Data Recording and Pre-Processing</title>
  <meta name="description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing." />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="Module 2 Experimental Data Recording | Improving the Reproducibility of Experimental Data Recording and Pre-Processing" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Module 2 Experimental Data Recording | Improving the Reproducibility of Experimental Data Recording and Pre-Processing" />
  
  <meta name="twitter:description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing." />
  

<meta name="author" content="Brooke Anderson, Michael Lyons, Mercedes Gonzalez-Juarrero, Marcela Henao-Tamayo, and Gregory Robertson" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="experimental-data-preprocessing.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Visualization in R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Rigor and reproducibility in computation</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#overview-of-these-modules"><i class="fa fa-check"></i><b>1.1</b> Overview of these modules</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i><b>1.2</b> License</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html"><i class="fa fa-check"></i><b>2</b> Experimental Data Recording</a>
<ul>
<li class="chapter" data-level="2.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module1"><i class="fa fa-check"></i><b>2.1</b> Separating data recording and analysis</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#popularity-of-spreadsheets"><i class="fa fa-check"></i><b>2.1.1</b> Popularity of spreadsheets</a></li>
<li class="chapter" data-level="2.1.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#data-recording-versus-data-analysis"><i class="fa fa-check"></i><b>2.1.2</b> Data recording versus data analysis</a></li>
<li class="chapter" data-level="2.1.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#hazards-of-combining-recording-and-analysis"><i class="fa fa-check"></i><b>2.1.3</b> Hazards of combining recording and analysis</a></li>
<li class="chapter" data-level="2.1.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#approaches-to-separate-recording-and-analysis"><i class="fa fa-check"></i><b>2.1.4</b> Approaches to separate recording and analysis</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module2"><i class="fa fa-check"></i><b>2.2</b> Principles and power of structured data formats</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#data-recording-standards"><i class="fa fa-check"></i><b>2.2.1</b> Data recording standards</a></li>
<li class="chapter" data-level="2.2.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#defining-data-standards-for-a-research-group"><i class="fa fa-check"></i><b>2.2.2</b> Defining data standards for a research group</a></li>
<li class="chapter" data-level="2.2.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#two-dimensional-structured-data-format"><i class="fa fa-check"></i><b>2.2.3</b> Two-dimensional structured data format</a></li>
<li class="chapter" data-level="2.2.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#levels-of-standardizationresearch-group-to-research-community"><i class="fa fa-check"></i><b>2.2.4</b> Levels of standardization—research group to research community</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module3"><i class="fa fa-check"></i><b>2.3</b> The ‘tidy’ data format</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#what-makes-data-tidy"><i class="fa fa-check"></i><b>2.3.1</b> What makes data “tidy”?</a></li>
<li class="chapter" data-level="2.3.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#why-make-your-data-tidy"><i class="fa fa-check"></i><b>2.3.2</b> Why make your data “tidy”?</a></li>
<li class="chapter" data-level="2.3.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#using-tidyverse-tools-with-data-in-the-tidy-data-format"><i class="fa fa-check"></i><b>2.3.3</b> Using tidyverse tools with data in the “tidy data” format</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module4"><i class="fa fa-check"></i><b>2.4</b> Designing templates for “tidy” data collection</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#exampledata-on-rate-of-bacterial-growth"><i class="fa fa-check"></i><b>2.4.1</b> Example—Data on rate of bacterial growth</a></li>
<li class="chapter" data-level="2.4.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#features-that-make-data-collection-templates-untidy"><i class="fa fa-check"></i><b>2.4.2</b> Features that make data collection templates untidy</a></li>
<li class="chapter" data-level="2.4.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#converting-to-a-tidier-format-for-data-collection-templates"><i class="fa fa-check"></i><b>2.4.3</b> Converting to a “tidier” format for data collection templates</a></li>
<li class="chapter" data-level="2.4.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#learning-more-about-tidy-data-collection-in-the-laboratory"><i class="fa fa-check"></i><b>2.4.4</b> Learning more about tidy data collection in the laboratory</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module5"><i class="fa fa-check"></i><b>2.5</b> Example: Creating a template for “tidy” data collection</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#example-datadata-on-rate-of-bacterial-growth"><i class="fa fa-check"></i><b>2.5.1</b> Example data—Data on rate of bacterial growth</a></li>
<li class="chapter" data-level="2.5.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#limiting-the-template-to-the-collection-of-data"><i class="fa fa-check"></i><b>2.5.2</b> Limiting the template to the collection of data</a></li>
<li class="chapter" data-level="2.5.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#making-sensible-choices-about-rows-and-columns"><i class="fa fa-check"></i><b>2.5.3</b> Making sensible choices about rows and columns</a></li>
<li class="chapter" data-level="2.5.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#avoiding-problematic-characters-or-formatting"><i class="fa fa-check"></i><b>2.5.4</b> Avoiding problematic characters or formatting</a></li>
<li class="chapter" data-level="2.5.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#separating-data-analysis-from-data-collection"><i class="fa fa-check"></i><b>2.5.5</b> Separating data analysis from data collection</a></li>
<li class="chapter" data-level="2.5.6" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#applied-exercise"><i class="fa fa-check"></i><b>2.5.6</b> Applied exercise</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module6"><i class="fa fa-check"></i><b>2.6</b> Organizing project files</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#organizing-project-files"><i class="fa fa-check"></i><b>2.6.1</b> Organizing project files</a></li>
<li class="chapter" data-level="2.6.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#how-to-organize-project-files"><i class="fa fa-check"></i><b>2.6.2</b> How to organize project files</a></li>
<li class="chapter" data-level="2.6.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#what-is-a-project-template"><i class="fa fa-check"></i><b>2.6.3</b> What is a project template?</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module7"><i class="fa fa-check"></i><b>2.7</b> Creating project directory templates</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#designing-a-project-template"><i class="fa fa-check"></i><b>2.7.1</b> Designing a project template</a></li>
<li class="chapter" data-level="2.7.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#creating-and-using-a-project-template"><i class="fa fa-check"></i><b>2.7.2</b> Creating and using a project template</a></li>
<li class="chapter" data-level="2.7.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#project-directories-as-rstudio-projects"><i class="fa fa-check"></i><b>2.7.3</b> Project directories as RStudio Projects</a></li>
<li class="chapter" data-level="2.7.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#creating-project-templates-in-rstudio"><i class="fa fa-check"></i><b>2.7.4</b> Creating ‘Project’ templates in RStudio</a></li>
<li class="chapter" data-level="2.7.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#discussion-questions"><i class="fa fa-check"></i><b>2.7.5</b> Discussion questions</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module8"><i class="fa fa-check"></i><b>2.8</b> Example: Creating a project template</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#description-of-the-example-set-of-studies"><i class="fa fa-check"></i><b>2.8.1</b> Description of the example set of studies</a></li>
<li class="chapter" data-level="2.8.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#step-1-survey-of-data-collected-for-the-projects"><i class="fa fa-check"></i><b>2.8.2</b> Step 1: Survey of data collected for the projects</a></li>
<li class="chapter" data-level="2.8.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#step-2-organizing-a-project-directory"><i class="fa fa-check"></i><b>2.8.3</b> Step 2: Organizing a project directory</a></li>
<li class="chapter" data-level="2.8.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#step-3-establishing-file-name-conventions"><i class="fa fa-check"></i><b>2.8.4</b> Step 3: Establishing file name conventions</a></li>
<li class="chapter" data-level="2.8.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#step-4-designing-data-collection-templates"><i class="fa fa-check"></i><b>2.8.5</b> Step 4: Designing data collection templates</a></li>
<li class="chapter" data-level="2.8.6" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#step-5-designing-a-report-template"><i class="fa fa-check"></i><b>2.8.6</b> Step 5: Designing a report template</a></li>
<li class="chapter" data-level="2.8.7" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#applied-exercise-1"><i class="fa fa-check"></i><b>2.8.7</b> Applied exercise</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module9"><i class="fa fa-check"></i><b>2.9</b> Harnessing version control for transparent data recording</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#challenges-of-collaborating-on-evolving-research-materials"><i class="fa fa-check"></i><b>2.9.1</b> Challenges of collaborating on evolving research materials</a></li>
<li class="chapter" data-level="2.9.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#recording-data-in-the-laboratoryfrom-paper-to-computers"><i class="fa fa-check"></i><b>2.9.2</b> Recording data in the laboratory—from paper to computers</a></li>
<li class="chapter" data-level="2.9.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#defining-version-and-version-control"><i class="fa fa-check"></i><b>2.9.3</b> Defining “version” and “version control”</a></li>
<li class="chapter" data-level="2.9.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#what-are-the-key-elements-of-version-control"><i class="fa fa-check"></i><b>2.9.4</b> What are the key elements of version control?</a></li>
<li class="chapter" data-level="2.9.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#comparing-git-to-other-tools"><i class="fa fa-check"></i><b>2.9.5</b> Comparing git to other tools</a></li>
<li class="chapter" data-level="2.9.6" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#discussion-questions-1"><i class="fa fa-check"></i><b>2.9.6</b> Discussion questions</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module10"><i class="fa fa-check"></i><b>2.10</b> Enhance the reproducibility of collaborative research with version control platforms</a>
<ul>
<li class="chapter" data-level="2.10.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#what-are-version-control-platforms"><i class="fa fa-check"></i><b>2.10.1</b> What are version control platforms?</a></li>
<li class="chapter" data-level="2.10.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#why-use-version-control-platforms"><i class="fa fa-check"></i><b>2.10.2</b> Why use version control platforms?</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module11"><i class="fa fa-check"></i><b>2.11</b> Using git and GitLab to implement version control</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#how-to-use-version-control"><i class="fa fa-check"></i><b>2.11.1</b> How to use version control</a></li>
<li class="chapter" data-level="2.11.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#leveraging-git-and-github-as-a-project-director"><i class="fa fa-check"></i><b>2.11.2</b> Leveraging git and GitHub as a project director</a></li>
<li class="chapter" data-level="2.11.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#leveraging-git-and-github-as-a-scientist-who-programs"><i class="fa fa-check"></i><b>2.11.3</b> Leveraging git and GitHub as a scientist who programs</a></li>
<li class="chapter" data-level="2.11.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#applied-exercise-2"><i class="fa fa-check"></i><b>2.11.4</b> Applied exercise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html"><i class="fa fa-check"></i><b>3</b> Experimental Data Preprocessing</a>
<ul>
<li class="chapter" data-level="3.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#module12"><i class="fa fa-check"></i><b>3.1</b> Principles of pre-processing experimental data</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#what-is-data-preprocessing"><i class="fa fa-check"></i><b>3.1.1</b> What is data preprocessing?</a></li>
<li class="chapter" data-level="3.1.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#common-themes-and-processes-in-data-preprocessing"><i class="fa fa-check"></i><b>3.1.2</b> Common themes and processes in data preprocessing</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#module12a"><i class="fa fa-check"></i><b>3.2</b> Selecting software options for pre-processing</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#gui-versus-code-script"><i class="fa fa-check"></i><b>3.2.1</b> GUI versus code script</a></li>
<li class="chapter" data-level="3.2.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#open-source-versus-proprietary-software"><i class="fa fa-check"></i><b>3.2.2</b> Open-source versus proprietary software</a></li>
<li class="chapter" data-level="3.2.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#discussion-questions-2"><i class="fa fa-check"></i><b>3.2.3</b> Discussion questions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#module13"><i class="fa fa-check"></i><b>3.3</b> Introduction to scripted data pre-processing in R</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#what-is-a-code-script"><i class="fa fa-check"></i><b>3.3.1</b> What is a code script?</a></li>
<li class="chapter" data-level="3.3.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#how-code-scripts-improve-reproducibility-of-preprocessing"><i class="fa fa-check"></i><b>3.3.2</b> How code scripts improve reproducibility of preprocessing</a></li>
<li class="chapter" data-level="3.3.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#how-to-write-an-r-code-script"><i class="fa fa-check"></i><b>3.3.3</b> How to write an R code script</a></li>
<li class="chapter" data-level="3.3.4" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#how-to-run-code-in-an-r-script"><i class="fa fa-check"></i><b>3.3.4</b> How to run code in an R script</a></li>
<li class="chapter" data-level="3.3.5" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#exercise"><i class="fa fa-check"></i><b>3.3.5</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#module13a"><i class="fa fa-check"></i><b>3.4</b> Tips for improving reproducibility when writing R scripts</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#tips-and-guidelines-for-writing-code-scripts"><i class="fa fa-check"></i><b>3.4.1</b> Tips and guidelines for writing code scripts</a></li>
<li class="chapter" data-level="3.4.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#discussion-questions-3"><i class="fa fa-check"></i><b>3.4.2</b> Discussion questions</a></li>
<li class="chapter" data-level="3.4.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#potential-quotes"><i class="fa fa-check"></i><b>3.4.3</b> Potential quotes</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#module14"><i class="fa fa-check"></i><b>3.5</b> Simplify scripted pre-processing through R’s ‘tidyverse’ tools</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#an-overview-of-the-tidyverse-approach"><i class="fa fa-check"></i><b>3.5.1</b> An overview of the “tidyverse” approach</a></li>
<li class="chapter" data-level="3.5.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#useful-tidyverse-tools-for-data-preprocessing"><i class="fa fa-check"></i><b>3.5.2</b> Useful tidyverse tools for data preprocessing</a></li>
<li class="chapter" data-level="3.5.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#resources-to-learn-more-on-tidyverse-tools"><i class="fa fa-check"></i><b>3.5.3</b> Resources to learn more on tidyverse tools</a></li>
<li class="chapter" data-level="3.5.4" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#practice-quiz"><i class="fa fa-check"></i><b>3.5.4</b> Practice quiz</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#module15"><i class="fa fa-check"></i><b>3.6</b> Complex data types in experimental data pre-processing</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#how-the-bioconductor-and-tidyverse-approaches-differ"><i class="fa fa-check"></i><b>3.6.1</b> How the Bioconductor and tidyverse approaches differ</a></li>
<li class="chapter" data-level="3.6.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#why-is-the-bioconductor-approach-designed-as-it-is"><i class="fa fa-check"></i><b>3.6.2</b> Why is the Bioconductor approach designed as it is?</a></li>
<li class="chapter" data-level="3.6.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#why-is-it-sometimes-necessary-to-use-a-bioconductor-approach-with-biomedical-data"><i class="fa fa-check"></i><b>3.6.3</b> Why is it sometimes necessary to use a Bioconductor approach with biomedical data</a></li>
<li class="chapter" data-level="3.6.4" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#combining-bioconductor-and-tidyverse-approaches-in-a-workflow"><i class="fa fa-check"></i><b>3.6.4</b> Combining Bioconductor and tidyverse approaches in a workflow</a></li>
<li class="chapter" data-level="3.6.5" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#outlook-for-a-tidyverse-approach-to-biomedical-data"><i class="fa fa-check"></i><b>3.6.5</b> Outlook for a tidyverse approach to biomedical data</a></li>
<li class="chapter" data-level="3.6.6" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#practice-quiz-1"><i class="fa fa-check"></i><b>3.6.6</b> Practice quiz</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#module18"><i class="fa fa-check"></i><b>3.7</b> Introduction to reproducible data pre-processing protocols</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#introducing-reproducible-data-pre-processing-protocols"><i class="fa fa-check"></i><b>3.7.1</b> Introducing reproducible data pre-processing protocols</a></li>
<li class="chapter" data-level="3.7.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#using-knitted-documents-for-protocols"><i class="fa fa-check"></i><b>3.7.2</b> Using knitted documents for protocols</a></li>
<li class="chapter" data-level="3.7.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#advantages-of-using-knitted-documents-for-data-focused-protocols"><i class="fa fa-check"></i><b>3.7.3</b> Advantages of using knitted documents for data-focused protocols</a></li>
<li class="chapter" data-level="3.7.4" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#how-knitted-documents-work"><i class="fa fa-check"></i><b>3.7.4</b> How knitted documents work</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#module19"><i class="fa fa-check"></i><b>3.8</b> RMarkdown for creating reproducible data pre-processing protocols</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#creating-knitted-documents-in-r"><i class="fa fa-check"></i><b>3.8.1</b> Creating knitted documents in R</a></li>
<li class="chapter" data-level="3.8.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#formatting-text-with-markdown-in-rmarkdown"><i class="fa fa-check"></i><b>3.8.2</b> Formatting text with Markdown in Rmarkdown</a></li>
<li class="chapter" data-level="3.8.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#preambles-in-rmarkdown-documents"><i class="fa fa-check"></i><b>3.8.3</b> Preambles in Rmarkdown documents</a></li>
<li class="chapter" data-level="3.8.4" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#executable-code-in-rmarkdown-files"><i class="fa fa-check"></i><b>3.8.4</b> Executable code in Rmarkdown files</a></li>
<li class="chapter" data-level="3.8.5" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#more-advanced-rmarkdown-functionality"><i class="fa fa-check"></i><b>3.8.5</b> More advanced Rmarkdown functionality</a></li>
<li class="chapter" data-level="3.8.6" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#learning-more-about-rmarkdown."><i class="fa fa-check"></i><b>3.8.6</b> Learning more about Rmarkdown.</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#module20"><i class="fa fa-check"></i><b>3.9</b> Example: Creating a reproducible data pre-processing protocol</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#introduction-and-example-data"><i class="fa fa-check"></i><b>3.9.1</b> Introduction and example data</a></li>
<li class="chapter" data-level="3.9.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#advice-on-designing-a-pre-processing-protocol"><i class="fa fa-check"></i><b>3.9.2</b> Advice on designing a pre-processing protocol</a></li>
<li class="chapter" data-level="3.9.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#writing-data-pre-processing-protocols"><i class="fa fa-check"></i><b>3.9.3</b> Writing data pre-processing protocols</a></li>
<li class="chapter" data-level="3.9.4" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#applied-exercise-3"><i class="fa fa-check"></i><b>3.9.4</b> Applied exercise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>4</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Improving the Reproducibility of Experimental Data Recording and Pre-Processing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="experimental-data-recording" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Module 2</span> Experimental Data Recording<a href="experimental-data-recording.html#experimental-data-recording" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This section includes modules on:</p>
<ul>
<li><a href="experimental-data-recording.html#module1">Module 2.1: Separating data recording and analysis</a></li>
<li><a href="experimental-data-recording.html#module2">Module 2.2: Principles and power of structured data formats</a></li>
<li><a href="experimental-data-recording.html#module3">Module 2.3: The ‘tidy’ data format</a></li>
<li><a href="experimental-data-recording.html#module4">Module 2.4: Designing templates for “tidy” data collection</a></li>
<li><a href="experimental-data-recording.html#module5">Module 2.5: Example: Creating a template for “tidy” data collection</a></li>
<li><a href="experimental-data-recording.html#module6">Module 2.6: Organizing project files</a></li>
<li><a href="experimental-data-recording.html#module7">Module 2.7: Creating project directory templates</a></li>
<li><a href="experimental-data-recording.html#module8">Module 2.8: Example: Creating a project template</a></li>
<li><a href="experimental-data-recording.html#module9">Module 2.9: Harnessing version control for transparent data recording</a></li>
<li><a href="experimental-data-recording.html#module10">Module 2.10: Enhance the reproducibility of collaborative research with version control platforms</a></li>
<li><a href="experimental-data-recording.html#module11">Module 2.11: Using git and GitLab to implement version control</a></li>
</ul>
<div id="module1" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Separating data recording and analysis<a href="experimental-data-recording.html#module1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Many biomedical laboratories currently use spreadsheet programs to jointly
record, visualize, and analyze experimental data <span class="citation">(Broman and Woo 2018)</span>. These
software tools, such as Microsoft Excel or Google Sheets, provide for manual or
automated entry of data into rows and columns of cells. Standard or custom
formulas and other operations can be applied to the cells, and are commonly used
to reformat or clean the data, calculate various statistics, and to generate
simple plots; all of which are embedded as additional data entries and
programming elements within the spreadsheet. While these tools greatly improved
the paper worksheets on which they were originally based <span class="citation">(Campbell-Kelly 2007)</span>,
this all-in-one practice impedes the transparency and reproducibility of both
recording and analysis of the large and complex data sets that are routinely
generated in life science experiments.</p>
<p>To improve the computational reproducibility of a research project, it is
critical for biomedical researchers to learn the importance of maintaining
recorded experimental data as “read-only” files, separating data recording from
any data pre-processing or data analysis steps <span class="citation">(Broman and Woo 2018; Marwick, Boettiger, and Mullen 2018)</span>. Statisticians have outlined specific methods that a
laboratory-based scientist can take to ensure that data shared in an Excel
spreadsheet are shared in a reliable and reproducible way, including avoiding
macros or embedded formulas, using a separate Excel file for each dataset,
recording descriptions of variables in a separate code book rather than in the
Excel file, avoiding the use of color of the cells to encode information, using
“NA” to code missing values, avoiding spaces in column headers, and avoiding
splitting or merging cells <span class="citation">(Ellis and Leek 2018; Broman and Woo 2018)</span>. In this module,
we will describe this common practice and will outline alternative approaches
that separate the steps of data recording and data analysis.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Explain the difference between data recording and data analysis</li>
<li>Understand why collecting data on spreadsheets with embedded formulas impedes
reproducibility</li>
<li>List alternative approaches to improve reproducibility</li>
</ul>
<p>Many scientific laboratories use spreadsheets within their data collection
process, both to record data and to clean and analyze the data. Studies have
surveyed scientists about their work practices, and they’ve found that
spreadsheets are a common element in scientists’ toolboxes. Examples include
surveys of over 250 biomedical researchers at the University of Washington
<span class="citation">(Anderson et al. 2007)</span>, and of neuroscience researchers at the University of
Newcastle. In both these studies, most respondents reported that they used
spreadsheets and other general-purpose software in their research
<span class="citation">(AlTarawneh and Thorne 2017)</span>. A working group on bioinformatics and data-intensive
science similarly found spreadsheets were the most common tool used across
attendees <span class="citation">(Barga et al. 2011)</span>.</p>
<p>In this module, we’ll talk some about why spreadsheets are so popular, as well
as some of their features that are beneficial for researchers. However, there
are also many problems they can introduce, particularly when spreadsheets are
used in a way that combines data collection with data pre-processing and analysis.
We’ll walk through some of these problems, and in later modules we’ll walk you
through alternatives, where spreadsheets are limited to recording data (if they’re
used at all), while steps of pre-processing and analysis are done with other
tools.</p>
<div id="popularity-of-spreadsheets" class="section level3 hasAnchor" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Popularity of spreadsheets<a href="experimental-data-recording.html#popularity-of-spreadsheets" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>All of us authors are old enough to remember when home computers were a novelty.
When you first got a computer in your home, it opened up all kinds of new powers.</p>
<p>For one of us, one particularly exciting piece of software was something called
<em>The Print Shop</em>. This software let you be an amateur graphic designer. You could
design things like signs and invitations. Because the printer paper at the time
was connected from one sheet to the next, with perforations between, you could
even make long banners. “Happy Birthday” banners, “Congratulations” banners,
“Welcome Home” banners: you could do it all. For someone who’d never had these
tools before, it was just thrilling.</p>
<p>This was evidently how early spreadsheet software made business executives feel.
Before these programs, if an executive wanted to crunch some numbers, they’d
have to send a request to their accounting department. The initial spreadsheet
program (VisiCalc) disrupted this process. It allowed one person to quickly
apply and test different models or calculations on recorded data
<span class="citation">(Levy 1984)</span>. These spreadsheet programs allowed non-programmers to
engage with data, including data processing and analysis tasks, in a way that
previously required programming expertise <span class="citation">(Levy 1984)</span>.</p>
<p>With spreadsheet programs, then, an executive could just play with the numbers
themselves. Because an early target for spreadsheet programs was these business
executives, the programs were designed to be very simple and easy to use—just
one step up in complexity from crunching numbers on the back of an envelope
<span class="citation">(Campbell-Kelly 2007)</span>.Spreadsheet programs in fact became so popular within
businesses that many attribute these programs with driving the uptake of
personal computers <span class="citation">(Campbell-Kelly 2007)</span>.</p>
<p>These kinds of software tools were designer for amateurs to begin to do some
of the things that otherwise required outsourcing to a professional. They are
fantastic tools for amateur exploration. They make it fun to test out ideas.</p>
<p>However, these types of software tools are so easy and convenient to use that it
can be tempting to let them replace more solid, production-level tools. It’s
easy, in other words, to make them the <em>only</em> tool used to tackle a problem,
rather than just the <em>first</em> tool to use to explore a solution. These tools
often work most of the time and most of the time. They are also often the
cheapest option, either in monetary cost or in the time investment to learn
them. However, they often fail when they’re used to replace more professional
options. This can be the case with spreadsheet programs in biomedical research,
where spreadsheets are often used not only as a straightforward way to record
data, but also to develop complex pipelines that process and analyze the data
once its been collected.</p>
</div>
<div id="data-recording-versus-data-analysis" class="section level3 hasAnchor" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Data recording versus data analysis<a href="experimental-data-recording.html#data-recording-versus-data-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In some cases, researchers use spreadsheets solely to record data, as a simple
type of database <span class="citation">(Birch, Lyford-Smith, and Guo 2018)</span>. However, biomedical researchers often use
spreadsheets to both record and analyze experimental data <span class="citation">(Anderson et al. 2007)</span>.
In this case, data processing and analysis is implemented through the use of
formulas and macros embedded within the spreadsheet. When a spreadsheet has
formulas or macros within it, the spreadsheet program creates an internal record
of how cells are connected through these formulas. For example, if the value in
a specific cell is converted from Fahrenheit to Celsius to fill a second cell,
and then that value is combined with other values in a column to calculate the
mean temperature across several observations, then the spreadsheet program has
internally saved how the later cells depend on the earlier ones. When you change
the value recorded in a cell of a spreadsheet, the spreadsheet program queries
this record and only recalculates the cells that depend on that cell. This
process allows the program to quickly “react” to any change in cell inputs,
immediately providing an update to all downstream calculations and analyses
<span class="citation">(Levy 1984)</span>. Since early in their development, spreadsheet programs
have also included <em>macros</em>, “a single computer instruction that stands for a
sequence of operations” <span class="citation">(Creeth 1985)</span>.</p>
<p>Spreadsheets have become so popular in part because so many people know how to
use them, at least in basic ways, and so many people have the software on their
computers that files can be shared with almost a guarantee that everyone will be
able to open the file on their own computer <span class="citation">(Hermans et al. 2016)</span>.
Spreadsheets use the visual metaphore of a traditional gridded ledger sheet
<span class="citation">(Levy 1984)</span>, providing an interface that is easy for users to
immediately understand and for which they can easily create a mental map
<span class="citation">(Birch, Lyford-Smith, and Guo 2018; Barga et al. 2011)</span>. This visually clear interface also means that
spreadsheets can be printed or incorporated into other documents “as-is”, as a
workable and understandable table of data values. In fact, some of the most
popular plug-in software packages for the early spreadsheet program Lotus 1-2-3
were programs for printing and publishing spreadsheets <span class="citation">(Campbell-Kelly 2007)</span>.
This “What You See Is What You Get” interface was a huge advance from previous
methods of data analysis for the first spreadsheet program, VisiCalc, providing
a “window to the data” that was accessible to business executives and others
without programming expertise <span class="citation">(Creeth 1985)</span>. Several surveys of
researchers have found that spreadsheets were popular because of their
simplicity and ease-of-use <span class="citation">(Anderson et al. 2007; AlTarawneh and Thorne 2017; Barga et al. 2011)</span>. By contrast, databases and scripted programming
languages can be perceived as requiring a cognitive load and lengthy training
that is not worth the investment when an easier tool is available
<span class="citation">(Hermans et al. 2016; Anderson et al. 2007; Myneni and Patel 2010; Barga et al. 2011; Topaloglou et al. 2004)</span>.</p>
</div>
<div id="hazards-of-combining-recording-and-analysis" class="section level3 hasAnchor" number="2.1.3">
<h3><span class="header-section-number">2.1.3</span> Hazards of combining recording and analysis<a href="experimental-data-recording.html#hazards-of-combining-recording-and-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Raw data often lost</strong></p>
<p>One of the key tenets of ensuring that research is computationally reproducible
is to always keep a copy of all raw data, as well as the steps taken to get from
the raw data to a cleaned version of the data through to the results of data
analysis. However, maintaining an easily accessible copy of all original raw data
for a project is a common problem among biomedical researchers
<span class="citation">(Goodman et al. 2014)</span>, especially as team members move on from a laboratory group
<span class="citation">(Myneni and Patel 2010)</span>.</p>
<p>One thing that can contribute to this problem is the use of spreadsheets to
jointly record and analyze data. First, data in a spreadsheet is typically not
saved as “read-only”, so it is possible for it to be accidentally overwritten:
in situations where spreadsheets are shared among multiple users, original cell
values can easily be accidentally written over, and it may not be clear who last
changed a value, when it was changed, or why <span class="citation">(AlTarawneh and Thorne 2017)</span>.</p>
<p>A second problem is that raw and processed data are combined in a spreadsheet,
which makes it hard to identify which data points within the spreadsheet make up
the raw data and which are the result of processing that raw data. One study of
operational spreadsheets noted that:</p>
<blockquote>
<p>“The data used in most spreadsheets is undocumented and there is no practical
way to check it. Even the original developer would have difficulty checking the
data.” <span class="citation">(S. G. Powell, Baker, and Lawson 2009)</span></p>
</blockquote>
<p>Finally, many spreadsheets use a proprietary format. In the development of
spreadsheet programs, this use of proprietary binary file formats helped a
software program keep users, increasing barriers for a user to switch to a new
program (since the new program wouldn’t be able to read their old files)
<span class="citation">(Campbell-Kelly 2007)</span>. However, this file format may be hard to open in the
future, as software changes and evolves <span class="citation">(Michener 2015)</span>; by comparison, plain
text files should be widely accessible through general purpose tools—a text
editor is a type of software available on all computers, for
example—regardless of changes to proprietary software like Microsoft Excel.</p>
<p><strong>Opacity of analysis steps</strong></p>
<p>To keep analysis steps clear—whether the calculation is being done in scripted
code or in spreadsheets or in pen-and-paper calculations—it is important to
document what is being done at each step and why <span class="citation">(Goodman et al. 2014)</span>. Scripted
languages allow for code comments, which are written directly into the script
but not evaluated by the computer, and so can be used to document steps within
the code without changing the operation of the code. Further, the program file
itself often presents a linear, step-by-step view of the pipeline, stored
separated from the data itself <span class="citation">(Creeth 1985)</span>. Calculations done
with pen-and-paper (e.g., in a laboratory notebook) can be annotated with text
to document the steps. Spreadsheets, on the other hand, are often poorly
documented, or documented in ways that are hard to keep track of.</p>
<p>Within spreadsheets, the logic and methods behind the pipeline of data
processing and analysis is often not documented, or only documented with cell
comments (hard to see as a whole) or in emails, not the spreadsheet file.
One study that investigated a large collection of spreadsheets found that most
do not include documentation explaining the logic or implementation of data
processing and analysis implemented within the spreadsheet
<span class="citation">(Hermans et al. 2016)</span>. A survey of neuroscience researchers at a UK
institute found that about a third of respondents included no documentation
for spreadsheets used in their research laboratories <span class="citation">(AlTarawneh and Thorne 2017)</span>.</p>
<p>When spreadsheet pipelines are documented, it is often through methods that are
hard to find and interpret later. One study of scientific researchers found
that, when research spreadsheets were documented, it was often through “cell
comments” added to specific cells in the spreadsheet, which can be hard to
interpret inclusively to understand the flow and logic of a spreadsheet as a
whole <span class="citation">(AlTarawneh and Thorne 2017)</span>. In some cases, teams use email chains, rather than
the document itself, to discuss and document functionality and changes in
spreadsheets. They pass versions of the spreadsheet file as attachments of
emails discuss the spreadsheet in the email body. One research team investigated
over 700,000 emails from employees of Enron that were released during legal
proceedings <span class="citation">(Hermans and Murphy-Hill 2015)</span>. They specifically investigated the spreadsheets
attached to these emails (over 15,000 spreadsheets) and how teams discussed the
spreadsheets within the emails themselves . They found that the logic and
methods of calculations within the spreadsheets were often documented within the
bodies of emails. This means that, if someone needs to figure out why a step was
taken or identify when an error was introduced into a spreadsheet, they must dig
through the chain of old emails documenting that spreadsheet, rather than having
the relevant documentation within the spreadsheet’s own file.</p>
<p>Another problem comes up because there may only be one person on the team who
fully understands the spreadsheet: often, the person who created the spreadsheet
is the only person who fully knows how it works <span class="citation">(Myneni and Patel 2010)</span>,
particularly if the spreadsheet includes complex macros or a complicated
structure in the analysis pipeline <span class="citation">(Creeth 1985)</span>. Data processing
and analysis pipelines for spreadsheets are not carefully designed; instead,
it’s more typically for spreadsheet user to start by directly entering data and
formulas without a clear overall plan <span class="citation">(AlTarawneh and Thorne 2017)</span>. As a result,
research spreadsheets are often not designed to follow a common structure for
the research field or for the laboratory group <span class="citation">(Anderson et al. 2007)</span>.</p>
<p>This practice creates a heavy dependence on the person who created that
spreadsheet anytime the data or results in that spreadsheet need to be
interpreted. This is particularly problematic in projects where the spreadsheet
will be shared for collaboration or adapted to be used in a future project, as
is often done in scientific research groups. In this case, it
can be hard to “onboard” new people to use the file, and much of the work and
knowledge about the spreadsheet can be lost when that person moves on from the
business or laboratory group <span class="citation">(Creeth 1985; Myneni and Patel 2010)</span>. If you share a spreadsheet with numerous and complex
macros and formulas included to clean and analyze the data, it can take an
extensive amount of time, and in some cases may be impossible, for the
researcher you share it with to decipher what is being done to get from the
original data input in some cells to the final results shown in others and in
graphs. Further, if others can’t figure out the steps being done through macros
and formulas in a spreadsheet, they will not be able to check it for problems in
the logic of the overall analysis pipeline or for errors in the specific
formulas used within that pipeline. They also will struggle to extend and adapt
the spreadsheet to be used for other projects. These problems come up not only
when sharing with a collaborator, but also when reviewing spreadsheets that you
have previously created and used (as many have noted, your most frequent
collaborator will likely be “future you”). In fact, one survey of biomedical
researchers at the University of Washington noted that,</p>
<blockquote>
<p>“The profusion of individually created spreadsheets containing overlapping and
inconsistently updated data created a great deal of confusion within some labs.
There was little consideration to future data exchange of submission
requirements at the time of publication.”
<span class="citation">(Anderson et al. 2007)</span></p>
</blockquote>
<p><strong>Potential for errors</strong></p>
<p>Because spreadsheets often do a poor job of making the analysis steps
transparent, they can be prone to bugs in analysis. Indeed, previous studies
have found that errors are very common within spreadsheets
<span class="citation">(Hermans et al. 2016)</span>. For example, one study of 50 operational
spreadsheets found that about 90% contained at least one error
<span class="citation">(S. G. Powell, Baker, and Lawson 2009)</span>. In part, it is easier to make errors in spreadsheets and
harder to catch errors in later work with a spreadsheet because the formulas and
connections between cells aren’t visible when you look at the
spreadsheet—they’re behind the scenes <span class="citation">(Birch, Lyford-Smith, and Guo 2018)</span>. This makes it very
hard to get a clear and complete view of the pipeline of analytic steps in data
processing and analysis within a spreadsheet, or to discern how cells are
connected within and across sheets of the spreadsheet. As one early article on
the history of spreadsheet programs notes:</p>
<blockquote>
<p>“People tend to forget that even the most elegantly crafted spreadsheet is a
house of cards, ready to collapse at the first erroneous assumption. The
spreadsheet that looks good but turns out to be tragically wrong is becoming
a familiar phenomenon.” <span class="citation">(Levy 1984)</span></p>
</blockquote>
<p>Some characteristics of spreadsheets may heighten chances for errors. These
include high conditional complexity, which can result from lots of branching of
data flow through if / else structures, as well as formulas that depend on a
large number of cells or that incorporate many functions
<span class="citation">(Hermans et al. 2016)</span>. Following the logical chain of spreadsheet formulas
can be particularly difficult when several calculations are chained in a row
<span class="citation">(Hermans and Murphy-Hill 2015)</span>. In some cases, if you are trying to figure out very long
chains of dependent formulas across spreadsheet cells, you may even have to
sketch out by hand the flow of information through the spreadsheet to understand
what’s going on <span class="citation">(Nardi and Miller 1990)</span>. When a spreadsheet uses macros, it can
also make it particularly hard to figure out the steps of an analysis and to
diagnose and fix any bugs in those steps <span class="citation">(Nash 2006; Creeth 1985)</span>. One study investigated how spreadsheets are used in
practice and noted that, “Many spreadsheets are so chaotically designed that
auditing (especially of a few formulas) is extremely difficult or impossible.”
<span class="citation">(S. G. Powell, Baker, and Lawson 2009)</span></p>
<p>In some cases, formula dependencies might span across different sheets of a
spreadsheet file. These cross-sheet dependencies can make the analysis steps
even more opaque <span class="citation">(Hermans et al. 2016)</span>, as a change in the cell value of
one sheet might not be immediately visible as a change in another cell on that
sheet (the same is true for spreadsheets so large that all the cells in a sheet
are not concurrently visible on the screen). Other common sources of errors
included incorrect references to cells inside formulas and incorrect use of
formulas <span class="citation">(S. G. Powell, Baker, and Lawson 2009)</span> or errors introduced through the common practice of
copying and pasting when developing spreadsheets <span class="citation">(Hermans et al. 2016)</span>.</p>
<p>There are methods that have been brought from more traditional programming work
into spreadsheet programming to try to help limit errors, including a tool
called assertions that allows users to validate data or test logic within their
spreadsheets <span class="citation">(Hermans et al. 2016)</span>. However, these are often not
implemented, in part perhaps because many spreadsheet users see themselves as
“end-users”, creating spreadsheets for their own personal use rather than as
something robust to future use by others, and so don’t seek out strategies
adopted by programmers when creating stable tools for others to use
<span class="citation">(Hermans et al. 2016)</span>. In practice, though, a spreadsheet is often used
much longer, and by more people, than originally intended. From early in the
history of spreadsheet programs, users have shared spreadsheet files with
interesting functionality with other users <span class="citation">(Levy 1984)</span>, and the
lifespan of a spreadsheet can extend and extend—a spreadsheet created by one
user for their own personal use can end up being used and modified by that
person or others for years <span class="citation">(Hermans et al. 2016)</span>.</p>
<p><strong>Subpar software for analysis.</strong></p>
<p>While spreadsheets serve as a widely-used tool for data recording and analysis,
in many cases spreadsheets programs are poorly suited to clean and analyze
scientific data compared to other programs. As tools and interfaces continue to
develop that make other software more user-friendly to those new to programming,
scientists may want to reevaluate the costs and benefits, in terms of both time
required for training and aptness of tools, for spreadsheet programs compared to
using scripted programming languages like R and Python.</p>
<p>Several problems have been identified with spreadsheet programs in the context
of recording and, especially, analyzing scientific data. First, some statistical
methods may be inferior to those available in other statistical programming
language. Many statistical operations require computations that cannot be
perfectly achieved with a computer, since the computer must ultimately solve
many mathematical problems using numerical approximations rather than continuous
methods (e.g., calculus). The choice of the algorithms used for these
approximations heavily influence how closely a result approximates the true
answer. Since the most popular spreadsheet program (Excel) is closed source, it
is hard to identify and diagnose such problems, and there is likely less of an
incentive for problems in statistical methodology to be fixed (rather than using
development time and funds to increase easier-to-see functionality in the
program).</p>
<p>A series of papers examined the quality of statistical methods in several
statistical software programs, including Excel, starting in the 1990s
<span class="citation">(Bruce D. McCullough and Wilson 1999, 2005; Bruce D. McCullough 1999; B. D. McCullough and Wilson 2002; Bruce D. McCullough and Heiser 2008; Mélard 2014)</span>. In the
earliest studies, they found some concerns across all programs considered
<span class="citation">(Bruce D. McCullough and Wilson 1999; Bruce D. McCullough 1999)</span>. One of the biggest
concerns, however, was that there was little evidence over the years that the
identified problems in Excel were resolved, or at least improved, over time
<span class="citation">(B. McCullough 2001; Bruce D. McCullough and Heiser 2008)</span>. The authors note that there may
be little incentive for checking and fixing problems with algorithms for
statistical approximation in closed source software like Excel, where sales
might depend more on the more immediately evident functionality in the software,
while problems with statistical algorithms might be less evident to potential
users <span class="citation">(B. McCullough 2001)</span>.</p>
<p>Open source software, on the other hand, offers pathways for identifying and fixing
any problems in the software, including for statistical algorithms and methods
implemented in the software’s code. Since the full source code is available, researchers
can closely inspect the algorithms being used and compare them to the latest
knowledge in statistical computing methodology. Further, if an inferior algorithm is in
use, most open source software licenses allow a user to adapt and extend the software,
including to implement better statistical algorithms.</p>
<p>Second, spreadsheet programs can include automated functionality that’s meant to
make something easier for most users, but that might invisibly create problems
in some cases. A critical problem, for example, has been identified when using
Excel for genomics data. When Excel encounters a cell value in a format that
seems like it could be a date (e.g., “Mar-3-06”), it will try to convert that
cell to a “date” class. Many software programs save date as this special “date”
format, where it is printed and visually appears in a format like “3-Mar-06” but
is saved internally by the program as a number (for Microsoft Excel, the number
of days since January 1, 1900 <span class="citation">(Willekens 2013)</span>). By doing this, the
software can more easily undertake calculations with dates, like calculating the
number of days between two dates or which of two dates is earlier.
Bioinformatics researchers at the National Institutes of Health found that Excel
was doing this type of automatic and irreversible date conversion for 30 gene
names, including “MAR3” and “APR-4”, resulting in these gene names being lost
for further analysis <span class="citation">(Zeeberg et al. 2004)</span>.</p>
<p>Avoiding this automatic date conversion required specifying that columns with
columns susceptible to these problems, including columns of gene names, should
be retained in a “text” class in Excel’s file import process. While this
problem was originally identified and published in 2004 <span class="citation">(Zeeberg et al. 2004)</span>,
along with tips to identify and avoid the problem, a study in 2016 found that
approximately a fifth of genomics papers investigated in a large-scale review
had gene name errors resulting from Excel automatic conversion, with the rate of
errors actually increasing over time <span class="citation">(Ziemann, Eren, and El-Osta 2016)</span>.</p>
<p>Other automatic conversion problems caused the lost of clone identifiers with
composed of digits and the letter “E” <span class="citation">(Zeeberg et al. 2004; Welsh et al. 2017)</span>,
which were assumed to be expressing a number using scientific notation and so
automatically and irreversibly converted to a numeric class. Further automatic
conversion problems can be caused by cells that start with an operator (e.g., “+
control”) or with leading zeros in a numeric identifier (e.g., “007”)
<span class="citation">(Welsh et al. 2017)</span>.</p>
<p>Finally, spreadsheet programs can be limited as analysis needs become more
complex or large <span class="citation">(Topaloglou et al. 2004)</span>. For example, spreadsheets can be
problematic when integrating or merging large, separate datasets
<span class="citation">(Birch, Lyford-Smith, and Guo 2018)</span>. This can create barriers, for example, in biological studies
seeking to integrate measurements from different instruments (e.g., flow
cytometry data with RNA-sequencing data). Further, while spreadsheet programs
continue to expand in their capacity for data, for very large datasets they
continue to face limits that may be reached in practical applications
<span class="citation">(Birch, Lyford-Smith, and Guo 2018)</span>—until recently, for example, Excel could not handle more
than one million rows of data per spreadsheet. Even when spreadsheets can handle
larger data, their efficiency in running data processing and analysis pipelines
across large datasets can be slow compared to code implemented with other
programming languages.</p>
<p><strong>Difficulty collaborating with statisticians.</strong></p>
<p>Modern biomedical researchers requires large teams, with statisticians and
bioinformaticians often forming a critical part of the team to enable
sophisticated processing and analysis of experimental data. However, the process
of combining data recording and analysis of experimental data, especially
through the use of spreadsheet programs, can create barriers in working across
disciplines. One group defined these issues as “data friction” and “science
friction”—the extra steps and work required at each interface where data
passes, for example, from a machine to analysis or from a collaborator in one
discipline to one in a separate discipline <span class="citation">(Edwards et al. 2011)</span>.</p>
<p>When collaborating with statisticians or bioinformaticians, one of the key
sources of this “data friction” can result from the use of spreadsheets to
jointly record and analyze experiemental data. First, spreadsheets are easy to
print or copy into another format (e.g., PowerPoint presentation, Word
document), and so researchers often design spreadsheets to be immediately
visually appealing to viewers. For example, a spreadsheet might be designed to
include hierarchically organized headers (e.g., heading and subheading, some
within a cell merged across several columns), or to show the result of a
calculation at the bottom of a column of observations (e.g., “Total” in the last
cell of the column) <span class="citation">(Teixeira and Amaral 2016)</span>. Multiple separate small tables
might be included in the same sheet, with empty cells used for visual
separation, or use a “horizontal single entry” design , where the headers are in
the leftmost column rather than the top row <span class="citation">(Teixeira and Amaral 2016)</span>.</p>
<p>These spreadsheet design choices make it much more difficult for the contents of
the spreadsheet to be read into other statistical programs. These types of data
require several extra steps in coding, in some cases fairly complex coding, with
regular expressions or logical rules needed to parse out the data and convert it
to the needed shape, before the statistical work can be done for the dataset.
This is a poor use of time for a collaborating statistician, especially if it
can be avoided through the design of the data recording template. Further, it
introduces many more chances for errors in cleaning the data.</p>
<p>Further, information embedded in formulas, macros, and extra formatting like
color or text boxes is lost when the spreadsheet file is input into other
programs. Spreadsheets allow users to use highlighting to represent information
(e.g., measurements for control animals shown in red, those for experiment
animals in blue) and to include information or documentation in text boxes. For
example, one survey study of biomedical researchers at the University of
Washington included this quote from a respondent: “I have one spreadsheet that
has all of my chromosomes … and then I’ve gone through and color coded it for
homozygosity and linkage.” <span class="citation">(Anderson et al. 2007)</span> All the information encoded in
this sheet through color will be lost when the data from the spreadsheet is read
into another statistical program.</p>
</div>
<div id="approaches-to-separate-recording-and-analysis" class="section level3 hasAnchor" number="2.1.4">
<h3><span class="header-section-number">2.1.4</span> Approaches to separate recording and analysis<a href="experimental-data-recording.html#approaches-to-separate-recording-and-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the remaining modules in this section, we will present and describe techniques
that can be used to limit or remove these problems. First, in the next few modules,
we will walk through techniques to design data recording
formats so that data is saved in a consistent format across experiments within
a laboratory group, and in a way that removes “data friction” for collaboration
with statisticians or later use in scripted code. These techniques can be immediately
used to design a better spreadsheet to be used solely for data collection.</p>
<p>In later modules, we will discuss the use of R projects to coordinate data
recording and analysis steps within a directory, while using separate files for
data recording versus data processing and analysis. These more advanced formats
will enable the use of quality assurance / control measures like testing of data
entry and analysis functionality, better documentation of data analysis pipelines,
and easy use of version control to track projects and collaborate transparently and
with a recorded history.</p>

</div>
</div>
<div id="module2" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Principles and power of structured data formats<a href="experimental-data-recording.html#module2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The format in which experimental data is recorded can have a large influence on
how easy and likely it is to implement reproducibility tools in later stages of
the research workflow. Recording data in a “structured” format brings many
benefits. In this module, we will explain what makes a dataset “structured” and
why this format is a powerful tool for reproducible research.</p>
<p>Every extra step of data cleaning is another chance to introduce errors in
experimental biomedical data, and yet laboratory-based researchers often share
experimental data with collaborators in a format that requires extensive
additional cleaning before it can be input into data analysis
<span class="citation">(Broman and Woo 2018)</span>. Recording data in a “structured” format brings many
benefits for later stages of the research process, especially in terms of
improving reproducibility and reducing the probability of errors in analysis
<span class="citation">(Ellis and Leek 2018)</span>. Data that is in a structured, tabular, two-dimensional
format is substantially easier for collaborators to understand and work with,
without additional data formatting <span class="citation">(Broman and Woo 2018)</span>. Further, by using a
consistent structured format across many or all data in a research project, it
becomes much easier to create solid, well-tested code scripts for data
pre-processing and analysis and to apply those scripts consistently and
reproducibly across datasets from multiple experiments <span class="citation">(Broman and Woo 2018)</span>.
However, many biomedical researchers are unaware of this simple yet powerful
strategy in data recording and how it can improve the efficiency and
effectiveness of collaborations <span class="citation">(Ellis and Leek 2018)</span>.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>List the characteristics of a structured data format</li>
<li>Describe benefits for research transparency and reproducibility</li>
<li>Outline other benefits of using a structured format when recording data</li>
</ul>
<p>Guru Madhavan, the Senior Director of Programs at the National Academy of
Engineering, wrote a book in 2015 called <em>Applied Minds: How Engineers Think</em>.
In this book, he described a powerful tool for engineers—standards:</p>
<blockquote>
<p>“Standards are for products what grammar is for language. People sometimes
criticize standards for making life a matter of routine rather than inspiration.
Some argue that standards hinder creativity and keep us slaves to the past.
But try imagining a world without standards. From tenderloin beef cuts to
the geometric design of highways, standards may diminish variety and
authenticity, but they improve efficiency. From street signs to nutrition
labels, standards provide a common language of reason. From Internet
protocols to MP3 audio formats, standards enable systems to work together.
From paper sizes … to George Laurer’s Universal Product Code, standards
offer the convenience of comparability.” <span class="citation">(Madhavan 2015)</span></p>
</blockquote>
<p>Standards can be a powerful tool for biomedical researchers, as well, including
when it comes to recording data. In this module, we’ll walk through several
types of standards that can be used when recording biomedical data.</p>
<div id="data-recording-standards" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Data recording standards<a href="experimental-data-recording.html#data-recording-standards" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For many areas of biological data, there has been a push to create standards for
how data is recorded and communicated. Standards can clarify several elements: the content
that should be included in a dataset, the format in which that content is
stored, and the vocabulary used within this data. One article names these three
facets of a data standard as the <em>minimum information</em>, <em>file formats</em>, and
<em>ontologies</em> <span class="citation">(Ghosh et al. 2011)</span>.</p>
<p>Many people and organizations (including funders) are excited about the idea of
developing and using data standards. Good standards—ones that are widely
adapted by researchers—can help in making sure that data submitted to data
repositories are used widely and that software can be developed that is
interoperable with data from many research group’s experiments. This section
describes the elements that go into a data standard, discusses some choices to
be made when definining a data standard (especially choices on data structure
and file formats), and some of the advantages and disadvantages of developing
and using data recording standards at several levels, including research group
and community levels.</p>
<p>For a simple example, think about recording dates. The <em>minimum
information standard</em> for a date might always be the same—a recorded value
must include the day of the month, month, and year. However, this information
can be <em>structured</em> in a variety of ways. In many scientific data, it’s common
to record this information going from the largest to smallest units, so March
12, 2006, would be recorded “2006-03-12”. Another convention (especially in the
US) is to record the month first (e.g., “3/12/06”), while another (more common
in Europe) is to record the day of the month first (e.g., “12/3/06”).</p>
<p>If you are trying to combine data from different datasets with dates, and all
use a different structure, it’s easy to see how mistakes could be introduced
unless the data is very carefully reformatted. For example, March 12 (“3-12”
with month-first, “12-3” with day-first) could be easily mistaken to be December
3, and vice versa. Even if errors are avoided, combining data in different
structures will take more time than combining data in the same structure,
because of the extra needs for reformatting to get all data in a common
structure.</p>
<p><strong>Ontology standards.</strong></p>
<p>One type of standard is called an <em>ontology</em> (sometimes called a <em>terminology</em>
<span class="citation">(Sansone et al. 2012)</span>). An ontology helps define a vocabulary that is controlled
and consistent. It helps researchers, when they want to talk
about an idea or thing, to use one word, and just one word, and to ensure that
it will be the same word used by other researchers when they refer to that idea
or thing. Ontologies also help to define the relationships between ideas or
concrete things in a research area <span class="citation">(Ghosh et al. 2011)</span>, but here we’ll focus on
their use in provided a consistent vocabulary to use when recording data.</p>
<p>Let’s start with a very simple example to give you an idea of what an ontology
is. What do you call a small mammal that is often kept as a pet and that has
four legs and whiskers and purrs? If you are recording data that includes this
animal, do you record this as “cat” or “feline” or maybe, depending on the
animal, even “tabby” or “tom” or “kitten”? Similarly, do you record tuberculosis
as “tuberculosis” or “TB” or maybe even “consumption”? If you do not use the
same word consistently in a dataset to record an idea, then while a human might
be able to understand that two words should be considered equivalent, a computer
will not be able to immediately tell.</p>
<p>At a larger scale, if a research community can adapt an ontology—one they
agree to use throughout their studies—it will make it easier to understand and
integrate datasets produced by different research laboratories. If every
research group uses the term “cat” in the example above, then code can easily be
written to extract and combine all data recorded for cats across a large
repository of experimental data. On the other hand, if different terms are used,
then it might be necessary to first create a list of all terms used in datasets
in the respository, then pick through that list to find any terms that are
exchangeable with “cat”, then write script to pull data with any of those terms.</p>
<p>Several onotologies already exist or are being created for biological and other
biomedical research <span class="citation">(Ghosh et al. 2011)</span>. For biomedical science, practice, and
research, the BioPortal website (<a href="http://bioportal.bioontology.org/" class="uri">http://bioportal.bioontology.org/</a>) provides
access to over 1,000 ontologies, including several versions of the International
Classification of Diseases, the Medical Subject Headings (MESH), the National
Cancer Institute Thesaurus, the Orphanet Rare Disease Ontology and the National
Center for Biotechnology Information (NCBI) Organismal Classification. For each
ontology in the BioPortal website, the website provides a link for downloading
the ontology in several formats.</p>
<p>Try downloading one of the ontologies using a plaintext file format (the “CSV”
choice in the download options at the BioPortal link). Once you do, you can open
it in your favorite spreadsheet program and explore how it defines specific
terms to use for each idea or thing you might need to discuss within that topic
area, as well as synonyms for some of the terms.</p>
<p>To use an ontology when recording your own data, just make sure you use the
ontology’s suggested terms in your data. For example, if you’d like to use the
Ontology for Biomedical Investigations
(<a href="http://bioportal.bioontology.org/ontologies/OBI" class="uri">http://bioportal.bioontology.org/ontologies/OBI</a>) and you are recording how many
children a woman has had who were born alive, you should name that column of the
data “number of live births”, not “# live births” or “live births (N)” or
anything else. Other collections of ontologies exist for fields of scientific
research, including the Open Biological and Biomedical Ontology (OBO) Foundry
(<a href="http://www.obofoundry.org/" class="uri">http://www.obofoundry.org/</a>).</p>
<p>If there are community-wide ontologies in your field, it is worthwhile to use
them in recording experimental data in your research group. Even better is to
not only consistently use the defined terms, but also to follow any conventions
with capitalization. While most statistical programs provide tools to change
capitalization (for example, to change all letters in a character string to
lower case), this process does require an extra step of data cleaning and an
extra chance for confusion or for errors to be introduced into data.</p>
<p><strong>Minimum information standards.</strong> The next easiest facet of a data standard to
bring into data recording in a research group is <em>minimum information</em>. Within a
data recording standard, minimum information (sometimes also called <em>minimum
reporting guidelines</em> <span class="citation">(Sansone et al. 2012)</span> or <em>reporting requirements</em>
<span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span>) specify what should be included in a dataset
<span class="citation">(Ghosh et al. 2011)</span>. Using minimum information standards help ensure that data
within a laboratory, or data posted to a repository, contain a number of
required elements. This makes it easier to re-use the data, either to compare it
to data that a lab has newly generated, or to combine several posted datasets to
aggregate them for a new, integrated analysis, considerations that are growing
in importance with the increasing prevalence of research repositories and
research consortia in many fields of biomedical science <span class="citation">(Keller et al. 2017)</span>.</p>
<p>One article that discusses software for systems biology provides a definition
as well as examples of minimum information within this field:</p>
<blockquote>
<p>“Minimum information is a checklist of required supporting information for
datasets from different experiments. Examples include: Minimum Information About
a Microarray Experiment (MIAME), Minimum Information About a Proteomic
Experiment (MIAPE), and the Minimum Information for Biological and Biomedical
Investigations (MIBBI) project.” <span class="citation">(Ghosh et al. 2011)</span></p>
</blockquote>
<p><strong>Standardized file formats.</strong> While using a standard ontology and a standard for
minimum information is a helpful start, it just means that each dataset has the
required elements <em>somewhere</em>, and using a consistent vocabulary—it doesn’t
specify where those elements are in the data or that they’ll be in the same
place in every dataset that meets those standards. As a result, datasets that all
meet a common standard can still be very hard to combine, or to create common
data analysis scripts and tools for, since each dataset will require a different
process to pull out a given element.</p>
<p>Computer files serve as a way to organize data, whether that’s recorded
datapoints or written documents or computer programs <span class="citation">(Kernighan and Pike 1984)</span>. A
<em>file format</em> defines the rules for how the bytes in the chunk of memory that
makes up a certain file should be parsed and interpreted anytime you want to
meaningfully access and use the data within that file
<span class="citation">(Murrell 2009)</span>. There are many file formats you may be familiar
with—a file that ends in “.pdf” must be opened with a Portable Document Format
(PDF) Reader like Adobe Acrobat, or it won’t make much sense (you can try this
out by trying to open a “.pdf” file with a text editor, like TextEdit or
Notepad). The PDF Reader software has been programmed to interpret the data in a
“.pdf” file based on rules defining what data is stored where in the section of
computer memory for that file. Because most “.pdf” files conform to the same
<em>file format</em> rules, powerful software can be built that works with any file in
that format.</p>
<p>For certain types of biomedical data, the challenge of standardizing a format
has similarly been addressed through the use of well-defined rules for not only
the content of data, but also the way that content is structured. This can be
standardized through <em>standardized file formats</em> (sometimes also called <em>data
exchange formats</em> <span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span>) and often defines not only the
upper-level file format (e.g., use of a “.csv” file type), but also how data
within that file type should be organized. If data from different research
groups and experiments is recorded using the same file format, researchers can
develop software tools that can be repeatedly used to interpret and visualize
that data; on the other hand, if different experiments record data using
different formats, bespoke analysis scripts must be written for each separate
dataset.</p>
<p>This is a blow not only to the efficiency of data analysis, but also a
threat to the accuracy of that analysis. If a set of tools can be developed that
will work over and over, more time can be devoted to refining those tools and
testing them for potential errors and bugs, while one-shot scripts often can’t
be curated with similar care. One paper highlights the dangers that come with
working with files that don’t follow a defined format:</p>
<blockquote>
<p>“Beware of common pitfalls when working with <em>ad hoc</em> bioinformatics formats.
Simple mistakes over minor details like file formats can consume a
disproportionate amount of time and energy to discover and fix, so mind these
details early on.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<p>It also limits the usefulness of secondary data. As one article notes,</p>
<blockquote>
<p>“Vast swathes of bioscience data remain locked in esoteric formats, are
described using nonstandard terminology, lack sufficient contextual information,
or simply are never shared due to the perceived cost or futility of the
exercise.” <span class="citation">(Sansone et al. 2012)</span></p>
</blockquote>
<p>Some biomedical data file formats have been created to help smooth over the
transfer of data that’s captured by complex equipment into software that can
analyze that data. For example, many immunological studies need to measure
immune cell populations in experiments, and to do so they use piece of equipment
called a flow cytometer that probes cells in a sample with lasers and measures
resulting intensities to determine characteristics of that cell. The data
created by this equipment are large (often measurements from several lasers are
taken for a million or more cells in a single run). The data also are complex, as
they need to record not only the intensity measurements from each laser, but
also some metadata about the equipment and characteristics of the run.</p>
<p>If every company that makes flow cytometers used a different file format for
saving the resulting data, then a different set of analysis software would need
to be developed to accompany each piece of equipment. For example, a laboratory
at a university with flow cytometers from two different companies would need
licenses for two different software programs to work with data recorded by flow
cytometers, and they would need to learn how to use each software package
separately. There is a chance that software could be developed that used shared
code for data analysis, but only if it also included separate sets of code to
read in data from all types of equipment and to reformat them to a common
format.</p>
<p>This isn’t the case, however. Instead, there is a commonly agreed on file format
that flow cytometers should use to record the data they collect, called the the
<em>FCS file format</em>. This format has been defined through a series of papers
(e.g., <span class="citation">(Spidlen et al. 2021)</span>), with several separate versions as the file format
has evolved. It provides clear specifications on where to save each relevant
piece of information in the block of memory devoted to the data recorded by the
flow cytometer (in some cases, leaving a slot in the file blank if no relevant
information was collected on that element). As a result, people have been able
to create software, both proprietary and open-source, that can be used with any
data recorded by a flow cytometer, regardless of which company manufacturer the
piece of equipment that was used to generate the data.</p>
<p>Other types of biomedical data also have some standardized file formats,
including the FASTQ file format for sequencing data and the mzML file format for
metabolomics data. In some cases these were defined by an organization, society,
or initiative (e.g., the Metabolomics Standards Initiative)
<span class="citation">(Ghosh et al. 2011)</span>, while in some cases the file format developed by a
specific equipment manufacturer has become popular enough that it’s established
itself as the standard for recording a type of data <span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span>.</p>
</div>
<div id="defining-data-standards-for-a-research-group" class="section level3 hasAnchor" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Defining data standards for a research group<a href="experimental-data-recording.html#defining-data-standards-for-a-research-group" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If some of the data you record from your experiments comes from complex
equipment, like flow cytometers or mass spectrometers, you may be recording much
of that data in a standardized format without any extra effort, because that
format is the default output format for the equipment. However, you may have
more control over other data recorded from your experiments, including smaller,
less complex data that you record directly into a laboratory notebook or
spreadsheet. You can derive a number of benefits from defining and using a
standard for collecting these data, as well, which one paper describes as
the output of “traditional, low-throughput bench science” <span class="citation">(Wilkinson et al. 2016)</span>.</p>
<p>When recording this type of data, the data may be written down in an <em>ad hoc</em>
way—however the particular researcher doing the experiment thinks makes
sense—and that format might change with each experiment, even if many
experiments have similar data outputs. As a result, it becomes harder to create
standardized data processing and analysis scripts that work with this data or
that integrate it with more complex data types. Further, if everyone in a
laboratory sets up their spreadsheets for data recording in their own way, it is
much harder for one person in the group to look at data another person recorded
and immediately find what they need within the spreadsheet.</p>
<p>As a step in a better direction, the head of a research group may designate some
common formats (e.g., a spreadsheet template) that all researchers in the group
will use when recording the data from a specific type of experiments. One key
advantage to using standardized data formats even for recording simple,
“low-throughput” data is that everyone in the research group will be able to
understand and work with data recorded by anyone else in the group—data will
not become impenetrable once the person who recorded it leaves the group. Also,
once a group member is used to the format, the process of setting up to record
data from a new experiment will be quicker, as it won’t require the effort of
deciding and setting up a <em>de novo</em> format for a spreadsheet or other recording
file. Instead, a template file can be created that can be copied as a starting
point for any new data recording.</p>
<p>It also opens the possibility to create tools or scripts that read in and
analyze the data and that can be re-used across multiple experiments with minor
or no changes. This helps improve the efficiency and reproducibility of data
analysis, visualization, and reporting steps of the research project.</p>
<p>Developing these kinds of standards does require some extra time commitment
<span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span>. First, time is needed to design the format, and it does
take a while to develop a format that is inclusive enough that it includes a
place to put all data you might want to record for a certain type of experiment.
Second, it will take some time to teach each laboratory member what the format
is and how to make sure they comply with it when they record data.</p>
<p>On the flip side, the longer-term advantages of using a defined, structured
format will outweigh the short-term time investments for many laboratory groups
for frequently used data types. By creating and using a consistent structure to
record data of a certain type, members of a laboratory group can increase their
efficiency (since they do not need to re-design a data recording structure
repeatedly). They can also make it easier for downstream collaborators, like
biostatisticians and bioinformaticians, to work with their output, as those
collaborators can create tools and scripts that can be recycled across
experiments and research projects if they know the data will always come to them
in the same format. One paper suggests that the balance can be found, in terms of deciding whether
the benefits of developing a standard outweigh the costs, by considering how
often data of a certain type is generated and used:</p>
<blockquote>
<p>“To develop and deploy a standard creates an overhead, which can be expensive.
Standards will help only if a particular type of information has to be
exchanged often enough to pay off the development, implementation, and usage
of the standard during its lifespan.” <span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span></p>
</blockquote>
<p>These benefits are even more dramatic if data format standards
are created and used by a whole research field (e.g., if a standard data
recording format is always used for researchers conducting a certain type of
drug development experiment), because then the tools built at one institution
can be used at other insitutions. However, this level of field-wide coordination
can be hard to achieve, and so a more realistic immediate goal might be
formalizing data recording structures within your research group or department,
while keeping an eye out for formats that are gaining popularity as standards in
your field to adopt within your group.</p>
</div>
<div id="two-dimensional-structured-data-format" class="section level3 hasAnchor" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Two-dimensional structured data format<a href="experimental-data-recording.html#two-dimensional-structured-data-format" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>So far, this module has explored why you might want to use standardized
data formats for recording experimental data. The rest of the module
aims to give you tips for how to design and define your own standardized
data formats, if you decide that is worthwhile for certain data types
recorded within your research group.</p>
<p>Once you commit to creating a defined, structured format, you’ll need to decide
what that structure should be. There are many options here, and it’s very
tempting to use a format that is easy on human eyes
<span class="citation">(Buffalo 2015)</span>. For example, it may seem appealing to create a
format that could easily be copied and pasted into presentations and Word
documents and that will look nice in those presentation formats. To facilitate
this use, a laboratory might set up a recording format based on a spreadsheet
template that includes multiple tables of different data types on the same
sheet, or multi-level column headings.</p>
<p>Unfortunately, many of these characteristics—which make a format attractive to
human eyes—will make it harder for a computer to make sense of. For example,
if you include two tables in the same spreadsheet, it might make it easier for a
person to get a look at two small data tables without having to toggle to
different parts of the spreadsheet. However, if you want to read that data into
a statistical program (or work with a collaborator who would), it will likely
take some complex code to try to tell the computer how to find the second table
in the spreadsheet. The same applies if you include some blank lines at the top
of the spreadsheet, or use multi-level headers, or use “summary” rows at the
bottom of a table. Further, any information you’ve included with colors or with
text boxes in the spreadsheet will be lost when the data’s read into a
statistical program. These design elements make it much harder to read the data
embedded in a spreadsheet into other computer programs, including programs for
more complex data analysis and visualization, like R and Python.</p>
<p>As one article notes:</p>
<blockquote>
<p>“Data should be formatted in a way that facilitates computer readability. All
too often, we as humans record data in a way that maximizes its readability to
us, but takes a considerable amount of cleaning and tidying before it can be
processed by a computer. The more data (and metadata) that is computer readable,
the more we can leverage our computers to work with this data.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<p>One of the easiest format for a computer to read is a two-dimensional
“box” of data, where the first row of the spreadsheet gives the column names,
and where each row contains an equal number of entries. This type of
two-dimensional tabular structure forms the basis for several popular
“delimited” file formats that serve as a <em>lingua franca</em> across many simple
computer programs, like the comma-separated values (CSV) format, the
tab-delimited values (TSV) format, and the more general delimiter-separated
values (DSV) format, which are a common format for data exchange across
databases, spreadsheet programs, and statistical programs <span class="citation">(Janssens 2014; E. S. Raymond 2003; Buffalo 2015)</span>.</p>
<p>Any deviations from this two-dimensional “box” shape can crate problems
when a computer program tries to parse the data. For example, if
you have two tables in the same spreadsheet, with blank lines between them, the
computer will likely either think it’s read all the data after the first table,
and so not read in any data from the second table, or it will think the data
from both tables belong in a single table, with some rows of missing data in the
center. To write the code to read in data from two tables into two separate
datasets in a statistical program, it will be necessary to write some complex
code to tell the computer how to search out the start of the second table in the
spreadsheet.</p>
<p>Problems also come up if a spreadsheet uses multiple rows to create multi-level
column headers. In this case, anyone reading it into another program like R or
Python will need to either skip some of the rows of the column headers, and so
lose information in the original spreadsheet, or write complex code to parse the
column headers separately, then read in the later rows with data, and then stick
the two elements back together. Another thing that can cause problems is
“summary” rows at the end of a dataset (for example, the sums or means of all
values in a column). These will need to be trimmed off when the data is read
into other programs, since most of the analysis and visualization someone would
want to do in another program will calculate any summaries fresh, and will want
each row of a dataset to represent the same “type” and level of data (e.g., one
measurement from one animal).</p>
<p>For anything in a data format that requires extra coding when reading data into
another program, you are introducing a new opportunity for errors at the
interface between data recording and data analysis. If there are strong reasons
to use a format that requires these extra steps, it will still be possible to
create code to read in and parse the data in statistical programs, and if the
same format is consistently used, then scripts can be developed and thoroughly
tested to allow this. However, keep in mind that this will be an extra
burden on any data analysis collaborators who are using a program besides a
spreadsheet program. The extra time this will require could be large, since
this code should be vetted and tested thoroughly to ensure that the data
cleaning process is not introducing errors. By contrast, if the data is
recorded in a two-dimensional format with a single row of column names as
the first row, data analysts can likely read it quickly and cleanly into
other programs, with low risks of errors in the transfer of data from the
spreadsheet.</p>
</div>
<div id="levels-of-standardizationresearch-group-to-research-community" class="section level3 hasAnchor" number="2.2.4">
<h3><span class="header-section-number">2.2.4</span> Levels of standardization—research group to research community<a href="experimental-data-recording.html#levels-of-standardizationresearch-group-to-research-community" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Standards can operate both at the level of individual research groups and at the
level of the scientific community as a whole. The potential advantages of
community-level standards are big: they offer the chance to develop
common-purpose tools and code scripts for data analysis, as well as make it
easier to re-use and combine experimental data from previous research that is
posted in open data repositories. If a software tool can be reused, then more
time can be spent in developing and testing it, and as more people use it, bugs
and shortcomings can be identified and corrected. Community-wide standards can
lead to databases with data from different experiments, and from different
laboratory groups, structured in a way that makes it easy for other researchers
to understand each dataset, find pieces of data of interest within datasets, and
integrate different datasets <span class="citation">(Lynch 2008)</span>. Similarly, with community-wide
standards, it can become much easier for different research groups to
collaborate with each other or for a research group to use data generated by
equipment from different manufacturers <span class="citation">(Schadt et al. 2010)</span>. As
an article on interoperable bioscience data notes,</p>
<blockquote>
<p>“Without community-level harmonization and interoperability, many community
projects risk becoming data silos.” <span class="citation">(Sansone et al. 2012)</span></p>
</blockquote>
<p>However, there are important limitations to community-wide standards, as well.
It can be very difficult to impose such standards top-down and community-wide,
particularly for low-throughput data collection (e.g., laboratory bench
measurements), where research groups have long been in the habit of recording
data in spreadsheets in a format defined by individual researchers or research
groups. One paper highlights this point:</p>
<blockquote>
<p>“The data exchange formats PSI-MI and MAGE-ML have helped to get many of the
high-throughput data sets into the public domain. Nevertheless, from a bench
biologist’s point of view benefits from adopting standards are not yet
overwhelming. Most standardization efforts are still mainly an investment for
biologists.” <span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span></p>
</blockquote>
<p>Further, in some fields, community-wide standards have struggled to remain
stable, which can frustrate community members, as scripts and software must be
revamped to handle shifting formats <span class="citation">(Buffalo 2015; Barga et al. 2011)</span>. In some cases, a useful compromise is to follow a
general data recording format, rather than one that is very prescriptive. For
example, committing to recording data in a format that is “tidy” (which we
discuss extensively in the next module) may be much more flexible—and able to
meet the needs of a large range of experimental designs—than the use of a
common spreadsheet template or a more proscriptive standardized data format.</p>
<!-- ### Applied exercise -->

</div>
</div>
<div id="module3" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> The ‘tidy’ data format<a href="experimental-data-recording.html#module3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the previous module, we explained the benefits of saving data in a structured
format, and in particular using a two-dimensional format saved to a plain text
file when possible. In this section, we’ll talk about the “tidy” data format.
The tidy data format is one implementation of a tabular, two-dimensional
structured data format that has quickly gained popularity among statisticians
and data scientists since it was defined in a 2014 paper <span class="citation">(Wickham 2014)</span>.
These principles cover some basic rules for ordering the data, and even if you
haven’t heard the term <em>tidy data</em>, you may already be implementing many of its
standards in your own datasets. Datasets in this format tend to be very easily
to work with, including to further clean, model, and visualize the data, as well
as to integrate the data with other datasets. In particular, this data format is
compatible with a collection of open-source tools on the R platform called the
<em>tidyverse</em>. These characteristics mean that, if you are planning to use a
standardized data format for recording experimental data in your research group,
you may want to consider creating one that adheres to the tidy data format.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>List characteristics defining the “tidy” structured data format</li>
<li>Understand how to reformat a dataset to make it follow the “tidy” format</li>
<li>Explain the difference between the a structured data format (general concept)
and the “tidy’ data format (one popular implementation)</li>
<li>Understand benefits of recording data in a “tidy” format</li>
</ul>
<p>Adam Savage has built a career out of making things. He became famous as the
host of the TV show <em>Mythbusters</em>, where a crew builds contraptions to test
urban myths. For many years before that, he created models and special effects
for movies. He has thought a lot about how to effectively work in teams to make
things, and in 2019 he published a book about his life as a maker called <em>Every
Tool is a Hammer</em> <span class="citation">(Savage 2020)</span>.</p>
<p>Among many insights, Savage focuses on the importance
of tidying up as part of the creation process, saying “It’s time, when taken,
that you might feel is slowing you down in the moment, but in fact is saving you
time in the long run.” <span class="citation">(Savage 2020)</span> He introduces a new word for the
process of straightening up tools and materials—“knolling”. He borrowed the
term from an artist, Tom Sachs, whose rules for his own workshop include,
“Always Be Knolling”.</p>
<p>The idea of “knolling” includes a few key principles. First, only have what you
need out. Put everything else somewhere else. Removing any extras makes it
faster to find what you need when you need it. Second, for things you need, make
sure they’re out and available. “Drawers are where things go to die,” Savage
says, highlighting inefficiency when you have to look
for things that are hidden from site as you work. Finally, organize the things
that you have out. Put like things together, and arrange everything neatly,
aligning things in parallel or perpendicular patterns, rather than piling it
haphazardly.</p>
<p>Just as organizing tools and materials improves efficiency in a workshop,
organizing your data can dramatically improve the efficiency of data
preprocessing, analysis, and visualization. Indeed, “tidying up” your data
can give such dramatic improvements that a number of researchers have
developed systems and written papers that describe good organization schemes
to use to tidy up data (e.g., <span class="citation">(Wickham 2014)</span>).</p>
<p>The principles for tidying up data follow some of the principles for knolling.
For example, you want to make sure that you’re saving data in a file or
spreadsheet that only includes the data, removing any of the extras. Lab groups
will sometimes design spreadsheets for data collection that include a space for
recording data, but also space for notes, embedded calculations, and plots.
These extra elements can make it hard to extract and use the data itself. One
way to tidy up a dataset is to remove any of these extra elements. While you can
do this after you’ve collected your data, it’s more efficient to design a way to
record your data in the first place without extra elements in the file or
spreadsheet. You can further tidy up your data format by reformatting it to
follow the rules of a data format called the “tidy data” format.</p>
<p>We’ll start this module by describing rules a dataset format must follow for it
to be “tidy” and clarifying how you can set up your data recording to follow
these rules. In later parts of this module, we’ll talk more about why it’s
helpful to use a tidy data format, as well as a bit about the tidyverse tools
that you can use with data in this format.</p>
<div id="what-makes-data-tidy" class="section level3 hasAnchor" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> What makes data “tidy”?<a href="experimental-data-recording.html#what-makes-data-tidy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The “tidy” data format describes one way to structure tabular data. The name
follows from the focus of this data format and its associated set of tools—the
“tidyverse”—on preparing and cleaning (“tidying”) data, in contrast to sets of
tools more focused on other steps, like data analysis <span class="citation">(Wickham 2014)</span>. The
word “tidy” is not meant to apply that other formats are “dirty”, or that they
include data that is incorrect or subpar. In fact, the same set of datapoints
could be saved in a file in a way that is either “tidy” (in the sense of
<span class="citation">(Wickham 2014)</span>) or untidy, depending only on how the data are organized
across columns and rows.</p>
<p>Wickham notes in his article, where he first describes the tidy data format,
that his ideas about this format evolved from seeing many examples of
different ways that data could be organized within a two-dimensional structure.
He notes:</p>
<blockquote>
<p>“The development of tidy data has been driven by my experience from working
with real-world datasets. With few, if any, constraints on their organization,
such datasets are often constructed in bizarre ways. I have spent countless
hours struggling to get such datasets organized in a way that makes data
analysis possible, let alone easy.” <span class="citation">(Wickham 2014)</span></p>
</blockquote>
<p>To help you understand the tidy data format the Wickham developed, let’s start
with a checklist of rules that make a dataset tidy. Some of these are drawn
directly from the journal article that originally defined the data format
<span class="citation">(Wickham 2014)</span>. Other rules are based on common “untidy” patterns that show
up in data recording templates for laboratory research. The checklist is:</p>
<ul>
<li>Data are recorded in a tabular, two-dimensional format</li>
<li>The data collection file or spreadsheet does <strong>not</strong> include extra elements
like plots or embedded equations in the file</li>
<li>Each observation forms a row</li>
<li>Each variable forms a column</li>
<li>Column headers are variable names, not values</li>
<li>Each type of observational unit forms its own table</li>
<li>A single variable is not spread across multiple columns</li>
<li>Multiple variables are not stored in one column</li>
<li>Data types are consistent within a column</li>
<li>Files are named with a consistent naming convention</li>
<li>Column names are informative and follow a consistent naming convention</li>
<li>Missing values are noted in a consistent way (e.g., as “NA”)</li>
</ul>
<p>In previous modules, we’ve discussed the first two principles, highlighting how
important it is to separate data collection from further steps of data
processing and analysis. In this section of the module, we’ll go through other
items in this checklist, to help you understand what makes a dataset follow the
“tidy” data format. If so, you’ll be able to set up your data recording template
to follow this template, and you’ll be able to tell when you work with data that
others collect, if it is in this format and restructure it if not. In the next
part of this module, we’ll explain why it’s so useful to have your data in this
format.</p>
<p>Tidy data, first, must be in a tabular (i.e., two-dimensional, with columns and
rows, and with all rows and columns of the same length—nothing “ragged”),
without any “extras”, like embedded plots and calculations in a spreadsheet. If
you recorded data in a spreadsheet using a very basic strategy of saving a
single table per spreadsheet, with the first row giving the column names (as
described in the previous module), then your data will be in a tabular format.
In general, if your recorded data looks “boxy”, it’s probably in a
two-dimensional tabular format.</p>
<p>There are some additional criteria for the “tidy” data format, though, and so
not every structured, tabular dataset is in a “tidy” format. As Wickham notes
in his paper defining the format,</p>
<blockquote>
<p>“Most statistical datasets are rectangular tables made up of rows and columns
… [but] there are many ways to structure the same underlying data. …
Real datasets can, and often do, violate the three precepts of tidy data in
almost every way imaginable.” <span class="citation">(Wickham 2014)</span></p>
</blockquote>
<p>The first of these rules are that each row of a “tidy” dataset records the
values for a single observation, and that each column records values of a
variable: that is, characteristics or measurements of a certain type, in the
order of the observations given by the rows <span class="citation">(Wickham 2014)</span>.</p>
<p>To figure out if your data format follows these rules, it’s important to
determine the <em>unit of observation</em> of that data, which is the unit at which you
take measurements <span class="citation">(Sedgwick 2014)</span>. This idea is different than the <em>unit of
analysis</em>, which is the unit that you’re focusing on in your study hypotheses
and conclusions (this is sometimes also called the “sampling unit” or “unit of
investigation”) <span class="citation">(Altman and Bland 1997)</span>. In some cases, these two might be equivalent
(the same unit is both the unit of observation and the unit of measurement), but
often they are not <span class="citation">(Sedgwick 2014)</span>. Sedgwick notes:</p>
<blockquote>
<p>“The unit of observation and unit of analysis are often confused.
The unit of observation, sometimes referred to as the unit of
measurement, is defined statistically as the ‘who’ or ‘what’
for which data are measured or collected. The unit of analysis
is defined statistically as the ‘who’ or ‘what’ for which
information is analysed and conclusions are made.” <span class="citation">(Sedgwick 2014)</span></p>
</blockquote>
<p>As an example, say you are testing how the immune system of mice responds to a
certain drug over time. In this case, the <em>unit of analysis</em> might be the drug,
or a combination of drug and dose—ultimately, you may want to test something
like if one drug is more effective than another. To answer this research
question, you likely have several replicates of mice in each treatment group. If
a separate mouse (replicate) is used to collect each observation, and a mouse is
never measured twice (i.e., at different time points, or for a different
infection status), then the <em>unit of measurement</em>—the level at which each data
point is collected—is the mouse. This is because each mouse is providing a
single observation to help answer the larger research question.</p>
<p>As another example, say you conducted a trial on human subjects, to see how the
use of a certain treatment affects the speed of recovery, where each study
subject was measured at different time points. In this case, the unit of
observation is the combination of study subject and time point (while the unit
of analysis is the treatment). That means that Subject 1’s measurement at Time 1 would be one
observation, and the same person’s measurement at Time 2 would be a separate
observation. For a dataset to comply with the “tidy” data format, these two
observations would need to be recorded on separate lines in the data. If the
data instead had different columns to record each study subject’s measurements
at different time points, then the data would still be tabular, but it would not
be “tidy”.</p>
<p>Once you have divided your data into separate datasets based on the level of
observation, and structured each row to record data for a single observation
based on the unit of observation within that dataset, each column should be used
to measure a separate characteristic or measurement (a <em>variable</em>) for each
measurement <span class="citation">(Wickham 2014)</span>. A column could either give characteristics of
the data that were pre-defined by the study design—for example, the treatment
assigned to a mouse (a type of variable called a <em>fixed variable</em>, since its
value was fixed before the start of the experiment) or observed measurements,
like the level of infection measured in an animal (a type of variable called a
<em>measured variable</em>, since its value is determine through the experiment)
<span class="citation">(Wickham 2014)</span>.</p>
<p>In the example of human subjects measured at repeated time points, you may
initially find the “tidy” format unappealing, because it seems like it would
lead to a lot of repeated data. For example, if you wanted to record each study
subject’s sex, it seems like the “tidy” format would require you to repeat that
information in each separate line of data that’s used to record the measurements
for that subject for different time points. This isn’t the case—instead, with
a “tidy” data format, different “levels” of data observations should be recorded
in separate tables <span class="citation">(Wickham 2014)</span>. In other words, you should design a
separate table for each unit of observation if you have data at several of these
units for your experiment. For example, if you have some data on each study
subject that does not change across the time points of the study—like the
subject’s ID, sex, and age at enrollment—those form a separate dataset, one
where the unit of observation is the study subject, so there should be just one
row of data per study subject in that data table, while the measurements for
each time point should be recorded in a separate data table. A unique
identifier, like a subject ID, should be recorded in each data table so it can
be used to link the data in the two tables. If you are using a spreadsheet to
record data, this would mean that the data for these separate levels of
observation should be recorded in separate sheets, and not on the same sheet of
a spreadsheet file. Once you read the data into a scripting language like R or
Python, it will be easy to link the larger and smaller “tidy” datasets as needed
for analysis, visualizations, and reports.</p>
</div>
<div id="why-make-your-data-tidy" class="section level3 hasAnchor" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Why make your data “tidy”?<a href="experimental-data-recording.html#why-make-your-data-tidy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This may all seem like a lot of extra work, to make a dataset “tidy”, and why
bother if you already have it in a structured, tabular format? It turns out
that, once you get the hang of what gives data a “tidy” format, it’s pretty
simple to design recording formats that comply with these rules. What’s more,
when data is in a “tidy” format, it can be directly input into a collection
of tools in R that belong to something called the “tidyverse”.</p>
<p>R’s <em>tidyverse</em> framework enables powerful and user-friendly data management,
processing, and analysis by combining simple tools to solve complex, multi-step
problems <span class="citation">(Ross, Wickham, and Robinson 2017; Silge and Robinson 2016; Wickham 2016; Wickham and Grolemund 2016)</span>. Since the <em>tidyverse</em> tools are simple and share a common
interface, they are easier to learn, use, and combine than tools created in the
traditional base R framework <span class="citation">(Ross, Wickham, and Robinson 2017; Lowndes et al. 2017; McNamara 2016)</span>. This <em>tidyverse</em> framework is quickly becoming the standard
taught in introductory R courses and books <span class="citation">(Hicks and Irizarry 2017; B. Baumer 2015; Kaplan 2018; Stander and Dalla Valle 2017; McNamara 2016)</span>, ensuring ample training resources for researchers new to
programming, including books (e.g., <span class="citation">(B. S. Baumer, Kaplan, and Horton 2017; Irizarry and Love 2016; Wickham and Grolemund 2016)</span>), massive open online courses (MOOCs), on-site university courses
<span class="citation">(B. Baumer 2015; Kaplan 2018; Stander and Dalla Valle 2017)</span>, and Software
Carpentry workshops <span class="citation">(Wilson 2014; Pawlik et al. 2017)</span>. Further, tools
that extend the <em>tidyverse</em> have been created to enable high-quality data
analysis and visualization in several domains, including text mining
<span class="citation">(Silge and Robinson 2017)</span>, microbiome studies <span class="citation">(McMurdie and Holmes 2013)</span>, natural language
processing <span class="citation">(Arnold 2017)</span>, network analysis <span class="citation">(Tyner, Briatte, and Hofmann 2017)</span>, ecology
<span class="citation">(Hsieh, Ma, and Chao 2016)</span>, and genomics <span class="citation">(Yin, Cook, and Lawrence 2012)</span>.</p>
<p>This collection of tools is very straightforward to use and so powerful that
it’s well worth making an effort to record data in a format that works directly
with the tools, if possible. Outside of cases of very complex or very large
data, it should be possible. As Jeff Leek notes in a blog post on tidy data
analysis,</p>
<blockquote>
<p>“Tidy data is great for a huge fraction of data analyses you might
be interested in. It makes organizing, developing, and sharing data a lot
easier. It’s how I recommend most people share data.” <span class="citation">(Leek 2012)</span></p>
</blockquote>
<p>The “tidyverse” collection of tools united by a common philosophy: very complex
things can be done simply and efficiently with small, sharp tools that share a
common interface. Zev Ross, in an article about tidy tools and how they can
declutter a workflow, notes:</p>
<blockquote>
<p>“The philosophy of the tidyverse is similar to
and inspired by the “unix philosophy”, a set of loose principles that ensure
most command line tools play well together. … Each function should solve one
small and well-defined class of problems. To solve more complex problems, you
combine simple pieces in a standard way.” <span class="citation">(Ross, Wickham, and Robinson 2017)</span></p>
</blockquote>
<p>The tidyverse isn’t the only popular system that follows this
philosophy—one other favorite is Legos. Legos are small, plastic bricks, with
small studs on top and tubes for the studs to fit into on the bottom. The studs
all have the same, standardized size and are all spaced the same distance apart.
Therefore, the bricks can be joined together in any combination, since each
brick uses the same <em>input format</em> (studs of the standard size and spaced at the
standard distance fit into the tubes on the bottom of the brick) and the same
<em>output format</em> (again, studs of the standard size and spaced at the standard
distance at the top of the brick). Because of this design, bricks can be joined
regardless of whether the bricks are different colors or different heights or
different widths or depths. With Legos, even though each “tool” (brick) is very
simple, the tools can be combined in infinite variations to create very complex
structures.</p>
<p>The tools in the “tidyverse” operate on a similar principle. They all input a
tidy dataset (or a column from a tidy dataset) and they (almost) all output data
in the same format they input it. For most of the tools, their required format
for input and output is the “tidy data” format <span class="citation">(Wickham 2014)</span>, called a tidy
<em>dataframe</em> in R—this is a dataframe that follows the rules detailed earlier
in this section.</p>
<p>This common input / output interface, and the use of small tools that follow
this interface and can be combined in various ways, is what makes the tidyverse
tools so powerful. However, there are other good things about the tidyverse that
make it so popular. One is that it’s fairly easy to learn to use the tools, in
comparison to learning how to write code for other R tools <span class="citation">(D. Robinson 2017; R. Peng 2018)</span>. The developers who have created the tidyverse tools have
taken a lot of effort to try to make sure that they have a clear and consistent
<em>user interface</em> <span class="citation">(Wickham 2017; Bryan and Wickham 2017)</span>. Wickham highlights how this
standardization makes an approach focused on tidy data so powerful:</p>
<blockquote>
<p>“A standard makes initial data cleaning easier because you do not need to
start from scratch and reinvent the wheel every time. The tidy data standard has
been designed to facilitate initial exploration and analysis of the data, and to
simplify the development of data analysis tools that work well together.”
<span class="citation">(Wickham 2014)</span></p>
</blockquote>
<p>To help understand a user interface, and how having a consistent user interface
across tools is useful, let’s think about a different example—cars. When you
drive a car, you get the car to do what you want through the steering wheel, the
gas pedal, the break pedal, and different knobs and buttons on the dashboard.
When the car needs to give you feedback, it uses different gauges on the
dashboard, like the speedometer, as well as warning lights and sounds.
Collectively, these ways of interacting with your car make up the car’s <em>user
interface</em>. In the same way, each function in a programming language has a
collection of parameters you can set, which let you customize the way the
function runs, as well as a way of providing you output once the function has
finished running and the way to provide any messages or warnings about the
function’s run. For functions, the software developer can usually choose design
elements for the function’s user interface, including which parameters to
include for the function, what to name those parameters, and how to provide
feedback to the user through messages, warnings, and the final output.</p>
<p>If a collection of tools is similar in its user interfaces, it will make it
easier for users to learn and use any of the tools in that collection once
they’ve learned how to use one. For cars, this explains how the rental car
business is able to succeed. Even though different car models are very different
in many characteristics—their engines, their colors, their software—they are
very consistent in their user interfaces. Once you’ve learned how to drive one
car, when you get in a new car, the gas pedal, brake, and steering wheel are
almost guaranteed to be in about the same place and to operate about the same
way as in the car you learned to drive in. The exceptions are rare enough to be
memorable—think how many movies have a laughline from a character trying to
drive a car with the driver side on the opposite side of what they’re used to.</p>
<p>The tidyverse tools are similarly designed so that they all have a very similar
user interface. For example, many of the tidyverse functions use a parameter
named “.data” to refer to the input data. Similarly, parameters
named “.vars” and “.funs” are repeatedly used over tidyverse functions, with the
same meaning in each case. What’s more, the tidyverse functions are typically given names
that very clearly describe the action that the function does, like <code>filter</code>,
<code>summarize</code>, <code>mutate</code>, and <code>group</code>. As a result, the final code is very clear
and can almost be “read” as a natural language, rather than code. As Jenny
Bryan notes, in an article on data science:</p>
<blockquote>
<p>“The Tidyverse
philosophy is to rigorously (and ruthlessly) identify and obey common
conventions. This applies to the objects passed from one function to another
and to the user interface each function presents. Taken in isolation, each
instance of this seems small and unimportant. But collectively, it creates
a cohesive system: having learned one component you are more likely to be
able to guess how another different component works.”
<span class="citation">(Bryan and Wickham 2017)</span></p>
</blockquote>
<p>As a result, the tidyverse collection of tools is pretty easy to learn, compared
to other sets of functions in scripting languages, and pretty easy to expand
your knowledge of once you know some of its functions. Wickham notes:</p>
<blockquote>
<p>“The goal of [the tidy tools] principles is to provide a uniform interface so
that tidyverse packages work together naturally, and once you’ve mastered one,
you have a head start on mastering the others.” <span class="citation">(<strong>wickhem2017tidy?</strong>)</span></p>
</blockquote>
<p>Many people who teach
R programming now focus on first teaching the tidyverse, given these
characteristics <span class="citation">(D. Robinson 2017; R. Peng 2018)</span>, and it’s often a
first focus for online courses and workshops on R programming. Since its main
data structure is the “tidy data” structure, it’s often well worth recording
data in this format so that all these tools can easily be used to explore and
model the data.</p>
</div>
<div id="using-tidyverse-tools-with-data-in-the-tidy-data-format" class="section level3 hasAnchor" number="2.3.3">
<h3><span class="header-section-number">2.3.3</span> Using tidyverse tools with data in the “tidy data” format<a href="experimental-data-recording.html#using-tidyverse-tools-with-data-in-the-tidy-data-format" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The tidyverse includes tools for many of the tasks you might need to
do while managing and working with experimental data. When you download
R, you get what’s called <em>base R</em>. This includes the main code that drives
anything you do in R, as well as functions for doing many core tasks.
However, the power of R is that, in addition to base R, you can also add
onto R through what are called <em>packages</em> (sometimes also referred to
as <em>extensions</em> or <em>libraries</em>). These are kind of like “booster packs”
that add on new functions for R. They can be created and contributed
by anyone, and many are collected through a few key repositories like
CRAN and Bioconductor.</p>
<p>All the tidyverse tools are included in R extension packages, rather than base
R, so once you download R, you’ll need to download these packages as well to use
the tidyverse tools. The core tidyverse functions include functions to read in
data (the <code>readr</code> package for reading in plain text, delimited files, <code>readxl</code>
to read in data from Excel spreadsheets), clean or summarize the data (the
<code>dplyr</code> package, which includes functions to merge different datasets, make
new columns as functions of old ones, and summarize columns in the data, either
as a whole or by group), and reformat the data if needed to get it in a tidy
format (the <code>tidyr</code> package). The tidyverse also includes more precise tools,
including tools to parse dates and times (<code>lubridate</code>) and tools to work with
character strings, including using regular expressions as a powerful way to find
and use certain patterns in strings (<code>stringr</code>). Finally, the tidyverse
includes powerful functions for visualizing data, based around the <code>ggplot2</code>
package, which implements a “grammar of graphics” within R.</p>
<p>You can install and load any of these tidyverse packages one-by-one using the
<code>install.packages</code> and <code>library</code> functions with the package name from within R.
If you are planning on using many of the tidyverse packages, you can also
install and load many of the tidyverse functions by installing a package called
“tidyverse”, which serves as an umbrella for many of the tidyverse packages.</p>
<p>In addition to the original tools in the tidyverse, many people have developed
<em>tidyverse</em> extensions—R packages that build off the tools and principles in
the tidyverse. These often bring the tidyverse conventions into tools for
specific areas of science. For example, the <code>tidytext</code> package provides tools to
analyze large datasets of text, including books or collections of tweets, using
the tidy data format and tidyverse-style tools. Similar tidyverse extensions
exist for working with network data (<code>tidygraph</code>) or geospatial data (<code>sf</code>).
Extensions also exist for the visualization branch of the tidyverse
specifically. These include <em>ggplot extensions</em> that allow users to create
things like calendar plots (<code>sugrrants</code>), gene arrow maps (<code>gggene</code>), network
plots (<code>igraph</code>), phytogenetic trees (<code>ggtree</code>) and anatogram images
(<code>gganatogram</code>). These extensions all allow users to work with data that’s in a
“tidy data” format, and they all provide similar user interfaces, making it
easier to learn a large set of tools to do a range of data analysis and
visualization, compared to if the set of tools lacked this coherence.</p>
<!-- ### Practice quiz -->
<!-- Question 1: What is a fundamental characteristic of tidy data?  -->
<!-- a) All data are stored in a single column  -->
<!-- b) Each variable is stored in a separate column [Correct answer]  -->
<!-- c) Observations are mixed within a single column  -->
<!-- d) Redundant information is encouraged for completeness  -->
<!-- Question 2: In tidy data, how should missing values be handled?  -->
<!-- a) Replace them with zeros for consistency  -->
<!-- b) Remove all observations with missing values; tidy data cannot have missing values  -->
<!-- c) Keep them as-is, with no special treatment  -->
<!-- d) Use a consistent representation (e.g., “NA”, or consistently leave them as an empty cell)  -->
<!-- Question 3: Which of the following is the only statement that does not describe one of the advantages of using a tidy data format?  -->
<!-- a) This data format is commonly taught in beginning data analysis courses, and so there are numerous resources to help learn to use it.  -->
<!-- b) This format will always make the data easy to read and interpret when printed out in a paper document [Correct answer]  -->
<!-- c) Many functions and packages in the R programming environment are available to work with data stored in this format  -->
<!-- d) This format is appropriate for chaining together simple commands to create a more complex pipeline to analyze and process data  -->
<!-- Question 4:  -->
<!-- In a data collection template, which of the following are considered extra elements and should be removed to help make the template meet the standards for “tidy” data? Select all that apply.  -->
<!-- a) Embedded formulas that make calculations between cells [Should be selected]  -->
<!-- b) An area to record the observed data from the experiment  -->
<!-- c) Plots [Should be selected]  -->
<!-- Question 5: Which is not one of the guidelines for the tidy data format?  -->
<!-- a) A single variable is not spread across multiple columns  -->
<!-- b) Each observation forms a row  -->
<!-- c) All data from an experiment are included in the same table, even if data are collected at different units of observation [Correct answer]  -->
<!-- d) Each variable forms a column  -->
<!-- Question 6: What is a tabular data format?  -->
<!-- a) One in which the data are hierarchical, with different numbers of observations for each study subject  -->
<!-- b) One in which the data form a matrix, with the same number of rows and columns and the same type of data in each table cell  -->
<!-- c) One in which the data are arranged in a rectangular, two-dimensional format, with rows and columns [Correct answer]  -->
<!-- d) One in which the data adhere to all rules of the tidy data format  -->
<!-- Question 7: You are conducting a study that compares a drug to a placebo. You take measurements on human subjects over time, recording measurements at five timepoints for each study subject. Which of the following is true of this dataset?  -->
<!-- a) The unit of observation is the combination of study subject and timepoint, while the unit of analysis is the treatment (drug or placebo) [Correct answer]  -->
<!-- b) The unit of observation is the study subject, while the unit of analysis is the treatment (drug or placebo)  -->
<!-- c) The unit of observation is the timepoint, while the unit of analysis is the human subject  -->
<!-- d) The unit of observation and the unit of analysis are both the treatment (drug or placebo)  -->
<!-- Discussion questions -->
<!-- What are your main considerations when you decide how to record your data?   -->
<!-- Based on the reading, can you define the tidy data format? Were you familiar with this format before preparing for this discussion? Do you use some of these principles when recording your own data?  -->
<!-- Describe advantages, as well as potential limitations, of storing data in a tidy data format  -->
<!-- In data that you have collected, can you think of examples when the data collection format included extra elements, beyond simply space for recording the data? Examples might include plots, calculations, notes, and highlighting. What were some of the advantages of having these extra elements in the template? Based on the reading or your own experience, what are some disadvantages to including these extra elements in a data collection template?  -->
<!-- In research collaborations, have you experienced a case where the data format for one researcher created difficulties for the other?  -->

</div>
</div>
<div id="module4" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Designing templates for “tidy” data collection<a href="experimental-data-recording.html#module4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This module will move from the principles of the “tidy” data format to the
practical details of designing a “tidy” data format to use when collecting
experimental data. We will describe common issues that prevent biomedical
research datasets from being “tidy” and show how these issues can be avoided. We
will also provide rubrics and a checklist to help determine if a data collection
template complies with a “tidy” format.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Identify characteristics that keep a dataset from being “tidy”</li>
<li>Convert data from an “untidy” to a “tidy” format</li>
</ul>
<p>In this module, we will use a real example of data collected in a biomedical
laboratory. We’ll use this example to show how data is often collected in a way
that is not “tidy” (module 2.3), focusing on the features of data collection
that make it “untidy”. We’ll then describe some general principles for why and
how to instead create and use tidy (or at least tidier) templates to collect
data in the laboratory. We’ll also show how this can be the first step in a
pipeline to creating useful, attractive, and reproducible reports that describe
the data you collected. This module will focus on the principles of templates
for tidy data collection, while in the next module we’ll dig deeper into the
details of making this conversion for the example dataset that we use as a
demonstration in this module.</p>
<div id="exampledata-on-rate-of-bacterial-growth" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Example—Data on rate of bacterial growth<a href="experimental-data-recording.html#exampledata-on-rate-of-bacterial-growth" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Throughout this module, we’ll use a real dataset to illustrate principles of
data collection in a biomedical laboratory. First, let’s start by looking at the
original data collection template, and use this to walk through some details of
this dataset.</p>
<p>Figure <a href="experimental-data-recording.html#fig:growthexcel1">2.1</a> provides an annotated view of the data set, showing
the format used when the data were originally collected:</p>
<div class="figure"><span style="display:block;" id="fig:growthexcel1"></span>
<img src="figures/growth_curve_example.png" alt="Example of an Excel spreadsheet used to record and analyze data for a laboratory experiment. Annotations highlight where data is entered by hand, where calculations are done by hand, and where embedded Excel formulas are used. The figures are created automatically using values in a specified column." width="\textwidth" />
<p class="caption">
Figure 2.1: Example of an Excel spreadsheet used to record and analyze data for a laboratory experiment. Annotations highlight where data is entered by hand, where calculations are done by hand, and where embedded Excel formulas are used. The figures are created automatically using values in a specified column.
</p>
</div>
<p>These data were collected to measure the compare growth yield and doubling time
of <em>Mycobacterium tuberculosis</em> (the bacteria that causes tuberculosis in
humans) under two conditions—high oxygen and low oxygen. In humans, <em>M.
tuberculosis</em> can persist for years or decades in granulomas, and the centers of
these granulomas are often hypoxic (low in oxygen). Therefore, it’s important to
understand how these bacteria grow in hypoxic conditions.</p>
<p>To conduct this experiment, the researchers used test tubes that were capped
with sealed caps to prevent and air exchange between the contents of the tube
and the environment. Inside the tubes, the amount of oxygen was controlled
by shifting the ratio of the volume of the culture (the liquid with nutrients
in which the M. tuberculosis will grow) versus the volume of air.
In the high oxygen condition, a lower volume of culture was used, which leaves
room for a lot of air in the top of the tube. In the low oxygen condition,
the tube was filled almost to the top with culture, which left very little air
at the top of the tube.</p>
<p>Once the tubes were filled and capped, they were left to grow for about a week.
During this time, the researchers took several measurements to determine the
growth of the bacteria in each tube. To do this, they used a spectrophotometer
to track increases in optical density over time. This
method gives a measurement that is directly proportional
to the cell mass in each tube, and so provides a measure of how much the
bacteria has grown since the start of the experiment.</p>
<p>To record data from this experiment, researchers used the spreadsheet shown in
Figure <a href="experimental-data-recording.html#fig:growthexcel1">2.1</a>. This spreadsheet is an example of a data
collection template—it was created not only for this experiment, but also for
other experiments that this research group conducts to measure bacterial growth
under different conditions. It was designed to allow a researcher working in the
laboratory to record measurements over the course of the experiment.</p>
<p>Let’s take a closer look at some of the features of this spreadsheet. First, it
has a section on the top right that focuses on data collection during the
experiment, with one row for each time when the tubes were measured for the cell
mass. This section of the spreadsheet starts with several
columns related to the time of each measurement, including the clock time at
measurement (column A), the difference in time (hours) between each time point
in which data were collected (column B), the date on which data were gathered
(column C), and the time in hours for each data point from the start of the
study for graphing purposes (column D). The columns for clock time (A) and date
(C) were recorded by hand, while the columns for time since the start of the
experiment (B and D) were calculated or converted by hand from these values and
then entered in the column. The remaining columns (E–I) provide data on the
optical density (absorbance at 600 nm), which is directly proportional to cell
mass in the tube. There is one column per test tub, and each of these column
labels includes a test tube ID (A1, A3, L1, L2, L3). If a tube ID starts with
“A”, it was grown in high oxygen conditions, and if it starts with “L”, it was
grown in low oxygen conditions.</p>
<p>Next, the spreadsheet has areas that provide summaries of the data, calculated
using embedded formulas or through the spreadsheet’s plotting functions. For
example, rows 17–18 provide calculations of the doubling time of the bacteria
in each tube for two periods (early and late in the experiment), while two
growth curves are plotted at the bottom of the spreadsheet.</p>
<p>Finally, the spreadsheet includes a couple of other features, including some
written notes about one of the hand calculations and a macro in the top right
that can be used by the researcher to calculate the amount of the initial
inoculum to add to each tube at the start of the experiment.</p>
<p>What the researchers found appealing about the format of this spreadsheet was
the ease with which the researcher collecting data in the laboratory could
accomplish the study goals. They also cited transparency of the raw data and
ease with which additional sampling data points could be added. The data being
graphed in real time, and the inclusion of a simple macro to calculate doubling
time, allowed the research in the laboratory to see tangible differences between
the two assay conditions as data were collected over the one-week experiment.</p>
<p>However, many of these features can have undesired consequences. They can increase
the chance of errors in recording the data and in calculating summaries based on the
data. They also make it hard to move the data into a reproducible pipeline, and
so limit opportunities for more sophisticated analysis and visualization. In the
next section of this module, we’ll highlight features of data collection templates
like this one that can make data collection untidy. In the next module,
we’ll discuss how you could create a new data collection template for this example
data that would be tidier, and use this to open a more general discussion of
principles of tidy data collection templates.</p>
</div>
<div id="features-that-make-data-collection-templates-untidy" class="section level3 hasAnchor" number="2.4.2">
<h3><span class="header-section-number">2.4.2</span> Features that make data collection templates untidy<a href="experimental-data-recording.html#features-that-make-data-collection-templates-untidy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are several features of the data collection template shown in Figure
<a href="experimental-data-recording.html#fig:growthexcel1">2.1</a> that make it untidy. These will make it difficult read
the data into a statistical program like R or Python to conduct data analysis
and visualization. There are also some features that make it prone to errors in
data collection and analysis.</p>
<p>First, these data will be hard to read into a statistical program because the
raw data form only part of the spreadsheet (Figure <a href="experimental-data-recording.html#fig:extractraw">2.2</a>, area
highlighted by the blue box). The “extra” elements on the spreadsheet, which
include the output from calculations, plots, macros, and notes, make it harder
to isolate the raw data from the file when using a statistical program.</p>
<div class="figure"><span style="display:block;" id="fig:extractraw"></span>
<img src="figures/growth_curve_raw_data.png" alt="Isolating raw data collected in a template from extra elements. The box in this figure highlights the area of the spreadsheet where data are collected. All other elements of the spreadsheet focus on other aims (e.g., summarizing these data, adding notes, macros for experimental design). Those other elements make it difficult to extract the raw data for more advanced analysis and visualization through a statistical program like R, Python, or Perl." width="\textwidth" />
<p class="caption">
Figure 2.2: Isolating raw data collected in a template from extra elements. The box in this figure highlights the area of the spreadsheet where data are collected. All other elements of the spreadsheet focus on other aims (e.g., summarizing these data, adding notes, macros for experimental design). Those other elements make it difficult to extract the raw data for more advanced analysis and visualization through a statistical program like R, Python, or Perl.
</p>
</div>
<p>While these extra elements make it hard to extract the raw data, it isn’t
impossible. Programming languages like R include functions to read data in from
a spreadsheet, and these functions often provide options to specify the sheet of
the file to read in, as well as the rows and columns to read from a specific
sheet. In the example spreadsheet in Figure <a href="experimental-data-recording.html#fig:extractraw">2.2</a>, for example,
you could specify to read in only rows 1–15 of columns A–I, to focus on the
raw data. However, one goal of reproducible research is to create tools and
pipelines that are robust—that is, ones that still work as desired when the
raw data is changed in small ways, or even across different raw data files.</p>
<p>Therefore, while we could customize code to read in data from a specific part of
a complex spreadsheet, like that shown in Figure <a href="experimental-data-recording.html#fig:extractraw">2.2</a>, this
customization would make the code less robust. If we asked the statistical
program to read in rows 1–15 of columns A–I, for example, the code would
perform incorrectly if we later added one more time point to the experiment, or
if we tried to use the same template for an experiment that used more test
tubes. If we instead use a template that only records the raw data, without
additional elements, then we can create more robust tools, since we can write
code to read in whatever is in a spreadsheet, rather than restricting to certain
rows and columns.</p>
<p>Next, the example template helps demonstrate how specific ways of recording data
can make the template less tidy. First, let’s look at how the template records
the time of each measurement. It does this using four separate columns
(Figure <a href="experimental-data-recording.html#fig:extractraw">2.2</a>). In column C, the researcher records the date a
measurement was taken, and in Column A he or she records the clock time of the
measurement. The experiment was started, for example, at 12:00 PM (“12:00” in column A)
on July 9 (“9-Jul” in column C). These values are entered by hand by the researcher.
Next, these values are used to calculate, for each measurement, how long it had been
since the start of the experiment. This value is recorded in two separate ways—as
hours and minutes in column B and converted into hours and percents of hours (using
decimals) in column D. For example, the second measurement was taken at 4:05 PM
on July 9 (“16:05” in column A and “9-Jul” in column C), which is 4 hours and 5 minutes
after the start of the experiment (“4hr 5min” in column B) or, since 5 minutes is about
8% of an hour, 4.08 hours after the start of the experiment (“4.08” in column D).</p>
<div class="figure"><span style="display:block;" id="fig:timemeasures"></span>
<img src="figures/growth_curve_time_measures.png" alt="Measurements of time in the example data collection template. The four highlighted columns (columns A, B, C, and D) are all used in this spreadsheet to record time. The methods of recording time in this template, however, may make it more likely to create errors in data recording and collection and will make it harder to use the data in a reproducible pipeline." width="\textwidth" />
<p class="caption">
Figure 2.3: Measurements of time in the example data collection template. The four highlighted columns (columns A, B, C, and D) are all used in this spreadsheet to record time. The methods of recording time in this template, however, may make it more likely to create errors in data recording and collection and will make it harder to use the data in a reproducible pipeline.
</p>
</div>
<p>There are a few things that could be changed about how the time data are
recorded here that could make this data collection template tidier. First, it
would be better to focus only on recording the raw data, rather than adding
calculations based on that data. Columns B and D in Figure <a href="experimental-data-recording.html#fig:extractraw">2.2</a>
are both the output from calculations. Anytime a spreadsheet includes a
calculation, it creates the room for mistakes in data collection and analysis.
Often, calculations in a spreadsheet will be done using embedded formulas. These
can cause problems anytime new columns or rows are added to the data, as that
can shift the cells meant to be used in the calculation. Further, these formulas
are embedded in the spreadsheet, where they can’t be seen and checked very
easily, which makes it easy to miss a typo or other error in the formula. In the
example in Figure <a href="experimental-data-recording.html#fig:extractraw">2.2</a>, columns B and D aren’t calculated by
embedded formulas, but rather calculated by the researcher by hand and then
entered. This can create the room for user error with each calculation and each
data entry. Later, we’ll see how we can tidy this data collection template by
removing columns that calculate time (columns B and D) and instead doing that
calculation once the raw data are read into a statistical program.</p>
<p>The second thing that could be changed is how the template records the date and time
of the measurement. Currently, it uses two columns (A and C) to record this information.
However, each piece of information is useless without the other—instead, they must
be known jointly to do things like calculate the time since the start of the
experiment. It would therefore be tidier to record this information in a single column.
For example, instead of recording the starting time of the experiment as “12:00” in
column A and “9-Jul” in column C, you could record it as “July 9, 2019 12:00” in a
single date-time column. In this example, adding the year (“2019”) to the date will
also make this data point easier to work with in a programming language, as these often
have special functions to work with data in date-time classes, but all elements of the
date and/or time must be included to convert data points into these useful classes.</p>
<p>Next, let’s look at how the template collects data related to cell growth in each tube
(columns E–I, Figure <a href="experimental-data-recording.html#fig:growthmeasures">2.4</a>).</p>
<div class="figure"><span style="display:block;" id="fig:growthmeasures"></span>
<img src="figures/growth_curve_growth_measures.png" alt="Measurements of bacterial growth in the example data collection template. The five highlighted columns (columns E--I) are all used in this spreadsheet to record optical density in each test tube at each measurement time." width="\textwidth" />
<p class="caption">
Figure 2.4: Measurements of bacterial growth in the example data collection template. The five highlighted columns (columns E–I) are all used in this spreadsheet to record optical density in each test tube at each measurement time.
</p>
</div>
<p>These data are recorded in a format that will work pretty well. Strictly
speaking, they aren’t fully tidy (module 2.3), since the column headers include
information that we might want to use as variables in analysis and
visualization. Specifically, each test tube’s ID is incorporated in the column
name where measurements for that tube are recorded, since each test tube is
recorded using a separate column. If we want to run analysis where we estimate
values for each test tube, or create plots where each test tube’s measurements
are shown with a separate line, then we’ll need to convert the format of the
data a bit. However, that’s quite easy to do in more statistical programming
languages now, and so it’s reasonable to compromise on this element of
“tidiness” in the data collection format. As we’ll show in the next module,
changing this layout in the original data collection would require the
researcher to re-type the measurement date and time several times and would
result in the spreadsheet being longer, and so harder to see at once when
recording data. We’ll discuss this balance in designing data collection
templates more in a bit.</p>
<p>There is a final element we’d like to highlight on this example template that
could make the data hard to integrate into a reproducible pipeline. There are
cases in the example template where either column names or cell values are
formatted in a way that would be hard to work with when the data is read into a
more advanced program like R or Python (Figure <a href="experimental-data-recording.html#fig:growthformatting">2.5</a>). For
example, the column names include spaces and parentheses (e.g., “Time (clock)”).
If left as-is, when the data are read into another program, the column names
will need to be cleaned up to take these characters out, so that the column
names are composed only of alphabetical characters, numbers, or underscores.
While this can be done in code like R or Python, it will add to the data
cleaning process and could be avoided by using simpler column names in the
original data collection template.</p>
<div class="figure"><span style="display:block;" id="fig:growthformatting"></span>
<img src="figures/growth_curve_formatting.png" alt="Examples of special characters and formatting in the example template that could cause problems later in a data analysis pipeline." width="\textwidth" />
<p class="caption">
Figure 2.5: Examples of special characters and formatting in the example template that could cause problems later in a data analysis pipeline.
</p>
</div>
</div>
<div id="converting-to-a-tidier-format-for-data-collection-templates" class="section level3 hasAnchor" number="2.4.3">
<h3><span class="header-section-number">2.4.3</span> Converting to a “tidier” format for data collection templates<a href="experimental-data-recording.html#converting-to-a-tidier-format-for-data-collection-templates" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now that we’ve looked at characteristics that can make a data collection
template untidy, let’s go through some principles for creating tidy templates to
record the same data. There are three basic principles for designing tidy
templates that will go a long way to creating ways to collect data in a research
group that can be easily used within a reproducible analysis pipeline.</p>
<p>The first principle in designing a tidier template for collecting laboratory
data is to <strong>limit the template to the collection of data</strong>. The key here is the
word “collection”. A tidy template will avoid any calculations done on the
original data and instead focus only on the initial data that the researcher
records for the experiment. This means that you should exclude from the template
any element that provides a calculation, summary, or plot based on the initial
recorded element. You should also exclude any special formatting that you are
using to encode information. For example, say that you are collecting data, and
in some cases you get a warning that the reading may be below the instrument’s
detection limit. It may be tempting to highlight the cells with measurements
where this warning was displayed as you record the data. However, you should
avoid doing this, as any color or other formatting information will be lost when
you read the data in the file into a statistical program. Instead, you could add
a second column to indicate if the measurement included a warning.</p>
<p>The second principle is to <strong>make sensible choices when dividing data collection
into rows and columns</strong>. There are many different ways that you could spread the
data collection into rows and columns. One decision is how (and whether) to
divide recorded information across columns. Figure <a href="experimental-data-recording.html#fig:arrangingcolumns">2.6</a>,
for example, shows several ways that you could divide data on a date and time
into one or more columns. In this example, it typically makes the most sense
to use a single column to record all the date and time elements (the top example
in Figure <a href="experimental-data-recording.html#fig:arrangingcolumns">2.6</a>). Most statistical programs have powerful
functions for parsing dates and times, after which they store these data in special
classes that allow time-related operations (for example, calculating the time difference
between two date-time measurements). It will be most efficient to record all date and
time elements in a single column.</p>
<div class="figure"><span style="display:block;" id="fig:arrangingcolumns"></span>
<img src="figures/arranging_columns.png" alt="Examples of special characters and formatting in the example template that could cause problems later in a data analysis pipeline." width="\textwidth" />
<p class="caption">
Figure 2.6: Examples of special characters and formatting in the example template that could cause problems later in a data analysis pipeline.
</p>
</div>
<p>Conversely if you have complex data with different elements
(for example, height in components of inches and feet), it may make
sense to use separate columns for each of the components. For
example, rather than using one column to record <code>5'7"</code>, you
could divide the information into one column with the component
that is in feet (<code>5</code>) and one with the component in inches (<code>7</code>).
In the first case, when you read the data into a program like
R you would need to use complex code to split the value into
its parts to be able to use it. In the second case, you could
easy work with the values in the two separate columns to calculate
a value to use in further work (e.g., use a formula like
<code>height_ft * 12 + height_in</code> to calculate the full height in inches).</p>
<p>Another decision at this stage is how “long” versus “wide” you make your
template. A “wide” design will include more columns, while a “long” design will
include more rows. Often, you can create different designs that allow you to
collect the same values but with different designs on this wide-versus-long
spectrum. Figure <a href="experimental-data-recording.html#fig:longversuswide">2.7</a> gives two examples of templates that
collect the same data, but one is using a wider design and the other is using a
longer design.</p>
<div class="figure"><span style="display:block;" id="fig:longversuswide"></span>
<img src="figures/growth_curve_long_vs_wide.png" alt="Examples of two ways arranging the same data in a data recording template. The format on the left records the optical density measurements for each test tube in a separate column, and the column header identifies the test tube. This is an example of a 'wider' format. The format on the right records the optical density for all test tupbes in a single column, using a separate column to record which test tube the measurement represents. This is an example of a 'longer' format." width="\textwidth" />
<p class="caption">
Figure 2.7: Examples of two ways arranging the same data in a data recording template. The format on the left records the optical density measurements for each test tube in a separate column, and the column header identifies the test tube. This is an example of a ‘wider’ format. The format on the right records the optical density for all test tupbes in a single column, using a separate column to record which test tube the measurement represents. This is an example of a ‘longer’ format.
</p>
</div>
<p>In module 2.3, we described the rules for the tidy format for dataframes.
If you record data directly into a tidy format, it will be very easy to
read into a programming language to analyze and visualize. However,
this tidy format can sometimes result in datasets that are very long. It
may be more convenient to record data into a wider format, especially if you
are recording the data in a laboratory setting where it is inconvenient to
scroll up and down within a longer-format file. Fortunately, there are some
convenient tools in programs like R and Python that can be used to take
data that are collected in a wider format and reformat them to the tidy
format as soon as they are read into the software program. While this will
require some extra code, it is usually code that is fairly simple and straightforward.
Therefore, when you design your data collection template, you can balance
any practical advantages of using a wider data collection format against
the advantages of a fully tidy format that apply once your input the data
into a statistical program for analysis and visualization. Often, the wider
format might win out in this balance, and that’s fine.</p>
<p>The third principle is to <strong>avoid characters or formatting that will make it
hard for a computer program to process the data</strong>. This principle is
particularly important for the column names for each column. When you read data
into a statistical program like R, these names will automatically be used as the
column names in the R data frame object, and the code will regularly use these
column names to refer to parts of the data when analyzing and visualizing it.
You will find it easiest to use the data in a reproducible pipeline if you
follow a couple rules for the column names. The reason that these rules will help
is that they replicate the rules for naming objects in programming languages,
and so will help in seamlessly transitioning between the stages of data
collection and data analysis. First, always start a column name with a letter.
Second, only use letters, numbers, or the underscore character (“_“) for
the rest of the characters in the column name.</p>
<p>Based on these rules, then, you should avoid putting spaces in your column names
when you design a data collection template. It is tempting to include spaces to
make the names clearer for humans to read, and this is understandable. Often,
using an underscore in place of a space can allow for easy human comprehension
while still avoiding characters that are difficult for statistical programs.
For example, if you have a column named “Optical density”, you can change it
to “Optical_density” without making it much more difficult for a person to
understand. As with other choices in designing a data collection template,
these choices about column names can be a balance between making the template
easy for researchers to use in the laboratory and easy for the statistical
program to parse later in the pipeline. For example, statistical programs like
R have functions for working with character strings that can be used to
replace all the spaces in column names with another character. However, if
it isn’t unreasonable to follow the recommended rules in writing column names
for the data collection template, you can keep code later in the pipeline
much simpler, so it’s worth considering.</p>
<p>Beyond spaces, there are a number of other special characters that you might
be tempted to include in column names. These could include parentheses, dollar signs,
percent signs, hash marks (“#”), and so on. Any of these will require extra code
in later steps of an analysis pipeline, and some can cause more severe problems
because they have special functionality in the programming language. For example,
hash marks are used in the R programming language to add comments within code, while
dollar signs are used for subsetting elements of a list or data frame object.
It is worth the effort to avoid all these characters in column names in a data
collection template.</p>
<p>There are also considerations you can make in terms of how you record data within
cells of the data collection template, and these can make a big difference in terms
of how hard or easy it is to work with the data within a statistical program. While
statistical programs like R are very powerful in terms of being able to handle even
very “messy” input data, they require a lot of code to leverage this power. By being
thoughtful when you design the template to record the data, you can avoid having to
use a lot of code to input and clean the data in later stages of the pipeline.</p>
<p>Figure <a href="experimental-data-recording.html#fig:recordingtime">2.8</a> gives an example of a choice that you could make
in the format you use to record data. This figure shows two columns from the
original data collection template from the example experiment for this module.
This template includes two columns that record the time since the start of the
experiment, and they use different formats for doing this. In column B, time is
recorded in hours and minutes, with the characters “hr” and “min” used to
separate the two time components. In column D, the same information is recorded,
but in decimals of hours (e.g., 4.08 hours for 4 hours and 5 minutes). While the
format in column B is more similar to how humans think of time, it will take
more code to parse in a statistical program. When reading this data into a
program like R, you would need to use regular expressions to split apart the
different elements and then recombine them into a format that the program
understands. By contrast, the values recorded in column D could be easily read
in by a statistical program, with minimal code needed before they could be used
in analysis and visualizations.</p>
<div class="figure"><span style="display:block;" id="fig:recordingtime"></span>
<img src="figures/growth_curve_recording_time.png" alt="Examples of two ways of recording time in the original template from the example experiment. Column B uses hours and minutes, with characters embedded to separate hours from minutes, while column D uses hours in decimal degrees. The format in column D will be much easier to integrate into a larger data analysis pipeline." width="\textwidth" />
<p class="caption">
Figure 2.8: Examples of two ways of recording time in the original template from the example experiment. Column B uses hours and minutes, with characters embedded to separate hours from minutes, while column D uses hours in decimal degrees. The format in column D will be much easier to integrate into a larger data analysis pipeline.
</p>
</div>
<p>These three principles are an excellent starting point for designing a tidy
template for collecting data. By using these, you will be well on your way to
collecting data in a way that is easy to integrate in a longer reproducible data
analysis pipeline.</p>
<p>When you convert data collection templates to “tidier” formats, they will
typically look much simpler than the templates that your research group may have
been using. In the example experiment that we described earlier in this module,
this process of tidying the template results in a template like that shown in
Figure <a href="experimental-data-recording.html#fig:growthexcel1">2.1</a> (in the next module, we’ll walk through all the
steps to create this tidier template, using this principles we’ve covered in
this module). By comparison, the starting template for data collection for this
experiment is shown in Figure <a href="experimental-data-recording.html#fig:growthexcel1">2.1</a>.</p>
<div class="figure"><span style="display:block;" id="fig:growthsimple1"></span>
<img src="figures/growth_curve_simple.png" alt="Example of an simpler format that can be used to record and analyze data for the same laboratory experiment as the previous figure. Annotations highlight where data is entered by hand. No calculations are conducted or figures created---these are all done later, using a code script." width="\textwidth" />
<p class="caption">
Figure 2.9: Example of an simpler format that can be used to record and analyze data for the same laboratory experiment as the previous figure. Annotations highlight where data is entered by hand. No calculations are conducted or figures created—these are all done later, using a code script.
</p>
</div>
<p>By comparing these two templates, you can see that the simpler template does
not, by itself, provide immediate, real-time summaries of the collected data. The
simpler template has removed elements like plots and values calculated by embedded
formulas. At first glance, this might seem like a disadvantage of using a tidier
template to collect data. However, by combining other tools in a pipeline, it is
easy to connect the tidier raw data file to reporting tools. In this way, you can
quickly create real-time summaries of the data that are similar to those shown in
Figure <a href="experimental-data-recording.html#fig:growthexcel1">2.1</a>, but that are created and reported outside the file
used to originally record the data.</p>
<p>Figure <a href="experimental-data-recording.html#fig:growthreport1">2.10</a> shows an example of a simple report that could
be created for the example experiment. This report is generated using a
statistical program, R, which inputs the data from the simple template shown in
Figure <a href="experimental-data-recording.html#fig:growthsimple1">2.9</a>. The report then uses R code to generate a PDF
or Word file with the output shown below. The file for this report is created in
a way that the output can be quickly regenerated with a single button click, and
so it can be applied to other data saved using the same template. In fact, you can
create templates for reports that coordinate with each data collection template
that you create. In the next module, we’ll walk through how you could create
the generating file for this report, and in later modules (3.7–3.9), we provide
a thorough overview of creating these types of “knitted” documents.</p>
<div class="figure"><span style="display:block;" id="fig:growthreport1"></span>
<img src="figures/growth_curve_report.png" alt="Examples of an automated report that can be created to quickly generate summaries and estimates of the data collected in the simplified data collection template for the example experiment." width="\textwidth" />
<p class="caption">
Figure 2.10: Examples of an automated report that can be created to quickly generate summaries and estimates of the data collected in the simplified data collection template for the example experiment.
</p>
</div>
<p>The report shown in Figure <a href="experimental-data-recording.html#fig:growthreport1">2.10</a> repeats some of the same summaries
that were shown in the more complex original data collection template
(Figure <a href="experimental-data-recording.html#fig:growthexcel1">2.1</a>). There are a number of advantages, however, to using
separate steps and files for the processes of collecting versus analyzing the data.
The separate report (Figure <a href="experimental-data-recording.html#fig:growthexcel1">2.1</a>) provides a starting point that can
be easily adapted to make more complex figures and analysis, as well as to integrate
the collected data with data measured in other ways for the experiment.</p>
</div>
<div id="learning-more-about-tidy-data-collection-in-the-laboratory" class="section level3 hasAnchor" number="2.4.4">
<h3><span class="header-section-number">2.4.4</span> Learning more about tidy data collection in the laboratory<a href="experimental-data-recording.html#learning-more-about-tidy-data-collection-in-the-laboratory" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>It may take some iteration to develop the data collection templates that are both
convenient and appropriate to input to more complex programs for pre-processing,
analysis, and visualization. This module and the next module provide guidance and
examples, but it can be helpful to see more examples. Two excellent resources on this
topic are articles by <span class="citation">Ellis and Leek (2018)</span> and <span class="citation">Broman and Woo (2018)</span>.</p>
<!-- ### Applied exercise -->

</div>
</div>
<div id="module5" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Example: Creating a template for “tidy” data collection<a href="experimental-data-recording.html#module5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will walk through an example of creating a template to collect data in a
“tidy” format for a laboratory-based research project, based on a research
project on drug efficacy in murine tuberculosis models. We will show the initial
“untidy” format for data recording and show how we converted it to a “tidy”
format. Finally, we will show how the data can then easily be analyzed and
visualized using reproducible tools.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Understand how the principles of “tidy” data can be applied for a real, complex research project;</li>
<li>List advantages of the “tidy” data format for the example project</li>
</ul>
<p>In the last module, we covered three principles for designing tidy templates for
data collection in a biomedical laboratory, motivated by an example dataset from
a real experiment. In this module, we’ll show you how to apply those principles
to create a tidier template for the example dataset from the last module.
As a reminder, those three principles are:</p>
<ol style="list-style-type: decimal">
<li>Limit the template to the collection of data.</li>
<li>Make sensible choices when dividing data collection into rows and columns.</li>
<li>Avoid characters or formatting that will make it hard for a computer program to process the data.</li>
</ol>
<p>It is important to note that there’s no reason that you can’t continue to use a
spreadsheet program like Excel or Google Sheets to collect data. The spreadsheet
program itself can easily be used to create a simple template to use as you
collect data. In fact, we’ll continue using a spreadsheet format in the rest of
this module and in the next one as we show how to redesign the data collection
for this example experiment. It is important, however, to think through how you
will arrange that template spreadsheet to make it most useful in the larger
context of reproducible research.</p>
<div id="example-datadata-on-rate-of-bacterial-growth" class="section level3 hasAnchor" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> Example data—Data on rate of bacterial growth<a href="experimental-data-recording.html#example-datadata-on-rate-of-bacterial-growth" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here, we’ll walk through an example using real data collected in a laboratory
experiment. We described these data in detail in the previous module. As a
reminder, they were collected to measure the growth rate of <em>Mycobacteria
tuberculosis</em> under two conditions—high oxygen and low oxygen. They were
collected from five test tubes that were measured regularly over one week for
bacteria growth using a measure of optical density. Figure
<a href="experimental-data-recording.html#fig:growthexcel2">2.11</a> shows the original template that the research group used
to record these data.</p>
<div class="figure"><span style="display:block;" id="fig:growthexcel2"></span>
<img src="figures/growth_curve_example.png" alt="Example of an Excel spreadsheet used to record and analyze data for a laboratory experiment. Annotations highlight where data is entered by hand, where calculations are done by hand, and where embedded Excel formulas are used. The figures are created automatically using values in a specified column." width="\textwidth" />
<p class="caption">
Figure 2.11: Example of an Excel spreadsheet used to record and analyze data for a laboratory experiment. Annotations highlight where data is entered by hand, where calculations are done by hand, and where embedded Excel formulas are used. The figures are created automatically using values in a specified column.
</p>
</div>
<p>In the previous module, we described features that make this template “untidy”
and potentially problematic to include in a larger pipeline of reproducible
research. In the next few sections of this module, we’ll walk step-by-step
through changes that you could make to make this template tidier. We’ll finish
the module by showing how you could then easily design a further step of the
analysis pipeline to visualize and analyze the collected data, so that the
advantages of real-time plotting from the more complex spreadsheet are not
missed when moving to a tidier template.</p>
</div>
<div id="limiting-the-template-to-the-collection-of-data" class="section level3 hasAnchor" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> Limiting the template to the collection of data<a href="experimental-data-recording.html#limiting-the-template-to-the-collection-of-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The example template (Figure <a href="experimental-data-recording.html#fig:growthexcel2">2.11</a>) includes a number of
“extra” elements beyond simple data collection—all the elements outside rows
1–15 of columns A–I. Outside this area of the original spread, there are a
number of extra elements, including plots that visualize the data, summaries
generated based on the data (rows 16–18, for example), notes about the data,
and even a macro (top right) that wasn’t involved in data collection but instead
was used by the researcher to calculate the initial volume of inoculum to
include in each test tube. None of these “extras” can be easily read into a
statistical program like R or Python—at best, they will be ignored by the program.
They can even complicate reading in the cells with measurements (rows
1–15 of columns A–I), as most statistical programs will try to read in all the
non-empty cells of a spreadsheet unless directed otherwise.</p>
<p>A good starting point, then, would be to start designing a tidy data collection
template for this experiment by extracting only the content from the box in
Figure <a href="experimental-data-recording.html#fig:extractraw">2.2</a>. This would result in a template that looks like
Figure <a href="experimental-data-recording.html#fig:step1">2.12</a>.</p>
<div class="figure"><span style="display:block;" id="fig:step1"></span>
<img src="figures/growth_curve_step1.png" alt="First step in designing a tidy data collection template for the example project. A template has been created that focuses only on the raw data, removing all extra elements like plots, notes, macros, and summaries." width="\textwidth" />
<p class="caption">
Figure 2.12: First step in designing a tidy data collection template for the example project. A template has been created that focuses only on the raw data, removing all extra elements like plots, notes, macros, and summaries.
</p>
</div>
<p>Notice that we’ve also removed any of the color formatting from the spreadsheet. It is fine to
keep color in the spreadsheet if it will help the research to find the right spot to record data
while working in the laboratory, but you should make sure that you’re not using it to encode
information about the data—all color formatting will be ignored when the data are read by a
statistical program like R.</p>
<p>While the template shown in Figure <a href="experimental-data-recording.html#fig:step1">2.12</a> has removed a lot of the calculated values from the
original template, it has not removed all of them. Two of the columns are still values that were
determined by calculation after the original data were collected. Column B and column D both provide
measures of the length of time since the start of the experiment, and both are calculated by
comparing a measurement time to the time at the start of the experiment.</p>
<p>The time since the start of the experiment can easily be calculated later in the analysis pipeline,
once you read the data into a statistical program like R. By delaying this step, you can both
simplify the data collection template (requiring fewer columns for the research in the laboratory
to fill out) and also avoid the chance for mistakes, which could occur both in the hand calculations
of these values and in data entry, when the researcher enters the results of the calculations in the
spreadsheet cell. Figure <a href="experimental-data-recording.html#fig:step2">2.13</a> shows a new version of the template, where these calculated
columns have been removed. This template is now restricted to only data points originally collected
in the course of the experiment, and has removed all elements that are based on calculations or other
derivatives of those original, raw data points.</p>
<div class="figure"><span style="display:block;" id="fig:step2"></span>
<img src="figures/growth_curve_step2.png" alt="Second step in designing a tidy data collection template for the example project. This template started from the previous one, but removed columns that were hand-calculated and then entered by the researcher in the previous template. This version has removed all calculated values on the template, limiting it to only the original recorded values required for the experiment." width="\textwidth" />
<p class="caption">
Figure 2.13: Second step in designing a tidy data collection template for the example project. This template started from the previous one, but removed columns that were hand-calculated and then entered by the researcher in the previous template. This version has removed all calculated values on the template, limiting it to only the original recorded values required for the experiment.
</p>
</div>
</div>
<div id="making-sensible-choices-about-rows-and-columns" class="section level3 hasAnchor" number="2.5.3">
<h3><span class="header-section-number">2.5.3</span> Making sensible choices about rows and columns<a href="experimental-data-recording.html#making-sensible-choices-about-rows-and-columns" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The second principle is to <strong>make sensible choices when dividing data collection
into rows and columns</strong>. There are many different ways that you could spread the
data collection into rows and columns, and in this step, you can consider which
method would meet a reasonable balance between making the template easy for the
researcher in the laboratory to use to record data and also making the resulting
data file easy to incorporate in a reproducible data analysis pipeline.</p>
<p>For the example experiment, Figure <a href="experimental-data-recording.html#fig:extractraw">2.2</a> shows three examples
that we can consider for how to arrange data collection across rows and columns.
All three build on the changes we made in the earlier step of “tidying” the template,
which resulted in the template shown in Figure <a href="experimental-data-recording.html#fig:step2">2.13</a>.</p>
<div class="figure"><span style="display:block;" id="fig:columnoptions"></span>
<img src="figures/growth_curve_column_options.png" alt="Examples of ways that data collection could be divided into rows and columns in the example template. Panel A shows an example where date and time are recorded in different columns. Panel B is similar to Panel A, but in this case, date and time are recorded in a single column. Panel C shows a classically 'tidy' data format, where each measurement date-time is repeated for each of the five test tubes, and columns give the test tube ID and absorbance measurement at that time for that tube (only part of the data is shown for this format, while remaining rows are off the page). While Panel C provides the 'tidiest' format, it may have some practical constraints when used in a laboratory setting. For example, it would require more data entry during data collection (since date-time is entered five times at each measurement time), and its long format prevent it all from being seen at once without scrolling on a computer screen." width="\textwidth" />
<p class="caption">
Figure 2.14: Examples of ways that data collection could be divided into rows and columns in the example template. Panel A shows an example where date and time are recorded in different columns. Panel B is similar to Panel A, but in this case, date and time are recorded in a single column. Panel C shows a classically ‘tidy’ data format, where each measurement date-time is repeated for each of the five test tubes, and columns give the test tube ID and absorbance measurement at that time for that tube (only part of the data is shown for this format, while remaining rows are off the page). While Panel C provides the ‘tidiest’ format, it may have some practical constraints when used in a laboratory setting. For example, it would require more data entry during data collection (since date-time is entered five times at each measurement time), and its long format prevent it all from being seen at once without scrolling on a computer screen.
</p>
</div>
<p>Panel A (an exact repeat of the template shown in Figure <a href="experimental-data-recording.html#fig:step2">2.13</a>) shows
an example where date and time are recorded in different columns. Panel B is
similar to Panel A, but in this case, date and time are recorded in a single
column. Panel C shows a classically “tidy” data format, where each measurement’s
date-time is repeated for each of the five test tubes, and columns give the test
tube ID and absorbance measurement at that time for that tube (only part of the
data is shown for this format, while remaining rows are off the page).</p>
<p>In this example, the template that may be the most reasonable is the one shown
in Panel B. While Panel C provides the “tidiest” format, it has some practical
constraints when used in a laboratory setting. For example, it would require
more data entry during data collection (since date-time is entered five times at
each measurement time), and its long format prevent it all from being seen at
once without scrolling on a computer screen. When comparing Panels A and B, the
template in Panel B has an advantage. The information on date and time are
useful together, but not individually. For example, to calculate the time since
the start of the experiment, you cannot just calculate the difference in dates
or just the difference in times, but instead must consider both the date and
time of the measurement in comparison to the date and time of the start of the
experiment. As a result, at some point in the data analysis pipeline, you’ll
need to combine information about the date and the time to make use of the two
elements. While this combination of two columns can be easily done within a
statistical program like R, it can also be directly designed into the original
template for collecting the data. Therefore, unless there is a practical reason
why it would be easier for the researcher to enter date and time separately, the
template shown in Panel B is preferable to that shown in Panel A in terms of
allowing for the “tidy” collection of research data into a file that is easy to
include in a reproducible pipeline. Figure <a href="experimental-data-recording.html#fig:step3">2.15</a> shows the template
design at this stage in the process of tidying it, highlighting the column that
combines date and time elements in a single column. In this version of the
template, we’ve also been careful about how date and time are recorded, a
consideration that we’ll discuss more in the next section.</p>
<div class="figure"><span style="display:block;" id="fig:step3"></span>
<img src="figures/growth_curve_step3.png" alt="Third step in designing a tidy data collection template for the example project. This template started from the previous one, but combined collection of the date and time of the measurement into a single column and revised the format to include all date elements and to prevent automatic conversion by the spreadsheet program." width="\textwidth" />
<p class="caption">
Figure 2.15: Third step in designing a tidy data collection template for the example project. This template started from the previous one, but combined collection of the date and time of the measurement into a single column and revised the format to include all date elements and to prevent automatic conversion by the spreadsheet program.
</p>
</div>
</div>
<div id="avoiding-problematic-characters-or-formatting" class="section level3 hasAnchor" number="2.5.4">
<h3><span class="header-section-number">2.5.4</span> Avoiding problematic characters or formatting<a href="experimental-data-recording.html#avoiding-problematic-characters-or-formatting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The third principle is to <strong>avoid characters or formatting that will make it
hard for a computer program to process the data</strong>. There are a number of special
characters and formatting conventions that can be hard for a statistical program to
handle. In the example template shown in Figure <a href="experimental-data-recording.html#fig:step3">2.15</a>, for example,
the column names include spaces (for example, in “Date and time”), as well as
parentheses (for example, in “VA 001 (A1)”). While most statistical programs have
tools that allow you to handle and convert these characters once the data are
read in, it’s even simpler to use simpler column names in the original data collection
template, and this will save some extra coding further along in the analysis pipeline.
Two general rules for creating easy-to-use column names in a data collection template
are: (1) start each column name with a letter and (2) for the rest of the column
name, use only letters, numbers, or the underscore character (“_“). For example,”aerated1” would work well, but “1–aerated” would not.</p>
<p>Within the cell values below the column names, there is more flexibility. For example,
if you have a column that gives the IDs of different samples, it would be fine to include
spaces and other characters in those IDs.</p>
<p>There are a few exceptions, however. A big one
is with values that record dates or date-time combinations. First, it is important to include
all elements of the date (or date and time, if both are recorded). For example, the year
should be included in the recorded date, even if the experiment only took a few days.
This is because statistical programs have excellent functions for working with data that
are dates or date-times, but to take advantage of these, the data must be converted into
a special class in the program, and conversion to that class requires specific elements
(for example, a date must include the year, month, and day of month).</p>
<p>Second, it is
useful to avoid recording dates and date-times in a way that results in a spreadsheet
program automatically converting them. Surrounding the information about a date in
quotation marks when entering it (as shown in Figure <a href="experimental-data-recording.html#fig:step3">2.15</a>) can avoid this.</p>
<p>Finally, consider using a format to record the date that is unambiguous and so less likely
to have recording errors. Dates, for example, are sometimes recorded using only numbers—for
example, the first date of “July 9, 2019” in the example data could be recorded as
“7/9/2019” or “7/9/19”, to be even more concise. However, this format has some ambiguity.
It can be unclear if this refers to July 9 or to September 7, both of which could be
written as “7/9”. For the version that uses two digits for the year, it can be unclear
if the date is for 2019 or 1919 (or any other century). Using the format “July 9, 2019”,
as done in the latest version of the sample template, avoids this potential ambiguity.</p>
<p>Figure <a href="experimental-data-recording.html#fig:growthsimple2">2.16</a> shows the template for the example experiment after the
column names have been revised to avoid any problematic characters. This template is now in
a very useful format for a reproducible research pipeline—the data collected using this
template can be very easily read into and processed using further statistical programs like
R or Python.</p>
<div class="figure"><span style="display:block;" id="fig:growthsimple2"></span>
<img src="figures/growth_curve_simple.png" alt="Example of an simpler format that can be used to record and analyze data for the same laboratory experiment as the previous figure. Annotations highlight where data is entered by hand. No calculations are conducted or figures created---these are all done later, using a code script." width="\textwidth" />
<p class="caption">
Figure 2.16: Example of an simpler format that can be used to record and analyze data for the same laboratory experiment as the previous figure. Annotations highlight where data is entered by hand. No calculations are conducted or figures created—these are all done later, using a code script.
</p>
</div>
</div>
<div id="separating-data-analysis-from-data-collection" class="section level3 hasAnchor" number="2.5.5">
<h3><span class="header-section-number">2.5.5</span> Separating data analysis from data collection<a href="experimental-data-recording.html#separating-data-analysis-from-data-collection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Once you have created a “tidy” template for collecting your data in the laboratory,
you can create a report template that will input that data and then provide summaries
and visualizations. This allows you to separate the steps (and files) for collecting data
from those for analyzing data. Figure <a href="experimental-data-recording.html#fig:growthreport2">2.17</a> shows an example of
a report template that could be created to pair with the data collection template shown
in Figure <a href="experimental-data-recording.html#fig:growthsimple2">2.16</a>.</p>
<div class="figure"><span style="display:block;" id="fig:growthreport2"></span>
<img src="figures/growth_curve_report.png" alt="Examples of an automated report that can be created to quickly generate summaries and estimates of the data collected in the simplified data collection template for the example experiment." width="\textwidth" />
<p class="caption">
Figure 2.17: Examples of an automated report that can be created to quickly generate summaries and estimates of the data collected in the simplified data collection template for the example experiment.
</p>
</div>
<p>To create a report template like this, you can use tools for reproducible
reports from statistical programs like R and Python. In this section, we will
give an overview of how you could create the report template shown in Figure
<a href="experimental-data-recording.html#fig:growthreport2">2.17</a>.</p>
<p>This report is written using a framework called RMarkdown, which allows you to
include executable code inside a nicely-formatted document, resulting in a
document in Word, PDF, or HTML that is easy for humans to read while also
generating results based on R code. We will cover this format in details in
modules 3.7–3.9. In the rest of this section, we’ll walk through some of the R
code that is “powering” the analysis in this document, but if you’d like to
learn how to combine it into an RMarkdown document to create the report shown in
Figure <a href="experimental-data-recording.html#fig:growthreport2">2.17</a>, you can learn much more about RMarkdown in
those later modules. The code within the report uses the R language. We’ll cover
a few examples here, but if you would like to use R, you’ll find it helpful to
learn more. Numerous excellent (and free) resources exist to help learn R. One
of the best is the book “R for Data Science” by Hadley Wickham and Garrett
Grolemund. It is available in print, as well as free online at
<a href="https://r4ds.had.co.nz/" class="uri">https://r4ds.had.co.nz/</a>.</p>
<p>The code used to generate the results in Figure <a href="experimental-data-recording.html#fig:growthreport2">2.17</a> is all
in the programming language R. A programming language can seem, at first glance,
much more difficult to learn and use than using a spreadsheet program like Excel
to set up formulae and macros. However, languages like R have evolved
substantially in recent years to allow for much more straightforward coding than
you may have seen in the past, and the barrier to learning to use them for
straightforward data management and analysis is not much higher than the effort
required to become proficient in using a spreadsheet program. To demonstrate
this, let’s look through a few of the tasks required to generate the results
shown in Figure <a href="experimental-data-recording.html#fig:growthreport2">2.17</a>. We won’t cover all the code, just
highlight some of the key steps. If you’d like to look in details over the code
and the output document, you can download those files and explore them: you can
access the file for the <a href="https://github.com/geanders/improve_repro/blob/master/data/growth_curve_data_in_excel%20(1)/Example_report.Rmd">Rmarkdown
file</a>,
and you can download the <a href="https://github.com/geanders/improve_repro/raw/master/data/growth_curve_data_in_excel%20(1)/Example_report.pdf">output
PDF</a>.
If you’d like to try out the code in the Rmarkdown file, you’ll also need the
example data, which you can download by clicking
<a href="https://github.com/geanders/improve_repro/raw/master/data/growth_curve_data_in_excel%20(1)/growth%20curve%20data_GR.xls">here</a>.</p>
<p>One key step is to read the collected data into R. When you use a spreadsheet
for both data collection and analysis, you don’t need to read the data to start
working with them, since everything is saved in the same file. Once you separate
the steps of data collection and data analysis, however, you do need to take an
extra step to read the data file into another program for analysis. Fortunately,
this is very simple in R. The data in this example are recorded using an Excel
spreadsheet, and there is a simple function in R that lets you read data in from
this type of spreadsheet (Figure <a href="experimental-data-recording.html#fig:growthreadexcel">2.18</a>). After this step of
code, you will have an object in R called <code>growth_data</code>, which contains the data
in a two-dimensional form very similar to how it is recorded in the spreadsheet
(this type of object in R is called a dataframe).</p>
<div class="figure"><span style="display:block;" id="fig:growthreadexcel"></span>
<img src="figures/growth_curve_readxl.png" alt="Code to read data from the data collection template into R for cleaning, analysis, and visualization. The data were recorded in the tidy data collection template described earlier in this module. Here, those data are read into R (code shown at top). The resulting data in R are stored in a format that is very similar to the design of a spreadsheet, with rows for observations and columns for the values recorded for each observation (bottom)." width="\textwidth" />
<p class="caption">
Figure 2.18: Code to read data from the data collection template into R for cleaning, analysis, and visualization. The data were recorded in the tidy data collection template described earlier in this module. Here, those data are read into R (code shown at top). The resulting data in R are stored in a format that is very similar to the design of a spreadsheet, with rows for observations and columns for the values recorded for each observation (bottom).
</p>
</div>
<p>Another key step is to calculate, for each observation, the time since the start
of the experiment. In the original data collection template shown in Figure
<a href="experimental-data-recording.html#fig:growthexcel2">2.11</a>, this calculation was done by hand by the researcher and
entered into the spreadsheet. When we converted the spreadsheet to a tidier
version, we took out all steps that involved calculations with the data, and
instead limited the data collection to only raw, observed values. This helps us
avoid errors and typos—instead of having the researcher calculate the
difference in time as they are running the experiment, they can just record the
time, and we can write code in the analysis document that handles the further
calculations, using well-designed and well-tested tools to do this calculation.</p>
<p>Figure <a href="experimental-data-recording.html#fig:growthadddifftime">2.19</a> shows code that can be used for this
calculation. At the start of this code, the data are stored in an object named
<code>growth_data</code>. The <code>mutate</code> function adds a column to the data, named
<code>sampling_delta_time</code>, that will give the difference between the time of an
observation and the start of the experiment. Within the <code>mutate</code> call, a special
function named <code>difftime</code> calculates the difference in two time points. This
function lets us specify the time units we’d like to use, and here we can pick
<code>"hours"</code> for the units. The <code>first</code> function lets us pull out the first value
in the data for a recorded time—in other words, the time when the experiment
started. This lets us compare each observation time to the time of the start of
the experiment. The result of this code is a new version of the <code>growth_data</code>
dataframe, with a new column giving time since the start of the experiment:</p>
<div class="figure"><span style="display:block;" id="fig:growthadddifftime"></span>
<img src="figures/growth_curve_adddifftime.png" alt="Code to add a column to the data that gives the time since the start of the experiment. This code (top) uses the time recorded for each experiment and compares it to the first recorded time, at the start of the experiment. This determines the time since the start of the experiment for each observation, given in a new column in the data (bottom)." width="\textwidth" />
<p class="caption">
Figure 2.19: Code to add a column to the data that gives the time since the start of the experiment. This code (top) uses the time recorded for each experiment and compares it to the first recorded time, at the start of the experiment. This determines the time since the start of the experiment for each observation, given in a new column in the data (bottom).
</p>
</div>
<p>Another key step is to plot results from the data. In R, there is a package
called <code>ggplot2</code> that provides tools for visualization. The tools in this
package work by building a plot using “layers”, adding on small elements
line by line through simple functions that each do one simple thing.
While the resulting code can be long, each step is simple, and so it
becomes simple to learn these different “layers” and learn how to combine
them to create complex plots.</p>
<p>Figure <a href="experimental-data-recording.html#fig:growthplotcode">2.20</a> walks through the code for one of the
visualizations in the report. At this point in the report code, the data have
been reformatted into an object called <code>growth_data_tidy</code>, which has columns for
each observation on the time since the start of the experiment
(<code>sampling_delta_time</code>), the measured optical density (<code>optical_density</code>),
whether the tube was aerated or low oxygen (<code>growth_conditions</code>), and a short ID
for the test tube (<code>short_tube_id</code>). The code starts by creating a plot object,
specifying that in this plot the color will show the growth conditions, the
position on the x-axis will show the time since the start of the experiment, and
the y-axis will show the optical density. Layers are then added to this plot
object that add points and lines to the plot based on these mappings, and for
the lines, it’s further specified that the type of line should show the test
tube ID (for example, one tube will be shown with a dotted line, another with a
dashed line). Further layers are added to customize the scale labels with
<code>labs</code>, including the labels for the x-axis and y-axis and the legends of the
color and linetype scales. Another layer is used to customize the appearance of
the plot—things like the background color and the font used—and another
layer is added to use a log-10 scale for the x-axis.</p>
<div class="figure"><span style="display:block;" id="fig:growthplotcode"></span>
<img src="figures/growth_curve_plot_code.png" alt="Code to plot growth curves from the data. When the plotting code is run, the data have been transformed into a 'tidy' format (top), with columns that include the time since the start of the experiment, a test tube ID, the growth condition for the test tube, and the optical density measured in that test tube. The code (middle) add layers to implement each element of the plot based on this input data. The final plot is shown at the bottom." width="\textwidth" />
<p class="caption">
Figure 2.20: Code to plot growth curves from the data. When the plotting code is run, the data have been transformed into a ‘tidy’ format (top), with columns that include the time since the start of the experiment, a test tube ID, the growth condition for the test tube, and the optical density measured in that test tube. The code (middle) add layers to implement each element of the plot based on this input data. The final plot is shown at the bottom.
</p>
</div>
<p>While this looks like a lot of code, the process isn’t any longer than it would
be to customize elements of a plot in a spreadsheet program. The advantages of
the coded approach are that you maintain a full record of all the steps you took
to customize the plot. This is something that you can use to reproduce your plot
later, or even to use as a starting point for creating a similar plot with new
data.</p>
<p>The next key step that we’d like to point out is how you can write and use small
functions to do customized tasks for the experimental data. As one example, for
the data in this example, we want to estimate doubling times based on the
observed data. The principal investigator has decided that we should do this
based on comparing bacteria levels at two times points—the measured time that
is closest to 65 hours after the start of the experiment, and the time that is
closest to 24 hours after the start of the experiment.</p>
<p>In the original data collection template—where the data were both recorded
and analyzed in a spreadsheet—this step was done by hand by the researcher,
looking through the data and selecting the cell closest to each of these
times, and then connecting that cell to a spreadsheet formula calculation
to calculate the doubling time. We can make this process more rigorous and
less prone to error by writing a small function that does the same thing,
then using that function to automate the process of identifying the
relevant observations to use in calculating the doubling rate.</p>
<p>Figure <a href="experimental-data-recording.html#fig:growthfunction">2.21</a> shows how you can write and then use a small
function in R. This function will input your <code>growth_data</code> dataset, as well as a
time that you are aiming for, and will output the sampling time in the data that
is closest to—while not larger than—that time. It does that in a few steps
within the body of the function. First, the code in the function filters to only
observations earlier than the target time. Then it measures the difference
between each of the times for these observations and the target time, and uses
this to identify the observation with the closest time to the target. It pulls
out the time of this observation and returns it.</p>
<div class="figure"><span style="display:block;" id="fig:growthfunction"></span>
<img src="figures/growth_curve_function.png" alt="Code to create and apply a small function. The code at the top can be used to create a function that can input your dataframe and determine the observation time in that data that is closest to (without being larger than) a target time. The function does this through a series of small steps. This function can then be applied to find the observation time in the data that is closest to specific target times, like 24 hours and 64 hours (bottom)." width="\textwidth" />
<p class="caption">
Figure 2.21: Code to create and apply a small function. The code at the top can be used to create a function that can input your dataframe and determine the observation time in that data that is closest to (without being larger than) a target time. The function does this through a series of small steps. This function can then be applied to find the observation time in the data that is closest to specific target times, like 24 hours and 64 hours (bottom).
</p>
</div>
<p>Small functions like this can easily be reused in other code for your research
group. By writing the logic of the step out as a function—rather than redoing
the steps by hand or step-by-step each time you need to do it—you can save
time later, and in return, you have extra time that you can spend in writing
the original function and carefully checking to make sure that it works
correctly.</p>
<p>Finally, many of these steps require extensions to base R. When you download R,
you are getting a base set of tools. Many people have developed helpful
extensions that build on this base. These are stored and shared in what are
called <em>R packages</em>. You can install these extra packages for free, and you
use the <code>library</code> function in R to load a package you’ve installed, giving you
access to the extra functions that it provides. Figure
<a href="experimental-data-recording.html#fig:growthloadpackage">2.22</a> shows the spot in the Rmarkdown code where we
loaded packages we needed for this report. These include packages with functions
to read data in R from Excel (the <code>readxl</code>) package, as well as a suite of
packages with tools for cleaning and visualizing data (the <code>tidyverse</code> package).
In later modules, we’ll talk some more about R coding tools that you might find
useful for working with biomedical data, including the tools in the powerful and
popular <code>tidyverse</code> suite of packages.</p>
<div class="figure"><span style="display:block;" id="fig:growthloadpackage"></span>
<img src="figures/growth_curve_load_packages.png" alt="Code to load packages with additional functionality. These provide functions that are not offered in base R, but that are useful in working with the example data. They include packages with functions for reading in data from an Excel file, as well as packages with functions for cleaning and visualizing data." width="\textwidth" />
<p class="caption">
Figure 2.22: Code to load packages with additional functionality. These provide functions that are not offered in base R, but that are useful in working with the example data. They include packages with functions for reading in data from an Excel file, as well as packages with functions for cleaning and visualizing data.
</p>
</div>
<p>Overall, you can see that the code in this document provides a step-by-step
recipe that documents all the calculations and cleaning that we do with the
data, as well as how we create the plots. This code runs every time we create
the report shown in Figure <a href="experimental-data-recording.html#fig:growthreport2">2.17</a>, and it gives us a good
starting point if we run additional experiments that generate similar data.</p>
</div>
<div id="applied-exercise" class="section level3 hasAnchor" number="2.5.6">
<h3><span class="header-section-number">2.5.6</span> Applied exercise<a href="experimental-data-recording.html#applied-exercise" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Rmarkdown document includes a number of other steps, and you might find
it interesting to download the document and the example data and walk through
them to get a feel for the process. All the steps are documented in the
Rmarkdown document with extensive code comments, to explain what’s happening
along the way.</p>
<!-- ### Discussion questions -->

</div>
</div>
<div id="module6" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> Organizing project files<a href="experimental-data-recording.html#module6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To improve the computational reproducibility of a research project, researchers
can use a single ‘Project’ directory to collectively store all research data,
meta-data, pre-processing code, and research products (e.g., paper drafts,
figures). We will explain how this practice improves the reproducibility and
list some of the common components and subdirectories to include in the
structure of a ‘Project’ directory, including subdirectories for raw and
pre-processed experimental data.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Describe a ‘Project’ directory, including common components and subdirectories</li>
<li>List how a single ‘Project’ directory improves reproducibility</li>
</ul>
<p>In earlier modules, we discussed how to separate data collection from data
analysis. By separating data collection and analysis into separate files, we can
make the file for each step simpler. Further, by separating steps into different
files, we can save the files in plain text, which makes it easier to track them
using version control software (discussed in later modules). This helps create a
record of changes made to the data or analysis code during the research process.</p>
<p>While this process helps in reproducibility, it results in more files being
collected for an experiment. Instead of data and its analysis collected within a
single spreadsheet file, you may end up with multiple files of data collected
from the experiment, as well as separate files with scripts for processing,
analyzing, and visualizing the data. With more complex experiments, there may be
different data files containing the data collected from different assays. For example,
you may run an experiment where you collect data from each research animal on
bacterial load, as well as flow cytometry data, as well as a measure of antibody
levels through ELISA. As a result, you may have one raw data file from each
assay and, for some assays, even one file per study subject (e.g., flow
cytometry). The files for a research project will also include files with
writing and presentations (posters and slides) associated with the project, as
well as code scripts for preprocessing data, for conducting data analysis, and
for creating and sharing final figures and tables.</p>
<p>In the next few modules, we’ll discuss how you can organize the files for an
experiment using a single directory that is designed to follow a similar format
across all your projects. The modules will discuss the advantages
of well-designed project directories, tips for arranging files within a project
directory, and how to create a directory template that allows you to use
consistent file organization across many experiments.</p>
<div id="organizing-project-files" class="section level3 hasAnchor" number="2.6.1">
<h3><span class="header-section-number">2.6.1</span> Organizing project files<a href="experimental-data-recording.html#organizing-project-files" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As the files for a project accumulate, do you have a clear plan for keeping them
organized? Based on one analysis, many biomedical researchers do not. One study,
for example, surveyed over 250 biomedical researchers at the University of
Washington. They noted that, “Some researchers admitted to having no
organizational methodology at all, while others used whatever method best suited
their individual needs” <span class="citation">(Anderson et al. 2007)</span>. One respondent answered, “They’re
not organized in any way—they’re just thrown into files under different
projects,” while another said “I grab them when I need them, they’re not
organized in any decent way,” and another, “It’s not even organized—a file on
a central computer of protocols that we use, common lab protocols but those are
just individual Word files within a folder so it’s not searchable <em>per se</em>”
<span class="citation">(Anderson et al. 2007)</span>.</p>
<p>Similarly, in an article on organizing project files for research, Marwick
notes:</p>
<blockquote>
<p>“Virtually all researchers use computers as a central tool in their
workflow. However, our formal education rarely includes any training in
how to organise our computer files to make it easy to reproduce results
and share our analysis pipeline with others. Without clear instructions,
many researchers struggle to avoid chaos in their file structures, and so
are understandable reluctant to expose their workflow for others to see.
This may be one of the reasons that so many requests for details about
method, including requests for data and code, are turned down or go
unanswered.” <span class="citation">(Marwick, Boettiger, and Mullen 2018)</span></p>
</blockquote>
<p>In an earlier module, we introduced Adam Savage’s idea of “knolling” to keep a
workspace tidy (Module 2.3). He was talking about a physical workspace. When
you are working with data, computer files and directories are your workspace.
For any type of work, the design of the workspace plays a critical role in
how the workers approach tasks and solve problems. Rod Judkins, who is a
lecturer at St Martin’s College of Art, highlights this in a book on
creative thinking:</p>
<blockquote>
<p>“Your working environment, whether it’s a supermarket, office, studio, or
building site, persuades you to work and think in certain ways. The more aware
you are of that, and the more you understand your medium, the more you can use
it to your advantage.” <span class="citation">(Judkins 2016)</span></p>
</blockquote>
<p>Adam Savage describes how important this is in another type of working, gourmet
cooking, describing how this idea of an organized workspace is captured by the
technique of <em>mise en place</em>—of laying out all the elements needed for the
work ahead of time and in an organized way—introduced by the famous French
chef August Escoffier:</p>
<blockquote>
<p>“Kitchens are pressure cookers in which wasted movement and hasty technique
can ruin a dish, slice an artery, burn a hand, land you in the weeds, and
ultimately kill a restaurant. <em>Mise en place</em> is the only way to reliably create
a perfect dish, to exact specifications, over and over again, night after night,
for paying customers who demand nothing less.” <span class="citation">(Savage 2020)</span></p>
</blockquote>
<p>Good organization of your files can similarly encourage clear thinking and help
in reasoning through how to analyze data. One article notes that “mundane issues
such as organizing files and directories and documenting progress … are
important because poor organizational choices can lead to significantly slower
research progress.” <span class="citation">(Noble 2009)</span> In fact, if files are organized in a
consistent way across multiple projects, this can even allow you to start
automating some necessary tasks through code that is built to work with that
consistent structure <span class="citation">(Buffalo 2015)</span>.</p>
<p>Organization also helps you know where to find things. You even know where to
find things when you come back to a project after a while away from it (for
example, while the paper was out for review). You can teach someone else how to
find things quickly and consistently across your multiple projects, as well as
where to put things if they’re contributing to one of them. You have a place for
everything.</p>
<p>As one article notes, with good organization, “methods and data sections in
papers practically write themselves, with no time wasted in frenzied hunting for
missing information.” <span class="citation">(Baker 2016)</span> An article on organizing computational
biology projects also highlights how good organization can improve your
efficiency:</p>
<blockquote>
<p>“Everything you do, you will probably have to do over again. Inevitably, you
will discover some flaw in your initial preparation of the data being analyzed,
or you will get access to new data, or you will decide that your
parameterization of a particular model was not broad enough. This means that the
experiment you did last week, or even the set of experiments you’ve been working
on over the past month, will probably need to be redone. If you have organized
and documented your work clearly, then repeating the experiment with the new
data or the new parameterization will be much, much easier.” <span class="citation">(Noble 2009)</span></p>
</blockquote>
<p>Finally, when your research files are kept in one place and well-organized, it
makes it easier for you to share those as a supplement to articles you write
about the research, and are also more likely to do so. One article notes:</p>
<blockquote>
<p>Without clear instructions, many researchers struggle to avoid chaos in their
file structures, and so are understandably reluctant to expose their workflow
for others to see. This may be one of the reasons that so many requests for
details about method, including requests for data and code, are turned down or
go unanswered”. <span class="citation">(Marwick, Boettiger, and Mullen 2018)</span></p>
</blockquote>
<p>Sharing data and code is crucial to research reproducibility, especially for
projects that include extensive proprocessing and complex analysis of data, as
many biomedical research projects now do. As a further bonus, research articles
that include data with the publication may be more impactful, as measured by
citations that the paper receives <span class="citation">(Marwick, Boettiger, and Mullen 2018)</span>.</p>
</div>
<div id="how-to-organize-project-files" class="section level3 hasAnchor" number="2.6.2">
<h3><span class="header-section-number">2.6.2</span> How to organize project files<a href="experimental-data-recording.html#how-to-organize-project-files" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>First, and at a minimum, you should get in the habit of storing all of the files
for an experiment in the same place. Specifically, project files should all be
in a single directory within the file system of a computer <span class="citation">(Noble 2009)</span>.
While this can be an individual’s computer, it may also be on a dedicated
server or through an online, cloud-based program.</p>
<p>There are a number of advantages to keeping all files related to a single
project inside a dedicated file directory. First, this provides
a clear and obvious place to search for all project files throughout your work
on the project, including after lulls in activity (like waiting for reviews from
a paper submission). As one article notes:</p>
<blockquote>
<p>“During the course of a project, you’ll have amassed data files, notes,
scripts, and so on—if these were scattered all over your hard drive (or worse,
across many computers’ hard drives), it would be a nightmare to keep track of
everything.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<p>Another article highlights how helpful this organization was for a project
involving a large group:</p>
<blockquote>
<p>“Instead of squirrelling away data in individual folders and lab books,
researchers now archive all published data in a designated central drive, so
that the information is accessible for the long haul. Initially, people thought
the process was just extra bureaucratic work, or that it had been invented so I
could police their data. Now, it has become the norm, and researchers tell me
they save time and worry by having their data organized and archived.”
<span class="citation">(Winchester 2018)</span></p>
</blockquote>
<p>Second, by keeping all project files within a single directory, you also make it
easier to share the collection those files. There are several reasons you might
want to share these files. An obvious one is that you likely will want to share
the project files across members in your research team, so they can collaborate
together on the project. However, there are also other reasons you’d need to
share files, and one that is growing in importance is that you may be asked to
share files (data, code scripts, etc.) when you publish a paper describing your
results.</p>
<p>When files are all stored in one directory, the directory can be compressed and
shared as an email attachment or through a file sharing platform like Google
Drive. When all the materials for a project are stored in a single directory, it
also makes it easier to share the set of files through version control and
online version control platforms <span class="citation">(Vuorre and Crump 2021)</span>. (This also helps you to
track changes to files across the project <span class="citation">(Vuorre and Crump 2021)</span>). In later
modules in this book, we will introduce <code>git</code> version control software and the
GitHub platform for sharing files under this type of version control—this is
one example of this more dynamic way of sharing files within a directory.
Further, services like Google Drive and Microsoft Teams are steadily advancing
in their capabilities for versioning and tracking changes across shared files.</p>
<p>To gain the advantages of directory-based project file organization, all the
files need to be within a single directory, but they don’t all have to be within
the same “level” in that directory. Instead, you can use subdirectories to
structure and organize these files, while still retaining all the advantages of
directory-based file organization. Computer file systems are well-structured to
use a hierarchical design, with subdirectories nested inside directories. You
can leverage this to manage the complexity and breadth of files for your
project.</p>
<p>This will help limit the number of files in each “level” of the directory, so
none becomes an overwhelming collection of files of different types. It can help
you navigate the files in the directory, and also help someone you share the
directory with figure out what’s in it and where everything is. However,
to leverage these gains, you need to be thoughtful about exactly how you
organize the files into subdirectories.</p>
<p>As you decide how to organize files, keep in mind a concept called
“discoverability”. In the classic design book <em>The Design of Everyday
Things</em>, Don Norman presents discoverability as a key principle of good design,
explaining as the ability for a user to be able to figure out, from the design
of something, how to use that thing quickly, easily, and correctly. He
illustrates this with an example of discoverability in the design of doors.
For a door, the location of a pull handle and a push bar immediately shows
someone how to use the door: pull where you see a pull handle and push where you
see a push bar. If the door is lacking these, it makes it harder for a user
to “discover” how to use it at first glance, and they might try to push when
they need to pull or vice-versa.</p>
<p>The same idea applies to designing the way to organize research project files
within a directory. You want to make sure that a new user (or you in the future)
will be able to easily navigate through the directory to find what they need.
One article on organizing research project files notes that, when it comes to
deciding how to organize your files, “The core guiding principle is simple:
Someone unfamiliar with your project should be able to look at your computer
files and understand in detail what you did and why.” <span class="citation">(Noble 2009)</span> Another
notes, “Regardless of the particular project you’re working on, your project
directory should be laid out in a consistent and understandable fashion. Clear
project organization makes it easier for both you and collaborators to figure
out exactly where and what everything is.” <span class="citation">(Buffalo 2015)</span></p>
<p>One way to improve discoverability is to name your files and subdirectories
in meaningful ways. The computer will give you wide flexibility in setting
names for files and subdirectories, but a human will find it much easier to
navigate a directory when the names are clear labels that describe the contents.
For example, if you have data from different assays, you might organize them
all into a directory named “raw_data” that is then divided into subdirectories
named with the type of assay.</p>
<p>As you develop names that are discoverable, keep in mind that your users may
include some people outside your field, for whom some shorthand common in the
field might be unclear. For example, in some studies of infectious bacterial
disease, the bacterial load is measured in an assay that counts colony forming
units. Among bench scientists in this field, the assay is often called “CFUs”.
If you are collaborating with a biostatistician, however, they may find the
files more discoverable if you named the subdirectory with these files something
like “bacterial_load” rather than “cfus”, as they may not be familiar with that
shorthand.</p>
<p>One way to improve discoverability is to follow any standards that exist for
organizing project files <span class="citation">(Marwick, Boettiger, and Mullen 2018)</span>. As an example of how standards
can help discoverability, think about the design of cars. Most cars, regardless
of the manufacturer, will have the steering wheel, accelerator, and brake in
approximately the same position relative to the driver. By following this
standard, it’s easier (and safer) for a driver to learn to use a car they
haven’t driven before. When it comes to project file organization, these
standards will come in the form of which subdirectories are included, how
they’re organized hierarchically, and how subdirectories and files are named.</p>
<p>These standards could exist as several levels: for your discipline, for your lab
group, or even for you as an individual. For example, when people develop R
packages, the package consists of a set of files, and there is a very clear and
highly enforced standard for how these files are arranged in a directory and how
the subdirectories are named. By enforcing this standard, many different people
can create packages and have them work in a similar way. On the opposite end of
the spectrum, if there are not clear standards at the level of your discipline,
you could create a clear standard that you plan to follow either for your lab
group or even for your individual work. If you’re consistent in organizing your
files using that standard, it will make it easier to navigate files as you move
from one project to another. One article notes,</p>
<blockquote>
<p>“The key principle is to organize the compendium so that another person can
know what to expect from the plain meaning of the file and directory names.
Using widely held conventions… will help other people to understand how your
files relate to each other without having to ask you. Naming objects is
difficult to do well, so it is worth to put some effort into a logical and
systematic file naming convention if you have a complex project with many files
and directories (e.g., a multi-experiment study where each experiment has
numerous data and code files).” <span class="citation">(Marwick, Boettiger, and Mullen 2018)</span></p>
</blockquote>
<p>As an added bonus, subdirectory organization can also be used in clever ways
within code scripts applied to files in the directory. For example, there are
functions in all scripting languages that will list all the files in a specified
subdirectory. If you keep all your raw data files of a certain type (for
example, all output from running flow cytometry for the project) within a single
subdirectory, you can use this type of function with code scripts to list all
the files in that directory and then apply code that you’ve developed to
preprocess or visualize the data across all those files. This code would
continue to work as you added files to that directory, since it starts by
looking in that subdirectory each time it runs and working with all files there
as of that moment. One article notes:</p>
<blockquote>
<p>“Organizing data files into a single directory with consistent filenames
prepares us to iterate over <em>all</em> of our data, whether it’s the four example
files used in this example, or 40,000 files in a real project. Think of it this
way: remember when you discovered you could select many files with your mouse
cursor? With this trick, you could move 60 files as easily as six files. You
could also select certain file types (e.g., photos) and attach them all to an
email with one movement. By using consistent file naming and directory
organization, you can do the same programatically using the Unix shell and other
programming languages.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
</div>
<div id="what-is-a-project-template" class="section level3 hasAnchor" number="2.6.3">
<h3><span class="header-section-number">2.6.3</span> What is a project template?<a href="experimental-data-recording.html#what-is-a-project-template" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Louis Pastuer famously said that “Luck favors the prepared mind.” In file
organization, as with so much else, time spent preparing can pay off
exponentially later. In this case, the next step is to not only use a structured
directory for each project or experiment, but to start using the same,
standardized structure for every one of your projects and experiments. This
takes more work, in particular to design a structure that can be used across
many projects. However, the gains in terms of organization and efficiency can be
extraordinary. We have not run across many people who have taken the time to set
this up. However, among the people we know who have, we haven’t run across many
who ever went back once they started using this type of system.</p>
<p>This involves first designing a common template for the directory structure for
your projects. Once you have decided on a structure for this template, you can
create a version of it on your computer—a file directory with all the
subdirectories included, but without any files (or only template files you’d
want to use as a starting point in each project, like templates for data
collection and reports as presented in Modules 2.4 and 2.5). When you start a
new project, you can then just copy this template and rename it. If you are
using R and begin to use R Projects (described in the next section), you can
also create an R Studio Project template to serve as this kind of starting point
each time you start a new project.</p>
<p>In other areas of science and engineering, this idea of standardized directory
structures has allowed the development of powerful techniques for open-source
software developers to work together. For example, anyone may create their own
extensions to the R programming language and share these with others through
GitHub or several large repositories. As mentioned briefly earlier in this
module, this is coordinated by enforcing a common directory structure on these
extension “packages”—to create a new package, you must put certain types of
files in certain subdirectories within a project directory. With these
standardized rules of directory structure and content, each of these packages
can interact with the base version of R, since there are functions that can tap
into any of these new packages by assuming where each type of file will be
within the package’s directory of files.</p>
<p>In a similar way, if you impose a common directory structure across all the
project directories in your research lab, your collaborators will quickly be
able to learn where to find each element, even in projects they are new to, and
you will all be able to write code that can be easily applied across all project
directories, allowing you to improve reproducibility and comparability across
all projects by assuring that you are conducting the same preprocessing and
analysis across all projects (or, if you are conducting things differently for
different projects, that you are deliberate and aware that you are doing so).
Creating a project template that you copy and rename as you start a new
project is one way to facilitate this.</p>
<p>As you use a template for a project, you can customize it as you need. For
example, if you had included a subdirectory for flow cytometry data, but are not
running that assay in this experiment, you can remove that subdirectory.
Similarly, you can customize the report as you go to help it work well for this
specific experiment. However, you will aim to keep to the standard format as
much as possible, since it’s the standardization across projects that provides
many of the advantages.</p>
<p>In the next module, we will walk through the steps of designing a project
template that you can use across experiments for your laboratory group. In
module 2.8, we’ll walk through an example of creating and using this kind of
project template for an example set of studies.</p>
<!-- ### Practice quiz -->

</div>
</div>
<div id="module7" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> Creating project directory templates<a href="experimental-data-recording.html#module7" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Researchers can develop project directory templates to facilitate collecting research
files in a single, structured directory, with the added benefit of easy use of
version control. Researchers can gain even more benefits by consistently
structuring all their project directories. We will demonstrate how to
implement structured project directories through RStudio, as well as how RStudio
enables the creation of a ‘Project’ for initializing consistently-structured
directories for all of a research group’s projects.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Be able to designed a structured project directory template for
research projects</li>
<li>Understand how RStudio can be used to create ‘Project’ templates</li>
</ul>
<p>The last module described the advantages of organizing all the files for a
research project within a single file directory, and the added advantages of
using a consistent structure for the project directories for all of the
experiments or projects in your research group. In this module, we’ll walk
through the steps required to design and create a template for your project
directories. In particular, we’ll build on ideas from earlier modules about
creating reproducible data collection templates, as these can form key
components of the template for project directories.</p>
<p>At its most basic, a template for a project directory is a computer file
directory that includes the subdirectories (with standardized names for each
subdirectory) that you want to include—for example, you may know that you will
always want the project directory to include subdirectories for “raw_data” (with
its own subdirectories for different types of data, for example for “cfus” and
“flow”), “data” (with clean versions of the data, after conducting any needed
preprocessing, like calculating colony-forming units in a sample based on data
from plating at different dilutions, or the output from gating flow cytometry
data), “reports” (for writing, posters, and presentation slides), and “R” (for
common scripts that you use for preprocessing, visualization, and data
analysis). Creating and using a common template for your directory structure for
projects will help create consistency across projects in the directory
structure, which can facilitate the use and re-use of automated tools like code
scripts across different experiments.</p>
<p>Here are some key goals to consider when designing a template:</p>
<ul>
<li>Keeping data collection and analysis separate</li>
<li>Using simple file formats and tidy structures for recording data whenever
possible (a larger number of simple files is easier to organize and work
with than a smaller number of complex files)</li>
<li>Create reports and analysis that incorporate data from different assays
for an experiment</li>
<li>Make it easy to share all project files across the team</li>
<li>Make it easy to share project files on a public repository</li>
<li>Make it easy to track changes to files for a project</li>
</ul>
<p>To “define the problem” <span class="citation">(Osann, Mayer, and Wiele 2020)</span>:</p>
<ul>
<li>Files for the project need to be stored in a single directory</li>
<li>That directory needs to be easy to understand and navigate</li>
<li>We want to have the same structure for many different projects,
so it needs to be flexible enough to accommodate that</li>
<li>Be able to find things easily</li>
</ul>
<blockquote>
<p>“The goal of a research compendium is to provide a standard and easily
recognizable way for organizing the digital materials of a project to enable
others to inspect, reproduce, and extend the research. There are three generic
principles that define research compendia, independent of particular software
tools, and disciplinary contexts. 1. A research compendium should organize its
files according to the prevailing conventions of the scholarly community,
whether that be an academic discipline or a lab group. Following these
conventions will help other people recognize the structure of the project, and
also support tool building which takes advantage of the shared structure. 2. A
research compendium should maintain a clear separation of data, method, and
output, while unambiguously expressing the relationship between those three. In
practice, this means data files must be separate from code files. This is
important to let others easily identify how the original researcher operated on
the data to generate the results. Keeping data and method separate treats the
data as ‘read-only,’ so that the original data are untouched and all
modifications are transparently documented in the code. The output files should
be considered as disposable, with a mindset that one can always easily
regenerate the output using the code and data. The relationship between which
code operates on which data in which order to produce which outputs must be
specified as well. In his advice to industry data scientists, Ben Baumer’s
article in this collection similarly highlights the importance of keeping data
separate from the presentation of data, or research outputs. 3. A research
compendium should specify the computational environment that was used for the
original analysis. At its most basic, this could be a plain text file that
includes a short list of the names and version numbers of the software and other
critical tools used for the analysis. In more complex approaches, described
below, the computational environment can be automatically preserved or
reproduced as well.” <span class="citation">(Marwick, Boettiger, and Mullen 2018)</span></p>
</blockquote>
<div class="figure"><span style="display:block;" id="fig:templatedirectory"></span>
<img src="figures/project_template_directory.png" alt="A research group can create a file directory that will serve as a template for all the experiments of a certain type in your laboratory. The template can include templates of files for data recording and for generating reports. To start recording data for a new experiment, a researcher can copy and rename this template directory." width="\textwidth" />
<p class="caption">
Figure 2.23: A research group can create a file directory that will serve as a template for all the experiments of a certain type in your laboratory. The template can include templates of files for data recording and for generating reports. To start recording data for a new experiment, a researcher can copy and rename this template directory.
</p>
</div>
<p>Designing a project template will include two parts—first, designing a
<em>conceptual</em> template for your file organization and, second, creating a
<em>physical</em> implementation of that concept. The conceptual template will
develop a structure for how you’ll organize and name files within a project
directory. The physical template will use these ideas to develop a blank file
directory that follows that organization and that you can copy, paste, and
adapt each time you start a new project.</p>
<p>This is a process of <em>designing</em>, and so you will find it helpful to follow
principles that facilitate the design process. For example, as you design, it’s
useful to keep in mind the resources that you have available <span class="citation">(Osann, Mayer, and Wiele 2020)</span>.
In this case, the key resources are the file directory and organization system,
as well as simple file types, that are available through modern
computer operating systems. Many of these are based on the Unix file directory
system and file designs.</p>
<div id="designing-a-project-template" class="section level3 hasAnchor" number="2.7.1">
<h3><span class="header-section-number">2.7.1</span> Designing a project template<a href="experimental-data-recording.html#designing-a-project-template" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One critical step in the design process is to iterate: make a first version of
something (a prototype) and then try it out to see how it works, then revise and
improve based on what you find out in practice <span class="citation">(Osann, Mayer, and Wiele 2020)</span>. In this case,
that means creating a project directory template, but considering it a first
draft until you try it in practice. You can refine this template once you’ve
tried it and identified where it works and where it doesn’t.</p>
<p>Before you open your computer to make a “physical” template, you should design
it. This involves deciding what types of data will go into a project directory,
how those files will be structured within the directory, the naming conventions
for files, and so on. In other words, you should create a blueprint from
your template before you make it on your computer.</p>
<p>The step of creating a blueprint for your project directory template is a design
process. As you work on it, you will want to prioritize how the template will
fit the needs of the user—your research group. One of the key early steps in
the design process is to observe <span class="citation">(Osann, Mayer, and Wiele 2020)</span>. This lets you get a
firmer idea of what problem you’re trying to solve and what is needed to solve
this. Another early step is to synthesize based on what you’ve observed
<span class="citation">(Osann, Mayer, and Wiele 2020)</span>. This allows you to think about a variety of needs and
prioritize and refine them so that you have a very clear criteria to determine
if what you design is successful in addressing the problem you were trying to
solve.</p>
<p>As you decide how to organize project files within a directory, it is worthwhile
to take some time to think about the types of files that are often generated by
your research projects, because there are also big advantages to creating a
standard structure of subdirectories that you can use consistently across the
directories for all the projects in your research program. Of course, some
projects may not include certain files, and some might have a new or unusual
type of file, so you can customize the directory structure to some degree for
these types of cases, but it is still a big advantage to include as many common
elements as possible across all your projects.</p>
<p>One of the best ways to get an idea of what your research
group needs within a project directory is to take a survey of past research
projects from your group. Make a list of what types of data were collected and
what types of preprocessing and analysis were done using those data. For each
type of data, it’s helpful to make a note of the file type it’s usually stored
in and the typical size of the files. How are data for a specific assay divided
across files? Are the data for all animals and all timepoints included in a
single spreadsheet file? If so, are they saved in the same sheet, or divided
across sheets? Conversely, are different files used for the data from different
animals or different time points?</p>
<p>This is also a good stage to diagnose if there are data collection files that
are not successful in separating data collection from data preprocessing and
analysis (module 2.1). As you progress, you may also want to add templates that
serve as a starting point for data collection files and report files within this
project. For example, if you always want to collect observed data in a standard
way, you could create a template for data collection, for example as a CSV file.
This idea of creating data collection templates is described in detail in
modules 2.4 and 2.5.</p>
<p>With a data collection template for collecting a certain type of data, each
researcher in your lab could copy and rename this file each time they collect a
new set of data—by ensuring a common structure when collecting the data,
including file format, column names, and so on, you can build code scripts that
will work on data collected for all your experiments. You may also have some
standard reports that you want to create with types of data you commonly
collect, and so you could include templates for those reports in your R Project
template. Again, these can be copied and adapted within the project—the
template serves as a starting point so you don’t have to start with a blank
slate with every project, but it is not restrictive and can be adapted to each
project as you work on that project.</p>
<p>In addition to the data that you record in the laboratory by hand, you may also
typically have data that’s generated and recorded by laboratory equipment. For
example, the type of study may often include data collected from flow cytometry,
to measure certain cell populations in samples, or from mass spectometry, to
measure levels of certain molecules. For these data, the recording format will
typically be determined by the equipment, and so you won’t need to create data
collection templates for the data. However, you should store these data files in
your project directory as well, where they are easy to access and integrate with
other data as you analyze the data for the study.</p>
<p>The recorded data files and the files that come directly from equipment can all
be considered raw data files. In addition, you may typically create some files
with pre-processed data. For example, if you have flow cytometry data, you may
initially get large files in a format specific to flow cytometry (“fcs” files)
from the equipment. You may use a program to pre-process these files, for
example, to manually gate the data to identify specific cell populations. In
addition to saving the raw data files directly from the flow cytometer, you’ll
also want to save the processed data files in your project directory, since
these are the files that you’ll analyze and integrate with other data from the
project.</p>
<p>You could create
subdirectories both for the raw data and for the processed data that result from
pre-processing steps. For example, you might want to store the raw fcs files
within a subdirectory called “data_raw” and the processed (gated) csv files
within a subdirectory called “data”.</p>
<p>With the previous steps, you will have determined the types of files you
normally have for this type of study, as well as structured the project
directory to organize these files. The next step is to create a template report.
You can create this using tools for reproducible reports—in R, a key tool for
this is RMarkdown. Here, we’ll cover using this tool for creating a report
briefly, but there are many more details in modules 3.7 through 3.9. Briefly,
RMarkdown allows you to include both code and text meant for humans within a
single, plain text document. This document can then be rendered, a process that
executes the code and formats the text meant for humans, producing a document in
an easy-to-read format like Word or PDF.</p>
<p>Once you have determined the types of files that you’ll normally include in your
project, you can decide how to organize them into subdirectories in a project
file directory. You want to design a structure that will be clear and also
easy to work within, as you continue work on your project. As Adam Savage
notes:</p>
<blockquote>
<p>“Not all organizational methodologies are created equal. One could be spotlessly
organized, with everything put away and labeled and color coded, and it could feel
like a prison with the walls closing in around you. Another could be equally organized
but a bit more open and exposed, and it could untap creative genius like no other space
you’ve worked in.” <span class="citation">(Savage 2020)</span></p>
</blockquote>
<p>As you do decide how to organize a project directory into subdirectories, it
will be helpful to have example or template files for each file type. For data
that you will record yourself, these can be the templates that you developed to
collect the data in a tidy format (modules 2.3 through 2.5), while for data from
equipment, these can just be one or more example files from the equipment that
you have collected for a past project. Having these example files will help you
to develop a template project report that can input the type of data that you
typically collect for this type of project.</p>
<div class="figure"><span style="display:block;" id="fig:projecttemplatecomplex"></span>
<img src="figures/project_template_morecomplex.png" alt="Example of a more complex project directory structure that could be created, with directories added to store data collected through flow cytometry and single cell RNA sequencing." width="\textwidth" />
<p class="caption">
Figure 2.24: Example of a more complex project directory structure that could be created, with directories added to store data collected through flow cytometry and single cell RNA sequencing.
</p>
</div>
<p>When you create a project directory template, we recommend that you create a
subdirectory named something like “reports” to use to store any Rmarkdown report
files for the project. This organization will make it clear where you’ve stored
your reports in the project directory. You’ll be able to use file and directory
pathnames to access all the data in the project, so it will be easy to use the
study’s data in the report even if they’re in separate subdirectories.</p>
<p>In general, as you design the structure of subdirectories, as well as pick
naming conventions for both subdirectories and files, keep in mind that a key
aim is to create a structure that is general enough that you can use it
consistently for many projects, but also clear enough that you can quickly
find things within the directory. Adam Savage describes a similar aim when
he designs the layout and organization of his shops:</p>
<blockquote>
<p>“What truly unifies my shops, especially as I got more experienced, is that they are
each built on two, simple philosophical pillars: 1) I want to be able to see everything
easily; and 2) I want to be able to reach everything easily.” <span class="citation">(Savage 2020)</span></p>
</blockquote>
<p>As you design, keep in mind that you want to organize and to do it consistently:</p>
<ul>
<li>Put all code, data, and output for a project in a single directory</li>
<li>Organize those directories into subdirectories</li>
<li>Name those subdirectories consistently</li>
<li>Use meaningful file names</li>
<li>Remove unnecessary files</li>
</ul>
<p>A number of researchers have put a lot of thought into how to organize project
directories for scientific research.</p>
<p>One paper suggests structuring research project
files as R packages, with the following subdirectories in a project
directory <span class="citation">(Vuorre and Crump 2021)</span>:</p>
<ul>
<li>data: Processed R datasets</li>
<li>data-raw: Raw starting data, in any machine-readable format</li>
<li>docs</li>
<li>experiments: I think this is to write surveys, etc., that would run online?</li>
<li>man: Help files for the R functions</li>
<li>manuscript: Paper for the project, starting in a mark-up language and rendering to pdf</li>
<li>model: Code for running models, can be rendered to pdf or HTML</li>
<li>posters: Code for creating posters with RMarkdown (posterdown), renders to pdf or HTML</li>
<li>R: Code for project-specific functions</li>
<li>slides: Code for creating slides with RMarkdown, render to HTML</li>
<li>vignettes</li>
</ul>
<p>Here is the suggestion from another paper <span class="citation">(Marwick, Boettiger, and Mullen 2018)</span>:</p>
<blockquote>
<p>“An ideal package-based file organization for a more complex project would look like this:</p>
</blockquote>
<blockquote>
<p>. A README.md file that describes the overall project and where to get started. It can be helpful to include graphical summary of the interlocking pieces of the project.</p>
</blockquote>
<blockquote>
<p>. Script files with reusable functions go in the R/ directory. If these functions are documented using Roxygen, then the documentation will be automatically generated in a man/ directory.</p>
</blockquote>
<blockquote>
<p>.Raw data files are kept in the data/ directory. If your data are very large, or streaming, an alternative is to include a small-sample dataset so that people can try out the techniques without having to run very expensive computations.</p>
</blockquote>
<blockquote>
<p>. Analysis scripts and reports files go in the analysis/ directory. In many cases it can be useful to give the analysis scripts ascending names, for example 001-load.R, 002-clean.R etc. This kind of file-naming helps with organisation, but it does not capture the full tree of dependencies in the way a Makefile or an R Markdown file does. To manage more complex workflows, the analysis/ directory could include either an R markdown file, a Makefile or a Makefile.R file. These files are important because they control the order of the code execution. In more complex projects, careful use of caching or a Makefile can save time by only running code that has not changed since it was last run.</p>
</blockquote>
<blockquote>
<p>. A DESCRIPTION file in the project root provides formally structured, machine- and human-readable information about the authors, the project license, the software dependencies, and other metadata of the compendium. When a DESCRIPTION file is included along with the other items above, then the compendium is also a formal, installable R package. When your compendium is an R package, you can take advantage of many time-saving tools for package development, testing, and sharing (e.g., the devtools package that we noted above). R’s built-in citation() function can use that metadata, along with references to any publications that result from the project, to provide users with the necessary information to cite your work.</p>
</blockquote>
<p>They provide some more details on some subdirectories:</p>
<blockquote>
<p>“The R/ directory contains custom functions that are used repeatedly throughout the project. The man/ directory contains the manual (i.e., documentation) for the use of the functions.” <span class="citation">(Marwick, Boettiger, and Mullen 2018)</span></p>
</blockquote>
<p>Another project (workflowr) suggests the following subdirectories <span class="citation">(Blischak, Carbonetto, and Stephens 2019)</span>:</p>
<ul>
<li>analysis: Rmd files with code to visualize, model, etc.</li>
<li>code: “intended for longer-running scripts, compiled code (e.g., C++)
and other source code supporting the data analysis” <span class="citation">(Blischak, Carbonetto, and Stephens 2019)</span></li>
<li>data: “for storing raw data files” <span class="citation">(Blischak, Carbonetto, and Stephens 2019)</span></li>
<li>docs: Generated results from Rmd files (HTMLs that can be posted online)</li>
<li>output: “for saving processed data files and other outputs generated by
the scripts and analyses” <span class="citation">(Blischak, Carbonetto, and Stephens 2019)</span></li>
</ul>
<p>They note that you want a directory setup that is “flexible and configurable”
<span class="citation">(Blischak, Carbonetto, and Stephens 2019)</span>.</p>
<p>Another paper notes some of the key characteristics that can help make
a project reproducible:</p>
<blockquote>
<p>“Using this literature as a guideline, we identify several key features of reproducible work. These recommendations are a matter of opinion—due to the lack of agreement on which components of reproducibility are most important, we select those that are mentioned most often, as well as some that are mentioned less but that we view as important.
1. A well-designed file structure:
1. a. Separate folders for different file types.
1. b. No extraneous files.
1. c. Minimal clutter.
2. Good documentation:
2. a. Files are clearly named, preferably in a way where the order in which they should be run is clear.
2. b. A README is present.
2. c. Dependencies are noted.
2. d. Code files contain descriptive comments.
3. Reproducible file paths:
3. a. No absolute paths, or paths leading to locations outside of a project’s directory, are used in code—only portable (relative) paths.
[Others not related to directory structure].” <span class="citation">(Bertin and Baumer 2021)</span></p>
</blockquote>
<p>One R package, designed to set-up a project directory structure, suggests
including a README file not only in the top-level directory, but also in
each subdirectory <span class="citation">(Johnston 2022)</span>. That project incorporates the following
subdirectories:</p>
<ul>
<li>R, “Should contain the R scripts and functions used for the analysis”</li>
<li>data, “If relevant, is where the processed (or simulated) data that is
used for the project as well as the results of the project’s analysis”</li>
<li>data-raw “If relevant, is where the scripts that process the raw data
into the usable data are kept and, optionally where the raw data is
also kept”</li>
<li>doc, “Should contain the files related to presenting the project’s
scientific output. Already has the report / manuscript inside”. Can
include the report, but also slides and other reporting output.</li>
</ul>
<blockquote>
<p>“Within a given project, I use a top-level organization that is logical, with chronological organization at the next level, and logical organization below that. A sample project, called msms, is shown in Figure 1. At the root of most of my projects, I have a data directory for storing fixed data sets, a results directory for tracking computational experiments peformed on that data, a doc directory with one subdirectory per manuscript, and directories such as src for source code and bin for compiled binaries or scripts. Within the data and results directories, it is often tempting to apply a similar, logical organization. For example, you may have two or three data sets against which you plan to benchmark your algorithms, so you could create one directory for each of them under data. In my experience, this approach is risky, because the logical structure of your final set of experiments may look drastically different from the form you initially designed. This is particularly true under the results directory, where you may not even know in advance what kinds of experiments you will need to perform. If you try to give your directories logical names, you may end up with a very long list of directories with names that, six months from now, you no longer know how to interpret.
Instead, I have found that organizing my data and results directories chronologically makes the most sense. Indeed, with this approach, the distinction between data and results may not be useful. Instead, one could imagine a top-level directory called something like experiments, with subdirectories with names like 2008-12-19. Optionally, the directory name might also include a word or two indicating the topic of the experiment therein. In practice, a single experiment will often require more than one day of work, and so you may end up working a few days or more before creating a new subdirectory. Later, when you or someone else wants to know what you did, the chronological structure of your work will be self-evident.
Below a single experiment directory, the organization of files and directories is logical, and depends upon the structure of your experiment. In many simple experiments, you can keep all of your files in the current directory. If you start creating lots of files, then you should introduce some directory structure to store files of different types. This directory structure will typically be generated automatically from a driver script, as discussed below.” <span class="citation">(Noble 2009)</span></p>
</blockquote>
<p>This project also incorporates “TODO.md” in the top level of the project
directory, as well as a “DESCRIPTION” file that “includes metadata about
your project, in a machine readable format, and that also stores a list
of the R packages your project depends on” <span class="citation">(Johnston 2022)</span>.</p>
<blockquote>
<p>“<strong>Centralize the location of the raw data files and automate the derivation of
intermediate data.</strong> Store the input data on a centralized file server that is
profesionally backed up. Mark the files as read-only. Have a clear and linear
workflow for computing the derived data (e.g., normalized, summarized, transformed,
etc.) from the raw files, and store these in a separate directory. Anticipate that
this workflow will need to be run several times, and version it. Use the
<code>BiocFileCache</code> package to mirror these files on your personal computer.
[footnote: A more basic alternative is the rsync utility. A popular solution offered
by some organizations is based on ownCloud. Commercial options are Dropbox,
Google Drive and the like].” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
</div>
<div id="creating-and-using-a-project-template" class="section level3 hasAnchor" number="2.7.2">
<h3><span class="header-section-number">2.7.2</span> Creating and using a project template<a href="experimental-data-recording.html#creating-and-using-a-project-template" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Once you have a blueprint for a template for your project directories, you can
create this template as a directory on your computer. This will serve as a
prototype to test. A prototype doesn’t have to be fully refined, just built out
enough that the users can test it out <span class="citation">(Osann, Mayer, and Wiele 2020)</span>.</p>
<p>This process is, once you have designed the template, very easy. It involves no
fancy tools—in fact, it’s so straightforward that at first it might seem too
simple to be useful. For this basic approach, you will create an example file
directory that includes template files and that captures you desired project
directory structure. When you are ready to start a new project, you will copy
this template, rename the copy to be specific to the new project, and then use
this directory to store and work with the data you collect for the project.</p>
<p>This process, therefore involves creating a basic file
directory with the desired template files and file directory structure and then
copying this file directory every time you want to start a new project for a
study in this set of studies. When you are ready to start a new project, you
will copy this template, rename the copy to be specific to the new project, and
then use this directory to store and work with the data you collect for the
project.</p>
<p>Within your templates, you may find it useful to include “placeholders”. Instead
of leaving the areas where data will be recorded blank, you can put in examples
that show the format of how the data should be collected. By using a color for
these placeholders, you can clarify that they are meant to be erased and replaced
with the real data once a person starts using the template.</p>
<p>Figure <a href="experimental-data-recording.html#fig:replacingplaceholdertreatment">2.25</a> gives an example of how
placeholders can work in a data collection template that’s included in a
project directory template.</p>
<div class="figure"><span style="display:block;" id="fig:replacingplaceholdertreatment"></span>
<img src="figures/project_replacing_placeholder_treatment_data.png" alt="The template includes a file with experiment metadata, with a sheet for recording the details of each treatment. A user can open this file and replace the placeholder values (in red) with real values for the treatments in the experiment. By changing the text color to black, the user can have a visual confirmation that the placeholder data have been replaced with real study data." width="\textwidth" />
<p class="caption">
Figure 2.25: The template includes a file with experiment metadata, with a sheet for recording the details of each treatment. A user can open this file and replace the placeholder values (in red) with real values for the treatments in the experiment. By changing the text color to black, the user can have a visual confirmation that the placeholder data have been replaced with real study data.
</p>
</div>
<p>Here are some specific steps to create the files for the project directory
template:</p>
<ol style="list-style-type: decimal">
<li>Review the list of data you typically collect or files you create for that
type of study or experiment, a list you created when designing the blueprint for
the directory template</li>
<li>Create template files for any data collection that is typical for that type
of study or experiment. Use example or placeholder data to create examples of
those files.</li>
<li>Create a directory structure that divides the types of files into
subdirectories of similar types.</li>
<li>Create one or more templates of report files that access and report on the
data in the project template</li>
</ol>
<p>In modules 2.4 and 2.5, we showed how you can create tidy data collection
templates to use to collect data, and how these can be paired with reproducible
reporting tools to separate the steps of data collection and reporting (modules
3.7 through 3.9 go into much more depth on these reproducible reporting tools).
Once you have decided on the types of data that you will usually collect for the
type of study that this template is for, you can use that process to create tidy
data collection templates for each type of data.</p>
<p>Once you set up this template, a researcher in your group can initialize a project
for a new experiment by copying the template directory and renaming it to the name
of the experiment. They can then open the directory and replace any placeholder
data in the project files with real data from the experiment.</p>
<p>Figure <a href="experimental-data-recording.html#fig:basicprojecttemplateuse">2.26</a> gives a basic walk-through of the
simple steps you’ll use to start a new project directory once you’ve created
this type of template (we will cover this example in much more detail in the
next module). First, you will find the project directory template in your
computer’s file system, copy it to where you’d like to save the files for the
new project, and rename the directory to your new project’s name. Next, you’ll open
the data collection template files and replace the placeholder example data in
the template (shown in red font) with the real data from your study. The
placeholder data can help you remember the format you should use to record the
real data. Finally, once you’ve recorded the data for the study or experiment,
you can open the example report template file. If you’ve designed this report
template well, it should run with the new data you’ve recorded to create a
report for the experiment. At this stage, you can add to the report or customize
it for the new project by changing the Rmarkdown file and re-rendering it to
update the report.</p>
<div class="figure"><span style="display:block;" id="fig:basicprojecttemplateuse"></span>
<img src="figures/project_template_basic_use.png" alt="Steps in using a basic project directory template that you have created for a type of study or experiment." width="\textwidth" />
<p class="caption">
Figure 2.26: Steps in using a basic project directory template that you have created for a type of study or experiment.
</p>
</div>
<p>This template is not restrictive—it serves as a starting point, but it can
be adapted for each specific project. For example, if you are collecting
data from an assay that you have not used in past experiments, you can add
a new data subdirectory to your project directory to use for storing that new
type of data. However, you do want to keep a balance, where you avoid
unneeded changes to the project template within each specific project’s
directory. This is because many of the benefits of standardizing (e.g.,
knowing where things are, building tools that leverage the standardized
directory structure) are lost as the directories for specific projects grow
to be more and more different from each other.</p>
<p>The report template is included in the project directory template, so it will be
copied and available for you to use anytime you start a new project using that
template. However, you are not obligated to keep the report identical to the
template. Instead, the template report serves as a starting point, and you can
add to it or adapt it as you work on a study.</p>
</div>
<div id="project-directories-as-rstudio-projects" class="section level3 hasAnchor" number="2.7.3">
<h3><span class="header-section-number">2.7.3</span> Project directories as RStudio Projects<a href="experimental-data-recording.html#project-directories-as-rstudio-projects" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If you are using the R programming language for data preprocessing, analysis,
and visualization—as well as RMarkdown for writing reports and
presentations—then you can use RStudio’s “Project” functionality to make it
even more convenient to work with files within a research project’s directory.
You can make any file directory a “Project” in RStudio by chosing “File” -&gt;
“New Project” in RStudio’s menu. This gives you the option to create a
project from scratch or to make an existing directory and RStudio Project.</p>
<p>When you make a file directory an RStudio Project, it doesn’t change much in
the directory itself except adding a “.RProj” file. This file keeps track of
some things about the file directory for RStudio, including preferred settings
for RStudio to use when working in that project. Also, when you
open one of these Projects in RStudio, it will move your working directory
into that projects top-level directory. This makes it very easy and practical
to write code using relative pathnames that start from this top-level of the
project directory. This is good practice, because these relative pathnames
will work equally well on someone else’s computer, whereas if you use file
pathnames that are absolute (i.e., giving directions to the file from the root
directory on your computer), then when someone else tries on run the code on their
own computer, it won’t work and they’ll need to change the filepaths in the code,
since everyone’s computer has its files organized differently. For example, if you,
on your personal computer, have the project directory stored in your “Documents”
folder, while a colleague has stored the project directory in his or her “Desktop”
directory, then the absolute filepaths for each file in the directory will be
different for each of you. The relative pathnames, starting from the top level of
the project directory, will be the same for both of you, though, regardless of
where you each stored the project directory on your computer.</p>
<p>There are some other advantages, as well, to turning each of your research
project directories into RStudio Projects. One is that it is very easy to
connect each of these Projects with GitHub, which facilitates collaborative work
on the project across multiple team members while tracking all changes under
version control. This functionality is described in modules 2.9 through 2.11.</p>
<p>Having your project directories as R Projects makes it easy to navigate among
different projects. When you close RStudio and reopen it, it will automatically
open in the last Project you had open. There is a small tab in the top right
hand corner of the RStudio window that lists the project you are currently in.
To move to a different Project, you can click on the down arrow beside this
project name. There will be a list of your most recent projects, as well as
options to open any Project on your computer. If you want to work in RStudio,
but not in any of the Projects, you can choose to “Close Project”.</p>
<p>When you are working in an RStudio Project, RStudio will automatically move your
working directory to be the top-level directory of the Project directory. This
makes it easy to write code that uses this directory as the presumed working
directory, using relative file paths to identify and files within the directory.
Further, if you share the project directory with someone else, they
can similarly open the RStudio Project in their own version of RStudio, and all
the relative pathnames to files should work on their system without any problems.
This feature helps make code in an RStudio Project directory reproducible across
different people’s computers.</p>
<p>The RStudio Project environment has some other features, as well, that may be
useful for some projects. For example, if you are tracking the project directory
under the git version control system, then when you open the RStudio Project,
there will be a special tab in one of the panes to help in using git with the
project. This tab provides a visual interface for you to commit changes you’ve made,
so they are tracked and can be reversed if needed, and also so you can easily
push and pull these committed changes to and from a remote repository, like a
GitHub repository, if you are collaborating with others.</p>
</div>
<div id="creating-project-templates-in-rstudio" class="section level3 hasAnchor" number="2.7.4">
<h3><span class="header-section-number">2.7.4</span> Creating ‘Project’ templates in RStudio<a href="experimental-data-recording.html#creating-project-templates-in-rstudio" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As you continue to use R and RStudio’s Project functionality, you may want to
take the template directory for your project and create an RStudio Project
template based on its structure. Once you do, when you start a new research
project, you can create the full directory for your project’s files from within
RStudio by going to “File” -&gt; “New Project” and then choosing to create a new
project based on that template. The new project will already be set up with the
“.RProj” file that allows you to easily navigate into and out of that project,
to connect it to GitHub, and all the other advantages of setting a file
directory as an RStudio Project. This takes a bit of time to set-up, but can
be a powerful tool in ensuring that researchers in your laboratory use a
standardized format for project directories across many experiments.</p>
<p>When you create a new project in R, you will have the option to use any of
the available project templates currently downloaded to your copy of R
<span class="citation">(<span>“<span>RStudio Project Templates</span>”</span> 2021)</span>. To create a new project, go to the “File” menu
in the top menu bar in RStudio, and then choose “New Project”. This will open
a pop-up box like the one shown in Figure <a href="experimental-data-recording.html#fig:createnewproject">2.27</a>.</p>
<div class="figure"><span style="display:block;" id="fig:createnewproject"></span>
<img src="figures/create_new_project.png" alt="Creating a new project in RStudio. When you chose 'File' then 'New Project' from the RStudio menu, it opens the New Project Wizard shown here. You have the option to create a new project that is not based on a project template by selecting 'New Project'. You also have the chance to create a project using a template by selecting one of the templates. The listed templates will depend on which packages you have downloaded for your copy of R. For example, here the `bookdown` package has been installed for the local copy of R, and so a template is available for 'Book Project using bookdown'." width="\textwidth" />
<p class="caption">
Figure 2.27: Creating a new project in RStudio. When you chose ‘File’ then ‘New Project’ from the RStudio menu, it opens the New Project Wizard shown here. You have the option to create a new project that is not based on a project template by selecting ‘New Project’. You also have the chance to create a project using a template by selecting one of the templates. The listed templates will depend on which packages you have downloaded for your copy of R. For example, here the <code>bookdown</code> package has been installed for the local copy of R, and so a template is available for ‘Book Project using bookdown’.
</p>
</div>
<p>This pop-up contains the New Project Wizard in RStudio. Here, you can either
create a new Project without using a template (click on “New Project”) or you
can create a Project starting from a template. The templates available in your
copy of R will be listed below the “New Project” listing. Depending on which
packages you’ve installed for your copy of R, you will have different choices of
project templates available, as project templates tend to be created and shared
within R packages <span class="citation">(<span>“<span>RStudio Project Templates</span>”</span> 2021)</span>. In the example shown in
Figure <a href="experimental-data-recording.html#fig:createnewproject">2.27</a>, for example, one of the template options is
for a “Book Project using bookdown”, available because the <code>bookdown</code> R package
has been installed locally.</p>
<p>Your research group can create your own Project templates. This will allow
you to use a standard template for your projects, just like we showed in the
last section. However, instead of needing to copy, paste, and rename the template
each time, if you create an official RStudio Project template, then the researcher
can chose to use this template under the “New Project” option in RStudio
(Figure <a href="experimental-data-recording.html#fig:exampleprojectwizard">2.28</a>).</p>
<div class="figure"><span style="display:block;" id="fig:exampleprojectwizard"></span>
<img src="figures/project_example_project_template.png" alt="To make it easier for members of a group to use a project template, the group can create an official R template for the type of project. Once this type of template is created, a user can access it as a choice when creating a new R Project from RStudio. When doing so, a box will pop up with options for setting up the project. In this example, the user can specify the members of the research team and indicate if the experiment will include data from flow cytometry or single cell RNA-sequencing, in which case the Project will include subdirectories to store these types of data, as well as data recorded in the laboratory." width="\textwidth" />
<p class="caption">
Figure 2.28: To make it easier for members of a group to use a project template, the group can create an official R template for the type of project. Once this type of template is created, a user can access it as a choice when creating a new R Project from RStudio. When doing so, a box will pop up with options for setting up the project. In this example, the user can specify the members of the research team and indicate if the experiment will include data from flow cytometry or single cell RNA-sequencing, in which case the Project will include subdirectories to store these types of data, as well as data recorded in the laboratory.
</p>
</div>
<p>To create your own Project template that can be used in this way, you will need to
create them within an R package, but this package does not need to be posted to
a public site like CRAN. Instead, it can be shared exclusively among the
research group as a zipped file that can be installed directly from source onto
each person’s computer. Alternatively, you can post the package code as a GitHub
repository, and there are straightforward tools for installing R package code
from GitHub onto each team member’s computer. RStudio has provided a
detailed guide to creating your own project template at
<a href="https://rstudio.github.io/rstudio-extensions/rstudio_project_templates.html" class="uri">https://rstudio.github.io/rstudio-extensions/rstudio_project_templates.html</a>.
This topic has also been discussed through a short talk at the
yearly RStudio::conf: <a href="https://rstudio.com/resources/rstudioconf-2020/rproject-templates-to-automate-and-standardize-your-workflow/" class="uri">https://rstudio.com/resources/rstudioconf-2020/rproject-templates-to-automate-and-standardize-your-workflow/</a>.</p>
</div>
<div id="discussion-questions" class="section level3 hasAnchor" number="2.7.5">
<h3><span class="header-section-number">2.7.5</span> Discussion questions<a href="experimental-data-recording.html#discussion-questions" class="anchor-section" aria-label="Anchor link to header"></a></h3>

</div>
</div>
<div id="module8" class="section level2 hasAnchor" number="2.8">
<h2><span class="header-section-number">2.8</span> Example: Creating a project template<a href="experimental-data-recording.html#module8" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will walk through a real example, based on the experiences of one of our
Co-Is, of establishing the format for a research group’s ‘Project’ template,
creating that template using RStudio, and initializing a new research project
directory using the created template. This example will be from a
laboratory-based research group that studies the efficacy of tuberculosis drugs
in a murine model.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Create a ‘Project’ template in RStudio to initialize consistently-formatted
‘Project’ directories</li>
<li>Initialize a new ‘Project’ directory using this template</li>
</ul>
<p>For this module, we’ll show how to create an R Project template to manage data
from an example set of projects. We will walk through the process of creating a
project directory template that could be used to manage and analyze data from
any of the specific studies in this set of studies.</p>
<p>The full directory of files for this example can be found at
<a href="https://github.com/geanders/example_for_improve_repro" class="uri">https://github.com/geanders/example_for_improve_repro</a> [need to make public],
where you can download them or explore them online. All files for this project
can be stored within a well-designed directory, and this directory can be
enhanced into something called an R Project very easily. In this module, we’ll
explore how to use an R Project and what advantages it offers compared to other
ways of organizing the files associated with a study. In particular, we’ll build
on ideas from earlier modules about creating reproducible data collection
templates, as in this example, the use of a common template across many studies
in a set makes it very easy to create and apply a common reporting template to
the data, easily creating a reproducible report for each of the nineteen studies
in the example set of studies. Further, we’ll look at how this organization
allows not only for reporting on specific studies in a reproducible way, but
also makes it easier to create an overall report that combines results and
details from all studies in the set.</p>
<div id="description-of-the-example-set-of-studies" class="section level3 hasAnchor" number="2.8.1">
<h3><span class="header-section-number">2.8.1</span> Description of the example set of studies<a href="experimental-data-recording.html#description-of-the-example-set-of-studies" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As a motivating example, we’ll use an example based on a set of real immunology
experiments. This example highlights how a research laboratory will often
conduct a similar type of experiment many times, so it lets us demonstrate how
the design of the project’s files within a project directory can be reused
across similar experiments. It will allow us to show you how you can move from
designing a file directory for a single experiment to designing one that can be
used repeatedly, and then how you can take advantage of consistency in the
directory structure across projects to make tools and templates that can be
reused. [Get reference for these studies]</p>
<p>This example set of studies covers a group of studies to explore novel
treatments for tuberculosis. While treatments exist for tuberculosis, the
current treatment regime is lengthy and involves a combination of multiple
drugs. If the treatment is not completed, it can result in the development and
spread of drug-resistant tuberculosis strains, and so the treatment is sometimes
required to be done under observation <span class="citation">(Barry and Cheung 2009)</span>. If the patient has a
strain of tuberculosis that is resistant to some of the first-line drugs, they
need to be treated with second-line drugs, which can have serious side effects
<span class="citation">(Barry and Cheung 2009)</span>. There is a critical need to develop more candidate drugs
against this disease, given all the limitations and struggles of the current
treatment regime.</p>
<p>Each study investigates how mice that are challenged with tuberculosis respond
to different treatments, both in terms of how well they handle the treatment
(assessed by checking if their weight decreases notably while on treatment) and
also how well the treatment manages to limit the growth of tuberculosis in the
mouse’s lungs.</p>
<p>These example studies were conducted with similar designs and similar
goals—all aimed to test candidate treatments for tuberculosis. Most studies in
this set tested one or more treatments as well as one or more controls. The
controls could include negative controls, like saline solution, or positive
controls, like a drug already in use to treat the disease, isoniazid. A few of
the studies tested only controls, to help in developing baseline expectations
for things like the bacterial load in different mouse strains used in studies in
the set. The set of studies tested some treatments that were monotherapies (only
one drug given to the animal) as well as some that were combinations of two or
three different drugs. For many of the drugs that were tested, they were tested
at different doses and, in some cases, different methods of delivery or
different mouse models.</p>
<p>Each of the treatments were given to several mice that had been infected with
<em>Mycobacterium tuberculosis</em>. During the treatment, the mice were weighed
regularly. This weight measurement helps to determine if a particular treatment
is well-tolerated by the animals—if not, it may show through the treated mice
losing weight during treatment. For convenience, the mice were not weighed
individually. Instead, mice with the same treatment were kept in a single cage,
and the entire cage was weighed, the weight of the cage itself factored out, and
the average weight of mice for that treatment determined by dividing the weight
of all mice in the cage by the number of mice in the cage. After a period of
time, the mice were sacrificed and one lobe from their lungs was used to
determine each mouse’s bacterial load, through plating the material from the
lobe and counting the colony forming units (CFUs). One aim of the data analysis
is to compare the bacterial load of mice under various treatments to the
bacterial load of mice in the control group.</p>
<p>The full set of studies included 19 different studies. These were conducted at
different times, but the data for all of the studies can be collected using a
common format, and we’ll talk about how both data collection templates and a
project directory template could be designed to accomodate these experiments.</p>
<p>In this module, as well as the following two, we’ll be exploring how you can use
RStudio’s Project functionality to organize data from one or more studies. We’ll
particularly focus on how, by using a common format for data collection, you can
create tools that can be used repeatedly for different experiments to ensure
that methods are the same across all studies of a similar type, as well as to
improve the reproducibility of the studies.</p>
</div>
<div id="step-1-survey-of-data-collected-for-the-projects" class="section level3 hasAnchor" number="2.8.2">
<h3><span class="header-section-number">2.8.2</span> Step 1: Survey of data collected for the projects<a href="experimental-data-recording.html#step-1-survey-of-data-collected-for-the-projects" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The first step in developing a project template is to take a survey of the
typical types of files that are included in your research projects. Let’s walk
through the types of data that were collected for each study. First, there was
some metadata recorded for each study. Figure <a href="experimental-data-recording.html#fig:metadata">2.29</a> gives an
example. This includes information about the strain of mouse that was used in
the study, treatment details (including the method of giving the drug or drugs,
how often they were given each week, and for how many weeks), how much bacteria
the animals were exposed to (measured both in terms of the inoculum they were
given and their bacterial load one day after they were given that inoculum,
which was based on sacrificing one animal the day after challenging all the
animals with the bacteria), and, if the study included a novel drug as part of
the tested treatment, the batch number of that drug.</p>
<div class="figure"><span style="display:block;" id="fig:metadata"></span>
<img src="figures/project_metadata.png" alt="Example of recording metadata for a study in the set of example studies for this module." width="\textwidth" />
<p class="caption">
Figure 2.29: Example of recording metadata for a study in the set of example studies for this module.
</p>
</div>
<p>Next, the researchers recorded some information about each treatment group
within the experiment. This typically included at least one negative control. In
some cases, there was also a positive control, in which the animals were treated
with a drug that’s in standard use against tuberculosis already (e.g.,
isoniazid). Most studies would also test one or more treatments, which could
include monotherapies or combined therapies. Figure <a href="experimental-data-recording.html#fig:treatmentdetails">2.30</a>
shows an example of the data that were recorded on each treatment in the study.
These data include the names and doses of up to three drugs in each treatment,
as well as a column where the researcher can provide detailed specifications of
the treatment.</p>
<div class="figure"><span style="display:block;" id="fig:treatmentdetails"></span>
<img src="figures/project_treatment_details.png" alt="Example of recording treatment details for a study in the set of example studies for this module." width="\textwidth" />
<p class="caption">
Figure 2.30: Example of recording treatment details for a study in the set of example studies for this module.
</p>
</div>
<p>Once the animals were challenged with the bacteria, treatment began, and two
main types of data were measured and recorded. First, the mice were weighed once
a week. For convenience, the mice were not weighed
individually. Instead, mice with the same treatment were kept in a single cage,
and the entire cage was weighed, the weight of the cage itself factored out, and
the average weight of mice for that treatment determined by dividing the weight
of all mice in the cage by the number of mice in the cage. These weights were converted to a measure of the percent change in
weight since the start of treatment. If the animals’ weights decrease during the
treatment, it is a marker that the treatment is not well-tolerated by the
animals. Figure <a href="experimental-data-recording.html#fig:mouseweight">2.31</a> shows an example of how these data could be
recorded. All animals within a treatment group were kept in the same cage, and
this cage was measured once a week. By dividing the weight of all animals in the
cage by the number of animals, the researchers could estimate the average weight
of animals in that treatment group, which is recorded as shown in Figure
<a href="experimental-data-recording.html#fig:mouseweight">2.31</a>.</p>
<div class="figure"><span style="display:block;" id="fig:mouseweight"></span>
<img src="figures/project_mouse_weights.png" alt="Example of recording weekly weights of mice in each treatment group for the example set of studies." width="\textwidth" />
<p class="caption">
Figure 2.31: Example of recording weekly weights of mice in each treatment group for the example set of studies.
</p>
</div>
<p>Finally, after the treatment period, the mice were sacrificed and a portion of
each mouse’s lung was used to estimate the bacterial load in that mouse. Figure
<a href="experimental-data-recording.html#fig:bacterialload">2.32</a> shows an example of how the data on the bacterial load
in each mouse can be recorded.</p>
<div class="figure"><span style="display:block;" id="fig:bacterialload"></span>
<img src="figures/project_bacterial_load.png" alt="Example of recording the bacterial load in the lungs of each mouse at the end of treatment for the example set of studies." width="\textwidth" />
<p class="caption">
Figure 2.32: Example of recording the bacterial load in the lungs of each mouse at the end of treatment for the example set of studies.
</p>
</div>
<p>As you can see, these data were all recorded in a format designed for the tidy
collection of laboratory data (see modules 2.4 and 2.5). These spreadsheets are
meant only to record the data, and then processing, analysis, and visualization
can then be done in separate files.</p>
</div>
<div id="step-2-organizing-a-project-directory" class="section level3 hasAnchor" number="2.8.3">
<h3><span class="header-section-number">2.8.3</span> Step 2: Organizing a project directory<a href="experimental-data-recording.html#step-2-organizing-a-project-directory" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Once you’ve determined the types of files that you’ll normally include in your
project, you then need to decide how to organize them into subdirectories in the project
file directory.</p>
<p>In this case, we’ve organized the project directory template to include just
a few things at the top level:</p>
<ul>
<li>A Excel file that stores meta-data about the experiment</li>
<li>A subdirectory named “raw_data”, where we’ll store original raw files of
data, before it is preprocessed</li>
<li>A subdirectory named “data”, which will store experimental data once it
has been preprocessed</li>
<li>A subdirectory named “R”, which will store code</li>
<li>A subdirectory named “reports”, which will store the files to generate
reports, as well as any reports that are ultimately generated</li>
</ul>
<p>In this structure, we’ve selected subdirectory names that are generic enough
(e.g., “data”, “reports”) that they can be reused across many of our projects
without modification. These names should also be clear to any researcher that
explores this directory in the future, since the names are clear and
unambiguous. However, you might make different choices—for example, if
some of your team aren’t familiar with R as a programming language, you may
want to use the subdirectory name “code” rather than “R”.</p>
<p>Within some of the subdirectories, we can include more subdirectories to
further organize files. For example, within the “data” subdirectory, we can
have subdirectories for different types of data:</p>
<ul>
<li>A subdirectory named “flow_data” for data from flow cytometry</li>
<li>A subdirectory named “recorded_data”, for data that are recorded “by hand”
in the laboratory (for example, the weights of animals)</li>
<li>A subdirectory named “sc_rna_seq_data” for data from single-cell RNA
sequencing</li>
</ul>
<p>Again, these subdirectories are named in a way that will generalize to many
different experiments and yet also clearly labels the contents. Similar
subdirectory diversions could also be used within the “raw_data” subdirectory.</p>
<p>The exact combination of subdirectories within the “data” subdirectory might
change from experiment to experiment. For example, some experiments might
include single-cell RNA sequencing assays, while some may not. When we use the
template, it will be easy to delete any “data” subdirectories for assays we are
not conducting, but by including them in the template, we can insure that we use
a consistent name for each subdirectory when we include it.</p>
<p>When you create a project directory template, we recommend that you create a
subdirectory named something like “reports” to use to store any Rmarkdown report
files for the project. This organization will make it clear where you’ve stored your
reports in the project directory. You’ll be able to use file and directory pathnames
to access all the data in the project, so it will be easy to use the study’s data
in the report even if they’re in separate subdirectories. There’s only one tool
you’ll need to do this—you’ll need to learn how to use relative pathnames
within R code to access files in a different part of your project directory.</p>
</div>
<div id="step-3-establishing-file-name-conventions" class="section level3 hasAnchor" number="2.8.4">
<h3><span class="header-section-number">2.8.4</span> Step 3: Establishing file name conventions<a href="experimental-data-recording.html#step-3-establishing-file-name-conventions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>[Add here on the file name conventions we picked]</p>
</div>
<div id="step-4-designing-data-collection-templates" class="section level3 hasAnchor" number="2.8.5">
<h3><span class="header-section-number">2.8.5</span> Step 4: Designing data collection templates<a href="experimental-data-recording.html#step-4-designing-data-collection-templates" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The next step is to create any necessary data collection templates. We’ll create
a separate spreadsheet for each type of data, but we can group them into files
if we’d like (e.g., one spreadsheet file with several separate sheets). In our
example, we created two files to store this type of data, one for the metadata
that are recorded at the start of the experiment (overall experiment details and
the details of each tested treatment) and one for the data that are collected
over the course of the experiment (mouse weights and bacterial loads). Within
each file, we’ve used separate sheets to record the different types of data.
This allows us to keep similar types of data together in the same file, while
having a tidy collection format for each specific type of data (Figure
<a href="experimental-data-recording.html#fig:projectdatacollection">2.33</a>).</p>
<div class="figure"><span style="display:block;" id="fig:projectdatacollection"></span>
<img src="figures/project_data_collection.png" alt="Data collection templates for the example project directory template. These templates were created in two files, one for metadata, which is saved in the main directory of the project, and one for data collected in the laboratory during the experiment, which is saved in the 'data' subdirectory. Each file is saved as a spreadsheet file, with two sheets in each file to store different types of data." width="\textwidth" />
<p class="caption">
Figure 2.33: Data collection templates for the example project directory template. These templates were created in two files, one for metadata, which is saved in the main directory of the project, and one for data collected in the laboratory during the experiment, which is saved in the ‘data’ subdirectory. Each file is saved as a spreadsheet file, with two sheets in each file to store different types of data.
</p>
</div>
<p>All of these data collection files are designed using the principles of tidy
data collection. In modules 2.4 and 2.5, we showed how you can create tidy data
collection templates to use, and how these can be paired with
reproducible reporting tools to separate the steps of data collection and
reporting (modules 3.7 through 3.9 go into much more depth on these reproducible
reporting tools). Once you have decided on the types of data that you will
usually collect for the type of study that this template is for, you can use
that process to create tidy data collection templates for each type of data.</p>
<p>When we created the template for each type of data, we added placeholder data
(formatted in red to indicate that it is placeholder, rather than final
data). This is so the researcher can see an example of how to enter data in
the template when they start a new project.</p>
<p>Figure <a href="experimental-data-recording.html#fig:replacingplaceholdermetadata">2.34</a> gives an example of this process.
One of the files that is included in the example template directory shown
earlier is a spreadsheet to record metadata on the experiment. This spreadsheet
file has two sheets, one that records overall metadata on the study (for
example, the weeks of treatment given and the strain of mouse used) and one that
records details on each of the treatments that was tested. In the file in the
template directory, these spreadsheet pages include placeholder data. These are
formatted in red, so that they visually can be identified as placeholders. By
including these placeholder data, the researcher can see an example of the
format that you expect to be used in recording data in this file. Once the
project template is copied, the researcher will replace these data with the real
data, and then change the font color to black to indicate that the placeholder
data have been replaced (Figure <a href="experimental-data-recording.html#fig:replacingplaceholdermetadata">2.34</a>).</p>
<div class="figure"><span style="display:block;" id="fig:replacingplaceholdermetadata"></span>
<img src="figures/project_replace_placeholder_metadata.png" alt="The template includes a file with experiment metadata, with a sheet for recording the overall details of the experiment. A user can open this file and replace the placeholder values (in red) with real values for the experiment. By changing the text color to black, the user can have a visual confirmation that the placeholder data have been replaced with real study data." width="\textwidth" />
<p class="caption">
Figure 2.34: The template includes a file with experiment metadata, with a sheet for recording the overall details of the experiment. A user can open this file and replace the placeholder values (in red) with real values for the experiment. By changing the text color to black, the user can have a visual confirmation that the placeholder data have been replaced with real study data.
</p>
</div>
<p>Another sheet of this spreadsheet allows the researcher to record the details of
each of the treatments that were tested in the experiment. Again, placeholder
data are included in the template in a red font to help show the researcher how
to record the data, and these are meant to be replaced with real data from the
specific experiment (Figure <a href="experimental-data-recording.html#fig:replacingplaceholdertreatment2">2.35</a>). A
similar format is used in the template file to record data from the experiment,
including the weights of each animal over each week of treatment and the final
bacterial load in each animal at the end of treatment. Again, there are
placeholder values in the template file, which the researcher will replace with
real data after copying the project template for a new experiment.</p>
<div class="figure"><span style="display:block;" id="fig:replacingplaceholdertreatment2"></span>
<img src="figures/project_replacing_placeholder_treatment_data.png" alt="The template includes a file with experiment metadata, with a sheet for recording the details of each treatment. A user can open this file and replace the placeholder values (in red) with real values for the treatments in the experiment. By changing the text color to black, the user can have a visual confirmation that the placeholder data have been replaced with real study data." width="\textwidth" />
<p class="caption">
Figure 2.35: The template includes a file with experiment metadata, with a sheet for recording the details of each treatment. A user can open this file and replace the placeholder values (in red) with real values for the treatments in the experiment. By changing the text color to black, the user can have a visual confirmation that the placeholder data have been replaced with real study data.
</p>
</div>
</div>
<div id="step-5-designing-a-report-template" class="section level3 hasAnchor" number="2.8.6">
<h3><span class="header-section-number">2.8.6</span> Step 5: Designing a report template<a href="experimental-data-recording.html#step-5-designing-a-report-template" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A final and optional step is to create one or more template reports. You can
create this using tools for reproducible reports—in R, a key tool for this is
RMarkdown. Here, we’ll cover using this tool for creating a report briefly, but
there are many more details in modules 3.7 through 3.9. Having example files
will help you to develop a template project report that can input the type of
data that you typically collect for this type of project.</p>
<p>We created an Rmarkdown file that does this analysis and visualization and
included it in the project template directory. This means that the report file
will be copied and available each time someone copies the project template
directory at the start of a new project. However, you are not obligated to keep
the report identical to the template. Instead, the template report serves as a
starting point, and you can add to it or adapt it as you work on a study.</p>
<p>In many cases, you may have a more complex design for your project directory. For
example, if you were collecting flow cytometry data for the project as well, then
you would want a subdirectory in the project that is specifically designed to
store files from the flow cytometry component of the experiment. This subdirectory
would likely include several files, rather than just one. Further, you would not
know ahead of time what the name of these files would be (as you do with the data
collection template files that are included in the template directory). However,
you can still easily write code for a template report file that will work with
multiple files of a similar type, even if you don’t know what the names will be,
as long as you know what the name of their subdirectory will be. There are functions
in R like <code>list.files</code> that can be used to list all the file names for the files
in a given directory. You can use this function to create a vector of all the file
names and then “map” a function or group of functions across these
files to read them in, process them, and join them into a single dataframe in R.
By using this process, you can write template code in the report for the project
that should work in most cases for the data that you collect for a given type of
study.</p>
<p>This file is created using the RMarkdown format,
which combines text with executable code. You can create this template so that it
inputs the experimental data from the file formats created for the data recording
files in the project template. By doing this, the researcher should be able to “knit”
this report for a new experiment, and it should recreate the report based on the
data recorded for that experiment (Figure <a href="experimental-data-recording.html#fig:makingareport">2.36</a>). By knitting
this template report, you can create a nicely formatted version of the report for
the experimental data (Figure <a href="experimental-data-recording.html#fig:examplereport1">2.37</a>).</p>
<div class="figure"><span style="display:block;" id="fig:makingareport"></span>
<img src="figures/project_opening_and_running_report.png" alt="Example of how a user can create a report from the template. The template includes an example report, which is written using RMarkdown. The user can open this template report file and use the 'Knit' button in RStudio to render the file. As long as the experimental data are recorded using the data template files, the code for this report can process the data to generate a report from the data. The user can also make changes and additions to the template report." width="\textwidth" />
<p class="caption">
Figure 2.36: Example of how a user can create a report from the template. The template includes an example report, which is written using RMarkdown. The user can open this template report file and use the ‘Knit’ button in RStudio to render the file. As long as the experimental data are recorded using the data template files, the code for this report can process the data to generate a report from the data. The user can also make changes and additions to the template report.
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:examplereport1"></span>
<img src="figures/project_example_report_study001.png" alt="Example of the output from 'knitting' a report from the project template" width="\textwidth" />
<p class="caption">
Figure 2.37: Example of the output from ‘knitting’ a report from the project template
</p>
</div>
<p>Specifically, for this set of studies a preliminary report was designed, with an
example shown in Figure <a href="experimental-data-recording.html#fig:prelimreport">2.38</a>. This report uses the first page
to provide a nicely format version of the metadata for the study, including a
table with overall details and a table with details for each specific treatment
that was tested. The second page provides a graph that shows the percent weight
change for mice in each treatment group compared to the weight of that group at
the start of treatment. The third page provides a graph that shows the bacterial
loads in each mouse, grouped by treatment, as well as the results of running a
statistical test, for each treatment group, of the hypothesis that the mean
of a transformed version of the measure of bacterial load (log-10) for the group
was the same as for the untreated control group.</p>
<div class="figure"><span style="display:block;" id="fig:prelimreport"></span>
<img src="figures/project_prelim_report.png" alt="Example of the preliminary report generated for each study in the set of example studies for this module. The first page includes metadata on the study, as well as details on each treatment that was tested. The second page shows how mouse weights in each treatment group changed over the course of treatment, to help identify if a treatment was well-tolerated. The third page graphs the bacterial load in each mouse, grouped by treatment, and gives the result of a statistical analysis to test which treatment groups had outcomes that were significantly different from the untreated control group." width="\textwidth" />
<p class="caption">
Figure 2.38: Example of the preliminary report generated for each study in the set of example studies for this module. The first page includes metadata on the study, as well as details on each treatment that was tested. The second page shows how mouse weights in each treatment group changed over the course of treatment, to help identify if a treatment was well-tolerated. The third page graphs the bacterial load in each mouse, grouped by treatment, and gives the result of a statistical analysis to test which treatment groups had outcomes that were significantly different from the untreated control group.
</p>
</div>
<p>Let’s take a closer look at a few of these elements. For example, Figure
<a href="experimental-data-recording.html#fig:studytable">2.39</a> shows the tables from the first page of the report shown
in Figure <a href="experimental-data-recording.html#fig:prelimreport">2.38</a>. If you look back to the data collection for
this study (e.g., Figures <a href="experimental-data-recording.html#fig:metadata">2.29</a> and <a href="experimental-data-recording.html#fig:treatmentdetails">2.30</a>),
you can see that all of the information in these tables was pulled from data
recorded at the start of the study.</p>
<div class="figure"><span style="display:block;" id="fig:studytable"></span>
<img src="figures/project_study_info_table.png" alt="Example of one element of the preliminary report generated for each study in the set of example studies for this module. The first page provides tables with metadata about the study and details about each treatment that was tested." width="\textwidth" />
<p class="caption">
Figure 2.39: Example of one element of the preliminary report generated for each study in the set of example studies for this module. The first page provides tables with metadata about the study and details about each treatment that was tested.
</p>
</div>
<p>Figure <a href="experimental-data-recording.html#fig:mouseweightsplot">2.40</a> shows the second page of the report. This
figure has taken the mouse weights—which were recorded in one of the data
collection templates for the project (Figure <a href="experimental-data-recording.html#fig:mouseweight">2.31</a>)—and used
them to generate a plot of how average mouse weight in each treatment group
changed over the course of the treatment.</p>
<div class="figure"><span style="display:block;" id="fig:mouseweightsplot"></span>
<img src="figures/project_mouse_weights_graph.png" alt="Example of one element of the preliminary report generated for each study in the set of example studies for this module. The second page provides a plot of how the weights of mice in each treatment changed over the course of treatment." width="\textwidth" />
<p class="caption">
Figure 2.40: Example of one element of the preliminary report generated for each study in the set of example studies for this module. The second page provides a plot of how the weights of mice in each treatment changed over the course of treatment.
</p>
</div>
<p>Figure <a href="experimental-data-recording.html#fig:bactcompare">2.41</a> shows the last page of the report. This page
starts with a figure that shows the bacterial load in the lungs of each mouse in
the study at the end of the treatment period. In this figure, the measurement
for each mouse is shown with a point, and these points are grouped by the
treatment group of the mouse. Boxplots are added to show the distribution across
the mice in each group. The color is used to show whether the treatment was a
negative control, a positive control, a monotherapy, or a combined therapy. The
second part of the page gives a table with the results from running a
statistical analysis to compare the bacterial load for mice in each treatment
group to the bacterial load in the mice in the untreated control group. Color is
added to the table to highlight treatments that had a large difference in
bacterial load from the untreated control, as well as treatments for which the
difference from the untreated control was estimated to be statistically
significant. All the data for these results, including the labels for the plot,
are from the data collected in the data collection templates shown earlier.</p>
<div class="figure"><span style="display:block;" id="fig:bactcompare"></span>
<img src="figures/project_bact_compare_plot.png" alt="Example of one element of the preliminary report generated for each study in the set of example studies for this module. The third page provides results on how bacterial load in the lungs compares among treatments at the end of the treatment period." width="\textwidth" />
<p class="caption">
Figure 2.41: Example of one element of the preliminary report generated for each study in the set of example studies for this module. The third page provides results on how bacterial load in the lungs compares among treatments at the end of the treatment period.
</p>
</div>
<p>In the report, we’ll design the script for the report (the RMarkdown file) so
that it can leverage the order in how we’ve arranged files in the file system,
since this is enforced by the project directory template and so is the same
across different projects. This will let us repeat and reuse code scripts across
all the projects that use this template. This strategy is used often in handling
complex bioinformatics data <span class="citation">(Buffalo 2015)</span>, but it can also be
leveraged to improve the reproducibility and reliability when only using less
complex data recorded in the laboratory, as with the data shown in the example
for this module.</p>
<p>When it comes to project directories, it turns out that you can use the
directory structure in your favor when you create script-based reports, like
RMarkdown reports. There are functions in R, for example, that will allow you to
print all the files in a specified subdirectory. Say that you have several flow
cytometry files in a subdirectory of the “data” subdirectory called “flow_data”.
You could use this function in R to create a list of all the files in that
subdirectory, and then you can run other functions to do the same operations on
all of those files.</p>
<p>We wrote the code in the report in a way that it will still run if there are
more or fewer observations in any of the data collection files, so the report
template has some flexibility in terms of how each study in the set of studies
might vary. For example, in the example set of studies, some of the experiments
were run using only a control group of mice, while others were run to test
several different treatment groups. The report template can accommodate
these differences across studies in the set of studies.</p>
</div>
<div id="applied-exercise-1" class="section level3 hasAnchor" number="2.8.7">
<h3><span class="header-section-number">2.8.7</span> Applied exercise<a href="experimental-data-recording.html#applied-exercise-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>

</div>
</div>
<div id="module9" class="section level2 hasAnchor" number="2.9">
<h2><span class="header-section-number">2.9</span> Harnessing version control for transparent data recording<a href="experimental-data-recording.html#module9" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As a research project progresses, researchers will often end up with many files
(e.g., ‘draft1.doc’, ‘draft2.doc’). This can result in an explosion of files,
and it becomes hard to track which files represent the ‘current’ state of a
project. Version control allows researchers to edit and change research project
files more cleanly, while including messages to explain changes and maintaining
the power to ‘backtrack’ to previous versions. We will explain what version
control is and how it can be used in research projects to improve the
transparency and reproducibility of research, particularly for data recording.
In this module, we’ll introduce you to the basic idea of version control, using
the git software program as an example. In later modules, we’ll explain version
control platforms like GitHub, as well as give some tips on how to use both
within your research projects.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Define “version” and “version control”<br />
</li>
<li>Identify examples of versioning in a digital context (data, code, files)</li>
<li>Describe strategies and tools to save and refer to specific versions of data
and other files or directories</li>
<li>Discuss how version control principles can improve collaboration in
scientific projects</li>
</ul>
<div id="challenges-of-collaborating-on-evolving-research-materials" class="section level3 hasAnchor" number="2.9.1">
<h3><span class="header-section-number">2.9.1</span> Challenges of collaborating on evolving research materials<a href="experimental-data-recording.html#challenges-of-collaborating-on-evolving-research-materials" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When research groups—or any other professional teams—collaborate on
publications and research, the process can be a bit haphazard. Teams often use
emails and email attachments to share updates on the project, and email
attachments to pass around the latest version of a document for others to review
and edit. For example, one group of researchers investigated a large collection
of emails from people at Enron who were doing work involving spreadsheets
<span class="citation">(Hermans and Murphy-Hill 2015)</span>. They found that passing Excel files through email
attachments was a common practice, and that messages within emails suggested
that spreadsheets were stored locally, rather than in a location that was
accessible to all team members <span class="citation">(Hermans and Murphy-Hill 2015)</span>, which meant that team
members might often be working on different versions of the same spreadsheet
file. They note that “the practice of emailing spreadsheets is known to result
in serious problems in terms of accountability and errors, as people do not have
access to the latest version of a spreadsheet, but need to be updated of changes
via email.” <span class="citation">(Hermans and Murphy-Hill 2015)</span> The same process for collaboration is often used
in scientific research: one study found, “Team members regularly pass
data files back and forth by hand, by email, and by using shared lab or project
servers, websites, and databases.” <span class="citation">(Edwards et al. 2011)</span></p>
<p>Eric Raymond, in his book <em>The Art of Unix Programming</em>, calls this type of
project tracking “hand-hacking”. He notes:</p>
<blockquote>
<p>“The most primitive (but still very common) method [of version control] is all
hand-hacking. You snapshot the project periodically by manually copying
everything in it to a backup. You include history comments in source files. You
make verbal or email arrangements with other developers to keep their hands off
certain files while you hack them. … The hidden costs of this hand-hacking
method are high, especially when (as frequently happens) it breaks down. The
procedures take time and concentration; they’re prone to error, and tend to get
slipped under pressure or when the project is in trouble—that is exactly when
they are needed.” <span class="citation">(E. S. Raymond 2003)</span></p>
</blockquote>
<p>These practices make it very difficult to keep track of all project files, and
in particular, to track which version of each file is the most current. Further,
this process constrains patterns of collaboration—it requires each team member
to take turns in editing each file, or for one team member to attempt to merge
in changes that were made by separate team members at the same time when all
versions are collected.</p>
<p>This process also makes it difficult to keep track of why changes were made, and
often requires one team member to approve the changes of other team members.
While the “Track changes” and comment features can help the team communicate
with each other, these features often lead to a very messy document at stages in
the editing, where it is hard to pick out the current versus suggested wording,
and once a change is accepted or a comment deleted, these conversations can be
lost forever. Finally, word processing tools are poorly suited to track changes
or add suggestions directly to data or code, as both data and code are usually
saved in formats that aren’t native to word processing programs, and copying
them into a format like Word can introduce problematic hidden formatting that
can cause the data or code to malfunction.</p>
</div>
<div id="recording-data-in-the-laboratoryfrom-paper-to-computers" class="section level3 hasAnchor" number="2.9.2">
<h3><span class="header-section-number">2.9.2</span> Recording data in the laboratory—from paper to computers<a href="experimental-data-recording.html#recording-data-in-the-laboratoryfrom-paper-to-computers" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>How does version control—traditionally a tool of software engineers—relate
to collaborating to collect and analyze scientific research data? Traditionally,
experimental data collected in a laboratory was recorded in a paper laboratory
notebook. These laboratory notebooks played a role not only as the initial
recording of data, but also keep a legal record of the data recorded in the lab
<span class="citation">(Mascarelli 2014)</span>. They were also a resource for collaborating across a
team and for passing on a research project from one lab member to another
<span class="citation">(Butler 2005)</span>.</p>
<p>However, paper laboratory notebooks have a number of limitations. First, they
can be very inefficient. In a time when almost all data analyses—even simple
calculations—are done on a computer, recording research data on paper rather
than directly entering it into a computer is inefficient. Also, any stage of
copying data from one format to another, especially when done by a human rather
than a machine, introduces the chance to copying errors. Handwritten laboratory
notebooks can be hard to read <span class="citation">(Butler 2005; J. M. Perkel 2011)</span>, and
they may lack adequate flexibility to handle the complex experiments often conducted.
Further, electronic alternatives can also be easier to search, allowing for
deeper and more comprehensive investigations of the data collected across
multiple experiments <span class="citation">(Giles 2012; Butler 2005; J. M. Perkel 2011)</span>. Further, physical lab notebooks can be inefficient to
search; as one article notes, they are “usually chaotic and always unsearchable”
<span class="citation">(J. M. Perkel 2011)</span>.</p>
<p>Given a widespread recognition of the limitations of paper laboratory notebooks,
in the past couple of decades, there have been a number of efforts, both formal
and informal, to move from paper laboratory notebooks to electronic
alternatives. In some fields that rely heavily on computational analysis, there
are very few research labs (if any) that use paper laboratory notebooks
<span class="citation">(Butler 2005)</span>. In other fields, where researchers have traditionally
used paper lab notebooks, companies have been working for a while to develop
electronic laboratory notebooks specifically tailored to scientific research
needs <span class="citation">(Giles 2012)</span>. Some early adapters were pharmaceutical industrial
labs, where companies had the budgets to get customized versions and the
authority to require their use. In academic laboratories, electronic lab
notebooks have taken longer to be adapted <span class="citation">(Giles 2012; Butler 2005)</span>. Indeed, a widely adopted platform for electronic laboratory
notebooks has yet to be taken up by the scientific community <span class="citation">(Kwok 2018)</span>,
despite clear advantages of recording data directly into a computer rather than
first using a paper notebook. As Kwok notes in a 2018 commentary,</p>
<blockquote>
<p>“Since at least the 1990s, articles on technology have predicted the imminent,
widespread adoption of electronic laboratory notebooks (ELNs) by researchers. It has
yet to happen” <span class="citation">(Kwok 2018)</span></p>
</blockquote>
<p>Instead of using customized electronic laboratory notebook software, some
academics are moving their data recording online, but are using more generalized
electronic alternatives, like Dropbox, Google applications, OneNote, and
Evernote <span class="citation">(J. M. Perkel 2011; Kwok 2018; Giles 2012; K. Powell 2012)</span>.
Some scientists have started using version control software, especially the
combination of git and GitHub, as a way to improve laboratory data recording,
and in particular to improve transparency and reproducibility standards.
These pieces of software share the same pattern as Google applications or
Dropbox—they are generalized tools that have been honed and optimized for ease
of use through their role outside of scientific research, but can be harnessed
as a powerful tool in a scientific laboratory, as well. They are also free—at
least, for GitHub, at the entry and academic levels—and, even better, one
(git) is open source.</p>
</div>
<div id="defining-version-and-version-control" class="section level3 hasAnchor" number="2.9.3">
<h3><span class="header-section-number">2.9.3</span> Defining “version” and “version control”<a href="experimental-data-recording.html#defining-version-and-version-control" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Most scientific research today involves collaboration across a team of
researchers, rather than an individual scientist working alone. Collaboration
drives interdisciplinary science, but it also creates additional challenges. One
challenge comes with coordinating versions of research materials. These
materials can include data collection files, but can also include other
documents like study protocols, as well as physical materials like cell lines,
antibodies, and model organisms.</p>
<p>A <em>version</em> is one iteration of a research material that is evolving. For
example, a draft of a research paper is one version of that paper. Research data
that you collect may also go through several versions. For example, if you
identify a typo in data after you record it, you may need to correct the typo
and add a note or signature to explain that update. Further, if you are
collecting data at multiple timepoints, you may have new versions of a data file
as you complete each timepoint.</p>
<p>As materials evolve across versions, it introduces challenges in maintaining a
research process that is smooth, efficient, and error-free. One challenge is to
make sure it is always clear which version is the most current, as well as which
version should be used for specific purposes. For example, if several coauthors
are editing a paper draft, it is important to ensure they are all working on the
most recent version.</p>
<p>Another challenge is to coordinate the changes that different people make if
they work on the material at the same time. Scientific collaboration often does
not operate as an assembly line, where one person finishes their work on a
document or material and then hands it off to the next person. Instead, there
will often be several copies of a version in different peoples’ hands, with all
of them working on it at once. One example is a paper draft—often coauthors
all edit the latest draft at the same time, rather than one-by-one.
This creates the challenge of taking the contributions of each person and
coordinating their changes and additions into one primary copy.</p>
<p>A third challenge is to keep track of the changes that are made at each step,
as the document moves from version to version. This record can help in auditing
for errors or bugs that might be introduced as the document evolves. The record will
also ideally will include some information about why changes were made at each
step.</p>
<p>These challenges can be addressed through a process called <em>version control</em>.
While the term is most commonly used in reference to software development, the
idea of version control is widely relevant. Any process that creates evolving
versions of a document or material can benefit from the idea of version control,
which aims to record and document changes to the material over time, coordinate
the contributions of different members of a team, and revert back to older
versions if needed. In this module, we’ll focus on version control as it applies
to research materials that are electronic (files and directories), but you may
also find it useful to think about how the principles and elements of version
control can be applied to other research materials, like cell lines and
antibodies.</p>
</div>
<div id="what-are-the-key-elements-of-version-control" class="section level3 hasAnchor" number="2.9.4">
<h3><span class="header-section-number">2.9.4</span> What are the key elements of version control?<a href="experimental-data-recording.html#what-are-the-key-elements-of-version-control" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The term <em>version</em> in <em>version control</em> refers to one iteration or state of a
document or set of documents, for example the current version of a data file.
The word <em>control</em> captures the idea of allowing for safe changes and updates to
the version, especially when more than one person is working on it. Part of this
“control” will also include recording the changes made from one version to the
next and annotating reasons for those changes.</p>
<p>The general term <em>version control</em> can refer to any method of syncing
contributions from several people to a file or set of files. Version control of
computer files can be done “by hand”, with a person manually loggin each change,
and originally was <span class="citation">(Irving 2011)</span>. However, it’s much more efficient
to use a computer program to handle this tracking and to coordinate
contributions from multiple people.As Eric Raymond notes in <em>The Art of Unix
Programming</em>, “tracking all that detail is just the sort of thing computers are
good at and humans are not” <span class="citation">(E. S. Raymond 2003)</span>. He goes on to describe version
control as “a suite of programs that automates away most of the drudgery
involved in keeping an annotated history of your project and avoiding
modification conflicts” <span class="citation">(E. S. Raymond 2003)</span>.</p>
<p>Software for this purpose—<em>version control software</em>—first developed for
software programming projects. Some popular version control software today
comes from these roots. In this section, we’ll introduce the key features of
version control, and to do so we’ll use examples and terminology from a common
version control software program called <em>git</em>. While these terms are derived
from this particular software program, they represent ideas that are important
in any implementation of version control. Later, we’ll touch on how some of these
ideas are incorporated in other software, like Google Docs.</p>
<p>The software available for version control tracks electronic files. While the
very earliest version control software systems tracked single files, these
systems quickly moved to tracking sets of files, called <em>repositories</em>. A
repository is almost identical to a file directory (which you may also know as a
file folder), and indeed a repository starts from a file directory. The only
difference is the repository is enhanced with some additional overhead
<span class="citation">(Klemens 2014)</span>. This overhead is added to record how the files in the
directory have changed over time. You can compare this to how you might track
document changes if the documents were paper rather than electronic—you could
store the documents in a paper folder and add a piece of paper where you record
a log of each change you make to the documents in the folder. The extra overhead
that changes a regular file directory to a repository is very similar to the log
in this example. A repository, in other words, is a directory that is under
version control.</p>
<p>In a repository of files that is under version control, the version control
software takes snapshots of how the files look during your work on them. Each
snapshot is called a <em>commit</em>, and it provides a record of which lines in each
file changed from one snapshot to another, as well as exactly how they changed.
The idea behind these commits—recording the differences, line-by-line, between
an older and newer version of each file derives from a longstanding Unix command
line tool called <em>diff</em>. This tool, developed early in the history of Unix at
AT&amp;T’s Bell Labs <span class="citation">(E. S. Raymond 2003)</span>, is a solid and well-tested tool that does
the simple but important job of generating a list of all the differences between
two plain text files. Each commit in a repository includes the same type of
information about the differences introduced in the files at the time of that
commit.</p>
<p>When you are working with a directory under version control, you explain your
changes as you make them—in other words, version control allows for annotation
of the developing and editing process <span class="citation">(E. Raymond 2009)</span>. Each commit
requires you to enter a <em>commit message</em> describing why the changes in that
commit were made. The commit messages can serve as a powerful tool for
explaining changes to other team members or for reminding yourself in the future
about why certain changes were made. A repository under version control, then,
can include not only a complete history of how files in a project directory have
changed over the course of the project, but also why. If this feature is used
thoughtfully, then the commit history of the project provides a well-documented
description of the project’s full evolution. If you’re working on a manuscript,
for example, when it’s time to edit, you can cut whole paragraphs, and if you
ever need to get them back, they’ll be right there in the commit history for
your project, with their own commit message about why they were cut (hopefully a
nice clear one that will make it easy to find that commit if you ever need those
paragraphs again).</p>
<p>Further, each of the commits is given its own ID tag (in the <em>git</em> software,
this is done through something called a unique SHA-1 hash <span class="citation">(Klemens 2014)</span>),
and version control systems have a number of commands that let you “roll back”
to earlier versions. This provides <em>reversability</em> within the project files,
allowing you to go back to the version as it was when a certain commit was made
<span class="citation">(E. Raymond 2009)</span>.</p>
<p>A key strength, then, of using version control is its ability to track every
change made to files in the project, why the change was made, and who made it.
Version control creates a full history of the evolution of each file in the
project. When a change is committed, the history records the exact change made,
including the previous version of the file. No change is ever fully lost,
therefore, unless a great deal of extra work is taken to erase something from
the project’s commit history.</p>
<p>It turns out that this functionality—of being able to roll back to earlier
versions—has a wonderful side benefit when it comes to working on a large
project. It means that you don’t need to save earlier versions of each file. You
can maintain one and only one version of each project file in the project’s
directory, with the confidence that you never “lose” old versions of the file
<span class="citation">(J. Perkel 2018; Blischak, Davenport, and Wilson 2016)</span>. This allows you to maintain a clean and
simple version of the project files, with only one copy of each, ensuring it’s
always clear which version of a file is the “current” one (since there’s only
one version) <span class="citation">(Klemens 2014)</span>. This also provides the reassurance that you can
try new directions in a project, and always roll back to the old version if that
direction doesn’t work well.</p>
<p>In a 2011 commentary in <em>Nature Methods</em>, Perkel tells a story about how this
functionality helped one researcher keep his project directories simpler:</p>
<blockquote>
<p>“Early in his graduate career, John Blischak found himself creating figures
for his advisor’s grant application. Blischak was using the programming language
R to generate the figures, and as he iterated and optimized his code, he ran
into a familiar problem: Determined not to lose his work, he gave each new
version a different filename—analysis_1, analysis_2, and so on, for
instance—but failed to document how they had evolved. ‘I had no idea what had
changed between them,’ says Blischak… Using Git, Blischak says, he no longer
needed to maintain multiple copies of his files. ‘I just keep overwriting it and
changing it and saving the snapshots. And if the professor comes back and says,
’oh, you sent me an email back in March with this figure’, I can say, ‘okay,
well, I’ll just bo back to the March version of my code and I can recreate
it’.” <span class="citation">(J. Perkel 2018)</span></p>
</blockquote>
<p>Modern version control systems like <em>git</em> take a distributed approach to
collaboration on project files. In earlier types of version control programs,
there was one central repository for the file or set of files the team was
working on <span class="citation">(E. Raymond 2009; Target 2018)</span>. Very early on, under
what is called a <em>centralized</em> framework, this was kept on one
computer <span class="citation">(Irving 2011)</span>. A team member who wanted to make a change
would “check out” the file he or she wanted to work on, make changes, and then
check it back in as the newest main version <span class="citation">(E. S. Raymond 2003)</span>. While one team
member had this file checked out, other members would be locked out of making
any changes to that file—they could look at it, but couldn’t make any edits
<span class="citation">(E. Raymond 2009; Target 2018)</span>. This meant that there was no chance
of two people trying to change the same part of a file at the same time. In
spirit, this early system is pretty similar to the idea of sending a file around
the team by email, with the understanding that only one person works on it at a
time. A slightly more modern analogy is the idea of having a single version of a
file in Dropbox or Google Docs, and avoiding working on the file when you see
that another team member is working on it.</p>
<p>This assembly-line approach is pretty clunky, though. In particular, it usually
increases the amount of time that it takes the team to finish the project,
because only one person can work on a file at a time. Later types of version
control programs moved toward a different style, allowing for distributed rather
than centralized collaborative work on a file or a set of files
<span class="citation">(E. Raymond 2009; Irving 2011)</span>. Under the distributed model,
all team members can have their own version of all the files, work on them and
make records of changes they make to the files, and then occassionally sync with
everyone else to share your changes with them and bring their changes into your
copy of the files. This functionality is called <em>concurrency</em>, since it allows
team members to concurrently work on the same set of files
<span class="citation">(E. Raymond 2009)</span>. This idea allowed for the development of other useful
features and styles of working, including <em>branching</em> to try out new ideas that
you’re not sure you’ll ultimately want to go with and <em>forking</em>, a key tool used
in open-source software development, which among other things facilitates
someone who isn’t part of the original team getting a copy of the files they can
work with and suggesting some changes that might be helpful. So, this is the
basic idea of modern version control—for a project that involves a set of
computer files, everyone on the team (even if that’s just one person) has their
own copy of a directory with those files on their own computer, makes changes at
the time and in the spots in the files that they want, and then regularly
re-syncs their local directory with everyone else’s to share changes and
updates.</p>
<p>This distributed model also means there is a copy of the full repository on
every team member’s computer, which has the side benefit of providing additional
backup of the project files. Remote repositories—which may be on a server in a
different location—can be added with another copy of the project, which can
similarly be synced regularly to update with any changes made to project files.</p>
<p>While there are a number of software systems for version control, one of the
most common currently used for scientific projects is <em>git</em>. This program was
created by Linus Torvalds, who also created the Linux operating system, in 2005
as a way to facilitate the team working on Linux development. This program for
version control thrives in large collaborative projects, for example open-source
software development projects that include numerous contributors, both regular
and occasional <span class="citation">(Brown 2018)</span>. As Target notes in a 2018 article about
version control:</p>
<blockquote>
<p>“While people sometimes grouse about its steep learning curve or unintuitive
interface, git has become everyone’s go-to for version control.”
<span class="citation">(Target 2018)</span></p>
</blockquote>
<p>In recent years, some complementary tools have been developed that make the
process of collaborating together using version control software easier. Other
tools, such as bug trackers or issue trackers, facilitate corroborative
file-based projects to allow the team to keep a running “to-do” list of what
needs to be done to complete the project. These tools—which are discussed in
modules 2.10 and 2.11—can be used to improve collaboration on scientific
projects done by teams. GitHub is one a very popular version control platform
with these additional tools. It was created in 2008 as a web-based platform to
facilitate collaborating on projects running under git version control. It can
provide an easier entry to using git for version control than trying to learn to
use git from the command line <span class="citation">(Perez-Riverol et al. 2016)</span>. It also interfaces well with RStudio,
making it easy to integrate a collaborative workflow through GitHub from the
same RStudio window on your computer where you are otherwise doing your analysis
<span class="citation">(Perez-Riverol et al. 2016)</span>.</p>
<p>Finally, while git version control software is one of the best established ways
of implementing version control, there are growing efforts to enable some level
of version control through other platforms. For example, Google Docs enables a
level of version control through its Version History feature. This feature
allows you name different versions of a document as they are is saved in Google
Docs. It also allows you to restore a document to earlier versions, as well as
see which changes have been made to a document and who made each change.</p>
</div>
<div id="comparing-git-to-other-tools" class="section level3 hasAnchor" number="2.9.5">
<h3><span class="header-section-number">2.9.5</span> Comparing git to other tools<a href="experimental-data-recording.html#comparing-git-to-other-tools" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>While some generalized tools like Google tools and Dropbox might be simpler to
initially learn, more powerful version control tools like git offer some key
advantages for recording scientific data and are worth the effort to adopt. A
key advantage is their ability to track the full history of files as they
evolve, including not only the history of changes to each file, but also a
record of why each change was made. Git excels in tracking changes made to plain
text files. For these files, whether they record code, data, or text, git can
show line-by-line differences between two versions of the file. This makes it
very easy to go through the history of “commits” to a plain text file in a
git-tracked repository and see what change was made at each time point, and then
read through the commit messages associated with those commits to see why a
change was made. For example, if a value was entered in the wrong row of a plain
text file or spreadsheet, and the researcher then made a commit to correct that
data entry mistake, the researcher could explain the problem and its resolution
in the commit message for that change. As Tippmannmy notes:</p>
<blockquote>
<p>“The purpose of a lab notebook is to provide a lasting record of events in a
laboratory. In the same way that a chemistry experiment would be nearly
impossible without a lab notebook, scientific computing would be a nightmare of
inefficiency and uncertainty without version-control systems.”
<span class="citation">(Tippmann 2014)</span></p>
</blockquote>
<p>There are, of course, some limitations to using version control tools when
recording experimental data. First, while ideally laboratory data is recorded in
a plain text format (see the module in section 2.2 for a deeper discussion of
why), some data may be recorded in a binary file format. Some version control
tools, including git, can be used to track changes in binary files. However, git
does not take to these types of files naturally. In particular, git typically
will not be able to show users a useful comparison of the differences between
two versions of a binary file.</p>
<p>More problems can arise if the binary file is
very large <span class="citation">(Perez-Riverol et al. 2016; Blischak, Davenport, and Wilson 2016)</span>, as some experimental research
data files are (e.g., if they are high-throughput output of laboratory equipment
like a mass spectrometer). However, there are emerging tools and strategies for
improving the ability to include and track large binary files when using git and
GitHub <span class="citation">(Blischak, Davenport, and Wilson 2016)</span>.</p>
<p>Finally, as with other tools and techniques described in this book, there is an
investment required to learn how to use git <span class="citation">(Perez-Riverol et al. 2016)</span>, as well
as some extra overhead when using version control tools in a project
<span class="citation">(E. S. Raymond 2003)</span>. However, git can bring dramatic gains to
efficiency, transparency, and organization of research projects, even if you
only use a small subset of its basic functionality <span class="citation">(Perez-Riverol et al. 2016)</span>. In module
2.11, we provide guidance on getting started with using git and Github to track
a scientific research project.</p>
<p>Third, the combination of git and GitHub can help as a way to backup study data
<span class="citation">(Blischak, Davenport, and Wilson 2016; Perez-Riverol et al. 2016; J. Perkel 2018)</span>. Together, git and GitHub
provide a structure where the project directory (repository) is copied on
multiple computers, both the users’ laptop or desktop computers and on a remote
server hosted by GitHub or a similar organization. This set-up makes it easy to
bring all the project files onto a new computer—all you have to do is clone
the project repository. It also ensures that there are copies of the full
project directory, including all its files, in multiple places
<span class="citation">(Blischak, Davenport, and Wilson 2016)</span>. Further, not only is the data backed up across multiple
computers, but so is the full history of all changes made to that data and the
recorded messages explaining those changes, through the repositories commit
messages <span class="citation">(Perez-Riverol et al. 2016)</span>.</p>
</div>
<div id="discussion-questions-1" class="section level3 hasAnchor" number="2.9.6">
<h3><span class="header-section-number">2.9.6</span> Discussion questions<a href="experimental-data-recording.html#discussion-questions-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>In your own research, do you collect data in paper laboratory notebooks, electronically, or a mixture of the two? What have you found to be advantages and disadvantages of the method you typically use? Are there ever cases where you have no choice and must either record on paper or electronically (examples might include when working behind a secure barrier or when data are recorded directly by equipment into a digital format)?<br />
</li>
<li>Have you used any of the following tools for recording, sharing, and versioning data or other research files (e.g., drafts of research papers, code):
<ul>
<li>Electronic laboratory notebooks</li>
<li>Dropbox</li>
<li>Google Docs / Google Drive</li>
<li>Microsoft Teams</li>
<li>Local server or drive run by your institution</li>
<li>GitHub / GitLab</li>
</ul></li>
<li>Describe how any of these tools have helped in version control, including tracking changes to the file and helping to coordinate several people working on a file at once. Are there aspects where the tools you’ve used have been limited in this capacity?</li>
<li>Can you think of any examples of times when you’ve experienced a failure of version control? Examples might include a case where some team members worked on the wrong version of a file, or when you lost track of the changes that had been made to a file. What did you learn from the experience? Have you developed methods to avoid similar problems in the future? How might a version control problem like this result in problems with the rigor and reproducibility of scientific research?</li>
<li>How does the idea of version control relate to physical research materials, like model organisms, antibodies, or cell lines? Do you have any examples you can share of issues that have come up in research related to the version of these types of physical research materials?</li>
<li>What steps do you think you could take in your research to improve version control? Do you see this as a higher or lower priority change to take compared to other steps that might improve rigor and reproducibility in your research? Discuss your reasoning.</li>
</ul>
<!-- ### Practice Quiz -->
<!-- Question 1: Which of the following research materials can have different versions and therefore is a material for which you can consider principles of version control?  -->
<!-- a) Electronically-recorded research data   -->
<!-- b) Study protocols  -->
<!-- c) Cell lines  -->
<!-- d) All of the above [Correct answer]  -->
<!-- Question 2: What is one challenge that is created when several people make changes to the same document concurrently?  -->
<!-- a) It is difficult to share copies of the document with everyone at once  -->
<!-- b) Science researchers tend to work alone on most projects, so it is difficult to convince them to collaborate  -->
<!-- c) It is difficult to resolve and coordinate the changes each person made to incorporate all the changes into a new version [Correct answer]  -->
<!-- d) Most researchers prefer to work one after another on a document, rather than all working at the same time  -->
<!-- Question 3:  -->
<!-- TRUE / FALSE: The idea of version control is only relevant in the context of developing software.  -->
<!-- [Correct answer: FALSE]  -->
<!-- Question 4: In the term version control, what does “control” mean?  -->
<!-- a) Allowing only one person to work on a document at a time while freezing it to changes from any other user  -->
<!-- b) Ensuring that none of the updates to a document introduce bugs or errors  -->
<!-- c) Allowing for safe changes and updates to a version of a document when more than one person is working on it [Correct answer]  -->
<!-- d) Giving priority to one primary user to overrule changes made to the document by other users  -->
<!-- Question 5: What is the difference between a normal file directory and a repository?  -->
<!-- a) There is no difference; these words are synonyms  -->
<!-- b) A file directory is under version control, while a repository may not be  -->
<!-- c) A repository is one version of a file directory   -->
<!-- d) A repository is under version control, while a file directory may not be [Correct answer]  -->
<!-- Question 6: Why is reversibility appealing in a version control system?  -->
<!-- a) It means that you can roll back to an earlier version of the document if you want to [Correct answer]  -->
<!-- b) It means that you have a well-documented history of all changes made to the document  -->
<!-- c) It means that multiple people can make changes to a document at once  -->
<!-- d) It means that new files can be added to a repository  -->
<!-- Question 7: In the git version control system, what is the term for a snapshot of a version of the files in a repository at a certain point in its history?  -->
<!-- a) Merge  -->
<!-- b) History  -->
<!-- c) Commit message  -->
<!-- d) Commit [Correct answer]  -->
<!-- Question 8: How can the combination of version control software and a version control platform (like GitHub) help provide backup for research documents?  -->
<!-- a) To enable this combination, you must also keep a copy of the research files on a dedicated server  -->
<!-- b) With this combination, each contributor has a full copy of the repository of research files on their local computer, which they can sync with other versions through the platform [Correct answer]  -->
<!-- c) This combination coordinates with Google Drive to archive files  -->
<!-- d) This combination provides a full history of all versions of the files in the repository, while using the version control software without the platform would not  -->

</div>
</div>
<div id="module10" class="section level2 hasAnchor" number="2.10">
<h2><span class="header-section-number">2.10</span> Enhance the reproducibility of collaborative research with version control platforms<a href="experimental-data-recording.html#module10" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Once a researcher has learned to use <em>git</em> on their own computer for
local version control, they can begin using version control platforms (e.g.,
<em>GitLab</em>, <em>GitHub</em>) to collaborate with others under version
control. We will describe how a research team can benefit from using a version
control platform to work collaboratively.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>List benefits of using a version control platform to collaborate
on research projects, particularly for reproducibility</li>
<li>Describe the difference between version control (e.g., <em>git</em>) and
a version control platform (e.g., <em>GitLab</em>)</li>
</ul>
<div id="what-are-version-control-platforms" class="section level3 hasAnchor" number="2.10.1">
<h3><span class="header-section-number">2.10.1</span> What are version control platforms?<a href="experimental-data-recording.html#what-are-version-control-platforms" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The last module introduced the idea of version control, including the popular
software tool often used for version control, git. In this module, we’ll go a
step further, telling you about how you can expand the idea of version control
to leverage it when collaborating across your research team, using <em>version
control platforms</em>. Version control platforms build on the functionality of
version control software like git. They can provide you and your team tools
for sharing, tools for visualization, and tools for project management.</p>
<p>A version control platform allows you to share project files across a group of
collaborators while keeping track of what changes are made, who made each
change, and why each change was made. It therefore combines the strengths of a
“Track changes” feature with those of a file sharing platform like Dropbox. To
some extent, Google Docs or Google Drive also combine these features, and some
spreadsheet programs are moving toward some rudimentary functionality for
version control <span class="citation">(Birch, Lyford-Smith, and Guo 2018)</span>. However, there are added advantages of
version control platforms. Since open-source version control platforms like
GitHub can be set up on a server that you own, they can be used to collaborate
on projects with sensitive data, and also can store data directly on the server
you would like to use to store large project datasets or to run
computationally-intensive pre-processing or analysis. Finally, most version
control platforms include tools that help you manage and track the project.
These include “Issue Trackers”, tools for exploring the history of each file
and each change, and features to assign project tasks to specific team members.
The next section will describe the features of version control platforms that
make them helpful as a tool for collaborating on scientific research. These
systems are being leveraged by some scientists, both to manage research projects
and collaborate on writing scientific manuscripts and grant proposals
<span class="citation">(Perez-Riverol et al. 2016)</span>.</p>
<p>Version control platforms are always used in conjunction with version control
software, like the <em>git</em> software described in the last module. The version
control platform leverages the history of commits that were made to the project,
as well as the version control software’s capabilities for merging changes made
by different people at different times. On top of these facilities, a version
control platform also adds attractive visual interfaces for working with the
project, free or low-cost online hosting of project files, and team management
tools for each project. In this sense, you can think of <em>git</em> as the engine, in
other words, and the version control platform as the driver’s seat, with
dashboard, steering wheel, and gears to leverage the power of the underlying
<em>git</em> software. One scientist, in an article about Git and GitHub for
scientists, highlighted that resources like GitHub are “essential for
collaborative software projects because they enable the organization and sharing
of programming tasks between different remote contributors.” <span class="citation">(Perez-Riverol et al. 2016)</span></p>
<p>A number of version control platforms are available. Two that are currently very
popular for scientific research are GitHub (<a href="https://github.com/" class="uri">https://github.com/</a>) and GitLab
(<a href="https://about.gitlab.com/" class="uri">https://about.gitlab.com/</a>). Both provide free options for scientific
researchers, including the capabilities for using both public and private
repositories in collaboration with other researchers.</p>
</div>
<div id="why-use-version-control-platforms" class="section level3 hasAnchor" number="2.10.2">
<h3><span class="header-section-number">2.10.2</span> Why use version control platforms?<a href="experimental-data-recording.html#why-use-version-control-platforms" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Version control platforms offer a number of advantages when collaborating on a
research project that can help to improve your efficiency, rigor, and
reproducibility. Further, there are several high-quality free versions of
version control platforms that are available for researchers, and as their use
becomes more popular, there are more and more resources to help you learn how to
use these platforms effectively. Open-source versions, like GitLab, even allow
you to set up a version control platform on a server you own, rather than
needing to post data or code on an outside platform, and so you can use these
tools even in cases involving sensitive data.</p>
<p>Some of the key advantages of using a version control platform like GitHub
to collaborate on research projects include:</p>
<ul>
<li>Ability to track and merge changes that different collaborators made to the
document</li>
<li>Ability to create alternative versions of project files (<em>branches</em>), and merge them into the main project as desired</li>
<li>Tools for project management, including Issue Trackers</li>
<li>Default backup of project files</li>
<li>Ability to share project information online, including through hosting websites related to the project or supplemental files related to a manuscript</li>
</ul>
<p>Many of these strengths draw directly on the functions provided by the
underlying version control software (e.g., <em>git</em>). However, the version control
platform will typically allow team members to explore and work with these
functions in an easier way than if they try to use the barebones version control
software. Years ago, the use of version control required users to
be familiar with the command line, and to send arcane commands to track the
project files through that interface. With the rising popularity of version
control platforms, version control for project management can be taught
relatively quickly to students with a few months—or even weeks—of coding
experience. In fact, version control is beginning to be used as a method of
turning in and grading homework in beginning programming classes, with students
learning these techniques in the first few weeks of class. This would be
practically unimaginable without the user-friendly interface of a version
control platform as a wrapper for the power of the version control software
itself.</p>
<p>The capacities of version control to track changes and histories of project
files becomes even more important when working in collaboration on a project,
and a version control platform helps in tracking and managing contributions from
team members. As the proverb about too many cooks in the kitchen captures, any
time you have multiple people working on a project, it introduces the chance for
conflicts. While higher-level conflicts, like about what you want the final
product to look like or who should do which jobs, can’t be easily managed by a
computer program, now the complications of integrating everyone’s
contributions—and letting people work in their own space and then bring
together their individual work into one final project—can be. While these
programs for version control were originally created to help with programmers
developing code, they can be used now to coordinate group work on numerous types
of file-based projects, including scientific manuscripts, books, and websites
<span class="citation">(E. Raymond 2009)</span>. And although they can work with projects that include
files saved in binary (Word documents, for example), they thrive in projects
with a heavier concentration of text-based files, and so they fit in nicely in a
scientific research / data analysis workflow that is based on data stored in
plain text formats and data analysis scripts written in plain text files, tools
we discuss in other modules.</p>
<p>There is one key feature of modern version control that’s critical to making
this work—resolving changes in files that started the same but were edited in
different ways by different people and now need to be put back together,
bringing along any changes made from the original version. This step is called
<em>merging</em> the files. While this is a feature driven by the git software itself,
you typically won’t use it until you’re collaborating on a project through
a version control platform like GitHub.</p>
<p>While this is typically described using the plural, “files”, at a higher-level,
you can think of this as just merging the <em>changes</em> that two people have made as
they edited a single file, a file where they both started out with identical
copies. Without version control, this process can be time-consuming and
frustrating. As one scientist notes:</p>
<blockquote>
<p>“You will likely share your code with multiple lab mates or collaborators,
and they may have suggestions on how to improve it. If you email the code
to multiple people, you will have to manually incorporate all the changes
each of them sends.” <span class="citation">(Blischak, Davenport, and Wilson 2016)</span></p>
</blockquote>
<p>Think of the file broken up into each of its separate lines. There will be some
lines that neither person changed. Those are easy to handle in the
“merge”—they stay the same as in the original copy of the file. Next, there
will be some lines that one person changed, but that the other person didn’t. It
turns out that these are pretty easy to handle, too. If only one person changed
the line, then you use their version—it’s the most up-to-date, since if both
people started out with the same version, it means that the other person didn’t
make any changes to that part of the file. Finally, there may be a few lines
that both people changed. These are called <em>merge conflicts</em>. They’re places in
the file where there’s not a clear, easy-to-automate way that the computer can
know which version to put into the integrated, latest version of the file.
Different version control programs handle these merge conflicts in different
ways.</p>
<p>For the most common version control program used today, <em>git</em>, these spots in
the file are flagged with a special set of symbols when you try to integrate the
two updated versions of the file. Along with the special symbols to denote a
conflict, there will also be <em>both</em> versions of the conflicting lines of the
file. Whoever is integrating the files must go in and pick the version of those
lines to use in the integrated version of the file, or write in some compromise
version of those lines that brings in elements from both people’s changes, and
then delete all the symbols denoting that was a conflict and save this latest
version of the file.</p>
<p>When you collaborate using a version control platform, you will also find that
the commit messages provide a way to communicate across the team members. For
example, if one person is the key person working on a certain file, but has run
into a problem with one spot and asks another team member to take a go, then the
second team member isn’t limited to just looking at the file and then emailing
some suggestions. Instead, the second person can make sure he or she has the
latest version of that file, make the changes they think will help,
<em>commit</em> those changes with a message (a <em>commit message</em>) about why they think
this change will fix the problem, and then push that latest version of the file
back to the first person. If there are several places where it would help to
change the file, then these can be fixed through several separate commits, each
with their own message. The first person, who originally asked for help, can
read through the updates in the file (most platforms for using version control
will now highlight where all these changes are in the file) and read the second
person’s message or messages about why each change might help. Even better, days
or months later, when team members are trying to figure out why a certain change
was made in that part of the file, can go back and read these messages to get an
explanation.</p>
<p>These commit messages help remind you of the logic behind evolutions to the
code. As Raymond notes:</p>
<blockquote>
<p>“You know your code has changed; do you know why? It’s easy to forget the
reasons for changes, and step on them later. If you have collaborators on a
project, how do you know what they have changed while you weren’t looking, and
who was responsible for each change?”</p>
</blockquote>
<p>Platforms for using git often include nice tools for visualizing differences
between two files, providing a more visual way to look at the differences between
files across time points in the project. For example, GitHub automatically shows
these using colors to highlight additions and subtractions of plain text for
one file compared to another version of it when you look through a repository’s
commit history. Similarly, RStudio provides a new “Commit” window that can be
used to compare differences between the original and revised version of plain
text files at a particular stage in the commit history.</p>
<p>In recent years, some complementary tools have been developed that make the
process of collaborating together using version control software easier. These
include <em>bug trackers</em> or <em>issue trackers</em>, which allow the team to keep a running “to-do”
list of what needs to be done to complete the project <span class="citation">(Perez-Riverol et al. 2016)</span>.</p>
<blockquote>
<p>“Lists aren’t external to the creative process, they are intrinsic to it. They are
a natural part of any project of scale, whether we like it or not.” <span class="citation">(Savage 2020)</span></p>
</blockquote>
<blockquote>
<p>“The maker in me knows that this is where lists really shine, that it is their capacity
for simplifying the complex that sets them apart from all other planning tools.
Not just at the beginning of a project, either, but at every step along the
creative process, because no matter how exacting the list you make at the outset,
there will always be things that you missed or, more frequently, that change. It’s
like trying to measure a coastline: it’s fractal.” <span class="citation">(Savage 2020)</span></p>
</blockquote>
<blockquote>
<p>“The value of a list is that it frees you up to think more creatively, by
defining a project’s scope and scale for you on the page, so your brain doesn’t
have to hold on to so much information. The beauty of the checkbox is that it
does the same thing with regard to progress, allowing you to monitor the status
of your project, without having to mentally keep track of everything.” <span class="citation">(Savage 2020)</span></p>
</blockquote>
<blockquote>
<p>“The best part of making a list is, you guessed it, crossing things off. But when you
physically cross them out, like with a pen, you can make them harder to read, which
destroys their informational value beyond that single project and, to me at least, makes
the whole thing feel incomplete. The checkbox allowed me to cross something off my list,
to see clearly <em>that</em> I’d crossed it off, and at the same time retain all its
information while not also adding to the cognitive load of interpreting the list.”
<span class="citation">(Savage 2020)</span></p>
</blockquote>
<p>If a project uses a version control platform, it is very easy to share data recorded
for the project publicly. In a project that uses git and GitHub version control
tools, it is easy to share the project data online once an associated manuscript
is published, an increasingly common request or requirement from journals and
funding agencies <span class="citation">(Blischak, Davenport, and Wilson 2016)</span>. Sharing data allows a more complete
assessment of the research by reviewers and readers and makes it easier for
other researchers to build off the published results in their own work,
extending and adapting the code to explore their own datasets or ask their own
research questions <span class="citation">(Perez-Riverol et al. 2016)</span>. On GitHub, you can set the access to a
project to be either public or private, a setting that can be converted easily from one
form to the other over the course of the project <span class="citation">(Metz 2015)</span>. A private
project can be viewed only by fellow team members, while a public project can be
viewed by anyone.</p>
<p>Further, because git tracks the full history of changes to
these documents, it includes functionality that lets you tag the code and data
at a specific point (for example, the date when a paper was submitted) so that
viewers can look at that specific version of the repository files, even while
the project team continues to move forward in improving files in the directory.
At the more advanced end of functionality, there are even ways to assign a
persistent digital identifier (e.g., a DOI, like those assigned to published articles)
to a specific version of a GitHub repository <span class="citation">(Perez-Riverol et al. 2016)</span>.</p>
<p>Version control platforms also help in providing a way to backup study data
<span class="citation">(Blischak, Davenport, and Wilson 2016; Perez-Riverol et al. 2016; J. Perkel 2018)</span>. Together, git and GitHub
provide a structure where the project directory (repository) is copied on
multiple computers, both the users’ laptop or desktop computers and on a remote
server hosted by GitHub or a similar organization. As you collaborate with
others using version control under a distributed model, each collaborator will
have their own copy of all project files on their local computer. All project
files are also stored on the remote repository to which you all push and pull
commits. If you are using the GitHub platform, this will be GitHub’s servers; if
you use GitLab, you can set up the system on your own server. Each time you push
or pull from the remote copy of the project repository, you are syncing your
copy of the project files with those on other computers.</p>
<p>This set-up makes it easy to bring all the project files onto a new
computer—all you have to do is clone the project repository. It also ensures
that there are copies of the full project directory, including all its files, in
multiple places <span class="citation">(Blischak, Davenport, and Wilson 2016)</span>. Further, not only is the data backed up
across multiple computers, but so is the full history of all changes made to
that data and the recorded messages explaining those changes, through the
repositories commit messages <span class="citation">(Perez-Riverol et al. 2016)</span>.</p>
<p>Leips highlights the importance of backup for research data and code:</p>
<blockquote>
<p>“Backup, backup, backup—this is the main action you can take to care for your
computers and your data. Many PIs assume that backup systems are inherently
permanent and foolproof, and it often takes a loss to remind one that
materials break, systems fail, and humans make mistakes. Even if your data
are backed up at work, have at least one other backup system. Keep at least
one backup off site, in case of a diaster in the lab (yes, fires and floods
do happen). It doesn’t make much sense to have two separate backup systems stored
next to each other in a drawer.” <span class="citation">(LEIPS 2010)</span></p>
</blockquote>
<p>Finally, version control platforms like GitHub can be used for a number
of supplementary tasks for your research project. These include publishing
webpages or other web resources linked to the project and otherwise improving
public engagement with the project, including by allowing other researchers
to copy and adapt your project through a process called <em>forking</em>. Version
control platforms also provide a supplemental backup to project files.</p>
<p>First, GitHub can be used to collaborate on, host, and publish websites and
other online content <span class="citation">(Perez-Riverol et al. 2016)</span>. Version control systems have been used by
some for a long time to help in writing longform materials like books (e.g.,
<span class="citation">(E. S. Raymond 2003)</span>); new tools are making the process even easier. The GitHub
Pages functionality, for example, is now being used to host a number of books
created in R using the <code>bookdown</code> package, including the online version of this
book. The <code>blogdown</code> package similarly can be used to create websites, either
for individual researchers, for research labs, or for specific projects or
collaborations.</p>
<p>Further, if a project includes the creation of scientific software, a version
control platform can be used to share that software—as well as associated
documentation—in a format that is easy for others to work with. The platform
can also be used to share supplemental material for a manuscript, including the
code used for preprocessing and analyzing data. Perez highlights this
functionality:</p>
<blockquote>
<p>“The traditional way to promote scientific software is by publishing an
associated paper in the peer-reviewed scientific literature, though, as pointed
out by Buckheir and Donoho, this is just advertising. Additional steps can boost
the visibility of an organization. For example, GitHub Pages are simple websites
freely hosted by GitHub. Users can create and host blog websites, help pages,
manuals, tutorials, and websites related to specific projects.” <span class="citation">(Perez-Riverol et al. 2016)</span></p>
</blockquote>
<p>The most popular version control platforms, GitHub and GitLab, both allow users
to toggle projects between “public” and “private” modes, which can be used to
work privately on a project prior to peer review and publication, and then
switch to a public mode after publication. This functionality will allow those
who access the code to see not only the final product, but also the history of
the development of the code and data for the project, providing more
transparency in the development process, but without jeopardizing the novelty of
the research results prior to publication.</p>
<p>With GitHub, while only collaborators on a public project can directly change
the code, anyone else can <em>suggest</em> changes through a process of copying a
version of the project (<em>forking</em> it). This allows someone to make the changes
they would like to suggest directly to a copy of the code, and then ask the
project’s owners to consider integrating the changes back into the main version
of the project through a <em>pull request</em>. GitHub therefore creates a platform
where people can explore, adapt, and add to other people’s coding projects,
enabling a community of coders <span class="citation">(Perez-Riverol et al. 2016)</span>, and because of this
functionality it has been described as “a social network for software
development” <span class="citation">(J. Perkel 2018)</span> and as “a kind of bazaar that offers just about
any piece of code you might want—and so much of it free.” <span class="citation">(Metz 2015)</span>.
This same process can be leveraged for others to copy and adapt code—this is
particularly helpful in ensuring that a software or research project won’t be
“orphaned” if its main developer is unavailable (e.g., retires, dies), but
instead can be picked up and continued by other interested researchers.
Copyright statements and licenses within code projects help to clarify
attribution and rights in these cases.</p>
<p>In the next module, we describe practical ways to leverage these resources
within your research group. We include instructions both for team leaders—who
may not code but may want to use GitHub within projects to help manage the
projects—as well as researchers who work directly with data and code for the
research team. There are also a number of excellent resources that are now
available that walk users through how to set up and use a version control
platform. The process is particularly straightforward when the research project
files are collected in an RStudio Project format, as described in earlier
modules.</p>
<!-- ### Discussion questions -->

</div>
</div>
<div id="module11" class="section level2 hasAnchor" number="2.11">
<h2><span class="header-section-number">2.11</span> Using git and GitLab to implement version control<a href="experimental-data-recording.html#module11" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For many years, use of version control required use of the command line,
limiting its accessibility to researchers with limited programming experience.
However, graphical interfaces have removed this barrier, and RStudio has
particularly user-friendly tools for implementing version control. In this
module, we will show how to use <em>git</em> through RStudio’s user-friendly
interface and how to connect from a local computer to <em>GitLab</em> through
RStudio.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Understand how to set up and use <em>git</em> through RStudio’s interface</li>
<li>Understand how to connect with <em>GitLab</em> through RStudio to collaborate on<br />
research projects while maintaining version control</li>
</ul>
<div id="how-to-use-version-control" class="section level3 hasAnchor" number="2.11.1">
<h3><span class="header-section-number">2.11.1</span> How to use version control<a href="experimental-data-recording.html#how-to-use-version-control" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this chapter, we will give you an overview of how to use git and GitHub
for your laboratory research projects. If you prefer an open-source version
control platform, GitLab has similar functionality and can be installed on
a server you own.</p>
<p>We’ll address two separate groups, in separate sections. As the main focus of
this module, we’ll provide an overview of how you can leverage and use these
tools as the director or manager of a project, without knowing how to code in a
language like R. We are focusing on this audience in this module, as we see this
as an area where there aren’t a lot of available resources to provide guidance.
GitHub provides a number of useful tools that can be used by anyone, providing a
common space for managing the data recording, analysis and reporting for a
scientific research project. In this case, there would need to be at least one
member of your team who is comfortable with a programming language, but all team
members can participate in many features of the GitHub repository regardless of
programming skill.</p>
<p>The other audience for information on using git and GitHub are researchers who
are comfortable coding. Fortunately, there are many good resources available for
this audience. We’ll end the module by providing advice to this audience to
point them to resources where they can go to learn more and fully develop these
skills.</p>
<p>As an example, we’ll show different elements from a real GitHub repository, used
for scientific projects and papers. The repository is available at
<a href="https://github.com/aef1004/cyto-feature_engineering" class="uri">https://github.com/aef1004/cyto-feature_engineering</a>. It provides example data
and code to accompany a published article on a pipeline for flow cytometry
analysis <span class="citation">(Fox et al. 2020)</span>.</p>
</div>
<div id="leveraging-git-and-github-as-a-project-director" class="section level3 hasAnchor" number="2.11.2">
<h3><span class="header-section-number">2.11.2</span> Leveraging git and GitHub as a project director<a href="experimental-data-recording.html#leveraging-git-and-github-as-a-project-director" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Because <code>git</code> has a history in software development, and because most
introductions to it quickly present arcane-looking code commands, you may have
hesitations about whether it would be useful in your scientific research group.
This is particularly likely to be the case if you, and many in your research
group, do not have experience programming.</p>
<p>This is not at all the case, and in fact, the combination of git and GitHub can
become a secret weapon for your research group if you are willing to encourage
those in your group who do know some programming (or are willing to learn a bit)
to take the time to learn to set up a project in this environment for project
management. Once a project has been set up in GitHub, there are a number of
features that can be used by all team members, whether they code or not. These
features facilitate collaboration between coders and non-coders as the data and
analysis code evolve. The major features and advantages of git and GitHub are
described in modules 2.9 and 2.10.</p>
<p>As mentioned in the previous two modules, repositories that are tracked with
git and shared through GitHub provide a number of tools that are useful in
managing a project, both in terms of keeping track of what’s been done in the
project and also for planning what needs to be done next, breaking those goals
into discrete tasks, assigning those tasks to team members, and maintaining a
discussion as you tackle those tasks.</p>
<p>While <code>git</code> itself traditionally has been used with a command-line interface
(think of the black and green computer screens shown when movies portray
hackers), GitHub has wrapped <code>git</code>’s functionality with an attractive and easy
to understand graphical user interface. This is how you will interact with a
project repository if you are online and logged into GitHub, rather than
exploring it on your own computer (although there are also graphical user
interfaces you can use to more easily explore git repositories locally, on your
computer).</p>
<p>Successfully using GitHub to help track and manage a research project does not
require using all of the available tools, and in fact you can go a long way by
just starting with a subset. At the end of this module, there is a video
demonstration that walks you through the elements we’ve highlighted. Key project
management tools for GitHub that you can leverage, all demonstrated in
subsections below, are:</p>
<ul>
<li>Exploring commits and commit history</li>
<li>Tracking and making progress on issues</li>
<li>Managing repository access and ownership</li>
</ul>
<p>GitHub is free to join; while there are paid plans, the free plan is adequate
for getting started, and academic researchers can request free use of some of
the more extensive versions if needed. To create an account, visit
<a href="https://github.com/" class="uri">https://github.com/</a>. Even if you are not coding, you will need to be logged in
to your GitHub account to contribute to a repository. For some actions, you need
to be a collaborator on a project to take the action; in the later sections of
this module, we describe how people can be added as collaborators in a GitLab
repository.</p>
<p><strong>Exploring commits and the commit history</strong></p>
<p>Each time a team member makes a change to files in a GitHub repository, the
change is recorded as a <em>commit</em>, and the team member must include a short
<em>commit message</em> describing the change. Each file in the project will have its
own page on GitHub (Figure <a href="experimental-data-recording.html#fig:githubcommits1">2.42</a> shows an example). You can
see the history of changes to that files by clicking the “History” link on that
page.</p>
<div class="figure"><span style="display:block;" id="fig:githubcommits1"></span>
<img src="figures/github_commits1.png" alt="Example of a file page within a GitHub repository. Each file in a repository has its own page. On this page, you can see the history of changes made to the file by looking at 'History'. You can also make a commit an edit directly in GitHub by clicking on the 'Edit' icon." width="\textwidth" />
<p class="caption">
Figure 2.42: Example of a file page within a GitHub repository. Each file in a repository has its own page. On this page, you can see the history of changes made to the file by looking at ‘History’. You can also make a commit an edit directly in GitHub by clicking on the ‘Edit’ icon.
</p>
</div>
<p>For team members who are working a lot on coding, they will usually make changes
to a file locally, on the repository copy on their own computers and then push
their latest changes to the GitHub version. This workflow will allow them to
test the code locally before they update the GitHub version.</p>
<p>However, it is also possible to make a commit directly on GitHub, and this may
be a useful option for team members who are not coding and would like to make
small changes to the writing files. On the file’s page on GitHub, there is an
“Edit” icon (Figure <a href="experimental-data-recording.html#fig:githubcommits1">2.42</a>). By clicking on this, you will
get to a page where you can directly edit the file (Figure
<a href="experimental-data-recording.html#fig:githubcommits2">2.43</a> shows an example of what this page looks like). Once
you have made your edits, you will need to commit them, along with a short
description of the commit, the “commit message”. If you would like to include a
longer explanation of your changes, there is space for that, as well, when you
make the commit (Figure <a href="experimental-data-recording.html#fig:githubcommits2">2.43</a>). These commits will show up
in the repository’s history, attributed to you and with your commit message
attached to the change.</p>
<div class="figure"><span style="display:block;" id="fig:githubcommits2"></span>
<img src="figures/github_commits2.png" alt="Committing changes directly in GitHub. When you click on the 'Edit' button in a file's GitHub page (see previous figure), it will take you to a page where you can edit the file directly. You save the changes with a 'commit', including a commit message describing why you made the change. The change will be tagged with the message and your name." width="\textwidth" />
<p class="caption">
Figure 2.43: Committing changes directly in GitHub. When you click on the ‘Edit’ button in a file’s GitHub page (see previous figure), it will take you to a page where you can edit the file directly. You save the changes with a ‘commit’, including a commit message describing why you made the change. The change will be tagged with the message and your name.
</p>
</div>
<p>You can see the full history of changes that have been made to each file in the
project, as shown in the example in Figure <a href="experimental-data-recording.html#fig:githubcommithistory">2.44</a>. Each
change is tracked through a commit, which includes markers of who made the
change and a message describing the change. This allows you to quickly pinpoint
changes in a file in your research project. Near the commit message are listings
of which team member made the commit and when it was made. This also helps you
see how team members have contributed as the file evolves.</p>
<div class="figure"><span style="display:block;" id="fig:githubcommithistory"></span>
<img src="figures/github_commit_history.png" alt="Commit history in GitHub. Each file in a repository has a 'History' page, where you can explore each change commited for the file. Each commit has a unique identifier and commit message describing the change. You can click on the entry for any of these commits to see the changes made to the file with the commit (see next figure)." width="\textwidth" />
<p class="caption">
Figure 2.44: Commit history in GitHub. Each file in a repository has a ‘History’ page, where you can explore each change commited for the file. Each commit has a unique identifier and commit message describing the change. You can click on the entry for any of these commits to see the changes made to the file with the commit (see next figure).
</p>
</div>
<p>If you click on one of the commits listed on a file’s History page (Figure
<a href="experimental-data-recording.html#fig:githubcommithistory">2.44</a> points to one example of where you would click),
it will take you to a page providing information on the changes made with that
commit (Figure <a href="experimental-data-recording.html#fig:githubcommithistory2">2.45</a>). This page provides a
line-by-line view of each change that was made to project files with that
commit, as well as the commit message for that commit. If the person
committing the change included a longer description or commentary,
this information will also be included.</p>
<p>Within the body of the page, you can see the changes made with the commit. Added
lines will be highlighted in green while deleted lines are highlighted in red.
If only part of a line was changed, it will be shown twice, once in red as its
version before the commit, and once in green showing its version following the
commit. You can visually compare the two versions of the line to see how it was
changed with the commit.</p>
<div class="figure"><span style="display:block;" id="fig:githubcommithistory2"></span>
<img src="figures/github_commit_history2.png" alt="Commit history in GitHub. Each commit has its own page, where you can explore what changes were made with the commit, who made them, and when they were committed." width="\textwidth" />
<p class="caption">
Figure 2.45: Commit history in GitHub. Each commit has its own page, where you can explore what changes were made with the commit, who made them, and when they were committed.
</p>
</div>
<p><strong>Issues</strong></p>
<p>GitHub, as well as other version control platforms, includes functionality that
will help your team collaborate on a project. A key tool is the “Issues”
tracker. Each repository includes this type of tracker, and it can be easily
used by all team members, whether they are comfortable coding or not. Figure
<a href="experimental-data-recording.html#fig:githubissues1">2.46</a> gives an example of <a href="https://github.com/aef1004/cyto-feature_engineering/issues">the Issues tracker
page</a> for the
repository we are using as an example.</p>
<div class="figure"><span style="display:block;" id="fig:githubissues1"></span>
<img src="figures/github_issues.png" alt="Issues tracker page for an example GitHub repository. Arrows highlight the tab to click to get to the Issues tracker page in a repository, as well as where to go to find open and closed Issues for the repository." width="\textwidth" />
<p class="caption">
Figure 2.46: Issues tracker page for an example GitHub repository. Arrows highlight the tab to click to get to the Issues tracker page in a repository, as well as where to go to find open and closed Issues for the repository.
</p>
</div>
<p>The main Issues tracker page provides clickable links to all open issues for
the repository. You can open a new issue using the “New Issue” on this main
page or on the specific page of any of the repository’s issues. See Figure
<a href="experimental-data-recording.html#fig:githubissues2">2.47</a> for an example of this button.</p>
<div class="figure"><span style="display:block;" id="fig:githubissues2"></span>
<img src="figures/github_issues2.png" alt="Conversation about an Issue on Issues tracker page of an example GitHub repository. In this example, you can see how GitHub Issues trackers allow you to discuss how to resolve an issue across your team. From this page, you can read the current conversation about Issue \#1 of the repository and add your own comments. Once the Issue is resolved, you can 'Close' the Issue, which moves it off the list of active issues, but allows you to still re-read the conversation and, if necessary, re-open the issue later. You can also open a new issue from this page, using the button highlighted at the top right." width="\textwidth" />
<p class="caption">
Figure 2.47: Conversation about an Issue on Issues tracker page of an example GitHub repository. In this example, you can see how GitHub Issues trackers allow you to discuss how to resolve an issue across your team. From this page, you can read the current conversation about Issue #1 of the repository and add your own comments. Once the Issue is resolved, you can ‘Close’ the Issue, which moves it off the list of active issues, but allows you to still re-read the conversation and, if necessary, re-open the issue later. You can also open a new issue from this page, using the button highlighted at the top right.
</p>
</div>
<p>On the page for a specific issue (e.g., Figure <a href="experimental-data-recording.html#fig:githubissues2">2.47</a>), you
can have a conversation with your team to determine how to resolve the issue.
This conversation can include web links, figures, and “To-do” check boxes, to
help you discuss and plan how to resolve the issue. Each issue is numbered,
which allows you to track each individually as you work on the project.</p>
<p>Once you have resolved an issue, you will close it. This moves the issue
from the active list into a “Closed” list. Each closed issue still has its
own page, where you can read through the conversation describing how it
was resolved. If you need to, you can re-open a closed issue later, if you
determine that it was not fully resolved.</p>
<div class="figure"><span style="display:block;" id="fig:githubissues3"></span>
<img src="figures/github_issues3.png" alt="Labeling and assigning Issues. The GitHub Issues tracker allows you to assign each issue to one or more team members, clarifying that they will take the lead in resolving the issue. It also allows you to tag each issue with one or more labels, so you can easily navigate to issues of a specific type or identify the category of a specific issue." width="\textwidth" />
<p class="caption">
Figure 2.48: Labeling and assigning Issues. The GitHub Issues tracker allows you to assign each issue to one or more team members, clarifying that they will take the lead in resolving the issue. It also allows you to tag each issue with one or more labels, so you can easily navigate to issues of a specific type or identify the category of a specific issue.
</p>
</div>
<p>The Issues tracker page includes some more advanced functionality, as well
(Figure <a href="experimental-data-recording.html#fig:githubissues3">2.48</a>). For example, you can assign an issue to one
of more team members, indicating that they are responsible for resolving that
issue. You can also tag each issue with one of more labels, allowing you to
group issues into common categories. For example, you could tag all issues that
cover questions about pre-processing the data using a “pre-processing” label,
and all that are related to creating figures for the final manuscript with a
“figures” label.</p>
<p><strong>Repository access and ownership</strong></p>
<p>Repositories include functionality for inviting team members, assigning
roles, and otherwise managing access to the repository. First, a repository
can be either public or private. For a public repository, anyone will be
able to see the full contents of the repository through GitHub. You can
also set a repository to be private. In this case, the repository can only
be seen by those who have been invited to collaborate on the repository, and
only when they are logged in to their GitHub accounts. The private / public
status of a repository can be changed at any time, so if you want you can
maintain a repository for a project as private until you publish the results,
and then switch it to be public, to allow others to explore the code and data
that are linked to your published results.</p>
<p>You can invite team members to collaborate on a repository, as long as they have
GitHub accounts (these are free to sign up for). While public repositories can
be seen by anyone, the only people who can add to or change the contents of the
repository are people who have been invited to collaborate on the repository.
The person who creates the repository (the repository “owner”) can invite other
collaborators through the “Settings” tab of the repository, which will have a
“Manage access” function for the repositories maintainer. Only the owner of the
repository will have access to this tab for the repo. On this page, you can
invite other collaborators by searching using their GitHub “handle” (the short
name they chose to be identified by in GitHub). You can also change access
rights, for example, allowing some team members to be able to make major changes
to the repository—like deleting it—while others can make only smaller
modifications.</p>
<p>[Add: Roles on a repository]</p>
</div>
<div id="leveraging-git-and-github-as-a-scientist-who-programs" class="section level3 hasAnchor" number="2.11.3">
<h3><span class="header-section-number">2.11.3</span> Leveraging git and GitHub as a scientist who programs<a href="experimental-data-recording.html#leveraging-git-and-github-as-a-scientist-who-programs" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To be able to leverage GitHub to manage projects and share data, you will need
to have at least one person in the research group who can set up the initial
repository. GitHub repositories can be created very easily starting from an
RStudio Project, a format for organizing project files that was described in
module 3.7.</p>
<p>There are many excellent resources that provide instructions on this topic
meant for researchers who are comfortable with using R and RStudio. An
excellent place to start is with</p>
<p>[Jenny Bryan’s book]</p>
<p>Once you’ve explored that resource, here are some others you might also find
useful:</p>
<p>[Other resources]</p>
</div>
<div id="applied-exercise-2" class="section level3 hasAnchor" number="2.11.4">
<h3><span class="header-section-number">2.11.4</span> Applied exercise<a href="experimental-data-recording.html#applied-exercise-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="experimental-data-preprocessing.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/01-separating.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["improve_repro.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
