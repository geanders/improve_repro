<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Experimental Data Recording | Improving the Reproducibility of Experimental Data Recording and Pre-Processing</title>
  <meta name="description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Experimental Data Recording | Improving the Reproducibility of Experimental Data Recording and Pre-Processing" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Experimental Data Recording | Improving the Reproducibility of Experimental Data Recording and Pre-Processing" />
  
  <meta name="twitter:description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing." />
  

<meta name="author" content="Brooke Anderson, Michael Lyons, Mercedes Gonzalez-Juarrero, Marcela Henao-Tamayo, and Gregory Robertson" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="experimental-data-preprocessing.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Visualization in R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Overview</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i><b>1.1</b> License</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html"><i class="fa fa-check"></i><b>2</b> Experimental Data Recording</a>
<ul>
<li class="chapter" data-level="2.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module1"><i class="fa fa-check"></i><b>2.1</b> Separating data recording and analysis</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#data-recording-versus-data-analysis"><i class="fa fa-check"></i><b>2.1.1</b> Data recording versus data analysis</a></li>
<li class="chapter" data-level="2.1.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#hazards-of-combining-recording-and-analysis"><i class="fa fa-check"></i><b>2.1.2</b> Hazards of combining recording and analysis</a></li>
<li class="chapter" data-level="2.1.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#approaches-to-separate-recording-and-analysis"><i class="fa fa-check"></i><b>2.1.3</b> Approaches to separate recording and analysis</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module2"><i class="fa fa-check"></i><b>2.2</b> Principles and power of structured data formats</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#data-recording-standards"><i class="fa fa-check"></i><b>2.2.1</b> Data recording standards</a></li>
<li class="chapter" data-level="2.2.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#defining-data-standards-for-a-research-group"><i class="fa fa-check"></i><b>2.2.2</b> Defining data standards for a research group</a></li>
<li class="chapter" data-level="2.2.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#two-dimensional-structured-data-format"><i class="fa fa-check"></i><b>2.2.3</b> Two-dimensional structured data format</a></li>
<li class="chapter" data-level="2.2.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#saving-two-dimensional-structured-data-in-plain-text-file-formats"><i class="fa fa-check"></i><b>2.2.4</b> Saving two-dimensional structured data in plain text file formats</a></li>
<li class="chapter" data-level="2.2.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#occassions-for-more-complex-data-structures-and-file-formats"><i class="fa fa-check"></i><b>2.2.5</b> Occassions for more complex data structures and file formats</a></li>
<li class="chapter" data-level="2.2.6" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#levels-of-standardizationresearch-group-to-research-community"><i class="fa fa-check"></i><b>2.2.6</b> Levels of standardization—research group to research community</a></li>
<li class="chapter" data-level="2.2.7" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#applied-exercise"><i class="fa fa-check"></i><b>2.2.7</b> Applied exercise</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module3"><i class="fa fa-check"></i><b>2.3</b> The ‘tidy’ data format</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#what-makes-data-tidy"><i class="fa fa-check"></i><b>2.3.1</b> What makes data “tidy?”</a></li>
<li class="chapter" data-level="2.3.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#why-make-your-data-tidy"><i class="fa fa-check"></i><b>2.3.2</b> Why make your data “tidy?”</a></li>
<li class="chapter" data-level="2.3.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#using-tidyverse-tools-with-data-in-the-tidy-data-format"><i class="fa fa-check"></i><b>2.3.3</b> Using tidyverse tools with data in the “tidy data” format</a></li>
<li class="chapter" data-level="2.3.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#practice-quiz"><i class="fa fa-check"></i><b>2.3.4</b> Practice quiz</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module4"><i class="fa fa-check"></i><b>2.4</b> Designing templates for “tidy” data collection</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#exampledata-on-rate-of-bacterial-growth"><i class="fa fa-check"></i><b>2.4.1</b> Example—Data on rate of bacterial growth</a></li>
<li class="chapter" data-level="2.4.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#features-that-make-data-collection-templates-untidy"><i class="fa fa-check"></i><b>2.4.2</b> Features that make data collection templates “untidy”</a></li>
<li class="chapter" data-level="2.4.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#converting-to-a-tidier-format-for-data-collection-templates"><i class="fa fa-check"></i><b>2.4.3</b> Converting to a “tidier” format for data collection templates</a></li>
<li class="chapter" data-level="2.4.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#learning-more-about-tidy-data-collection-in-the-laboratory"><i class="fa fa-check"></i><b>2.4.4</b> Learning more about tidy data collection in the laboratory</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module5"><i class="fa fa-check"></i><b>2.5</b> Example: Creating a template for “tidy” data collection</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#example-datadata-on-rate-of-bacterial-growth"><i class="fa fa-check"></i><b>2.5.1</b> Example data—Data on rate of bacterial growth</a></li>
<li class="chapter" data-level="2.5.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#limiting-the-template-to-the-collection-of-data"><i class="fa fa-check"></i><b>2.5.2</b> Limiting the template to the collection of data</a></li>
<li class="chapter" data-level="2.5.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#making-sensible-choices-about-rows-and-columns"><i class="fa fa-check"></i><b>2.5.3</b> Making sensible choices about rows and columns</a></li>
<li class="chapter" data-level="2.5.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#avoiding-problematic-characters-or-formatting"><i class="fa fa-check"></i><b>2.5.4</b> Avoiding problematic characters or formatting</a></li>
<li class="chapter" data-level="2.5.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#moving-further-data-analysis-to-later-in-the-pipeline"><i class="fa fa-check"></i><b>2.5.5</b> Moving further data analysis to later in the pipeline</a></li>
<li class="chapter" data-level="2.5.6" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#exampledata-on-rate-of-bacterial-growth-1"><i class="fa fa-check"></i><b>2.5.6</b> Example—Data on rate of bacterial growth</a></li>
<li class="chapter" data-level="2.5.7" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#exampledata-on-bacteria-colony-forming-units"><i class="fa fa-check"></i><b>2.5.7</b> Example—Data on bacteria colony forming units</a></li>
<li class="chapter" data-level="2.5.8" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#exampledata-from-multiple-related-experiments"><i class="fa fa-check"></i><b>2.5.8</b> Example—Data from multiple related experiments</a></li>
<li class="chapter" data-level="2.5.9" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#issues-with-these-data-sets"><i class="fa fa-check"></i><b>2.5.9</b> Issues with these data sets</a></li>
<li class="chapter" data-level="2.5.10" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#final-tidy-examples"><i class="fa fa-check"></i><b>2.5.10</b> Final “tidy” examples</a></li>
<li class="chapter" data-level="2.5.11" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#options-for-recording-tidy-data"><i class="fa fa-check"></i><b>2.5.11</b> Options for recording tidy data</a></li>
<li class="chapter" data-level="2.5.12" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#examples-of-how-tidy-data-can-be-easily-analyzed-visualized"><i class="fa fa-check"></i><b>2.5.12</b> Examples of how “tidy” data can be easily analyzed / visualized</a></li>
<li class="chapter" data-level="2.5.13" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#discussion-questions"><i class="fa fa-check"></i><b>2.5.13</b> Discussion questions</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module6"><i class="fa fa-check"></i><b>2.6</b> Power of using a single structured ‘Project’ directory for storing and tracking research project files</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#example-project"><i class="fa fa-check"></i><b>2.6.1</b> Example project</a></li>
<li class="chapter" data-level="2.6.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#creating-an-organized-directory-for-project-files"><i class="fa fa-check"></i><b>2.6.2</b> Creating an organized directory for project files</a></li>
<li class="chapter" data-level="2.6.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#making-the-project-directory-and-r-project"><i class="fa fa-check"></i><b>2.6.3</b> Making the project directory and R Project</a></li>
<li class="chapter" data-level="2.6.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#organizing-project-files-through-the-file-system"><i class="fa fa-check"></i><b>2.6.4</b> Organizing project files through the file system</a></li>
<li class="chapter" data-level="2.6.5" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#organizing-files-within-a-project-directory"><i class="fa fa-check"></i><b>2.6.5</b> Organizing files within a project directory</a></li>
<li class="chapter" data-level="2.6.6" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#using-rstudio-projects-with-project-file-directories"><i class="fa fa-check"></i><b>2.6.6</b> Using RStudio Projects with project file directories</a></li>
<li class="chapter" data-level="2.6.7" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-1"><i class="fa fa-check"></i><b>2.6.7</b> Subsection 1</a></li>
<li class="chapter" data-level="2.6.8" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-2"><i class="fa fa-check"></i><b>2.6.8</b> Subsection 2</a></li>
<li class="chapter" data-level="2.6.9" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#practice-quiz-1"><i class="fa fa-check"></i><b>2.6.9</b> Practice quiz</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module7"><i class="fa fa-check"></i><b>2.7</b> Creating ‘Project’ templates</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#making-an-existing-file-directory-an-rstudio-project"><i class="fa fa-check"></i><b>2.7.1</b> Making an existing file directory an RStudio Project</a></li>
<li class="chapter" data-level="2.7.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#making-an-rstudio-project-template"><i class="fa fa-check"></i><b>2.7.2</b> Making an RStudio Project Template</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#creating-project-templates-in-rstudio"><i class="fa fa-check"></i><b>2.8</b> Creating ‘Project’ templates in RStudio</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#discussion-questions-1"><i class="fa fa-check"></i><b>2.8.1</b> Discussion questions</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module8"><i class="fa fa-check"></i><b>2.9</b> Example: Creating a ‘Project’ template</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-1-1"><i class="fa fa-check"></i><b>2.9.1</b> Subsection 1</a></li>
<li class="chapter" data-level="2.9.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#subsection-2-1"><i class="fa fa-check"></i><b>2.9.2</b> Subsection 2</a></li>
<li class="chapter" data-level="2.9.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#applied-exercise-1"><i class="fa fa-check"></i><b>2.9.3</b> Applied exercise</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module9"><i class="fa fa-check"></i><b>2.10</b> Harnessing version control for transparent data recording</a>
<ul>
<li class="chapter" data-level="2.10.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#what-is-version-control"><i class="fa fa-check"></i><b>2.10.1</b> What is version control?</a></li>
<li class="chapter" data-level="2.10.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#recording-data-in-the-laboratoryfrom-paper-to-computers"><i class="fa fa-check"></i><b>2.10.2</b> Recording data in the laboratory—from paper to computers</a></li>
<li class="chapter" data-level="2.10.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#discussion-questions-2"><i class="fa fa-check"></i><b>2.10.3</b> Discussion questions</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module10"><i class="fa fa-check"></i><b>2.11</b> Enhance the reproducibility of collaborative research with version control platforms</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#what-are-version-control-platforms"><i class="fa fa-check"></i><b>2.11.1</b> What are version control platforms?</a></li>
<li class="chapter" data-level="2.11.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#why-use-version-control-platforms"><i class="fa fa-check"></i><b>2.11.2</b> Why use version control platforms?</a></li>
<li class="chapter" data-level="2.11.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#how-to-use-github"><i class="fa fa-check"></i><b>2.11.3</b> How to use GitHub</a></li>
<li class="chapter" data-level="2.11.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#discussion-questions-3"><i class="fa fa-check"></i><b>2.11.4</b> Discussion questions</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#module11"><i class="fa fa-check"></i><b>2.12</b> Using git and GitLab to implement version control</a>
<ul>
<li class="chapter" data-level="2.12.1" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#how-to-use-version-control"><i class="fa fa-check"></i><b>2.12.1</b> How to use version control</a></li>
<li class="chapter" data-level="2.12.2" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#leveraging-git-and-github-as-a-project-director"><i class="fa fa-check"></i><b>2.12.2</b> Leveraging git and GitHub as a project director</a></li>
<li class="chapter" data-level="2.12.3" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#leveraging-git-and-github-as-a-scientist-who-programs"><i class="fa fa-check"></i><b>2.12.3</b> Leveraging git and GitHub as a scientist who programs</a></li>
<li class="chapter" data-level="2.12.4" data-path="experimental-data-recording.html"><a href="experimental-data-recording.html#applied-exercise-2"><i class="fa fa-check"></i><b>2.12.4</b> Applied exercise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html"><i class="fa fa-check"></i><b>3</b> Experimental Data Preprocessing</a>
<ul>
<li class="chapter" data-level="3.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#module12"><i class="fa fa-check"></i><b>3.1</b> Principles and benefits of scripted pre-processing of experimental data</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#what-is-pre-processing"><i class="fa fa-check"></i><b>3.1.1</b> What is pre-processing?</a></li>
<li class="chapter" data-level="3.1.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#approaches-to-simple-preprocessing-tasks"><i class="fa fa-check"></i><b>3.1.2</b> Approaches to simple preprocessing tasks</a></li>
<li class="chapter" data-level="3.1.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#approaches-to-more-complex-preprocessing-tasks"><i class="fa fa-check"></i><b>3.1.3</b> Approaches to more complex preprocessing tasks</a></li>
<li class="chapter" data-level="3.1.4" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#scripting-preprocessing-tasks"><i class="fa fa-check"></i><b>3.1.4</b> Scripting preprocessing tasks</a></li>
<li class="chapter" data-level="3.1.5" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#potential-quotes"><i class="fa fa-check"></i><b>3.1.5</b> Potential quotes</a></li>
<li class="chapter" data-level="3.1.6" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#discussion-questions-4"><i class="fa fa-check"></i><b>3.1.6</b> Discussion questions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#module13"><i class="fa fa-check"></i><b>3.2</b> Introduction to scripted data pre-processing in R</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#compiled-versus-interpreted-programming-languages"><i class="fa fa-check"></i><b>3.2.1</b> Compiled versus interpreted programming languages</a></li>
<li class="chapter" data-level="3.2.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#code-scripts-versus-interactive-coding"><i class="fa fa-check"></i><b>3.2.2</b> Code scripts versus interactive coding</a></li>
<li class="chapter" data-level="3.2.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#process-of-building-a-code-script"><i class="fa fa-check"></i><b>3.2.3</b> Process of building a code script</a></li>
<li class="chapter" data-level="3.2.4" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#applied-exercise-3"><i class="fa fa-check"></i><b>3.2.4</b> Applied exercise</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#module14"><i class="fa fa-check"></i><b>3.3</b> Simplify scripted pre-processing through R’s ‘tidyverse’ tools</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#key-elements"><i class="fa fa-check"></i><b>3.3.1</b> Key elements</a></li>
<li class="chapter" data-level="3.3.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#limitations-of-object-oriented-programming"><i class="fa fa-check"></i><b>3.3.2</b> Limitations of object-oriented programming</a></li>
<li class="chapter" data-level="3.3.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#the-tidyverse-approach"><i class="fa fa-check"></i><b>3.3.3</b> The “tidyverse” approach</a></li>
<li class="chapter" data-level="3.3.4" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#how-to-tidyverse"><i class="fa fa-check"></i><b>3.3.4</b> How to “tidyverse”</a></li>
<li class="chapter" data-level="3.3.5" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#subsection-1-2"><i class="fa fa-check"></i><b>3.3.5</b> Subsection 1</a></li>
<li class="chapter" data-level="3.3.6" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#subsection-2-2"><i class="fa fa-check"></i><b>3.3.6</b> Subsection 2</a></li>
<li class="chapter" data-level="3.3.7" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#practice-quiz-2"><i class="fa fa-check"></i><b>3.3.7</b> Practice quiz</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#module15"><i class="fa fa-check"></i><b>3.4</b> Complex data types in experimental data pre-processing</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#introduction"><i class="fa fa-check"></i><b>3.4.1</b> Introduction</a></li>
<li class="chapter" data-level="3.4.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#data-structures-and-the-dataframe"><i class="fa fa-check"></i><b>3.4.2</b> Data structures and the dataframe</a></li>
<li class="chapter" data-level="3.4.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#more-complex-data-structures-in-r"><i class="fa fa-check"></i><b>3.4.3</b> More complex data structures in R</a></li>
<li class="chapter" data-level="3.4.4" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#limitations-to-complex-data-structures"><i class="fa fa-check"></i><b>3.4.4</b> Limitations to complex data structures</a></li>
<li class="chapter" data-level="3.4.5" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#complex-versus-simple-structures-for-storing-data"><i class="fa fa-check"></i><b>3.4.5</b> Complex versus simple structures for storing data</a></li>
<li class="chapter" data-level="3.4.6" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#practice-quiz-3"><i class="fa fa-check"></i><b>3.4.6</b> Practice quiz</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#module16"><i class="fa fa-check"></i><b>3.5</b> Complex data types in R and Bioconductor</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#rs-list-data-structure-and-list-based-structures"><i class="fa fa-check"></i><b>3.5.1</b> R’s list data structure and list-based structures</a></li>
<li class="chapter" data-level="3.5.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#exploring-and-extracting-data-from-r-list-data-structures"><i class="fa fa-check"></i><b>3.5.2</b> Exploring and extracting data from R list data structures</a></li>
<li class="chapter" data-level="3.5.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#interfacing-between-object-based-and-tidyverse-workflows"><i class="fa fa-check"></i><b>3.5.3</b> Interfacing between object-based and tidyverse workflows</a></li>
<li class="chapter" data-level="3.5.4" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#extras"><i class="fa fa-check"></i><b>3.5.4</b> Extras</a></li>
<li class="chapter" data-level="3.5.5" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#subsection-2-3"><i class="fa fa-check"></i><b>3.5.5</b> Subsection 2</a></li>
<li class="chapter" data-level="3.5.6" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#applied-exercise-4"><i class="fa fa-check"></i><b>3.5.6</b> Applied exercise</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#module17"><i class="fa fa-check"></i><b>3.6</b> Example: Converting from complex to ‘tidy’ data formats</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#combining-bioconductor-and-tidyverse-approaches-in-a-workflow"><i class="fa fa-check"></i><b>3.6.1</b> Combining Bioconductor and tidyverse approaches in a workflow</a></li>
<li class="chapter" data-level="3.6.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#the-biobroom-package"><i class="fa fa-check"></i><b>3.6.2</b> The <code>biobroom</code> package</a></li>
<li class="chapter" data-level="3.6.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#the-ggbio-package"><i class="fa fa-check"></i><b>3.6.3</b> The <code>ggbio</code> package</a></li>
<li class="chapter" data-level="3.6.4" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#subsection-2-4"><i class="fa fa-check"></i><b>3.6.4</b> Subsection 2</a></li>
<li class="chapter" data-level="3.6.5" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#applied-exercise-5"><i class="fa fa-check"></i><b>3.6.5</b> Applied exercise</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#module18"><i class="fa fa-check"></i><b>3.7</b> Introduction to reproducible data pre-processing protocols</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#introducing-reproducible-data-pre-processing-protocols"><i class="fa fa-check"></i><b>3.7.1</b> Introducing reproducible data pre-processing protocols</a></li>
<li class="chapter" data-level="3.7.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#using-knitted-documents-for-protocols"><i class="fa fa-check"></i><b>3.7.2</b> Using knitted documents for protocols</a></li>
<li class="chapter" data-level="3.7.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#advantages-of-using-knitted-documents-for-data-focused-protocols"><i class="fa fa-check"></i><b>3.7.3</b> Advantages of using knitted documents for data-focused protocols</a></li>
<li class="chapter" data-level="3.7.4" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#how-knitted-documents-work"><i class="fa fa-check"></i><b>3.7.4</b> How knitted documents work</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#module19"><i class="fa fa-check"></i><b>3.8</b> RMarkdown for creating reproducible data pre-processing protocols</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#creating-knitted-documents-in-r"><i class="fa fa-check"></i><b>3.8.1</b> Creating knitted documents in R</a></li>
<li class="chapter" data-level="3.8.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#formatting-text-with-markdown-in-rmarkdown"><i class="fa fa-check"></i><b>3.8.2</b> Formatting text with Markdown in Rmarkdown</a></li>
<li class="chapter" data-level="3.8.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#preambles-in-rmarkdown-documents"><i class="fa fa-check"></i><b>3.8.3</b> Preambles in Rmarkdown documents</a></li>
<li class="chapter" data-level="3.8.4" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#executable-code-in-rmarkdown-files"><i class="fa fa-check"></i><b>3.8.4</b> Executable code in Rmarkdown files</a></li>
<li class="chapter" data-level="3.8.5" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#more-advanced-rmarkdown-functionality"><i class="fa fa-check"></i><b>3.8.5</b> More advanced Rmarkdown functionality</a></li>
<li class="chapter" data-level="3.8.6" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#learning-more-about-rmarkdown."><i class="fa fa-check"></i><b>3.8.6</b> Learning more about Rmarkdown.</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#module20"><i class="fa fa-check"></i><b>3.9</b> Example: Creating a reproducible data pre-processing protocol</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#introduction-and-example-data"><i class="fa fa-check"></i><b>3.9.1</b> Introduction and example data</a></li>
<li class="chapter" data-level="3.9.2" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#advice-on-designing-a-pre-processing-protocol"><i class="fa fa-check"></i><b>3.9.2</b> Advice on designing a pre-processing protocol</a></li>
<li class="chapter" data-level="3.9.3" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#writing-data-pre-processing-protocols"><i class="fa fa-check"></i><b>3.9.3</b> Writing data pre-processing protocols</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="experimental-data-preprocessing.html"><a href="experimental-data-preprocessing.html#applied-exercise-6"><i class="fa fa-check"></i><b>3.10</b> Applied exercise</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>4</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Improving the Reproducibility of Experimental Data Recording and Pre-Processing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="experimental-data-recording" class="section level1" number="2">
<h1><span class="header-section-number">Chapter 2</span> Experimental Data Recording</h1>
<p>This section includes modules on:</p>
<ul>
<li><a href="experimental-data-recording.html#module1">Module 2.1: Separating data recording and analysis</a></li>
<li><a href="experimental-data-recording.html#module2">Module 2.2: Principles and power of structured data formats</a></li>
<li><a href="experimental-data-recording.html#module3">Module 2.3: The ‘tidy’ data format</a></li>
<li><a href="experimental-data-recording.html#module4">Module 2.4: Designing templates for “tidy” data collection</a></li>
<li><a href="experimental-data-recording.html#module5">Module 2.5: Example: Creating a template for “tidy” data collection</a></li>
<li><a href="experimental-data-recording.html#module6">Module 2.6: Power of using a single structured ‘Project’ directory for storing and tracking research project files</a></li>
<li><a href="experimental-data-recording.html#module7">Module 2.7: Creating ‘Project’ templates</a></li>
<li><a href="experimental-data-recording.html#module8">Module 2.8: Example: Creating a ‘Project’ template</a></li>
<li><a href="experimental-data-recording.html#module9">Module 2.9: Harnessing version control for transparent data recording</a></li>
<li><a href="experimental-data-recording.html#module10">Module 2.10: Enhance the reproducibility of collaborative research with version control platforms</a></li>
<li><a href="experimental-data-recording.html#module11">Module 2.11: Using git and GitLab to implement version control</a></li>
</ul>
<div id="module1" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Separating data recording and analysis</h2>
<p>Many biomedical laboratories currently use spreadsheet programs to jointly
record, visualize, and analyze experimental data <span class="citation">(Broman and Woo 2018)</span>. These
software tools, such as Microsoft Excel[Ref, copyright?] or Google Sheets[Ref,
copyright?], provide for manual or automated entry of data into rows and columns
of cells. Standard or custom formulas and other operations can be applied to the
cells, and are commonly used to reformat or clean the data, calculate various
statistics, and to generate simple plots; all of which are embedded as
additional data entries and programming elements within the spreadsheet. While
these tools greatly improved the paper worksheets on which they were originally
based <span class="citation">(Campbell-Kelly 2007)</span>, this all-in-one practice impedes the transparency
and reproducibility of both recording and analysis of the large and complex data
sets that are routinely generated in life science experiments.</p>
<p>To improve the computational reproducibility of a research project, it is
critical for biomedical researchers to learn the importance of maintaining
recorded experimental data as “read-only” files, separating data recording from
any data pre-processing or data analysis steps <span class="citation">(Broman and Woo 2018; Marwick, Boettiger, and Mullen 2018)</span>. Statisticians have outlined specific methods that a
laboratory-based scientist can take to ensure that data shared in an Excel
spreadsheet are shared in a reliable and reproducible way, including avoiding
macros or embedded formulas, using a separate Excel file for each dataset,
recording descriptions of variables in a separate code book rather than in the
Excel file, avoiding the use of color of the cells to encode information, using
“NA” to code missing values, avoiding spaces in column headers, and avoiding
splitting or merging cells <span class="citation">(Ellis and Leek 2018; Broman and Woo 2018)</span>. In this module,
we will describe this common practice and will outline alternative approaches
that separate the steps of data recording and data analysis.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Explain the difference between data recording and data analysis</li>
<li>Understand why collecting data on spreadsheets with embedded formulas impedes
reproducibility</li>
<li>List alternative approaches to improve reproducibility</li>
</ul>
<div id="data-recording-versus-data-analysis" class="section level3" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Data recording versus data analysis</h3>
<p><strong>History of spreadsheets.</strong></p>
<p>Spreadsheets have long been an extremely popular tool for recording and
analyzing data, in part because they allow people without programming experience
to conduct a range of standard computations and statistical analyses through a
visual interface that is more immediately user-friendly to non-programmers than
programs with command line interfaces. An early target for spreadsheet programs
in terms of users was business executives, and so the programs were designed to
be very simple and easy to use—just one step up in complexity from crunching
numbers on the back of an envelope <span class="citation">(Campbell-Kelly 2007)</span>. Spreadsheet programs
in fact became so popular within businesses that many attribute these programs
with driving the uptake of personal computers <span class="citation">(Campbell-Kelly 2007)</span>.</p>
<p>Spreadsheets were innovative and rapidly adapted in part because they allowed
users to combine data recording and analysis—while previously, in business
settings, any complicated data analysis task needed to be outsourced to
mainframe computers and data processing teams, the initial spreadsheet program
(VisiCalc) allowed one person to quickly apply and test different models or
calculations on recorded data <span class="citation">(Levy 1984)</span>. These spreadsheet programs
allowed non-programmers to engage with data, including data processing and
analysis tasks, in a way that previously required programming expertise
<span class="citation">(Levy 1984)</span>.</p>
<p><strong>Use of spreadsheets.</strong></p>
<p>Many scientific laboratories use spreadsheets within their data collection
process, both to record data and to clean and analyze the data. Illustrative
examples can be found in surveys of over 250 biomedical researchers at the University
of Washington <span class="citation">(Anderson et al. 2007)</span>, and of neuroscience researchers at the
University of Newcastle, with most respondents reporting the use of spreadsheets
and other general-purpose software in their research <span class="citation">(AlTarawneh and Thorne 2017)</span>.
A working group on bioinformatics and data-intensive science similarly found
spreadsheets were the most common tool used across attendees
<span class="citation">(Barga et al. 2011)</span>.</p>
<p>In some cases, a spreadsheet is used solely to record data, as a simple type of
database <span class="citation">(Birch, Lyford-Smith, and Guo 2018)</span>. However, biomedical researchers often use
spreadsheets to both record and analyze experimental data <span class="citation">(Anderson et al. 2007)</span>.
In this case, data processing and analysis is implemented through the use of
formulas and macros embedded within the spreadsheet. When a spreadsheet has
formulas or macros within it, the spreadsheet program creates an internal record
of how cells are connected through these formulas. For example, if the value in
a specific cell is converted from Fahrenheit to Celsius to fill a second cell,
and then that value is combined with other values in a column to calculate the
mean temperature across several observations, then the spreadsheet program has
internally saved how the later cells depend on the earlier ones. When you change
the value recorded in a cell of a spreadsheet, the spreadsheet program queries
this record and only recalculates the cells that depend on that cell. This
process allows the program to quickly “react” to any change in cell inputs,
immediately providing an update to all downstream calculations and analyses
<span class="citation">(Levy 1984)</span>. Starting from the spreadsheet program Lotus 1-2-3,
spreadsheet programs also included <em>macros</em>, “a single computer instruction that
stands for a sequence of operations” <span class="citation">(Creeth 1985)</span>.</p>
<p>Spreadsheets have become so popular in part because so many people know how to
use them, at least in basic ways, and so many people have the software on their
computers that files can be shared with the virtual guarantee that everyone will
be able to open the file on their own computer <span class="citation">(Hermans et al. 2016)</span>.
Spreadsheets use the visual metaphore of a traditional gridded ledger sheet
<span class="citation">(Levy 1984)</span>, providing an interface that is easy for users to
immediately understand and create a mental map of <span class="citation">(Birch, Lyford-Smith, and Guo 2018; Barga et al. 2011)</span>. This visually clear interface also means that
spreadsheets can be printed or incorporated into other documents (Word files,
PowerPoint presentations) “as-is,” as a workable and understandable table of
data values. In fact, some of the most popular plug-in software packages for the
early spreadsheet program Lotus 1-2-3 were programs for printing and publishing
spreadsheets <span class="citation">(Campbell-Kelly 2007)</span>. This “What You See Is What You Get”
interface was a huge advance from previous methods of data analysis for the
first spreadsheet program, VisiCalc, providing a “window to the data” that was
accessible to business executives and others without programming expertise
<span class="citation">(Creeth 1985)</span>. Several surveys of researchers have found that
spreadsheets were popular because of their simplicity and ease-of-use
<span class="citation">(Anderson et al. 2007; AlTarawneh and Thorne 2017; Barga et al. 2011)</span>. By
contrast, databases and scritped programming lanugages can be perceived as
requiring a cognitive load and lengthly training that is not worth the
investment when an easier tool is available <span class="citation">(Hermans et al. 2016; Anderson et al. 2007; Myneni and Patel 2010; Barga et al. 2011; Topaloglou et al. 2004)</span>.</p>
</div>
<div id="hazards-of-combining-recording-and-analysis" class="section level3" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Hazards of combining recording and analysis</h3>
<p><strong>Raw data often lost.</strong></p>
<p>One of the key tenets of ensuring that research is computationally reproducible
is to always keep a copy of all raw data, as well as the steps taken to get from
the raw data to a cleaned version of the data through to the results of data
analysis. However, maintaining an easily accessible copy of all original raw data
for a project is a common problem among biomedical researchers
<span class="citation">(Goodman et al. 2014)</span>, especially as team members move on from a laboratory group
<span class="citation">(Myneni and Patel 2010)</span>.</p>
<p>The use of spreadsheets to jointly record and analyze data can contribute to
this problem. Spreadsheets allow for the immediate and embedded processing of
data. As a result, it may become very difficult to pull out the raw data
originally recorded in a spreadsheet. At the least, the combination of raw and
processed data in a spreadsheet makes it hard to identify which data points
within a spreadsheet make up the raw data and which are the result of processing
that raw data. One study of operational spreadsheets noted that:</p>
<blockquote>
<p>“The data used in most spreadsheets is undocumented and there is no practical
way to check it. Even the original developer would have difficulty checking the
data.” <span class="citation">(S. G. Powell, Baker, and Lawson 2009)</span></p>
</blockquote>
<p>Further, data in a spreadsheet is typically not saved as “read-only,” so it is
possible for it to be accidentally overwritten: in situations where spreadsheets
are shared among multiple users, original cell values can easily be accidentally
written over, and it may not be clear who last changed a value, when it was
changed, or why <span class="citation">(AlTarawneh and Thorne 2017)</span>.</p>
<p>Finally, many spreadsheets use a proprietary format. In the development of
spreadsheet programs, this use of proprietary binary file formats helped a
software program keep users, increasing barriers for a user to switch to a new
program (since the new program wouldn’t be able to read their old files)
<span class="citation">(Campbell-Kelly 2007)</span>. However, this file format may be hard to open in the
future, as software changes and evolves <span class="citation">(Michener 2015)</span>; by comparison, plain
text files should be widely accessible through general purpose tools—a text
editor is a type of software available on all computers, for
example—regardless of changes to proprietary software like Microsoft Excel.</p>
<p><strong>Opacity of analysis steps and potential for errors.</strong></p>
<p>Previous studies have found that errors are very common within spreadsheets
<span class="citation">(Hermans et al. 2016)</span>. For example, one study of 50 operational
spreadsheets found that about 90% contained at least one error
<span class="citation">(S. G. Powell, Baker, and Lawson 2009)</span>. In part, it is easier to make errors in spreadsheets and
harder to catch errors in later work with a spreadsheet because the formulas and
connections between cells aren’t visible when you look at the
spreadsheet—they’re behind the scenes <span class="citation">(Birch, Lyford-Smith, and Guo 2018)</span>. This makes it very
hard to get a clear and complete view of the pipeline of analytic steps in data
processing and analysis within a spreadsheet, or to discern how cells
are connected within and across sheets of the spreadsheet. As one early article on
the history of spreadsheet programs notes:</p>
<blockquote>
<p>“People tend to forget that even the most elegantly crafted spreadsheet is a
house of cards, ready to collapse at the first erroneous assumption. The
spreadsheet that looks good but turns out to be tragically wrong is becoming
a familiar phenomenon.” <span class="citation">(Levy 1984)</span></p>
</blockquote>
<p>Some characteristics of spreadsheets may heighten chances for errors. These
include high conditional complexity (i.e., lots of branching of data flow
through if / else structures), formulas that depend on a large number of cells
or that incorporate many functions <span class="citation">(Hermans et al. 2016)</span>. Following the
logical chain of spreadsheet formulas can be particularly difficult when several
calculations are chained in a row <span class="citation">(Hermans and Murphy-Hill 2015)</span>. Very long chains of
dependent formulas across spreadsheet cells may in some case requiring sketching
out by hand the flow of information through the spreadsheet to understand what’s
going on <span class="citation">(Nardi and Miller 1990)</span>. The use of macros can also make it
particularly hard to figure out the steps of an analysis and to diagnose and fix
any bugs in those steps <span class="citation">(Nash 2006; Creeth 1985)</span>. One
study of spreadsheets used in real life applications noted that, “Many
spreadsheets are so chaotically designed that auditing (especially of a few
formulas) is extremely difficult or impossible.” <span class="citation">(S. G. Powell, Baker, and Lawson 2009)</span></p>
<p>In some cases, formula dependences might span across different sheets of a
spreadsheet file. For the example given previously of a spreadsheet that converts
temperature from one unit to another and then averages across observations, for
example, the original temperature might be recorded in one sheet while the
converted temperature value is calculated and shown in a second sheet. These
cross-sheet dependencies can make the analysis steps even more opaque
<span class="citation">(Hermans et al. 2016)</span>, as a change in the cell value of one sheet might not
be immediately visible as a change in another cell on that sheet (the same is
true for spreadsheets so large that all the cells in a sheet are not
concurrently visible on the screen). Other common sources of errors included
incorrect references to cells inside formulas and incorrect use of formulas
<span class="citation">(S. G. Powell, Baker, and Lawson 2009)</span> or errors introduced through the common practice of copying
and pasting when developing spreadsheets <span class="citation">(Hermans et al. 2016)</span>.</p>
<p>To keep analysis steps clear, whether in scripted code or in spreadsheets or
pen-and-paper calculations, it is important to document what is being done at
each step and why <span class="citation">(Goodman et al. 2014)</span>. Scripted languages allow for code comments,
which are written directly into the script but not evaluated by the computer,
and so can be used to document steps within the code without changing the
operation of the code. Further, the program file itself often presents a linear,
step-by-step view of the pipeline, stored separated from the data itself
<span class="citation">(Creeth 1985)</span>. Calculations done with pen-and-paper (e.g., in a
laboratory notebook) can be annotated with text to document the steps.
Spreadsheets, on the other hand, are often poorly documented, or documented in
ways that are hard to keep track of. Before spreadsheets,</p>
<blockquote>
<p>“The formulas appeared in one place and the results in another. You could see
what you were getting. That cannot be said of electronic spreadsheets, which
don’t display the formulas that govern their calculations. As Mitch Kapor
explained, with electronic spreadsheets, ‘You can just randomly make formulas,
all of which depend on each other. And when you look at the final results, you
have no way of knowing what the rules are, unless someone tells you.’” <span class="citation">(Levy 1984)</span></p>
</blockquote>
<p>Within spreadsheets, the logic and methods behind the pipeline of data
processing and analysis is often not documented, or only documented with cell
comments (hard to see as a whole) or in emails, not the spreadsheet file.
One study that investigated a large collection of spreadsheets found that most
do not include documentation explaining the logic or implementation of data
processing and analysis implemented within the spreadsheet
<span class="citation">(Hermans et al. 2016)</span>. A survey of neuroscience researchers at a UK
institute found that about a third of respondents included no documentation
for spreadsheets used in their research laboratories <span class="citation">(AlTarawneh and Thorne 2017)</span>.</p>
<p>When spreadsheet pipelines are documented, it is often through methods that are
hard to find and interpret later. One study of scientific researchers found
that, when research spreadsheets were documented, it was often through “cell
comments” added to specific cells in the spreadsheet, which can be hard to
interpret inclusively to understand the flow and logic of a spreadsheet as a
whole <span class="citation">(AlTarawneh and Thorne 2017)</span>. In some cases, teams discuss and document
functionality and changes in spreadsheets through email chains, passing
different versions of the spreadsheet file as attachments of emails with
discussion of the spreadsheet in the email body. One research team investigated
over 700,000 emails from employees of Enron that were released during legal
proceedings and investigated the spreadsheets attached to these emails (over
15,000 spreadsheets) as well as discussion of the spreadsheets within the emails
themselves <span class="citation">(Hermans and Murphy-Hill 2015)</span>. They found that the logic and methods of
calculations within the spreadsheets were often documented within the bodies of
emails that team members used to share and discuss spreadsheets. This means
that, if someone needs to figure out why a step was taken or identify when an
error was introduced into a spreadsheet, they may need to dig through the chain
of old emails documenting that spreadsheet, rather than being able to find the
relevant documentation within the spreadsheet’s own file.</p>
<p>Often spreadsheets are designed, and their structure determined, by one person,
and this is often done in an <em>ad hoc</em> fashion, rather than designing the
spreadsheet to follow a common structure for the research field or for the
laboratory group <span class="citation">(Anderson et al. 2007)</span>. Often, data processing and analysis
pipelines for spreadsheets are not carefully designed; instead, it’s more
typically for spreadsheet user to start by directly entering data and formulas
without a clear overall plan <span class="citation">(AlTarawneh and Thorne 2017)</span>. Often, the person who
created the spreadsheet is the only person who fully knows how it works
<span class="citation">(Myneni and Patel 2010)</span>, particularly if the spreadsheet includes complex
macros or a complicated structure in the analysis pipeline
<span class="citation">(Creeth 1985)</span>.</p>
<p>This practice creates a heavy dependence on the person who created that
spreadsheet anytime the data or results in that spreadsheet need to be
interpreted. This is particularly problematic in projects where the spreadsheet
will be shared for collaboration or adapted to be used in a future project, as
is often done in scientific research groups. One survey of neuroscience
researchers at a UK institute, for example, found that “on average, 2–5
researchers share the same spreadsheet.” <span class="citation">(AlTarawneh and Thorne 2017)</span> In this case, it
can be hard to “onboard” new people to use the file, and much of the work and
knowledge about the spreadsheet can be lost when that person moves on from the
business or laboratory group <span class="citation">(Creeth 1985; Myneni and Patel 2010)</span>. If you share a spreadsheet with numerous and complex
macros and formulas included to clean and analyze the data, it can take an
extensive amount of time, and in some cases may be impossible, for the
researcher you share it with to decipher what is being done to get from the
original data input in some cells to the final results shown in others and in
graphs. Further, if others can’t figure out the steps being done through macros
and formulas in a spreadsheet, they will not be able to check it for problems in
the logic of the overall analysis pipeline or for errors in the specific
formulas used within that pipeline. They also will struggle to extend and adapt
the spreadsheet to be used for other projects. These problems come up not only
when sharing with a collaborator, but also when reviewing spreadsheets that you
have previously created and used (as many have noted, your most frequent
collaborator will likely be “future you”). In fact, one survey of biomedical
researchers at the University of Washington noted that,</p>
<blockquote>
<p>“The profusion of individually created spreadsheets containing overlapping and
inconsistently updated data created a great deal of confusion within some labs.
There was little consideration to future data exchange of submission
requirements at the time of publication.”
<span class="citation">(Anderson et al. 2007)</span></p>
</blockquote>
<p>There are methods that have been brought from more traditional programming work
into spreadsheet programming to try to help limit errors, including a tool
called assertions that allows users to validate data or test logic within their
spreadsheets <span class="citation">(Hermans et al. 2016)</span>. However, these are often not
implemented, in part perhaps because many spreadsheet users see themselves as
“end-users,” creating spreadsheets for their own personal use rather than as
something robust to future use by others, and so don’t seek out strategies
adopted by “programmers” when creating stable tools for others to use
<span class="citation">(Hermans et al. 2016)</span>. In practice, though, often a spreadsheet is used
much longer, and by more people, than originally intended. From early in the
history of spreadsheet programs, users have shared spreadsheet files with
interesting functionality with other users <span class="citation">(Levy 1984)</span>, and the
lifespan of a spreadsheet can be much longer than originally intended—a
spreadsheet created by one user for their own personal use can end up being used
and modified by that person or others for years <span class="citation">(Hermans et al. 2016)</span>.</p>
<p><strong>Subpar software for analysis.</strong></p>
<p>While spreadsheets serve as a widely-used tool for data recording and analysis,
in many cases spreadsheets programs are poorly suited to clean and analyze
scientific data compared to other programs. As tools and interfaces continue to
develop that make other software more user-friendly to those new to programming,
scientists may want to reevaluate the costs and benefits, in terms of both time
required for training and aptness of tools, for spreadsheet programs compared to
using scripted programming languages like R and Python.</p>
<p>Several problems have been identified with spreadsheet programs in the context of
recording and, especially, analyzing scientific data. First, some statistical
methods may be inferior to those available in other statistical programming language.
Since the most popular spreadsheet program (Excel) is closed source, it is hard to
identify and diagnose such problems, and there is likely less of an incentive for
problems in statistical methodology to be fixed (rather than using development time
and funds to increase easier-to-see functionality in the program). Many statistical
operations require computations that cannot be perfectly achieved with a
computer, since the computer must ultimately solve many mathematical problems using
numerical approximations rather than continuous methods (e.g., calculus). The choice of
the algorithms used for these approximations heavily influence how closely a result
approximates the true answer.</p>
<p>A series of papers examined the quality of statistical methods in several
statistical software programs, including Excel, starting in the 1990s
<span class="citation">(Bruce D. McCullough and Wilson 1999, 2005; Bruce D. McCullough 1999; B. D. McCullough and Wilson 2002; Bruce D. McCullough and Heiser 2008; Mélard 2014)</span>. In the
earliest studies, they found some concerns across all programs considered
<span class="citation">(Bruce D. McCullough and Wilson 1999; Bruce D. McCullough 1999)</span>. One of the biggest
concerns, however, was that there was little evidence over the years that the
identified problems in Excel were resolved, or at least improved, over time
<span class="citation">(B. McCullough 2001; Bruce D. McCullough and Heiser 2008)</span>. The authors note that there may
be little incentive for checking and fixing problems with algorithms for
statistical approximation in closed source software like Excel, where sales
might depend more on the more immediately evident functionality in the software,
while problems with statistical algorithms might be less evident to potential
users <span class="citation">(B. McCullough 2001)</span>.</p>
<p>Open source software, on the other hand, offers pathways for identifying and fixing
any problems in the software, including for statistical algorithms and methods
implemented in the software’s code. Since the full source code is available, researchers
can closely inspect the algorithms being used and compare them to the latest
knowledge in statistical computing methodology. Further, if an inferior algorithm is in
use, most open source software licenses allow a user to adapt and extend the software,
for example to implement better statistical algorithms.</p>
<p>Second, spreadsheet programs can include automated functionality that’s meant to
make something easier for most users, but that might invisibly create problems
in some cases. A critical problem, for example, has been identified when using
Excel for genomics data. When Excel encounters a cell value in a format that
seems like it could be a date (e.g., “Mar-3-06”), it will try to convert that
cell to a “date” class. Many software programs save date as this special “date”
format, where it is printed and visually appears in a format like “3-Mar-06” but
is saved internally by the program as a number (for Microsoft Excel, the number
of days since January 1, 1900 <span class="citation">(Willekens 2013)</span>). By doing this, the
software can more easily undertake calculations with dates, like calculating the
number of days between two dates or which of two dates is earlier.
Bioinformatics researchers at the National Institutes of Health found that Excel
was doing this type of automatic and irreversible date conversion for 30 gene
names, including “MAR3” and “APR-4,” resulting in these gene names being lost
for further analysis <span class="citation">(Zeeberg et al. 2004)</span>.</p>
<p>Avoiding this automatic date conversion required specifying that columns with
columns susceptible to these problems, including columns of gene names, should
be retained in a “text” class in Excel’s file import process. While this
problem was originally identified and published in 2004 <span class="citation">(Zeeberg et al. 2004)</span>,
along with tips to identify and avoid the problem, a study in 2016 found that
approximately a fifth of genomics papers investigated in a large-scale review
had gene name errors resulting from Excel automatic conversion, with the rate of
errors actually increasing over time <span class="citation">(Ziemann, Eren, and El-Osta 2016)</span>.</p>
<p>Other automatic conversion problems caused the lost of clone identifiers with
composed of digits and the letter “E” <span class="citation">(Zeeberg et al. 2004; Welsh et al. 2017)</span>,
which were assumed to be expressing a number using scientific notation and so
automatically and irreversibly converted to a numeric class. Further automatic
conversion problems can be caused by cells that start with an operator (e.g., “+
control”) or with leading zeros in a numeric identifier (e.g., “007”)
<span class="citation">(Welsh et al. 2017)</span>.</p>
<p>Finally, spreadsheet programs can be limited as analysis needs become more
complex or large <span class="citation">(Topaloglou et al. 2004)</span>. For example, spreadsheets can be
problematic when integrating or merging large, separate datasets
<span class="citation">(Birch, Lyford-Smith, and Guo 2018)</span>. This can create barriers, for example, in biological studies
seeking to integrate measurements from different instruments (e.g., flow
cytometry data with RNA-sequencing data). Further, while spreadsheet programs
continue to expand in their capacity for data, for very large datasets they
continue to face limits that may be reached in practical applications
<span class="citation">(Birch, Lyford-Smith, and Guo 2018)</span>—until recently, for example, Excel could not handle more
than one million rows of data per spreadsheet. Even when spreadsheets can handle
larger data, their efficiency in running data processing and analysis pipelines
across large datasets can be slow compared to code implemented with other
programming languages.</p>
<p><strong>Difficulty collaborating with statisticians.</strong></p>
<p>Modern biomedical researchers requires large teams, with statisticians and
bioinformaticians often forming a critical part of the team to enable
sophisticated processing and analysis of experimental data. However, the process
of combining data recording and analysis of experimental data, especially
through the use of spreadsheet programs, can create barriers in working across
disciplines. One group defined these issues as “data friction” and “science
friction”—the extra steps and work required at each interface where data
passes, for example, from a machine to analysis or from a collaborator in one
discipline to one in a separate discipline <span class="citation">(Edwards et al. 2011)</span>.
From a survey of scientific labs, for example, one respondent said:</p>
<blockquote>
<p>“I can give data that I think are appropriate to answer a question to a
biostatistician, but when they look at it, they see it from a different point of
view. And that spreadsheet does not really encapsulate where it came from very
well, how was it generated, was it random, how was this data collected. You
would run a series of queries that you think are pertinent to what this
biostatistician would want to know. They become a part of the exploration and
not just a receiver of whatever I decided to put in my spreadsheet on that day.
What I get back is almost never fully documented in any way that I can really
understand and add more to the process.” <span class="citation">(Myneni and Patel 2010)</span></p>
</blockquote>
<p>When collaborating with statisticians or bioinformaticians, one of the key
sources of this “data friction” can result from the use of spreadsheets to
jointly record and analyze experiemental data. First, spreadsheets are easy to
print or copy into another format (e.g., PowerPoint presentation, Word
document), and so researchers often design spreadsheets to be immediately
visually appealing to viewers. For example, a spreadsheet might be designed to
include hierarchically organized headers (e.g., heading and subheading, some
within a cell merged across several columns), or to show the result of a
calculation at the bottom of a column of observations (e.g., “Total” in the last
cell of the column) <span class="citation">(Teixeira and Amaral 2016)</span>. Multiple separate small tables
might be included in the same sheet, with empty cells used for visual
separation, or use a “horizontal single entry” design , where the headers are in
the leftmost column rather than the top row <span class="citation">(Teixeira and Amaral 2016)</span>.</p>
<p>These spreadsheet design choices make it much more difficult for the contents of
the spreadsheet to be read into other statistical programs. These types of data
require several extra steps in coding, in some cases fairly complex coding, with
regular expressions or logical rules needed to parse out the data and convert it
to the needed shape, before the statistical work can be done for the dataset.
This is a poor use of time for a collaborating statistician, especially if it
can be avoided through the design of the data recording template. Further, it
introduces many more chances for errors in cleaning the data.</p>
<p>Further, information embedded in formulas, macros, and extra formatting like
color or text boxes is lost when the spreadsheet file is input into other
programs. Spreadsheets allow users to use highlighting to represent information
(e.g., measurements for control animals shown in red, those for experiment
animals in blue) and to include information or documentation in text boxes. For
example, one survey study of biomedical researchers at the University of
Washington included this quote from a respondent: “I have one spreadsheet that
has all of my chromosomes … and then I’ve gone through and color coded it for
homozygosity and linkage.” <span class="citation">(Anderson et al. 2007)</span> All the information encoded in
this sheet through color will be lost when the data from the spreadsheet is read
into another statistical program.</p>
</div>
<div id="approaches-to-separate-recording-and-analysis" class="section level3" number="2.1.3">
<h3><span class="header-section-number">2.1.3</span> Approaches to separate recording and analysis</h3>
<p>In the remaining modules in this section, we will present and describe techniques
that can be used to limit or remove these problems. First, in the next few modules,
we will walk through techniques to design data recording
formats so that data is saved in a consistent format across experiments within
a laboratory group, and in a way that removes “data friction” for collaboration
with statisticians or later use in scripted code. These techniques can be immediately
used to design a better spreadsheet to be used solely for data collection.</p>
<p>In later modules, we will discuss the use of R projects to coordinate data
recording and analysis steps within a directory, while using separate files for
data recording versus data processing and analysis. These more advanced formats
will enable the use of quality assurance / control measures like testing of data
entry and analysis functionality, better documentation of data analysis pipelines,
and easy use of version control to track projects and collaborate transparently and
with a recorded history.</p>
<!---

### Additional notes / quotes for revisions

> "Many scientists spend a lot of time using Excel, and without batting an eye will
change the value in a cell and save the results. I strongly discourage modifying 
data this way. Instead, a better approach is to treat all data as *read-only* and
only allow programs to read data and create new, separate files of results. Why 
is treating data as read-only important in bioinformatics? First, modifying the 
data in place can easily lead to corrupted results. For example, suppose you 
wrote a script that directly modifies a file. Midway through processing a large
file, your script encounters an error and crashes. Because you've modified the
original file, you can't undo the changes and try again (unless you have a backup)!
Essentially, this file is corrupted and can no longer be used. Second, it's easy to 
lose track of how we've changed a file when we modify it in place. Unlike a workflow
where each step has an input file and an output file, a file modified in place
doesn't give us any indication of what we've done to it. Were we to lose track of how
we've changed a file and don't have a backup copy of the original data, our changes
are essentially irreproducible. Treating data as read-only may seem counterintuitive
may seem counterintuitive to scientists familiar with working extensively in Excel, 
but it's essential to robust research (and prevents catastrophe, and helps reproducibility).
The initial difficulty is well worth it; it also fosters reproducibility. Additionally, 
any step of the analysis can easily be redone, as the input data is unchanged by the 
program." [@buffalo2015bioinformatics]

> "'Plain text' data files are encoded in a format (typically UTF-8) that can be read by humans and computers alike. The great thing about plain text is their simplicity and their ease of use: any programming language can read a plain text file. The most common plain text format is .csv, comma-separated values, in which columns are separated by commas and rows are separated by line breaks." [@gillespie2016efficient]

--->

</div>
</div>
<div id="module2" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Principles and power of structured data formats</h2>
<p>The format in which experimental data is recorded can have a large influence on
how easy and likely it is to implement reproducibility tools in later stages of
the research workflow. Recording data in a “structured” format brings many
benefits. In this module, we will explain what makes a dataset “structured” and
why this format is a powerful tool for reproducible research.</p>
<p>Every extra step of data cleaning is another chance to introduce errors in
experimental biomedical data, and yet laboratory-based researchers often share
experimental data with collaborators in a format that requires extensive
additional cleaning before it can be input into data analysis
<span class="citation">(Broman and Woo 2018)</span>. Recording data in a “structured” format brings many
benefits for later stages of the research process, especially in terms of
improving reproducibility and reducing the probability of errors in analysis
<span class="citation">(Ellis and Leek 2018)</span>. Data that is in a structured, tabular, two-dimensional
format is substantially easier for collaborators to understand and work with,
without additional data formatting <span class="citation">(Broman and Woo 2018)</span>. Further, by using a
consistent structured format across many or all data in a research project, it
becomes much easier to create solid, well-tested code scripts for data
pre-processing and analysis and to apply those scripts consistently and
reproducibly across datasets from multiple experiments <span class="citation">(Broman and Woo 2018)</span>.
However, many biomedical researchers are unaware of this simple yet powerful
strategy in data recording and how it can improve the efficiency and
effectiveness of collaborations <span class="citation">(Ellis and Leek 2018)</span>.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>List the characteristics of a structured data format</li>
<li>Describe benefits for research transparency and reproducibility</li>
<li>Outline other benefits of using a structured format when recording data</li>
</ul>
<div id="data-recording-standards" class="section level3" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Data recording standards</h3>
<p>For many areas of biological data, there has been a push to create standards for
how data is recorded and communicated. Standards can clarify both the <em>content</em>
that should be included in a dataset, the <em>format</em> in which that content is
stored, and the <em>vocabulary</em> used within this data. One article names these three
facets of a data standard as the <strong>minimum information</strong>, <strong>file formats</strong>, and
<strong>ontologies</strong> <span class="citation">(Ghosh et al. 2011)</span>.</p>
<pre class="marginfigure"><code>&quot;It is important to distinguish between standards that specify how to actually
do experiments and standards that specify how to describe experiments.
Recommendations such as what standard reporters (probes) should be printed on
microarrays or what quality control steps should be used in an experiment belong
to the first category. Here we focus on the standards that specify how to
describe and communicate data and information.&quot;

[@brazma2006standards]</code></pre>
<p>Many people and organizations (including funders) are excited about the idea
of developing and using data standards, especially at the community level.
Good standards, that are widely adapted by researchers, can help in making
sure that data submitted to data repositories are used widely and that software
can be developed that is <em>interoperable</em> with data from many research group’s
experiments. There are also many advantages, if there are not community-level
standards for recording a certain type of data, to develop and use local
data standards for recording data from your own experiments. This section
describes the elements that go into a data standard, discusses some choices
to be made when definining a data standard (especially choices on data
structure and file formats), and some of the advantages and disadvantages
of developing and using data recording standards at both the research group
and community levels.</p>
<pre class="marginfigure"><code>**On the root causes for irreproducibility in biomedical research:** &quot;First, 
a lack of standards for data generation leads to problems with the comparability 
and integration of data sets.&quot;

[@waltemath2016modeling]</code></pre>
<p><strong>Ontology standards.</strong></p>
<p>Although it has the most complex name, an <em>ontology</em>
(sometimes called a <em>terminology</em> <span class="citation">(Sansone et al. 2012)</span>) might be the easiest and
quickest to adapt in recording data. An ontology helps define a vocabulary that
is controlled and consistent to use that researchers can use to refer to
concepts and concrete things within an area of research. It helps researchers,
when they want to talk about an idea or thing, to use one word, and just one
word, and to ensure that it will be the same word used by other researchers when
they refer to that idea or thing. Ontologies also help to define the relationships
between ideas or concrete things in a research area <span class="citation">(Ghosh et al. 2011)</span>, but
here we’ll focus on their use in provided a consistent vocabulary to use when
recording data.</p>
<p>For example, when recording a dataset, what do you call a small mammal that is
often kept as a pet and that has four legs and whiskers and purrs? Do you record
this as “cat” or “feline” or maybe, depending on the animal, even “tabby” or
“tom” or “kitten?” Similarly, do you record tuberculosis as “tuberculosis” or
“TB” or or maybe even “consumption?” If you do not use the same word
consistently in a dataset to record an idea, then while a human might be able to
understand that two words should be considered equivalent, a computer will not
be able to immediately tell that “TB” should be treated equivalently to
“tuberculosis.”</p>
<p>At a larger scale, if a research community can adapt an ontology they agree to
use throughout their studies, it will make it easier to understand and integrate
datasets produced by different research laboratories. If every research group
uses the term “cat,” then code can easily be written to extract and combine
all data recorded for cats across a large repository of experimental data.
On the other hand, if different terms are used, then it might be necessary to
first create a list of all terms used in datasets in the respository, then pick
through that list to find any terms that are exchangeable with “cat,” then write
script to pull data with any of those terms.</p>
<p>Several onotologies already exist or are being created for biological and other
biomedical research <span class="citation">(Ghosh et al. 2011)</span>. For biomedical science,
practice, and research, the BioPortal website
(<a href="http://bioportal.bioontology.org/" class="uri">http://bioportal.bioontology.org/</a>) provides access to almost 800 ontologies,
including several versions of the International Classification of Diseases, the
Medical Subject Headings (MESH), the National Cancer Institute Thesaurus, the
Orphanet Rare Disease Ontology and the National Center for Biotechnology
Information (NCBI) Organismal Classification. For each ontology in the BioPortal
website, the website provides a link for downloading the ontology in several
formats. If you download the ontology using the “CSV” format, you can open it in your
favorite spreadsheet program and explore how it defines specific terms to use
for each idea or thing you might need to discuss within that topic area, as well
as synonyms for some of the terms. To use an ontology when recording your own
data, just make sure you use the ontology’s suggested terms in your data. For
example, if you’d like to use the Ontology for Biomedical Investigations
(<a href="http://bioportal.bioontology.org/ontologies/OBI" class="uri">http://bioportal.bioontology.org/ontologies/OBI</a>) and you are recording how many
children a woman has had who were born alive, you should name that column of the
data “number of live births,” not “# live births” or “live births (N)” or
anything else. Other collections of ontologies exist for fields of scientific
research, including the Open Biological and Biomedical Ontology (OBO) Foundry
(<a href="http://www.obofoundry.org/" class="uri">http://www.obofoundry.org/</a>).</p>
<p>If there are community-wide ontologies in your field, it is worthwhile to use
them in recording experimental data in your research group. Even better is to
not only consistently use the defined terms, but also to follow any conventions
with capitalization. While most statistical programs provide tools to change
capitalization (for example, to change all letters in a character string to
lower case), this process does require an extra step of data cleaning and an
extra chance for confusion or for errors to be introduced into data.</p>
<p><strong>Minimum information standards.</strong> The next easiest facet of a data standard to
bring into data recording in a research group is <em>minimum information</em>. Within a
data recording standard, <em>minimum information</em> (sometimes also called <em>minimum
reporting guidelines</em> <span class="citation">(Sansone et al. 2012)</span> or <em>reporting requirements</em>
<span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span>) specify <em>what</em> should be included in a dataset
<span class="citation">(Ghosh et al. 2011)</span>. Using minimum information standards help ensure that data
within a laboratory, or data posted to a repository, contain a number of
required elements. This makes it easier to re-use the data, either to compare it
to data that a lab has newly generated, or to combine several posted datasets to
aggregate them for a new, integrated analysis, considerations that are growing
in importance with the increasing prevalence of research repositories and
research consortia in many fields of biomedical science <span class="citation">(Keller et al. 2017)</span>.</p>
<pre class="marginfigure"><code>&quot;Minimum information is a checklist of required supporting information for
datasets from different experiments. Examples include: Minimum Information About
a Microarray Experiment (MIAME), Minimum Information About a Proteomic
Experiment (MIAPE), and the Minimum Information for Biological and Biomedical
Investigations (MIBBI) project.&quot;

[@ghosh2011software]</code></pre>
<p><em>Standardized file formats.</em> While using a standard ontology and a standard for
minimum information is a helpful start, it just means that each dataset has the
required elements <em>somewhere</em>, and using a consistent vocabulary—it doesn’t
specify where those elements are in the data or that they’ll be in the same
place in every dataset that meets those standards. As a result, datasets that all
meet a common standard can still be very hard to combine, or to create common
data analysis scripts and tools for, since each dataset will require a different
process to pull out a given element.</p>
<p>Computer files serve as a way to organize data, whether that’s recorded
datapoints or written documents or computer programs <span class="citation">(Kernighan and Pike 1984)</span>. As
the programmer Paul Ford writes,</p>
<blockquote>
<p>“Data is just stuff, or rather, structured stuff: The cells of a spreadsheet,
the structure of a Word document, computer programs themselves—all data.” <span class="citation">(Ford 2015)</span></p>
</blockquote>
<p>A <em>file format</em> defines the rules for how the bytes in the chunk of
memory that makes up a certain file should be parsed and interpreted anytime you
want to meaningfully access and use the data within that file
<span class="citation">(Murrell 2009)</span>. There are many file formats you may be familiar
with—a file that ends in “.pdf” must be opened with a Portable Document Format
(PDF) Reader like Adobe Acrobat, or it won’t make much sense (you can try this
out by trying to open a “.pdf” file with a text editor, like TextEdit or
Notepad). The PDF Reader software has been programmed to interpret the data in a “.pdf” file
based on rules defining what data is stored where in the section of computer
memory for that file. Because most “.pdf” files conform to the same <em>file
format</em> rules, powerful software can be built that works with any file in that
format.</p>
<p>For certain types of biomedical data, the challenge of standardizing a format
has similarly been addressed through the use of well-defined rules for not only
the content of data, but also the way that content is <em>structured</em>. This can be
standardized through <em>standardized file formats</em> (sometimes also called <em>data
exchange formats</em> <span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span>) and often defines not only the
upper-level file format (e.g., use of a “.csv” file type), but also how data
within that file type should be organized. If data from different research
groups and experiments is recorded using the same file format, researchers can
develop software tools that can be repeatedly used to interpret and visualize
that data; on the other hand, if different experiments record data using
different formats, bespoke analysis scripts must be written for each separate
dataset. This is a blow not only to the efficiency of data analysis, but also a
threat to the accuracy of that analysis. If a set of tools can be developed that
will work over and over, more time can be devoted to refining those tools and
testing them for potential errors and bugs, while one-shot scripts often can’t
be curated with similar care. One paper highlights the dangers that come with
working with files that don’t follow a defined format:</p>
<blockquote>
<p>“Beware of common pitfalls when working with <em>ad hoc</em> bioinformatics formats.
Simple mistakes over minor details like file formats can consume a
disproportionate amount of time and energy to discover and fix, so mind these
details early on.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<p>Some biomedical data file formats have been created to help smooth over the
transfer of data that’s captured by complex equipment into software that can
analyze that data. For example, many immunological studies need to measure
immune cell populations in experiments, and to do so they use piece of
equipment called a flow cytometer that probes cells in a sample with lasers
and measures resulting intensities to determine characteristics of that cell.
The data created by this equipment is large (often measurements from [x] or more
lasers are taken for [x] cells in a single run) and somewhat complex, with a need
to record not only the intensity measurements from each laser, but also some metadata
about the equipment and characteristics of the run.
If every company that makes flow cytometers used a different file format for
saving the resulting data, then a different set of analysis software would need to
be developed to accompany each piece of equipment. For example, a laboratory at
a university with flow cytometers from two different companies would need licenses
for two different software programs to work with data recorded by flow cytometers,
and they would need to learn how to use each software package separately. There is a
chance that software could be developed that used shared code for data analysis, but only
if it also included separate sets of code to read in data from all types of equipment
and to reformat them to a common format.</p>
<p>This isn’t the case, however. Instead, there is a commonly agreed on file
format that flow cytometers should use to record the data they collect, called
the the <em>FCS file format</em>. This format has been defined through a series of
papers [refs], with several separate versions as the file format has evolved
over the years. It provides clear specifications on where to save each relevant
piece of information in the block of memory devoted to the data recorded by the
flow cytometer (in some cases, leaving a slot in the file blank if no relevant
information was collected on that element). As a result, people have been able
to create software, both proprietary and open-source, that can be used with any
data recorded by a flow cytometer, regardless of which company manufacturer the
piece of equipment that was used to generate the data. Other types of biomedical
data also have standardized file formats, including [example popular file
formats for biomedical data]. In some cases these were defined by an
organization, society, or initiative (e.g., the Metabolomics Standards
Initiative) <span class="citation">(Ghosh et al. 2011)</span>, while in some cases the file format developed
by a specific equipment manufacturer has become popular enough that it’s
established itself as the standard for recording a type of data
<span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span>.</p>
<p>For an even simpler example, thing about recording dates. The <em>minimum
information standard</em> for a date might always be the same—a recorded value
must include the day of the month, month, and year. However, this information
can be <em>structured</em> in a variety of ways. In many scientific data, it’s common
to record this information going from the largest to smallest units, so March
12, 2006, would be recorded “2006-03-12.” Another convention (especially in the
US) is to record the month first (e.g., “3/12/06”), while another (more common
in Europe) is to record the day of the month first (e.g., “12/3/06”).</p>
<p>If you are trying to combine data from different datasets with dates, and all
use a different structure, it’s easy to see how mistakes could be introduced
unless the data is very carefully reformatted. For example, March 12 (“3-12”
with month-first, “12-3” with day-first) could be easily mistaken to be December
3, and vice versa. Even if errors are avoided, combining data in different
structures will take more time than combining data in the same structure,
because of the extra needs for reformatting to get all data in a common
structure.</p>
<pre class="marginfigure"><code>&quot;Vast swathes of bioscience data remain locked in esoteric formats, are
described using nonstandard terminology, lack sufficient contextual information,
or simply are never shared due to the perceived cost or futility of the
exercise.&quot;

[@sansone2012toward]</code></pre>
</div>
<div id="defining-data-standards-for-a-research-group" class="section level3" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Defining data standards for a research group</h3>
<p>If some of the data you record from your experiments comes from complex
equipment, like flow cytometers or mass spectrometers, you may be recording much of that data in
a standardized format without any extra effort, because that format is the
default output format for the equipment. However, you may have more control over
other data recorded from your experiments, including smaller, less complex data
recorded directly into a laboratory notebook or spreadsheet. You can derive a
number of benefits from defining and using a standard for collecting this data,
as well.</p>
<p>As already mentioned, for many of the complex types of biological data,
standardized file formats exist. For example, flow cytometry data is typically
collected and recorded in <em>.fcs</em> files. Every piece of flow cytometry equipment
can then be built to output data in this format, and every piece of software to
analyze flow cytometry data can be built to read in this input. The <em>.fcs</em> file
format species how both raw data and metadata (e.g., compensation information,
equipment details) can be saved within the file—everyone who uses that file
format knows where to store data and where to find data of a certain type.</p>
<p>Much of the data collected in a laboratory is smaller, less complex, or less
structured than these types of data, data that is recorded “by hand,” often into
a laboratory notebook or a spreadsheet. One paper describes this type of data as
the output of “traditional, low-throughput bench science” <span class="citation">(Wilkinson et al. 2016)</span>.
For this data recording, the data may be written down in an <em>ad hoc</em>
way—however the particular researcher doing the experiment thinks makes
sense—and that format might change with each experiment, even if many
experiments have similar data outputs. As a result, it becomes harder to create
standardized data processing and analysis scripts that work with this data or
that integrate it with more complex data types. Further, if everyone in a
laboratory sets up their spreadsheets for data recording in their own way, it is
much harder for one person in the group to look at data another person recorded
and immediately find what they need within the spreadsheet.</p>
<p>As a step in a better direction, the head of a research group may
designate some common formats (e.g., a spreadsheet template) that all
researchers in the group should use when recording the data from a specific type
of experiments. This provides consistency across the recorded data for the
laboratory, making easier for one lab member to quickly understand and navigate
data saved by another lab member. It also opens the possibility to create tools
or scripts that read in and analyze the data that can be re-used across multiple
experiments with minor changes. This helps improve the efficiency and
reproducibility of data analysis, visualization, and reporting steps of the
research project.</p>
<p>This does require some extra time commitment <span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span>. First, time
is needed to design the format, and it does take a while to develop a format
that is inclusive enough that it includes a place to put all data you might want
to record for a certain type of experiment. Second, it will take some time to
teach each laboratory member what the format is and how to make sure they comply
with it when they record data.</p>
<p>On the flip side, the longer-term advantages of using a defined, structured
format will outweigh the short-term time investments for many laboratory groups
for frequently used data types. By creating and using a consistent structure to
record data of a certain type, members of a laboratory group can increase their
efficiency (since they do not need to re-design a data recording structure
repeatedly). They can also make it easier for downstream collaborators, like
biostatisticians and bioinformaticians, to work with their output, as those
collaborators can create tools and scripts that can be recycled across
experiments and research projects if they know the data will always come to them
in the same format. These benefits increase even more if data format standards
are created and used by a whole research field (e.g., if a standard data
recording format is always used for researchers conducting a certain type of
drug development experiment), because then the tools built at one institution
can be used at other insitutions. However, this level of field-wide coordination
can be hard to achieve, and so a more realistic immediate goal might be
formalizing data recording structures within your research group or department,
while keeping an eye out for formats that are gaining popularity as standards in
your field to adopt within your group.</p>
<p>One key advantage to using standardized data formats even for recording simple,
“low-throughput” data is that everyone in the research group will be able to
understand and work with data recorded by anyone else in the group—data will
not become impenetrable once the person who recorded it leaves the group. Also,
once a group member is used to the format, the process of setting up to record
data from a new experiment will be quicker, as it won’t require the effort of
deciding and setting up a <em>de novo</em> format for a spreadsheet or other recording
file. Instead, a template file can be created that can be copied as a starting
point for any new data recording.</p>
<p>Finally, there are huge benefits further down the data analysis pipeline that
come with always recording data in the same format. If your group is working
with a statistician or data analyst, it becomes much easier for that person to
quickly understand a new file if it follows the same format as previous files.
Further, if you work with a statistician or data analyst, he or she probably
creates code scripts to read in, re-format, analyze, and visualize the data
you’ve shared. If you always record data using the same format, these scripts
can be reused with very little modification. This saves valuable time, and it
helps make more time for more interesting statistical analysis if your
collaborator can trim time off reading in and reformatting the data in their
statistical programming language.</p>
<p>One paper suggests that the balance can be found, in terms of deciding whether
the benefits of developing a standard outweigh the costs, by considering how
often data of a certain type is generated and used:</p>
<blockquote>
<p>“To develop and deploy a standard creates an overhead, which can be expensive.
Standards will help only if a particular type of information has to be
exchanged often enough to pay off the development, implementation, and usage
of the standard during its lifespan.” <span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span></p>
</blockquote>
</div>
<div id="two-dimensional-structured-data-format" class="section level3" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Two-dimensional structured data format</h3>
<p>So far, this module has explored <em>why</em> you might want to use standardized
data formats for recording experimental data. The rest of the module
aims to give you tips for how to design and define your own standardized
data formats, if you decide that is worthwhile for certain data types
recorded within your research group.</p>
<p>Once you commit to creating a defined, structured format, you’ll need to decide
what that structure should be. There are many options here, and it’s very
tempting to use a format that is easy on human eyes
<span class="citation">(Buffalo 2015)</span>. For example, it may seem appealing to create a
format that could easily be copied and pasted into presentations and Word
documents and that will look nice in those presentation formats. To facilitate
this use, a laboratory might set up a recording format base on a spreadsheet
template that includes multiple tables of different data types on the same
sheet, or multi-level column headings.</p>
<p>Unfortunately, many of the characteristics that make a format attractive
to human eyes will make it harder for a computer to make sense of. For example,
if you include two tables in the same spreadsheet, it might make it easier for a
person to get a one-screen look at two small data tables. However, if you want
to read that data into a statistical program (or work with a collaborator who
would), it will likely take some complex code to try to tell the computer how to
find the second table in the spreadsheet. The same applies if you include some
blank lines at the top of the spreadsheet, or use multi-level headers, or use
“summary” rows at the bottom of a table. Further, any information you’ve
included with colors or with text boxes in the spreadsheet will be lost when the
data’s read into a statistical program. These design elements in a data
format make it much harder to read the data embedded in a spreadsheet into other
computer programs, including programs for more complex data analysis and
visualization, like R and Python.</p>
<pre class="marginfigure"><code>&quot;Data should be formatted in a way that facilitates computer readability. All
too often, we as humans record data in a way that maximizes its readability to
us, but takes a considerable amount of cleaning and tidying before it can be
processed by a computer. The more data (and metadata) that is computer readable,
the more we can leverage our computers to work with this data.&quot;

[@buffalo2015bioinformatics]</code></pre>
<p>For most statistical programs, data can be easily read in from a spreadsheet if
the computer can parse it in the following way: first, read in the first row,
and assign each cell in that row as the <em>name</em> of a column. Then, read in the
second row, and put each cell in the column the corresponds with the name of the
cell in the same position in the first row. Also, set the data type for that
column (e.g., number, character) based on the data type in this cell. Then, keep
reading in rows until getting to a row that’s completely blank, and that will be
the end of the data. If any of the rows has more cell than the first row, then
that means that something went wrong, and should result in stopping or giving a
warning. If any of the rows have fewer cells than the first row, then that means
that there are missing data in that row, and should probably be recorded as
missing values for any cells the row is “short” compared to the first row.</p>
<p>One of the easiest format for a computer to read is therefore a two-dimensional
“box” of data, where the first row of the spreadsheet gives the column names,
and where each row contains an equal number of entries. This type of
two-dimensional tabular structure forms the basis for several popular
“delimited” file formats that serve as a <em>lingua franca</em> across many simple
computer programs, like the comma-separated values (CSV) format, the
tab-delimited values (TSV) format, and the more general delimiter-separated
values (DSV) format, which are a common format for data exchange across
databases, spreadsheet programs, and statistical programs <span class="citation">(Janssens 2014; E. S. Raymond 2003; Buffalo 2015)</span>.</p>
<pre class="marginfigure"><code>&quot;Tabular plain-text data formats are used extensively in computing. The basic
format is incrediably simple: each row (also known as a record) is kept on its
own line, and each column (also known as a field) is separate by some delimiter.&quot;

[@buffalo2015bioinformatics]</code></pre>
<p>If you think of the computer parsing a spreadsheet as described above, hopefully
it clarifies why some spreadsheet formats would cause problems. For example, if
you have two tables in the same spreadsheet, with blank lines between them, the
computer will likely either think it’s read all the data after the first table,
and so not read in any data from the second table, or it will think the data
from both tables belong in a single table, with some rows of missing data in the
center. To write the code to read in data from two tables into two separate
datasets in a statistical program, it will be necessary to write some complex
code to tell the computer how to search out the start of the second table in the
spreadsheet.</p>
<p>Similar problems come up if a spreadsheet diverges from a regular,
two-dimensional format, with a single row of column names to start the data.
For example, if the data uses multiple rows to create multi-level
column headers, anyone reading it into another program will need to
either skip some of the rows of the column headers, and so lose information
in the original spreadsheet, or write complex code to parse the column
headers separately, then read in the later rows with data, and then stick
the two elements back together. “Summary” rows at the end of a dataset
(for example, the sums or means of all values in a column) will need to
be trimmed off when the data is read into other programs, since most
of the analysis and visualization someone would want to do in another
program will calculate any summaries fresh, and will want each row of
a dataset to represent the same “type” and level of data (e.g., one measurement
from one animal).</p>
<p>For anything in a data format that requires extra coding when reading data into
another program, you are introducing a new opportunity for errors at the
interface between data recording and data analysis. If there are strong reasons
to use a format that requires these extra steps, it will still be possible to
create code to read in and parse the data in statistical programs, and if the
same format is consistently used, then scripts can be developed and thoroughly
tested to allow this. However, do keep in mind that this will be an extra
burden on any data analysis collaborators who are using a program besides a
spreadsheet program. The extra time this will require could be large, since
this code should be vetted and tested thoroughly to ensure that the data
cleaning process is not introducing errors. By contrast, if the data is
recorded in a two-dimensional format with a single row of column names as
the first row, data analysts can likely read it quickly and cleanly into
other programs, with low risks of errors in the transfer of data from the
spreadsheet.</p>
<pre class="marginfigure"><code>&quot;Cleaning data is a short-term solution, and preventing errors is promoted as
a permanent solution. The drawback to cleaning data is that the process never
ends, is costly, and may allow many errors to avoid detection.&quot;

[@keller2017evolution]</code></pre>
</div>
<div id="saving-two-dimensional-structured-data-in-plain-text-file-formats" class="section level3" number="2.2.4">
<h3><span class="header-section-number">2.2.4</span> Saving two-dimensional structured data in plain text file formats</h3>
<p>If you have recorded data in a two-dimensional structured format, you can choose
to save it in either a <em>plain text</em> format or a <em>binary</em> format. With a plain
text format, a file is “human readable” when it’s opened in a text editor
<span class="citation">(Hunt, Thomas, and Cunningham 2000; Janssens 2014)</span>, because each byte that encodes the file
translates to a single character <span class="citation">(Murrell 2009)</span>, usually using an
ASCII or Unicode encoding. Common plain text file formats used for biomedical
research include CSV and TSV files (these are distinguished only by the
character used as a delimiter—commas for CSV files versus taabs for TSV files)
<span class="citation">(Buffalo 2015)</span>, other more complex file formats like SAM and XML
are also typically saved in plain text.</p>
<p>A binary file format, on the other hand, encodes data within the file using an
encoding system that differs from ANSCII or Unicode. To extract the data in a
meaningful way, a computer program must know and use rules for the encoding and
structure of that file format, and those rules will be different for each
different binary file format <span class="citation">(Murrell 2009)</span>. Some binary file
formats are “open,” with all the information on these rules and encodings
available for anyone to read. On the other hand, other binary file formats are
proprietary, without available guidance on how to interpret or use the data
stored in them when creating new software tools. Binary files, because they
don’t follow the restrictions of plain text encoding and format, can encode and
organize data in a way that’s often much more compressed, because it’s optimized
to suit a specific type of data. This means that binary file formats can often
store more data within a certain amount of computer memory compared to plain
text file formats. Binary files can also be designed so that the computer can
find and read a specific piece of data, rather than needing to read data in
linearly from the start to the end of a file as with plain text formats. This
means that programs can often access specific bits of data much more quickly
from a binary file format that from a plain text format, making computation
processing run much faster.</p>
<p>However, even with the speed and size advantages of many binary file formats, it
is often worthwhile to record and save experimental data in a plain text, rather
than binary, file format. There are a number of advantages to using a plain text
format. A plain text format may take more space (in terms of computer memory)
and take longer to process within other programs; however, its benefits
typically outweigh these limitations <span class="citation">(Hunt, Thomas, and Cunningham 2000)</span>. Advantages include:
(1) humans can read the file directly <span class="citation">(Hunt, Thomas, and Cunningham 2000; Janssens 2014)</span>,
and should always be able to, regardless of changes in and future obsolescence
of computer programs; (2) almost all software programs for analyzing and
processing files can input plain-text files, while binary file formats often
require specialized software <span class="citation">(Murrell 2009)</span>; (3) the
Unix system, which has influenced many existing software programs, especially
open-source programs for data analysis and command-line tools, are based on
inputting and outputtin line-based plain-text files <span class="citation">(Janssens 2014)</span>; and (4)
plain-text files can be easily tracked with version control
<span class="citation">(Hunt, Thomas, and Cunningham 2000)</span>. These advantages might become particularly important in
cases where researchers need to combine and integrate heterogeneous data, for
example data coming from different instruments.</p>
<p>Another advantage of storing data in a plain text format is that it makes
version control, which we’ll discuss in a later module, a much more powerful
tool. With plain text files, you can use version control to see the specific
changes to a file. With binary files, you can typically see if a file was
changed, but it’s much harder to see exactly what within the file was changed.</p>
<p>The book <em>The Pragmatic Programmer</em> highlights some of the advantages of plain
text:</p>
<blockquote>
<p>“Human-readable forms of data, and self-describing data, will outlive all
other forms of data and the applications that created them. Period. As long as
the data survives, you will have a chance to be able to use it—potentially
long after the original application that wrote it is defunct. … Even in the
future of XML-based intelligent agents that travel the wild and dangerous
Internet autonomously, negotiating data interchange among themselves, the
ubiquitous text file will still be there. In fact, in heterogeneous environments
the advantages of plain text can outweight all of the drawbacks. You need to
ensure that all parties can communicate using a common standard. Plain text is
that standard.” <span class="citation">(Hunt, Thomas, and Cunningham 2000)</span></p>
</blockquote>
<p>Paul Ford, by contrast, describes some of the disadvantages of
a binary file format, using the Photoshop file format as an example:</p>
<blockquote>
<p>“A Photoshop file is a lump of binary data. It just sits there on your hard
drive. Open it in Photoshop, and there are your guides, your color swatches, and
of course, the manifold pixels of your intent. But outside of Photoshop that
file is an enigma. There is not ‘view source.’ You can, if you’re passionate,
read the standard on the web, and it’s all piled in there, the history of
pictures on computers. That’s when it becomes clear: only Photoshop’s creator
Adobe can understand this thing.”
<span class="citation">(Ford 2014)</span></p>
</blockquote>
<p>Structuring data in a gridded, two-dimensional format, as described in the last
section, will be helpful even if it is in a file format that is binary, like
Excel. However, there are added benefits to saving the structured data in a
plain text format. Older Excel spreadsheets are typically saved in a proprietary
file format (“.xls”), while more recently Excel has saved files to an open
binary format based on packaging XML files with the data (“.xlsx” file format)
<span class="citation">(Janssens 2014)</span>. While the open proprietary format is preferable, since
tools can be developed to work with them by people other than the Microsoft
team, both file formats still face some of the limitations of binary file
formats as a way of recording experimental data. However, even if you have used
a spreadsheet program like Excel to record data, it’s very easy to still save
that data in a plain text file format <span class="citation">(Murrell 2009)</span>. In most
spreadsheet programs, you can choose to save a file “As CSV.”</p>
</div>
<div id="occassions-for-more-complex-data-structures-and-file-formats" class="section level3" number="2.2.5">
<h3><span class="header-section-number">2.2.5</span> Occassions for more complex data structures and file formats</h3>
<p>There are some cases where a two-dimensional data format may not be adequate
for recording experimental data, despite this format’s advantages in
improving reproducibility through later data analysis steps. Similarly,
there may be cases where a binary file format, or use of a database, will
outweigh the benefits of saving data to a plain text format. Being familiar
with different file formats can also be helpful when you need to integrate
data stored in different formats <span class="citation">(Murrell 2009)</span>.</p>
<p><strong>Non-tabular plain-text formats.</strong>
First, some data has a linked or hierarchical nature, in terms of how data
points are connected through the dataset. For example, data on a family tree have a hierarchical structure, where different numbers of children are
recorded for each parent. As another example, if you were building a dataset
describing how scientists have collaborated together as coauthors, that data
might form a network. In many cases, it is possible to structure datasets with
these types of “non-tabular” structure using the “tidy data” tabular format
described in the next section. However, in very complex cases, it may work
better to use a non-tabular data format <span class="citation">(E. S. Raymond 2003)</span>. Popular data formats
that are non-tabular include the eXtensible Markup Language (XML) and JavaScript
Object Notation (JSON) formats, both of which are well-suited for
hierarchically-structured data. You may also have data you would like to use in
XML or JSON formats if you are using web services to pull datasets from online
repositories, as open data application programming interfaces (APIs) often
return data in these formats <span class="citation">(Janssens 2014)</span>.</p>
<p>Another use of file formats that are plain text but meant to be streamed, rather
than read in as a whole. When reading in data stored in a delimited plain text
file, like a CSV file, a statistical program like R will typically read in all
the data and then operate on the dataset as a whole. If a data file is very
large, then reading in all the data at once might require so much memory that it
slows down processing, or even exceed the program’s memory cap [?]. One strategy
is to design a data format so that the program can read in a small amount of the
file, process that piece of the data, write the result out, and remove that bit
of data from the program’s memory before moving into the next portion of data
<span class="citation">(Buffalo 2015)</span>. This <em>streaming</em> approach is sometimes used with
some file formats used for biomedical research, including FASTA and FASTQ files.</p>
<p><strong>Databases.</strong>
When research datasets include not only data that can be expressed in plain
text, but also data like images, photographs, or videos, it may be worth
considering using a database to store the data <span class="citation">(Murrell 2009)</span>.
Relational database managment system software, like [examples. MySQL?
PostgreSQL?] can be used to organize data in a way that records connections
(<em>relations</em>) between different pieces of data and allows you to access
different combinations of that data quickly using Structured Query Language, or
<em>SQL</em> <span class="citation">(Ford 2015)</span>. Further, some statistical programming languages, including
R, now have tools that allow you to directly access and work with data from a
database from within the statistical program, and in some cases using scripts
that are very similar or identical to the code that would be used if you’d read
the data into the program from a plain text file.</p>
<pre class="marginfigure"><code>&quot;The database is the unsung infrastructure of the world, the shared memory of
every corporation, and the foundation of every major web site. And they are
everywhere. Nearly every host-your-own-web-site package comes with access to a
database called MySQL; just about every cell phone has SQLite3, aa tiny,
pocket-sized database, built in.&quot;

[@ford2015i]</code></pre>
<p>It will be more complicated to set up a database for recording experimental
data, and so it’s often preferable to instead save data in plain text files
within a file directory, if the data is simple enough to allow that. However,
there are some fairly simple database solutions that are now available,
including SQLite <span class="citation">(Buffalo 2015)</span>.</p>
<p><strong>Binary file formats.</strong></p>
<p>There are cases where it may not be best to store laboratory-generated data in a
plain text format. For example, the output from a flow cytometer is large and
would take up a lot (more) computer memory if stored in a plain text format, and
it would take much longer to read and work with the data in analysis software if
it were in that format. For very large datasets like this, it may be necessary
to use a binary data format, either for size or speed or both
<span class="citation">(Kernighan and Pike 1984; Hunt, Thomas, and Cunningham 2000)</span>. For very large biomedical datasets,
binary file formats are sometimes designed for <em>out-of-memory approaches</em>
<span class="citation">(Buffalo 2015)</span>, where a file format is designed in a way that
allows computer programs to find and read only specific pieces of data in a file
through a process called <em>random access</em>, rather than needing to read the full
file into memory before a specific piece of data in the file can be accessed
(a.k.a., <em>sequential access</em>) <span class="citation">(Murrell 2009)</span>.</p>
</div>
<div id="levels-of-standardizationresearch-group-to-research-community" class="section level3" number="2.2.6">
<h3><span class="header-section-number">2.2.6</span> Levels of standardization—research group to research community</h3>
<p>Standards can operate both at the level of individual research groups and at the
level of the scientific community as a whole. The potential advantages of
community-level standards are big: they offer the chance to develop
common-purpose tools and code scripts for data analysis, as well as make it
easier to re-use and combine experimental data from previous research that is
posted in open data repositories. If a software tool can be reused, then more
time can be spent in developing and testing it, and as more people use it, bugs
and shortcomings can be identified and corrected. Community-wide standards can
lead to databases with data from different experiments, and from different
laboratory groups, structured in a way that makes it easy for other researchers
to understand each dataset, find pieces of data of interest within datasets, and
integrate different datasets <span class="citation">(Lynch 2008)</span>. Similarly, with community-wide
standards, it can become much easier for different research groups to
collaborate with each other or for a research group to use data generated by
equipment from different manufacturers <span class="citation">(Schadt et al. 2010)</span>.</p>
<pre class="marginfigure"><code>&quot;Without community-level harmonization and interoperability, many community
projects risk becoming data silos.&quot;

[@sansone2012toward]</code></pre>
<pre class="marginfigure"><code>&quot;Solutions to integrating the new generation of large-scale data sets require
approaches akin to those used in physics, climatology and other quantitative
disciplines that have mastered the collection of large data sets.&quot;

[@schadt2010computational]</code></pre>
<p>However, there are important limitations to community-wide standards, as well.
It can be very difficult to impose such standards top-down and community-wide,
particularly for low-throughput data collection (e.g., laboratory bench
measurements), where research groups have long been in the habit of recording
data in spreadsheets in a format defined by individual researchers or research
groups. One paper highlights this point:</p>
<blockquote>
<p>“The data exchange formats PSI-MI and MAGE-ML have helped to get many of the
high-throughput data sets into the public domain. Nevertheless, from a bench
biologist’s point of view benefits from adopting standards are not yet
overwhelming. Most standardization efforts are still mainly an investment for
biologists.” <span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span></p>
</blockquote>
<p>Further, in some fields, community-wide standards have struggled to remain
stable, which can frustrate community members, as scripts and software must be
revamped to handle shifting formats <span class="citation">(Buffalo 2015; Barga et al. 2011)</span>. In some cases, a useful compromise is to follow a
general data recording format, rather than one that is very prescriptive. For
example, committing to recording data in a format that is “tidy” (which we
discuss extensively in the next module) may be much more flexible—and able to
meet the needs of a large range of experimental designs—than the use of a
common spreadsheet template or a more proscriptive standardized data format.</p>
</div>
<div id="applied-exercise" class="section level3" number="2.2.7">
<h3><span class="header-section-number">2.2.7</span> Applied exercise</h3>
<!---

### Additional materials to consider working in

> **Designed data** is "data that have traditionally been used in scientific
discovery. Designed data include statistically designed data collections, such
as surveys or experiments, and intentional observational collections. Examples
of intentional observational collections include data obtained from specially
designed instruments such as telescopes, DNA sequencers, or sensors on an ocean
buoy, and also data from systematically designed case studies such as health
registries Researchers have frequently devoted decades of systematic research to
understanding and characterizing the properties of designed data collections."
This contrasts with administrative data and opportunity data.
[@keller2017evolution]

> "The need to address data quality is a persistent one in the physical and
biological sciences, where scientists often seek to understand subtle effects
that leave minute traces in large volumes of data. ... For most scientists,
three factors motivate their work on data quality: first, the need to create a
strong foundation of data from which to draw their own conclusions; second, the
need to protect their data and conclusions from the criticisms of others; and
third, the need to understand the potential flaws in data collected by others.
The work of these scientists in data quality primarily concentrates on the
design and execution of experiments, including in laboratory, field, and
clinical settings. The key ingredients are measurement implementation,
laboratory and experimental controls, documentation, analysis, and curation of
data." [@keller2017evolution]

> "The concept of data quality management developed in the 1980s in step with
the technological ability to access stored data in a random fashion.
Specifically, as data management encoding process moved from the highly
controlled and defined linear process of transcribing information to tape, to a
system allowing for the random updating and transformation of data fields in a
record, the need for a higher level of control over what exactly can go into a
data field (including type, size, and values) became evident. Two key data
quality concepts came from these data management advances---ensuring data
integrity and cleansing the legacy data. Data integrity refers to the rules and
processes put in place to maintain and assure the accuracy and consistency of a
system that stores, processes, and retrieves data. Data cleaning refers to the
identificatio of incomplete, incorrect, inaccurate, or irrelevant parts of the
data and then replacing, modifying, or deleting this so-called dirty or coarse
data." [@keller2017evolution]

> "As the capability to store increasing amounts of data grew, so did the
business motivation to improve the quality of administrative data and thereby
improve decision making, reduce costs, and gain trust of customers."
[@keller2017evolution]

> "Determine whether there is a community-based metadata schema or standard (i.e., 
preferred sets of metadata elements) that can be adopted." [@michener2015ten]

> Might make more sense in tidy data section: "Although many standards have been
defined for data and model representations, they only ensure that data and
models that comply with these standards can be used by software that support
these standards; they do not ensure that multiple software tools can be used
seamlessly. When software tools are developed by independent research groups or
companies without an explicit agreement as to how they can be integrated, this
can cause problems when forming a workflow of multiple tools. This is because
the tools are likely to be inconsistent in their operating procedures and their
use of various non-standardized data formats. Thus, users often have to convert
data formats, to learn operating procedures for each tool, and sometimes even to
adjust operating environments. This impedes productivity, undermines the
flexibility of the workflow, and is prone to errors." [@ghosh2011software]

> "Ontologies define the relationships and hierarchies between different terms
and allow the unique, semantic annotation of data." [@ghosh2011software]

> "There are several international and national bodies, such as OMG, W3C, IEEE,
ANSI, and IETF... that formally approve standards or provide a framework for
standards development. Although some of the systems biology standards have been
certified (for example, SBML is officially adapted by IETF), in general in life
sciences this procedure is not particularly important---many of the most
successful standards such as GO have not undergone any official approval
procedure, but instead have become *de facto* standards. In fact, many of the
most successful standards in other domains are *de facto* standards."
[@brazma2006standards]

> "There are four steps involved in developing a complete and self-contained
standard: conceptual model design, model formalization, development of a data
exchange format, and implementation of the supporting tools."
[@brazma2006standards]

> "Two competing goals should be balanced when developing the conceptual model
of a domain: domain specificity and the need to find the common ground for all
related applications. Arguably, the most useful standards are those that consist
of the minimum number of most informative parameters. Keeping the list short
makes it simple and practicable, while selecting the most informative features
ensures accuracy and efficiency. The need for minimalism is reflected by the
titles for many such standards---Minimum Information About XYZ."

> "For a standard to be successful, laboratory information management systems
(LIMS), databases and data analysis, and modeling tools shoudl comply with it.
One way of fostering this is to develop easy-to-use 'middleware'---software
components that hide the technical complexities of the standard and facilitate
manipulation of the standard format in an easy way." [@brazma2006standards]

> "**Semantics**: The meaning of something; in computer science, it is usually
used in opposition to syntax (that is, format)." [@brazma2006standards]

> "Excel is by far the most common spreadsheet software, but many other
spreaadsheet programs exist, notably the Open Office Calc software, which is an
open source alternative and stores spreadsheets in an XML-based open standard
format called Open Document Format (ODF). This allows the data to be accessed by
a wider variety of software. However, even ODF is not ideal is a storage format
for aa research data set because spreadsheet formats contain not only the data
that is stored in the spreadsheet, but also information about how to display the
data, such as fonts and colors, borders and shading." [@murrell2009introduction]

> "Spreadsheet software is useful because it displays a data set in a nice
rectangular grid of cells. The arrangement of cells into distinct columns shares
the same benefits as fixed-width format text files: it makes it very easy for a
human to view and navigate within the data. ... because most spreadsheet
software provides facilities to import a data set from a wide variety of
formats, these benefits of the spreadsheet *software* can be enjoyed without
having to suffer the negatives of using a spreadsheet *format* for data storage.
For example, it is possible to store the data set in a CSV format and use
spreadsheet software to view or explore the data." [@murrell2009introduction]

> "Thinking about what sorts of questions will be asked of the data is a good
way to guide the design of data storage." [@murrell2009introduction]

First, you can still use spreadsheets, but reduce their use to recording data, 
leaving all data cleaning and analysis to be handled with other software. To make 
it easier to collaborate with statisticians and to interface with a program like 
R for data cleaning and analysis, it will be easiest if you set up your data
recording to include with other statistical programs like R or Python. These
steps are described in a later section, "...".

- Each sheet of the spreadsheet should contain data from a single
experiment.
- Never use whitespace to represent a meaningful separation in data within 
a spreadsheet. Never include multiple tables of data in the same sheet. 
- The first row of the spreadsheet should include a short column name for each
column with data. All column name information should be within a single row
(i.e., avoid subheadings). Avoid any special characters (e.g., "%") in column
names. Instead, use only letters, numbers, and underscores ("_"), and start with
a letter. It is especially helpful if you can avoid spaces in column names.
- Missing data should be represented consistently in cells. "NA" is one choice.
If you want to clarify why data is missing, it's much better to add a column
(e.g., "why_missing") where you can provide those details in text, rather than
combining within a single column numerical observation data with textual reasons
for missingness in cells with missing values.

Next, you could record data using a statistical language like R. There is an
excellent Integrated Development Environment for R called RStudio, and it
creates a much clearer interface with R compared to running R from a commond
line, particularly for new users. RStudio allows you to open delimited plain
text files, like csvs, using a grid-style interface. This grid-style interface
looks very similar to a spreadsheet, but lacks the ability to include formulas
or macros. Therefore, this format enforces a separation of the recording of raw
data from the cleaning and analysis of the data.

[R Project templates]

Data cleaning and analysis can then be shifted away from the files used to 
record the data and into reproducible scripts. These scripts can be clearly 
documented, either through comments in the code or through open source 
documentation tools like RMarkdown than interweave code and text in a way that
allows the creation of documents that are easier to read than commented code. 

This documentation should explain why each step is being done. In cases where 
it is not immediately evident from the code *how* the step is being done, this
should be documented as well. Any assumptions being used should be clarified in 
the documentation.

> "When we have only the letters, digits, special symbols, and punctuation marks
that appear on a standard (US) English keyboard, then we can use a single byte
to store each character. This is called an ASCII encoding (American Standard
Code for Information Exchange). ... UNICODE is an attempt to allow computers to
work with all of the characters in all of the languages of the world. Every
character has its own number, called a 'code point', often written in the form
U+xxxxxx, where every x is a hexidecimal digit. ... There are two main
'encodings' that are used to store a UNICODE code point in memory. UTF-16 always
uses two bytes per character of text and UTF-8 uses one or more bytes, depending
on which characters are stored. If the text is only ASCII, UTF-8 will only use
one byte per character. ... This encoding is another example of additional
information that may have to be provided by a human before the computer can read
data correctly from a plain text file, although many software packages will cope
with different encodings automatically." [@murrell2009introduction]

> "A PDF document is primarily a description of how to *display* information.
Any data values within a PDF document will be hopelessly entwined with
information about how the data values should be displayed."
[@murrell2009introduction]

> "Another major weakness of free-form text files is the lack of information
*within the file itself* about the structure of the file. For example, plain
text files do not contain information about which special character is being
used to separate fileds in a delimited file, or any information about the widths
of fields within a fixed-width format. This means that the computer cannot
automatically determine where different fields are within each row of a plain
text file, or even how many fields there are. A fixed-width format avoids this
problem, but enforcing a fixed length for fields can create other difficulties
if we do not know the maximum possible length for all variables. Also, if the
values for a variable can have very different lengths, a fixed-width format can
be inefficient because we store lots of empty space for short values. The
simplicity of plain text files make it easy for a computer to read a file as a
series of characters, but the computer cannot easily distinguish individual data
values from the series of characters. Even worse, the computer has no way of
tellins what sort of data is stored in each field. ... In practice, humans must
suppy additional information about a plain text file before a computer can
successfully determine where the different fields are within a plain text file
*and* what sort of data is stored in each field." [@murrell2009introduction]

> "In bioinformatics, the plain-text data we work with is often encoded in
*ASCII*. ASCII is a character encoding scheme that uses 7 bits to represent 128
different values, including letters (upper- and lowercase), numbers, and special
nonvisible characters. While ASCII only uses 7 bits, nowadays computers use an
8-bit *byte* (a unit representing 8 bits) to store ASCII characters. ... Because
plain-text data uses characters to encode information, our encoding scheme
matters. When working with a plain-text file, 98% of the time you won't have to
worry about the details of ASCII and how your file is encoded. However, the 2%
of the time when encoding data does matter---usually when an invisible non-ASCII
character has entered the data---it can lead to major headaches."
[@buffalo2015bioinformatics]

> "Programs [in Unix] retreive the data in a file by a system call (a subroutine
in the kernel) called `read`. Each time `read` is called, it returns the next
part of a file---the next line of text typed on the terminal, for example.
`read` also says how many bytes of the file were returned, so end of file is
assumed when a `read` says 'zero bytes are being returned'. If there were any
bytes left, `read` would have returned some of them." [@kernighan1984unix]

> "The format of a file is determined by the programs that use it; there is a
wide variety of file types, perhaps because there is a wide variety of programs.
But since file types are not determined by the file system, the kernel can't
tell you the type of a file: it doesn't know it. The `file` command makes an
educated guess ... To determine the types, `file` doesn't pay attention to the
names (although it could have), because naming conventions are just conventions,
and thus not perfectly reliable. For example, files suffixed '.c' are almost
always C source, but there is nothing to prevent you from creating a '.c' file
with arbitrary contents. Instead, `file` reads the first hundred bytes of a file
and looks for clues to that file type. ... In Unix systems there is just one
kind of file, and all that is required to access a file is its name. The lack of
file formats is an advantage overall---programmers don't need to worry about
file types, and all the standard programs will work on any file."
[@kernighan1984unix]

> "The Unix file system is organized so you can maintain your own personal files
without interfering with files belonging to other people, and keep people from
interfering with you too." [@kernighan1984unix]

> "The clinical patient health record is a longitudinal administrative record of
an individual's health information: all the data related to an individual's or a
population's health. The health record is a set of nonstandardized data that
spans multiple levels of aggregation, from a single measurement element (blood
pressure) to collections of diagnoses and related clinical observations. This
complexity is compounded by the high degree of human interaction involved in the
productio of clinical records, including self-reported data, medical diagnosis,
and other patient information." [@keller2017evolution]

> "Sharing data through repositories enhances both the quality and the value of
the data through standardized processes for curation, analysis, and quality
control. By allowing broad access to data, these repositories encourage and
support the use of previously collected data to test and extend previous
results. Data repositories are quite common in science fields such as astronomy,
genomics, and earth sciences. ... These repositories have accelerated discovery
by expanding the reach of these data to scientists who are not involved in the
initial data collection and experiments. Repositories address challenges that
affect data quality through governance, interoperability across systems, and
costs." [@keller2017evolution]

> One example of a repository is "the sharing of cDNA microarray data through
research consortia, which has led to a common set of standards and relatively
homogeneous data classes. There are many issues with the sharing of these data,
which requires the transformation of biologic to numeric data. These issues may
include loss of context, such as laboratory processes followed, and therefore
lack of information about the quality of the data when they are transformed. To
avoid this loss of information, the consortium ensures that documentation is
comprehensive so that other researchers can assess the quality of the data and
make comparisons with other studies using the same data. This documentation also
include information on when incorrect assignments of sequence identity are made
so that errors are not perpetuated in other studies." [@keller2017evolution]


> "**Data exchange format:** A file or message format that is formally defined
so that software can be built that 'knows' where to find various pieces of
information." [@brazma2006standards]

When a well-defined file format is used for saving data, the advantages can be huge. 
In the case of the FCS file format, the use of this common format means that people have
been able to create both open-source and proprietory software that inputs raw 
flow cytometry data regardless of the laboratory that collected it and of the 
manufacturor of the flow cytometer used for data collection (as long as that manufacturor
output the data in the FCS file format).


> "Everything in the Unix system is a file. That is less of an
oversimplification than you might think. When the first version of the system
was designed, before it even had a name, the discussions focused on the
structure of a file system that would be clean and easy to use. The file system
is central to the success and convenience of the Unix system. It is one of the
best examples of the 'keep it simple' philosophy, showing the power achieved by
careful implementation of a few well-chosen ideas." [@kernighan1984unix]

> "A file is a sequence of bytes. (A byte is aa small chunk of information,
typically 8 bits long. For our purposes, a byte is equivalent to a character.)
No structure is imposed on a file by the system, and no meaning is attaached to
its contents---the meaning of the bytes depends solely on the programs that
interpret the file. Furthermore, ... this is true not just of disc files but of
peripheral devices as well. Magnetic tapes, mail messages, characters typed on
the keyboard, line printer output, data flowing in pipes---each of these is just
a sequence of bytes as far as the systems and the programs in it are concerned."
[@kernighan1984unix]

> "The Comma-Separated Value (CSV) format is a special case of a plain text
format. Although not a formal standard, CSV files are very common and are a
quite reliable plain text delimited format that at least solves the problem of
where the fields are in each row of the file. The main rules for the CSV format
are: (1) **Comma-delimited**: Each field is separated by a comma (i.e., the
character , is the delimiter).; (2) **Double-quotes are special**: Fields
containing commas must be surrounded by double-quotes ... . (3) **Double-quote
escape sequence:** Fields containing double quotes must be surrounded by
double-quotes *and* each embedded double-quote must be represented using two
double quotes ... .; (4) **Header information**: There can be a single header
containing the names of the fields." [@murrell2009introduction]

> "A data file metaformat is a set of syntactic and lexical conventions that is
either formally standardized or sufficiently well established by practice that
there are standard service libraries to handle marshaling and unmarshaling it.
Unix has evolved or adopted metaformats suitable for a wide range of
applications [including delimiter-separated values and XML]. It is good practice
to use one of these (rather than an idiosyncratic custom format) whenever
possible. The benefits begin with the amount of custom paarsing and generation
code that you may be able to avoid writing by using a service library. But the
most important benefit is that developers and even many users will instantly
recognize these formats and feel comfortable with them, which reduces the
friction costs of learning new programs." [@raymond2003art]

> "There are three flavors you will encounter: tab-delimited, comma-separated, and
variable space-delimited. Of these three formats, tab-delimited is the most
commonly used in bioinformatics. File formats such as BED, GTF/GFF, SAM, tabular
BLAST output, and VCF are all examples of tab-delimited files. Columns of a
tab-delimited file are separated by a single tab character (which has the escape
code \t). A common convention (but not a standard) is to include metadata on the
first few lines of a tab-delimited file. These metadata lines begin with # to
differentiate them from the tabular dataa records. Because tab-delimated files
use a tab to delimit columns, tabs in data are not allowed. Comma-separated
values (CSV) is another common format. CSV is similar to tab-delimited, except
the delimiter is a comma character. While not a common in bioinformatics, it is
possible that the data stored in CSV format contain commas (which would
interfere with the ability to parse it). Some variants just don't allow this,
while others use quotes around entries that could contain commas. Unfortunately,
there's no standard CSV format that defines how to handle this and many other
issues with CSV---though some guidelines are given in RFC 4180. Lastly, there
are space-delimited formats. A few stubborn bioinformatics programs use a
variable number of spaces to separate columns. In general, tab-delimited formats
and CSV are better choices than space-delimited formats because it's quite
common to encounter data containing spaces." [@buffalo2015bioinformatics]

> "There are long-standing Unix traditions aabout how textual data formats ought
to look. Most of these derive from one or more of the standard Unix metaformats
... just described [e.g., DSV, XML]. It is wise to follow these conventions
unless you have strong and specific reasons to do otherwise. ... (1) *One record
per newline-terminated line, if possible.* This makes it easy to extract records
with text-stream tools. For data interchange with other operating systems, it's
wise to make your file-format parser indifferent to whether the line ending is
LF or CR-LF. It's also conventional to ignore trailing whitespace in such
formats; this protects against common editor bobbles. (2) *Less than 80
characters per line if possible.* This makes the format browseable in an
ordinary-sized terminal window. If many records must be longer than 80
characters, consider a stanza format... (3) *Use # as an introducer for
comments.* It's good to have a way to embed annotations and comments in data
files. It's best if they're actually part of the file structure, and so will be
preserved by tools that know its format. For comments that are not preserved
during parsing, # is the conventional start character. (4) *Support the
backslash convention.* The least surprising way to support nonprintable control
characters is by parsing C-like backslash escapes ... " [@raymond2003art]




> "You can take apart these formats and find out which decisions were made to
create them ... even old Microsoft Word, which in a long and painful political
bottle, finally settled down and 'opened' its format, countless hundres of pages
of documentation defining how words apper, how tables of contents are
registered, how all of the things that make up a Word document are to be
represented. The Microsoft Office File Formats specifications are of a most
disturbing, fascinating quality: one can read through them and think: *Yes, I
see this, I think I understand. But why?* ... Even Word is opened now, just
regular XML. Strange XML to be sure. All the codes once hidden are revealed."
[@ford2015on]


> "We wish to draw a distinction between data that is machine-actionable as a
result of specific investment in software supporting that data-type, for
example, bespoke parsers that understand life science wwPDB files or space
science Space Physics Archive Search and Extract (SPASE) files, and data that is
machine-actionable exclusively through the utilization of general-purpose, open
technologies. To reiterate the earlier point---ultimate machine actionability
occurs when a machine can make a useful decision regarding data that it has not
encountered before. This distinction is important when considering both (a) the
rapidly growing and evolving data environment, with new technologies and new,
more complex data-types continuously being developed, and (b) the growth of
general-purpose repositories, where the data-types encounted by an agent are
unpredictable. Creating bespoke parsers, in all computer languages, is not a
sustainable activity." [@wilkinson2016fair]

> "[One] way data can come from the Internet is through a web API, which stands
for *application programming interface*. The number of APIs that are being
offered by organizations is growing at an ever increasing rate... Web APIs are
not meant to be presented in a nice layout, such as websites. Instead, most web
APIs return data in a structured format, such as JSON or XML. Having data in a
structured format has the advantage that the data can be easily processed by
other tools." [@janssens2014data]


> "The *pileup format* [is] a plain-text format that summarizes reads' bases at
each chromosome position by stacking or 'piling up' aligned reads."
[@buffalo2015bioinformatics]


> "Data compression, the process of condensing data so that it takes up less
space (on disk drives, in memory, or across network transfers), is an
indespensible technology in modern bioinformatics. For example, sequences from a
recent Illumina HiSeq run when compressed with Gzip take up 21,408,674,240
bytes, which is a bit under 20 gigabytes. Uncompressed, this file is a whopping
63,203,414,514 bytes (around 58 gigabytes). This FASTQ file has 150 million
200bp reads, which is 10x coverage of the hexaploid wheat genome. The
compression ratio (uncompressed size/ compressed size) of this data is
approximately 2.95, which translates to a significant space saving of about 66%.
Your own bioinformatics projects will likely contain much more data, especially
as sequencing costs continue to drop and it's possible to sequence genomes to
higher depth, include more biological replicates or time points in expression
studies, or sequence more individuals in genotyping studies. For the most part,
data can remain compressed on the disk throughout processing and analysis. Most
well-writted bioinformatics tools can work natively with compressed data as
input, without requiring us to decompress it to disk first. Using pipes and
redirection, we can stream compressed data and write compressed files directly
to the disk. Additionally, common Unix tools like *cat*, *grep*, and *less* all
have variants that work with compressed data, and Python's *gzip* module allows
us to read and write compressed data from within Python. So while working with
large datasets in bioinformatics can be challenging, using the compression tools
in Unix and software libraries make our lives much easier."
[@buffalo2015bioinformatics]

> "Non-text files definitely have their place. For example, very laarge
databases usually need extra address information for rapid access; this has to
be binary for efficiency. But every file format that is not text must have its
own family of support programs to do things that the standard tools could
perform if the format were text. Text files may be a little less efficient in
maachine cycles, but this must be balanced against the cost of extra software to
maintain more specialized formats. If you design a file format, you should think
carefully before choosing a non-textual representation." [@kernighan1984unix]


> "*Out-of-memory approaches* [are] computational strategies built arouond
storing and working with data kept out of memory on the disk. Reading data from
a disk is much, much slower than working with data in memory... but in many
cases this is the approach we have to take when in-memory (e.g., loading the
entire dataset into R) or streaming approaches (e.g., using Unix pipes ...)
aren't appropriate." [@buffalo2015bioinformatics]

> "In general, it is possible to jump directly to a specific location within a
binary format file, whereas it is necessary to read a text-based format from the
beginning and one character at a time. This feature of accessing binary formats
is called **random access** and it is generlaly faster than the typically
**sequential access** of text files." [@murrell2009introduction]

> "We often need fast read-only access to data linked to a genomic location or
range. For the scale of data we encounter in genomics, retrieving this type of
data is not trivial for a few reasons. First the data might not fit entirely in
memory, requiring an approach where data is kept out of memory (in other words,
on a slow disk). Second, even powerful relational database systems can be
sluggish when querying out millions of entries that overlap a specific
region---an incrediably common operation in genomics. [BGZF and Tabix] are
specifically designed to get around these limitations, allowing fast
random-access of tab-delimited genome position data."
[@buffalo2015bioinformatics]

> "Samtools now supports (after version 1) a new, highly compressed file format
known as *CRAM*. Compressing alignments with CRAM can lead to a 10%--30%
filesize reduction compared to BAM (and quite remarkably, with no significant
increase in compression or decompression time compared to BAM). CRAM is a
*reference-based* compression scheme, meaning only the aligned sequence that's
different from the reference sequence is recorded. This greatly reduces file
size, as many sequence may align with minimal difference from the reference. As
a consequence of this reference-based approach, it is imperative that the
reference is available and does not change, as this would lead to a loss of data
kept in the CRAM format. Because the reference is so important, CRAM files
contain an MD5 checksum of the reference file to ensure it has not changed. CRAM
also has support for multiple different *lossy compression* methods. Lossy
compression entails some information about an alignment and the original read is
lost. For example, it's possible to bin base quality scores using a lower
resolution binning scheme to reduce the filesize." [@buffalo2015bioinformatics]




> "Very often we need efficient random access to subsequences of a FASTA file
(given regions). At first glance, writing a script to do this doesn't seem
difficult. We could, for example, write a script that iterates through FASTA
entries, extracting sequences that overlaps the range specified. However, this
is not an efficient method when extracting a few *random* subsequences. To see
why, consider accessing the sequence from position chromosome 8 (123,407,082 to
123,419,742) from the mouse genome. This approach would needlessly parse and
load chromosomes 1 through 7 into memory, even though we don't need to extract
subsequences from these chromosomes. Reading entire chromosomes from disk and
copying them into memory can be quite inefficient---we would have to load all
125 megabytes of chromosome 8 to extract 3.6kb! Extracting numerous random
subsequences from a FASTA file can be quite computationaally costly. A common
computational strategy that allows for easy and fast random access is *indexing*
the file. Indexed files are ubiquitous in bioinformatics."
[@buffalo2015bioinformatics]

> "We can avoid needlessly reading the entire file off of the disk by using an
index that points to where certian blocks are in the file. In the case of our
FASTA file, the index essentially stores the location of where each sequence
begins in the file (as well as other necessary information). When we look up a
range like chromosome 8 (123,407,082--123,410,744), *samtools faidx* uses the
information in the index to quickly calculate exactly where in the file those
bases are. Then, using an operation called a file *seek*, the program jumps to
this exact position (called the *offset*) in the file and starts reading the
sequence. Having precomputed file offsets combined with the ability to jump to
those exact positions is what makes accessing sections of an indexed file fast."
[@buffalo2015bioinformatics]


> "The data revolution within the biological and physical science world is
generating massive amounts of data from ... a wide range of ... projects, such
as those undertaken at the Large Hadron Collider and
genomics-proteomics-metabolomics research." [@keller2017evolution]


> "Community standards for data description and exchange are crucial. These
facilitate data reuse by making it easier to import, export, compare, combine,
and understand data. Standards also eliminate the need for the data creator to
develop unique descriptive practices. They open the door to development of
disciplinary repositories for specific classes of data and specialized software
management tools. GenBank, the US NIH genetic sequence database, and the US
National Virtual Observatory are good examples of what is possible here. In
2007, the US National Science Foundation, recognizing the importance of such
standards, established the Community Based Data Interoperability Networks
(INTEROP) funding programme for the development of tools, standards, and data
management best practices within specific disciplinary communities. ... Although
many classes of scientific data aren't ready, or aren't appropriate, for
standardization, well chosen investments in standardization show a consistently
high pay-off." [@lynch2008big]


> "For certain types of important digital objects, there are well-curated,
deeply-integrated, special-purpose repositories such as Genbank, Worldwide
Protein Data Bank, and UniProt... However, not all datasets or even data types
can be captured by, or submitted to, these repositories. Many important datasets
emerging from traditional, low-throughput bench science don't fit in the data
models of these special-purpose repositories, yet these datasets are no less
important with respect to integrative research, reproducibility, and reuse in
general. Apparently in response to this, we see the emergence of numerous
general-purpose data repositories [e.g., FigShare, Mendeley]. ... Such
repositories accept a wide range of data types in a wide range of formats,
generally do not attempt to integrate or harmonize the distributed data, and
place few restrictions (or requirements) on the descriptors of the data
deposition. The resulting data ecosystem, therefore, appears to be moving away
from centralization, is becoming more diverse, and less integrated, thereby
exacerbating the discovery and re-usability problem for both human and
computational stakeholders." [@wilkinson2016fair]

> "It would be unwise to bet that these formats [SAM/BAM files] won't change (or
even be replaced at some point)---the field of bioinformatics is notorious for
inventing new data formats (the same goes with computing in general) ... So
learning how to work with specific bioinformatics formats may seem like a lost
cause, skills such as following a format specification, manipulating binary
files, extracting information from bitflags, and working with application
programming interfaces (API) are essential skills when working with any format."
[@buffalo2015bioinformatics]

> From a working group on bioinformatics and data-intensive science: "Many simple
analyses are not automated because data formats are a moving target. ... The
community has been slow to share tools, partially because tools are not robust
against different input formats." [@barga2011bioinformatics]

> "Different centres generate data in different formats, and some analysis tools
require data to be in particular formats or require different types of data to
be linked together. Thus, time is wasted reformatting and reintegrating data
multiple times during a single analysis. For example, next-generation sequencing
companies do not deliver raw sequencing data in a format common to all
platforms, as there is no industry-wide standard beyond simple text files that
include the nucleotide sequence and the corresponding quality values. As a
result, carrying out sequencing analyses across different platforms requires
tools to be adapted to specific platforms. It is therefore crucial to develop
interoperable sets of analysis tools that can be run on different computational
platforms depending on which is best suited for a given application, and then
stitch those tools together to form analysis pipelines."
[@schadt2010computational]

> "Many important datasets emerging from traditional, low-throughput bench
science don't fit in the data models of ... special-purpose repositories [like
Genbank, Worldwide Protein Data Bank, and UniProt], yet these datasets are no
less important with respect to integrative research, reproducibility, and reuse
in general. Apparently in response to this, we see the emergence of numerous
general-purpose data repositories [e.g., FigShare, Mendeley]. ... Such
repositories accept a wide range of data types in a wide range of formats,
generally do not attempt to integrate or harmonize the distributed data, and
place few restrictions (or requirements) on the descriptors of the data
deposition. The resulting data ecosystem, therefore, appears to be moving away
from centralization, is becoming more diverse, and less integrated, thereby
exacerbating the discovery and re-usability problem for both human and
computational stakeholders." [@wilkinson2016fair]

> "Simplicity, but not oversimplification, is the key to success [in developing
standards]." [@brazma2006standards]


> "Minimum reporting guidelines, terminologies, and formats (hereafter referred to 
as reporting standards) are increasingly used in the structuring and curation of
datasets, enabling data sharing to varying degrees. However, the mountain of 
frameworks needed to support data sharing between communities inhibits the 
development of tools for data management, reuse and integration. ... The same
framework [on the other hand] enables researchers, bioinformaticians, and data
managers to operate within an open data commons." [@sansone2012toward]


> "'One of the core issues of Bioinformatics is dealing with a profusion of (often poorly
defined or ambiguous) file formats. Some *ad hoc* simple human readable formats have
over time attained the status of de facto standards.'-- Peter Cock et al. (2010)"
[@buffalo2015bioinformatics]

> "Developing and using a standard is often an investment that will not pay off
immediately, therefore there is a much better chance of success if the user community
decides that the respective standard is needed." [@brazma2006standards]

For high-throughput data recording, including output from equipment like ..., the
use of data standards is in some cases enforced by equiment manufacturers, and 
research scientists are able to develop tools that address the specific file
formats imposed by the equipment.

> "Although standardization is not a goal in itself, its importance is growing
in a high-throughput era. This is similar to what happened to manufacturing
during industrialization. The data from high-throughput technologies are being
generated at a rate that makes managing and using these data sets impossible on
a case-by-case basis. Although some of the data generated by the newest
technologies might have a low signal-to-noise ratio to make data re-usable, the
data quality is improving as the technology matures, and it is a waste of
resources not to share and re-use these expensive datasets. However, this is
only possible if the instrumentation that generates these data, laboratory-based
storage information management systems and databases, data analysis tools, and
systems modeling software can talk to each other easily. This is the purpose of
standardization." [@brazma2006standards]

> "A standard is successful only if it is used, and it is important to ensure
that supporting software tools are designed and implemented."
[@brazma2006standards]



> "In the late 2000s, there arose the 'NoSQL movement', coalescing around a
collective desire of many programmers to move beyond the strictures of the
relational model and unshackle themeselves from SQL. *Our data varied and
diverse,* they said, even if programmers weren't that varied and diverse, *and
we are tired of pretending that one technology will address the need for speed.*
Dozens of new databases appeared, each with different merits. There were
key-value databases, like Kyoto Cabinet, which optimized for speed of retrieval.
There were search-engine libraries, like Apache Lucene, which made it relatively
eaasy to search through enormous corpora of text---your own Google. There was
Mongo DB, which allowed for 'documents', big arbitrary blobs of dataa, to be
stored without nice rows and consistent structure. People debated, and continue
to debate, the value of each. ... There is as yet no absolute challenger to the
relationship model. When people think *database*, they still think *SQL*."
[@ford2015i]


> "By information or data communication standard we mean a convention on how to
encode data or information about a particular domain (such as gene function)
that enables unambiguous transfer and interpretation of this information or
data." [@brazma2006standards]

> "The proper acquisition and handling of data is crucially important for both
the generation and verification of hypotheses. The rapid development of
high-throughput experimental techniques is transforming life-science research
into 'big data' science, and although numerous data-management systems exist,
the heterogeneity of formats, identifiers, and data schema pose serious
challenges. In this context, data-management systems need standardized formats
for data exchange, globally unique identifiers for data mapping, and common
interfaces that allow the integration of disparate software tools in a
computational workflow." [@ghosh2011software]

> "Data quality [for health registries data] is driven by multiple dimensions
such as clinical data standardization, the existence of common definitions of
data fields, and the validity of self-reported patient conditions and outcomes.
Recognized issues include the definitions of data fields and their relational
structure, the training of personnel related to data collection data processing
issues (data cleaning), and curation." [@keller2017evolution]

--->

</div>
</div>
<div id="module3" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> The ‘tidy’ data format</h2>
<p>The “tidy” data format is one implementation of a tabular, two-dimensional
structured data format that has quickly gained popularity among statisticians
and data scientists since it was defined in a 2014 paper <span class="citation">(Wickham 2014)</span>.
The “tidy” data format plugs into R’s <em>tidyverse</em> framework, which enables
powerful and user-friendly data management, processing, and analysis by
combining simple tools to solve complex, multi-step problems
<span class="citation">(Ross, Wickham, and Robinson 2017; Silge and Robinson 2016; Wickham 2016; Wickham and Grolemund 2016)</span>.
Since the <em>tidyverse</em> tools are simple and share a common interface, they are
easier to learn, use, and combine than tools created in the traditional base R
framework <span class="citation">(Ross, Wickham, and Robinson 2017; Lowndes et al. 2017; <strong>reviewer2017review?</strong>; McNamara 2016)</span>. This <em>tidyverse</em> framework is quickly becoming the standard
taught in introductory R courses and books <span class="citation">(Hicks and Irizarry 2017; B. Baumer 2015; Kaplan 2018; Stander and Dalla Valle 2017; <strong>reviewer2017review?</strong>; McNamara 2016)</span>, ensuring ample training resources for researchers new to
programming, including books (e.g., <span class="citation">(B. S. Baumer, Kaplan, and Horton 2017; Irizarry and Love 2016; Wickham and Grolemund 2016)</span>), massive open online courses (MOOCs), on-site university courses
<span class="citation">(B. Baumer 2015; Kaplan 2018; Stander and Dalla Valle 2017)</span>, and Software
Carpentry workshops <span class="citation">(Wilson 2014; Pawlik et al. 2017)</span>. Further, tools
that extend the <em>tidyverse</em> have been created to enable high-quality data
analysis and visualization in several domains, including text mining
<span class="citation">(Silge and Robinson 2017)</span>, microbiome studies <span class="citation">(McMurdie and Holmes 2013)</span>, natural language
processing <span class="citation">(Arnold 2017)</span>, network analysis <span class="citation">(Tyner, Briatte, and Hofmann 2017)</span>, ecology
<span class="citation">(Hsieh, Ma, and Chao 2016)</span>, and genomics <span class="citation">(Yin, Cook, and Lawrence 2012)</span>. In this section, we will
explain what characteristics determine if a dataset is “tidy” and how use of the
“tidy” implementation of a structure data format can improve the ease and
efficiency of “Team Science.”</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>List characteristics defining the “tidy” structured data format</li>
<li>Explain the difference between the a structured data format (general concept)
and the ‘tidy’ data format (one popular implementation)</li>
</ul>
<p>In the previous module, we explained the benefits of saving data in a structured
format, and in particular using a two-dimensional format saved to a plain text
file when possible. In this section, we’ll talk about the “tidy text” format—a
set of principles to use when structuring two-dimensional tabular data. These
principles cover some basic rules for ordering the data, and the resulting
datasets can be very easily worked with, including to further clean, model, and
visualize the data, and to integrate the data with other datasets, using a
series of open-source tools on the R platform called the “tidyverse.” These
characteristics mean that, if you are planning to use a standardized data format
for recording experimental data in your research group, you may want to consider
creating one that adheres to the “tidy data” rules.</p>
<p>We’ll start by describing what rules a dataset’s format must follow for it to be
“tidy,” and try to clarify how you can set up your data recording to follow
these rules. In a later part of this module, we’ll talk more about the tidyverse
tools that you can use with this data, as well as give some resources for
finding out more about the tidyverse and how to use its tools.</p>
<p>Since a key advantage of the “tidy data” format is that it works so well with
R’s “tidyverse” tools, we’ll also talk a bit in this section about the use of
scripting languages like R, and how using them to analyze and visualize the
data you collect can improve the overall reproducibility of your research.</p>
<div id="what-makes-data-tidy" class="section level3" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> What makes data “tidy?”</h3>
<p>The “tidy” data format describes one way to structure tabular data. The name
follows from the focus of this data format and its associated set of tools—the
“tidyverse”—on preparing and cleaning (“tidying”) data, in contrast to sets of
tools more focused on other steps, like data analysis <span class="citation">(Wickham 2014)</span>. The
word “tidy” is not meant to apply that other formats are “dirty,” or that they
include data that is incorrect or subpar. In fact, the same set of datapoints
could be saved in a file in a way that is either “tidy” (in the sense of
<span class="citation">(Wickham 2014)</span>) or untidy, depending only on how the data are organized
across columns and rows.</p>
<pre class="marginfigure"><code>&quot;The development of tidy data has been driven by my experience from working
with real-world datasets. With few, if any, constraints on their organization,
such datasets are often constructed in bizarre ways. I have spent countless
hours struggling to get such datasets organized in a way that makes data
analysis possible, let alone easy.&quot;

[@wickham2014tidy]</code></pre>
<p>The rules for making data “tidy” are pretty simple, and they are defined in
detail, and with extended examples, in the journal article that originally
defined the data format <span class="citation">(Wickham 2014)</span>. Here, we’ll go through those rules, with
the hope that you’ll be able to understand what makes a dataset follow the
“tidy” data format. If so, you’ll be able to set up your data recording
template to follow this template, and you’ll be able to tell if other data you
get is in this format and, if it’s not, restructure it so that it does. In
the next part of this module, we’ll explain why it’s so useful to have your
data in this format.</p>
<p>Tidy data, first, must be in a tabular (i.e., two-dimensional, with columns and
rows, and with all rows and columns of the same length—nothing “ragged”). If
you recorded data in a spreadsheet using a very basic strategy of saving a
single table per spreadsheet, with the first row giving the column names (as
described in the previous module), then your data will be in a tabular format.
It should not be saved in a hierarchical structure, like XML (although there are
now tools for converting data from XML to a “tidy” format, so you may still be
able to take advantage of the tidyverse even if you must use XML for your data
recording). In general, if your recorded data looks “boxy,” it’s probably in a
two-dimensional tabular format.</p>
<p>There are some additional criteria for the “tidy” data format, though, and so
not every structured, tabular dataset is in a “tidy” format. The first of these
rules are that each row of a “tidy” dataset records the values for a single
observation, and that each column provides characteristics or measurements of a
certain type, in the order of the observations given by the rows
<span class="citation">(Wickham 2014)</span>. For example, if you have collected data from several
experimental samples and plated each sample at several dilutions to count
viable bacteria, then you could record the results using one row per
dilution—specifying each dilution for each sample as your level of
observation for the data—to save the data in a tidy format.</p>
<pre class="marginfigure"><code>&quot;Most statistical datasets are rectangular tables made up of rows and columns
... [but] there are many ways to structure the same underlying data. ... 
Real datasets can, and often do, violate the three precepts of tidy data in
almost every way imaginable.&quot;

[@wickham2014tidy]</code></pre>
<p>To be able to decide if your data is tidy, then, you need to know what forms a
single observation in the data you’re collecting. The <em>unit of observation</em> of a
dataset is the unit at which you take measurements <span class="citation">(Sedgwick 2014)</span>. This
idea is different than the <em>unit of analysis</em>, which is the unit that you’re
focusing on in your study hypotheses and conclusions (this is sometimes also
called the “sampling unit” or “unit of investigation”) <span class="citation">(Altman and Bland 1997)</span>. In
some cases, these two might be equivalent (the same unit is both the unit of
observation and the unit of measurement), but often they are not <span class="citation">(Sedgwick 2014)</span>. Again, in the example of plating samples at several
dilutions each, the unit of observation for the resulting data might be
at the level of each dilution for each sample, where the unit of analysis
that you are ultimately interested in is simply each sample.</p>
<pre class="marginfigure"><code>&quot;The unit of observation and unit of analysis are often confused.
The unit of observation, sometimes referred to as the unit of
measurement, is defined statistically as the &#39;who&#39; or &#39;what&#39;
for which data are measured or collected. The unit of analysis
is defined statistically as the &#39;who&#39; or &#39;what&#39; for which
information is analysed and conclusions are made.&quot;

[@sedgwick2014unit]</code></pre>
<p>As another example, say you are testing how the immune system of mice responds
to a certain drug over time. You may have several replicates of mice measured at
several time points, and those mice might be in separate groups (for example,
infected with a disease versus uninfected). In this case, if a separate mouse
(replicate) is used to collect each observation, and a mouse is never measured
twice (i.e., at different time points, or for a different infection status),
then the unit of measurement is the mouse. There should be one and only one row
in your dataset for each mouse, and that row should include two types of
information: first, information about the unit being measured (e.g., the time
point, whether the mouse was infected, and a unique mouse identifier) and,
second, the results of that measurement (e.g., the weight of the mouse when it
was sacrificed, the levels of different immune cell populations in the mouse, a
measure of the extent of infection in the mouse if it was infected, and perhaps
some notes about anything of note for the mouse, like if it appeared noticeably
sick). In this case, the <em>unit of analysis</em> might be the drug, or a combination
of drug and dose—ultimately, you may want to test something like if one drug
is more effective than another. However, the <em>unit of observation</em>, the level at
which each data point is collected, is the mouse, with each mouse providing a
single observation to help answer the larger research question.</p>
<p>As another example, say you conducted a trial on human subjects, to see how the
use of a certain treatment affects the speed of recovery, where each study
subject was measured at different time points. In this case, the unit of
observation is the combination of study subject and time point (while the unit
of analysis is the study subject, if the treatments are randomized to the study
subjects). That means that Subject 1’s measurement at Time 1 would be one
observation, and the same person’s measurement at Time 2 would be a separate
observation. For a dataset to comply with the “tidy” data format, these two
observations would need to be recorded on separate lines in the data. If the
data instead had different columns to record each study subject’s measurements
at different time points, then the data would still be tabular, but it would not
be “tidy.”</p>
<p>In this second example, you may initially find the “tidy” format unappealing,
because it seems like it would lead to a lot of repeated data. For example, if
you wanted to record each study subject’s sex, it seems like the “tidy” format
would require you to repeat that information in each separate line of data
that’s used to record the measurements for that subject for different time
points. This isn’t the case—instead, with a “tidy” data format, different
“levels” of data observations should be recorded in separate tables <span class="citation">(Wickham 2014)</span>.
So, if you have some data on each study subject that does not change across the
time points of the study—like the subject’s ID, sex, and age at
enrollment—those form a separate dataset, one where the unit of observation is
the study subject, so there should be just one row of data per study subject in
that data table, while the measurements for each time point should be recorded
in a separate data table. A unique identifier, like a subject ID, should be
recorded in each data table so it can be used to link the data in the two
tables. If you are using a spreadsheet to record data, this would mean that the
data for these separate levels of observation should be recorded in separate
sheets, and not on the same sheet of a spreadsheet file. Once you read the data
into a scripting language like R or Python, it will be easy to link the larger
and smaller “tidy” datasets as needed for analysis, visualizations, and reports.</p>
<p>Once you have divided your data into separate datasets based on the level of
observation, and structured each row to record data for a single observation
based on the unit of observation within that dataset, each column should be used
to measure a separate characteristic or measurement (a <em>variable</em>) for each
measurment <span class="citation">(Wickham 2014)</span>. A column could either give characteristics of the data
that were pre-defined by the study design—for example, the treatment assigned
to a mouse, or the time point at which a measurement was taken if the study
design defined the time points when measurements would be taken. These types of
column values are also sometimes called <em>fixed variables</em> <span class="citation">(Wickham 2014)</span>. Other
columns will record observed measurements—values that were not set prior to
the experiment. These might include values like the level of infection measured
in an animal and are sometimes called <em>measured variables</em> <span class="citation">(Wickham 2014)</span>.</p>
<pre class="marginfigure"><code>&quot;While the order of variables and observations does not affect analysis, a
good ordering makes it easier to scan the raw values. One way of organizing
variables is by their role in the analysis: are values fixed by the design of
the data collection, or are they measured during the course of the experiment?
Fixed variables describe the experimental design and are known in advance.
... Measured variables are what we actually measure in the study. Fixed
variables should come first, followed by measured variables, each ordered so
that related variables are contiguous. Rows can then be ordered by the first
variable, breaking ties with the second and subsequent (fixed) variables.&quot;

[@wickham2014tidy]</code></pre>
</div>
<div id="why-make-your-data-tidy" class="section level3" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Why make your data “tidy?”</h3>
<p>This may all seem like a lot of extra work, to make a dataset “tidy,” and why
bother if you already have it in a structured, tabular format? It turns out
that, once you get the hang of what gives data a “tidy” format, it’s pretty
simple to design recording formats that comply with these rules. What’s more,
when data is in a “tidy” format, it can be directly input into a collection
of tools in R that belong to something called the “tidyverse.” This collection
of tools is very straightforward to use and so powerful that it’s well worth
making an effort to record data in a format that works directly with the
tools, if possible. Outside of cases of very complex or very large data, it
should be possible.</p>
<pre class="marginfigure"><code>&quot;A standard makes initial data cleaning easier because you do not need to
start from scratch and reinvent the wheel every time. The tidy data standard has
been designed to facilitate initial exploration and analysis of the data, and to
simplify the development of data analysis tools that work well together.&quot;

[@wickham2014tidy]</code></pre>
<pre class="marginfigure"><code>&quot;Tidy data is great for a huge fraction of data analyses you might
be interested in. It makes organizing, developing, and sharing data a lot
easier. It&#39;s how I recommend most people share data.&quot;

[@leek2017toward]</code></pre>
<p>The “tidyverse” is a collection of tools united by a common philosophy: <strong>very
complex things can be done simply and efficiently with small, sharp tools
that share a common interface</strong>.</p>
<pre class="marginfigure"><code>&quot;The philosophy of the tidyverse is similar to
and inspired by the “unix philosophy”, a set of loose principles that ensure
most command line tools play well together. ...  Each function should solve one
small and well-defined class of problems. To solve more complex problems, you
combine simple pieces in a standard way.&quot;

[@ross2017declutter]</code></pre>
<p>The tidyverse isn’t the only popular system that follows this philosophy—one
other favorite is Legos. Legos are small, plastic bricks, with small studs on
top and tubes for the studs to fit into on the bottom. The studs all have the
same, standardized size and are all spaced the same distance apart. Therefore,
the bricks can be joined together in any combination, since each brick uses the
same <em>input format</em> (studs of the standard size and spaced at the standard
distance fit into the tubes on the bottom of the brick) and the same <em>output
format</em> (again, studs of the standard size and spaced at the standard distance
at the top of the brick).</p>
<p>This is true if you want to build with bricks of different colors or different
heights or different widths or depths. It even allows you to include bricks at
certain spots that either don’t require input (for example, a solid sheet that
serves as the base) or that don’t give output (for example, the round smooth
bricks with painted “eyes” that are used to create different creatures). With
Legos, even though each “tool” (brick) is very simple, the tools can be combined
in infinite variations to create very complex structures.</p>
<p>The tools in the “tidyverse” operate on a similar principle. They all input one
of a few very straightforward data types, and they (almost) all output data in
the same format they input it. For most of the tools, their required format for
input and output is the “tidy data” format <span class="citation">(Wickham 2014)</span>, called a tidy
<em>dataframe</em> in R—this is a dataframe that follows the rules detailed earlier
in this section.</p>
<p>Some of the tools require input and output of <em>vectors</em> instead of tidy
dataframes <span class="citation">(Wickham 2014)</span>; a <em>vector</em> in R is a one-dimensional string of values,
all of which are of the same data type (e.g., all numbers, or all character
strings, like names). In a tidy dataframe, each column is a vector, and the
dataframe is essentially several vectors of the same length stuck together to
make a table. Having functions that input and output vectors, then, means that
you can use those functions to make changes to the columns in a tidy dataframe.</p>
<p>A few functions in the “tidyverse” input a tidy dataframe but output data in a
different format. For example, visualizations are created using a function
called <code>ggplot</code>, as well as its helper functions and extensions. This function
inputs data in a tidy dataframe but outputs it in a type of R object called a
“ggplot object.” This object encodes the plot the code created, so in this case
the fact that the output is in a different format from the endpoint is similar
to with the “eye” blocks in Legos, where it’s meant as a final output step, and
you don’t intend to do anything further in the code once you move into that
step.</p>
<p>This common input / output interface, and the use of small tools that follow
this interface and can be combined in various ways, is what makes the tidyverse
tools so powerful. However, there are other good things about the tidyverse that
make it so popular. One is that it’s fairly easy to learn to use the tools, in
comparison to learning how to write code for other R tools <span class="citation">(D. Robinson 2017; Peng 2018)</span>. The developers who have created the tidyverse tools have
taken a lot of effort to try to make sure that they have a clear and consistent
<em>user interface</em> across tools <span class="citation">(Wickham 2017; Bryan and Wickham 2017)</span>. So far, we’ve
talked about the interface between functions, and how a common <em>input / output
interface</em> means the functions can be chained together more easily. But there’s
another interface that’s important for software tools: the rules for how a
computer users employ that tool, or the <em>user interface</em>.</p>
<p>To help understand a user interface, and how having a consistent user interface
across tools is useful, let’s think about a different example—cars. When you
drive a car, you get the car to do what you want through the steering wheel, the
gas pedal, the break pedal, and different knobs and buttons on the dashboard.
When the car needs to give you feedback, it uses different gauges on the
dashboard, like the speedometer, as well as warning lights and sounds.
Collectively, these ways of interacting with your car make up the car’s <em>user
interface</em>. In the same way, each function in a programming language has a
collection of parameters you can set, which let you customize the way the
function runs, as well as a way of providing you output once the function has
finished running and the way to provide any messages or warnings about the
function’s run. For functions, the software developer can usually choose design
elements for the function’s user interface, including which parameters to
include for the function, what to name those parameters, and how to provide
feedback to the user through messages, warnings, and the final output.</p>
<p>If a collection of tools is similar in its user interfaces, it will make it
easier for users to learn and use any of the tools in that collection once
they’ve learned how to use one. For cars, this explains how the rental car
business is able to succeed. Even though different car models are very different
in many characteristics—their engines, their colors, their software—they are
very consistent in their user interfaces. Once you’ve learned how to drive one
car, when you get in a new car, the gas pedal, brake, and steering wheel are
almost guaranteed to be in about the same place and to operate about the same
way as in the car you learned to drive in. The exceptions are rare enough to be
memorable—think how many movies have a laughline from a character trying to
drive a car with the driver side on the right if they’re used to the left or
vice versa.</p>
<p>The tidyverse tools are similarly designed so that they all have a very similar
user interface. For example, many of the tidyverse functions use a parameter
named “.data” to refer to the tidy dataframe to input into the function, and
this parameter is often the first listed for functions. Similarly, parameters
named “.vars” and “.funs” are repeatedly used over tidyverse functions, with the
same meaning in each case. What’s more, the tidyverse functions are typically given names
that very clearly describe the action that the function does, like <code>filter</code>,
<code>summarize</code>, <code>mutate</code>, and <code>group</code>. As a result, the final code is very clear
and can almost be “read” as a natural language, rather than code.</p>
<pre class="marginfigure"><code>&quot;Another part of what makes the Tidyverse effective is harder to see and, 
indeed, the goal is for it to become invisible: conventions. The Tidyverse
philosophy is to rigorously (and ruthlessly) identify and obey common
conventions. This applies to the objects passed from one function to another
and to the user interface each function presents. Taken in isolation, each
instance of this seems small and unimportant. But collectively, it creates 
a cohesive system: having learned one component you are more likely to be
able to guess how another different component works.&quot;

[@bryan2017data]</code></pre>
<pre class="marginfigure"><code>&quot;The goal of [the tidy tools] principles is to provide a uniform interface so
that tidyverse packages work together naturally, and once you’ve mastered one,
you have a head start on mastering the others.&quot;

[@wickhem2017tidy]</code></pre>
<p>As a result, the tidyverse collection of tools is pretty easy to learn, compared
to other sets of functions in scripting languages, and pretty easy to expand
your knowledge of once you know some of its functions. Several people who teach
R programming now focus on first teaching the tidyverse, given these
characteristics <span class="citation">(D. Robinson 2017; Peng 2018)</span>, and it’s often a
first focus for online courses and workshops on R programming. Since it’s main
data structure is the “tidy data” structure, it’s often well worth recording
data in this format so that all these tools can easily be used to explore and
model the data.</p>
<pre class="marginfigure"><code>&quot;All our code is underpinned by the principles of tidy data, the 
grammar of data manipulation, and the tidyverse R packages developed 
by Wickham. This deliberate philosophy for thinking 
about data helped bridge our scientific questions with the data processing 
required to get there, and the readability and conciseness of 
tidyverse operations makes our data analysis read more as a story 
arc. Operations require less syntax---which can mean fewer potential 
errors that are easier to identify---and they can be chained together, 
minimizing intermediate steps and data objects that can cause clutter and 
confusion. The tidyverse tools for wrangling data have 
expedited our transformation as coders and made R less intimidating to 
learn.&quot;

[@lowndes2017our]</code></pre>
</div>
<div id="using-tidyverse-tools-with-data-in-the-tidy-data-format" class="section level3" number="2.3.3">
<h3><span class="header-section-number">2.3.3</span> Using tidyverse tools with data in the “tidy data” format</h3>
<p>The tidyverse includes tools for many of the tasks you might need to
do while managing and working with experimental data. When you download
R, you get what’s called <em>base R</em>. This includes the main code that drives
anything you do in R, as well as functions for doing many core tasks.
However, the power of R is that, in addition to base R, you can also add
onto R through what are called <em>packages</em> (sometimes also referred to
as <em>extensions</em> or <em>libraries</em>). These are kind of like “booster packs”
that add on new functions for R. They can be created and contributed
by anyone, and many are collected through a few key repositories like
CRAN and Bioconductor.</p>
<p>All the tidyverse tools are included in R extension packages, rather than base
R, so once you download R, you’ll need to download these packages as well to use
the tidyverse tools. The core tidyverse functions include functions to read in
data (the <code>readr</code> package for reading in plain text, delimited files, <code>readxl</code>
to read in data from Excel spreadsheets), clean or summarize the data (the
<code>dplyr</code> package, which includes functions to merge different datasets, make
new columns as functions of old ones, and summarize columns in the data, either
as a whole or by group), and reformat the data if needed to get it in a tidy
format (the <code>tidyr</code> package). The tidyverse also includes more precise tools,
including tools to parse dates and times (<code>lubridate</code>) and tools to work with
character strings, including using regular expressions as a powerful way to find
and use certain patterns in strings (<code>stringr</code>). Finally, the tidyverse
includes powerful functions for visualizing data, based around the <code>ggplot2</code>
package, which implements a “grammar of graphics” within R.</p>
<p>You can install and load any of these tidyverse packages one-by-one using the
<code>install.packages</code> and <code>library</code> functions with the package name from within R.
If you are planning on using many of the tidyverse packages, you can also
install and load many of the tidyverse functions by installing a package called
“tidyverse,” which serves as an umbrella for many of the tidyverse packages.</p>
<p>In addition to the original tools in the tidyverse, many people have developed
<em>tidyverse</em> extensions—R packages that build off the tools and principles in
the tidyverse. These often bring the tidyverse conventions into tools for
specific areas of science. For example, the <code>tidytext</code> package provides tools to
analyze large datasets of text, including books or collections of tweets, using
the tidy data format and tidyverse-style tools. Similar tidyverse extensions
exist for working with network data (<code>tidygraph</code>) or geospatial data (<code>sf</code>).
Extensions also exist for the visualization branch of the tidyverse
specifically. These include <em>ggplot extensions</em> that allow users to create
things like calendar plots (<code>sugrrants</code>), gene arrow maps (<code>gggene</code>), network
plots (<code>igraph</code>), phytogenetic trees (<code>ggtree</code>) and anatogram images
(<code>gganatogram</code>). These extensions all allow users to work with data that’s in a
“tidy data” format, and they all provide similar user interfaces, making it
easier to learn a large set of tools to do a range of data analysis and
visualization, compared to if the set of tools lacked this coherence.</p>
</div>
<div id="practice-quiz" class="section level3" number="2.3.4">
<h3><span class="header-section-number">2.3.4</span> Practice quiz</h3>
<!---

### Extra content to consider

If you have data in a structured, tabular format that doesn't follow these
rules, you don't need to consider it "dirty", though---just think of "tidy" as
the tagname for this particular structure of data (the name, in this case,
connects the data format with a set of tools in R called the "tidyverse").

> "Software systems are transparent when they don't have murky corners or hidden
depths. Transparency is a passive quality. A program is passive when it is possible
to form a simple mental model of its behavior that is actuaally predictive for all
or most cases, because you can see through the machinery to what is actually going 
on." [@raymond2003art]

> "Software systems are discoverable when they include features that are designed 
to help you build in your mind a correct mental model of what they do and how they
work. So, for example, good documentation helps discoverability to a programmer. Discoverability
is an active quality. To achieve it in your software, you cannot merely fail to be obscure, 
you have to go out of your way to be helpful." [@raymond2003art]

> "Elegant code does much with little. Elegant code is not only correct but visibly, 
*transparently* correct. It does not merely communicate an algorithm to a computer, 
but also conveys insight and assurance to the mind of a human that reads it. By seeking
elegance in our code, we build better code. Learning to write transparent code is a first, 
long step toward learning how to write elegant code---and taking care to make code 
discoverable helps us learn how to make it transparent. Elegant code is both transparent and
discoverable." [@raymond2003art]

> "To design for transparency and discoverability, you need to apply every tactic for
keeping your code simple, and also concentrate on the ways in which your code is a 
communication to other human beings. The first questions to ask, after 'Will this design
work?' are 'Will it be reaadable to other people? Is it elegant?' We hope it is clear ...
that these questions are not fluff and that elegance is not a luxury. These qualities
in the human reaction to software are essential for reducing its bugginess and
increasing its long-term maintainability." [@raymond2003art]

> "The Unix style of design applies the do-one-thing-well approach at the level of 
cooperating programs as well as cooperating routines within a program, 
emphasizing small programs connected by well-defined interprocess communication
or by shared files. Accordingly, the Unix operating system encourages us to break our
programs down into simple subprocesses, and to concentrate on the interfaces between
these subprocesses." [@raymond2003art]

> "The ability to combine programs [with piping] can be extremely useful. But the real
win here is not cute combinations; it's that because both pipes and *more(1)* exist, 
*other programs can be simpler*. Pipes mean that programs like *ls(1)* (and other
programs that write to standard out) don't have to grow their own pagers---and we're 
saved from a word of a thousand built-in pagers (each, naturally, with its own divergent
look and feel). Code bloat is avoided and global complexity reduced. As a bonus, if 
anyone needs to customize pager behavior, it can be done in *one* place, by changing
*one* program. Indeed, multiple pagers can exist, and will all be useful with every application
that writes to standard output." [@raymond2003art]

> "Unix was born in 1969 and has been in continuous production use ever since. That's several 
geological eras by computer industry standards. ... Unix's durability and adaptability have
been nothing short of astonishing. Other technologies have come and gone like mayflies. 
Machines have increased a thousand-fold in power, languages have mutated, industry practice
has gone through multiple revolutions---and Unix hangs in there, still producing, still paying 
the bills, and still commanding loyalty from many of the best and brightest software technologists
on the planet." [@raymond2003art]

> "One of the many consequences of the exponential power-versus-time curve in computing, and the
corresponding pace of software development, is that 50% of what one knows becomes obsolete over
every 18 months. Unix does not abolish this phenomenon, but does do a good job of containing it. 
There's a bedrock of unchanging basics---languages, system calls, and tool invocations---that one 
can actually keep for entire years, even decades. Elsewhere it is impossible to predict what will 
be stable; even entire operating systems cycle out of use. Under Unix, there is a fairly sharp 
distinction between transient knowledge and lasting knowledge, and one can know ahead of time
(with about 90% certainty) which category something is likely to fall in when one learns it. Thus
the loyalty Unix commands." [@raymond2003art]

> "Unix is famous for being designed around the philosophy of small, sharp tools, each
intended to do one thing well. This philosophy is enabled by using a common underlying
format---the line-oriented, plain text file. Databases used for system administration
(users and passwords, network configuration, and so on) are all kept as plain 
text files. ... When a system crashes, you may be faced with only a minimal 
environment to restore it (you may not be able to access graphics drivers, 
for instance). Situations such as this can really make you appreciate the simplicity of
plain text." [@hunt2000pragmatic]

> "Unix is the foundational computing environment in bioinformatics because its
design is the antithesis of [a] inflexible and fragile approach. The Unix shell
was designed to allow users to easily build complex programs by interfacing
smaller modular programs together. This approach is the Unix philosophy: 
'This is the Unix philosophy: Write programs that do one thing and do it well.
Write programs to work together. Write programs to handle text streams, because 
that is a universal interface.'--Doug McIlory". [@buffalo2015bioinformatics]

> "Passing the output of one program directly into the input of another 
program with pipes is a computationally efficient and simple way to interface
Unix programs. This is another reason why bioinformaticians (and software engineers
in general) like Unix. Pipes allow us to build larger, more complex tools from
modular parts. It doesn't matter what language a program is written in, either; pipes
will work between anything as long as both programs understand the data passed
between them. As the lowest common denominator between most programs, plain-text
streams are often used---a point that McIlroy makes in his quote about the 
Unix philosophy." [@buffalo2015bioinformatics]

If the data is the same regardless of whether it's "tidy" or not, then why all
the fuss about following the "tidy" principles when you're designing the format
you'll use to record your data? The magic here in this---if you follow these
principles, then your data can be immediately input into a collection of powerful
tools for visualizing and analyzing the data, without further cleaning steps. 
What's more, all those tools (the set of tools is calld the "tidyverse") will 
typically *output* your data in a "tidy" format, as well. 

These small tools can be combined together because they take the same input
(data in a "tidy" format) and they output in the same format (also data in a
"tidy" format). This is such a powerful idea that many of the best loved toys
work on the same principle. Think of interlocking plastic block sets, like Lego.
You can create almost anything with a large enough set of Legos, because they
can be combined in almost any kind of way. Why? Because they all follow a
standard size for the ... on top of each block, and they all "input" ... of that
same size on the bottom of the block. That means they can be joined together in
any order and combination, and as a result very complex structures can be
created. It also means that each piece can be small and easy to understand---if
you're building a Lego structure, even something very fancy, you'll probably use
lots of rectangular brinks that are two ... across and four ... long, and that's
easy enough to describe that you could probably get a young child to help you
find those pieces when you need them.

The "tidy" data format is an implementation of a structured data format popular
among statisticians and data scientists. By consistently using this data format,
researchers can combine simple, generalizable tools to perform complex tasks in
data processing, analysis, and visualization. 

> "Base R graphics came historically first: simple, procedural, conceptually 
motivated by drawing on a canvas. There are specialized functions for different
types of plots. These are easy to call---but when you want to combine them to 
build up more complex plots or exchange one for another, this quickly gets 
messy, or even impossible. The user plots ... directly onto a (conceptual)
canvas. She explictly needs to deal with decisions such as how much space to allocate
to margins, axis labels, titles, legends, subpanels; once something is 'plotted', 
it cannot be moved or erased. There is a more high-level approach: in the 
*grammar of graphics*, graphics are built up from modular logical pieces, so that
we can easily try different visualization types for our data in an intuitive and
easily deciphered way, just as we can switch in and out parts of a sentence in human
language. There is no concept of a canvas or a plotter; rather, the user gives
`ggplot2` a high-level description of the plot she wants, in the form of an 
R object, and the rendering engine takes a holistic view of the scene to lay out
the graphics and render them on the output device." [@holmes2018modern]

--->

</div>
</div>
<div id="module4" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Designing templates for “tidy” data collection</h2>
<p>This module will move from the principles of the “tidy” data format to the
practical details of designing a “tidy” data format to use when collecting
experimental data. We will describe common issues that prevent biomedical
research datasets from being “tidy” and show how these issues can be avoided. We
will also provide rubrics and a checklist to help determine if a data collection
template complies with a “tidy” format.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Identify characteristics that keep a dataset from being “tidy”</li>
<li>Convert data from an “untidy” to a “tidy” format</li>
</ul>
<p>In this module, we will use a real example of data collected in a biomedical laboratory.
We’ll use this example to show how data is often collected in a way that is
not “tidy,” focusing on the features of data collection that make it “untidy.”
We’ll then describe some general principles for why and how to instead create and
use tidy (or at least tidier) templates to collect data in the laboratory, and
show how this can be the first step in a pipeline to creating useful, attractive,
and reproducible reports that describe the data you collected. This module will
focus on the principles of templates for tidy data collection, while in the next
module we’ll dig deeper into the details of making this conversion for the
example dataset that we use as a demonstration in this module.</p>
<div id="exampledata-on-rate-of-bacterial-growth" class="section level3" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Example—Data on rate of bacterial growth</h3>
<p>Throughout this module, we’ll use a real dataset to illustrate principles of
data collection in a biomedical laboratory. First, let’s start by looking at the
original data collection template, and use this to walk through some details of
this dataset.</p>
<p>Figure <a href="experimental-data-recording.html#fig:growthexcel1">2.1</a> provides an annotated view of the data set, showing
the format used when the data were originally collected:</p>
<div class="figure"><span style="display:block;" id="fig:growthexcel1"></span>
<img src="figures/growth_curve_example.png" alt="Example of an Excel spreadsheet used to record and analyze data for a laboratory experiment. Annotations highlight where data is entered by hand, where calculations are done by hand, and where embedded Excel formulas are used. The figures are created automatically using values in a specified column." width="\textwidth" />
<p class="caption">
Figure 2.1: Example of an Excel spreadsheet used to record and analyze data for a laboratory experiment. Annotations highlight where data is entered by hand, where calculations are done by hand, and where embedded Excel formulas are used. The figures are created automatically using values in a specified column.
</p>
</div>
<p>These data were collected to measure the compare growth yield and doubling time
of <em>Mycobacterium tuberculosis</em> (the bacteria that causes tuberculosis in
humans) under two conditions—high oxygen and low oxygen. In humans, M.
tuberculosis can persist for years or decades in granulomas, and the centers of
these granulomas are often hypoxic (low in oxygen). Therefore, it’s important to
understand how these bacteria grow in hypoxic conditions.</p>
<p>To conduct this experiment, the researchers used test tubes that were capped
with sealed caps to prevent and air exchange between the contents of the tube
and the environment. Inside the tubes, the amount of oxygen was controlled
by shifting the ratio of the volume of the culture (the liquid with nutrients
in which the M. tuberculosis will grow) versus the volume of air.
In the high oxygen condition, a lower volume of culture was used, which leaves
room for a lot of air in the top of the tube. In the low oxygen condition,
the tube was filled almost to the top with culture, which left very little air
at the top of the tube.</p>
<p>Once the tubes were filled and capped, they were left to grow for about a week.
During this time, the researchers took several measurements to determine the
growth of the bacteria in each tube. To do this, they used a spectrophotometer
to track increases in optical density (absorbance at 600 nm) over time. This
method gives a measurement of turbidity in each tube that is directly proportional
to the cell mass in the tube, and so provides a measure of how much the
bacteria has grown since the start of the experiment.</p>
<p>To record data from this experiment, researchers used the spreadsheet shown in
Figure <a href="experimental-data-recording.html#fig:growthexcel1">2.1</a>. This spreadsheet is an example of a data
collection template—it was created not only for this experiment, but also for
other experiments that this research group conducts to measure bacterial growth
under different conditions. It was designed to allow a researcher working in the
laboratory to record measurements over the course of the experiment. This
specific spreadsheet allowed the researcher who was conducting the experiment to
(1) calculate the amount of initial inoculum (cell culture) to add to each tube
to begin the study, (2) record the raw data absorbance measurements, (3) graph
the data on both a log and linear scale, and (4) calculate doubling time in two
phases of growth using the equation listed above.</p>
<p>Let’s take a closer look at some of the features of this spreadsheet. First, it
has a section on the top right that focuses on data collection during the
experiment, with one row for each time when the tubes were measured for the cell
mass within the tube. This section of the spreadsheet starts with several
columns related to the time of each measurement, including the clock time at
measurement (column A), the difference in time (hours) between each time point
in which data were collected (column B), the date on which data were gathered
(column C), and the time in hours for each data point from the start of the
study for graphing purposes (column D). The columns for clock time (A) and date
(C) were recorded by hand, while the columns for time since the start of the
experiment (B and D) were calculated or converted by hand from these values and
then entered in the column. The remaining columns (E–I) provide data on the
optical density (absorbance at 600 nm), which is directly proportional to cell
mass in the tube. There is one column per test tub, and each of these column
labels includes a test tube ID (A1, A3, L1, L2, L3). If a tube ID starts with
“A,” it was grown in high oxygen conditions, and if it starts with “L,” it was
grown in low oxygen conditions.</p>
<p>Next, the spreadsheet has areas that provide summaries of the data, calculated
using embedded formulas or through the spreadsheet’s plotting functions. For
example, rows 17–18 provide calculations of the doubling time of the bacteria
in each tube for two periods (early and late in the experiment), while two
growth curves are plotted at the bottom of the spreadsheet.</p>
<p>Finally, the spreadsheet includes a couple of other features, including some
written notes about one of the hand calculations and a macro in the top right
that can be used by the researcher to calculate the amount of the initial
inoculum to add to each tube at the start of the experiment.</p>
<p>What the researchers found appealing about the format of this spreadsheet was
the ease with which the researcher collecting data in the laboratory could
accomplish the study goals. They also cited transparency of the raw data and
ease with which additional sampling data points could be added. The data being
graphed in real time, and the inclusion of a simple macro to calculate doubling
time, allowed the research in the laboratory to see tangible differences between
the two assay conditions as data were collected over the one-week experiment.</p>
<p>However, many of these features can have undesired consequences. They can increase
the chance of errors in recording the data and in calculating summaries based on the
data. They also make it hard to move the data into a reproducible pipeline, and
so limit opportunities for more sophisticated analysis and visualization. In the
next section of this module, we’ll highlight features of data collection templates
like this one that can make data collection “untidy.” In the section after that,
we’ll discuss how you could create a new data collection template for this example
data that would be tidier, and use this to open a more general discussion of
principles of “tidy” data collection templates.</p>
</div>
<div id="features-that-make-data-collection-templates-untidy" class="section level3" number="2.4.2">
<h3><span class="header-section-number">2.4.2</span> Features that make data collection templates “untidy”</h3>
<p>There are several features of the data collection template shown in Figure
<a href="experimental-data-recording.html#fig:growthexcel1">2.1</a> that make it “untidy,” in the sense of making it
difficult to integrate the collected data in a data analysis pipeline that
includes reading the data into a statistical program like R, Perl, or Python to
conduct data analysis and visualization. There are also features that make it
prone to errors in data collection and analysis.</p>
<p>First, these data will be hard to read into a statistical program from this
spreadsheet because the raw data (the time points each observation was collected
and the optical density for the sample at that time point) form only part of the
spreadsheet (Figure <a href="experimental-data-recording.html#fig:extractraw">2.2</a>, area highlighted by the blue box). The “extra” elements on the
spreadsheet, which include the output from calculations, plots, macros, and
notes, make it harder to isolate the raw data from the file when using a
statistical program.</p>
<div class="figure"><span style="display:block;" id="fig:extractraw"></span>
<img src="figures/growth_curve_raw_data.png" alt="Isolating raw data collected in a template from extra elements. The box in this figure highlights the area of the spreadsheet where data are collected. All other elements of the spreadsheet focus on other aims (e.g., summarizing these data, adding notes, macros for experimental design). Those other elements make it difficult to extract the raw data for more advanced analysis and visualization through a statistical program like R, Python, or Perl." width="\textwidth" />
<p class="caption">
Figure 2.2: Isolating raw data collected in a template from extra elements. The box in this figure highlights the area of the spreadsheet where data are collected. All other elements of the spreadsheet focus on other aims (e.g., summarizing these data, adding notes, macros for experimental design). Those other elements make it difficult to extract the raw data for more advanced analysis and visualization through a statistical program like R, Python, or Perl.
</p>
</div>
<p>While these extra elements make it hard to extract the raw data, it isn’t
impossible. Programming languages like R include functions to read data in from
a spreadsheet, and these functions often provide options to specify the sheet of
the file to read in, as well as the rows and columns to read from a specific
sheet. In the example spreadsheet in Figure <a href="experimental-data-recording.html#fig:extractraw">2.2</a>, for example,
you could specify to read in only rows 1–15 of columns A–I, to focus on the
raw data. However, one goal of reproducible research is to create tools and
pipelines that are <strong>robust</strong>—that is, ones that still work as desired when the
raw data is changed in small ways, or even across different raw data files. In
later modules, in fact, we’ll look at how we can use these principles to create
tools that can be applied consistently across multiple studies to make data
analysis of laboratory data both more efficient and reproducible. Therefore,
while we could customize code to read in data from a specific part of a complex
spreadsheet, like that shown in Figure <a href="experimental-data-recording.html#fig:extractraw">2.2</a>, this customization
would make the code less robust. If we asked the statistical program to read in
rows 1–15 of columns A–I, for example, the code would perform incorrectly if
we later added one more time point to the experiment, or if we tried to use the
same template for an experiment that used more test tubes. If we instead use a
template that only records the raw data, without additional elements, then we
can create more robust tools, since we can write code to read in whatever is in
a spreadsheet, rather than restricting to certain rows and columns. Any analysis,
summaries, or visualizations that we’d like to perform on the raw data can be
done through reproducible reports—which we’ll show an example of later for
this example data—rather than directly in a spreadsheet.</p>
<p>Next, the example template helps demonstrate how specific ways of recording data
can make the template less tidy. First, let’s look at how the template records
the time of each measurement. It does this using four separate columns
(Figure <a href="experimental-data-recording.html#fig:extractraw">2.2</a>). In column C, the researcher records the date a
measurement was taken, and in Column A he or she records the clock time of the
measurement. The experiment was started, for example, at 12:00 PM (“12:00” in column A)
on July 9 (“9-Jul” in column C). These values are entered by hand by the researcher.
Next, these values are used to calculate, for each measurement, how long it had been
since the start of the experiment. This value is recorded in two separate ways—as
hours and minutes in column B and converted into hours and percents of hours (using
decimals) in column D. For example, the second measurement was taken at 4:05 PM
on July 9 (“16:05” in column A and “9-Jul” in column C), which is 4 hours and 5 minutes
after the start of the experiment (“4hr 5min” in column B) or, since 5 minutes is about
8% of an hour, 4.08 hours after the start of the experiment (“4.08” in column D).</p>
<div class="figure"><span style="display:block;" id="fig:timemeasures"></span>
<img src="figures/growth_curve_time_measures.png" alt="Measurements of time in the example data collection template. The four highlighted columns (columns A, B, C, and D) are all used in this spreadsheet to record time. The methods of recording time in this template, however, may make it more likely to create errors in data recording and collection and will make it harder to use the data in a reproducible pipeline." width="\textwidth" />
<p class="caption">
Figure 2.3: Measurements of time in the example data collection template. The four highlighted columns (columns A, B, C, and D) are all used in this spreadsheet to record time. The methods of recording time in this template, however, may make it more likely to create errors in data recording and collection and will make it harder to use the data in a reproducible pipeline.
</p>
</div>
<p>There are a few things that could be changed about how the time data are
recorded here that could make this data collection template tidier. First, it
would be better to focus only on recording the raw data, rather than adding
calculations based on that data. Columns B and D in Figure <a href="experimental-data-recording.html#fig:extractraw">2.2</a>
are both the output from calculations. Anytime a spreadsheet includes a
calculation, it creates the room for mistakes in data collection and analysis.
Often, calculations in a spreadsheet will be done using embedded formulas. These
can cause problems anytime new columns or rows are added to the data, as that
can shift the cells meant to be used in the calculation. Further, these formulas
are embedded in the spreadsheet, where they can’t be seen and checked very
easily, which makes it easy to miss a typo or other error in the formula. In the
example in Figure <a href="experimental-data-recording.html#fig:extractraw">2.2</a>, columns B and D aren’t calculated by
embedded formulas, but rather calculated by the researcher by hand and then
entered. This can create the room for user error with each calculation and each
data entry. Later, we’ll see how we can “tidy” this data collection template by
removing columns that calculate time (columns B and D) and instead doing that
calculation once the raw data are read into a statistical program.</p>
<p>The second thing that could be changed is how the template records the date and time
of the measurement. Currently, it uses two columns (A and C) to record this information.
However, each piece of information is useless without the other—instead, they must
be known jointly to do things like calculate the time since the start of the
experiment. It would therefore be tidier to record this information in a single column.
For example, instead of recording the starting time of the experiment as “12:00” in
column A and “9-Jul” in column C, you could record it as “July 9, 2019 12:00” in a
single date-time column. In this example, adding the year (“2019”) to the date will
also make this data point easier to work with in a programming language, as these often
have special functions to work with data in date-time classes, but all elements of the
date and/or time must be included to convert data points into these useful classes.</p>
<p>Next, let’s look at how the template collects data related to cell growth in each tube
(columns E–I, Figure <a href="experimental-data-recording.html#fig:growthmeasures">2.4</a>).</p>
<div class="figure"><span style="display:block;" id="fig:growthmeasures"></span>
<img src="figures/growth_curve_growth_measures.png" alt="Measurements of bacterial growth in the example data collection template. The five highlighted columns (columns E--I) are all used in this spreadsheet to record optical density in each test tube at each measurement time." width="\textwidth" />
<p class="caption">
Figure 2.4: Measurements of bacterial growth in the example data collection template. The five highlighted columns (columns E–I) are all used in this spreadsheet to record optical density in each test tube at each measurement time.
</p>
</div>
<p>These data are recorded in a format that will work pretty well. Strictly speaking, they
aren’t fully “tidy” (module 2.3), since the column headers include information that we might want to
use as variables in analysis and visualization. Specifically, each test tube’s ID is
incorporated in the column name where measurements for that tube are recorded, since
each test tube is recorded using a separate column. If we want to run analysis where we
estimate values for each test tube, or create plots where each test tube’s measurements
are shown with a separate line, then we’ll need to convert the format of the data a bit.
However, that’s quite easy to do in more statistical programming languages now, and so it’s
reasonable to compromise on this element of “tidiness” in the data collection format. As we’ll show in the
next module, changing this layout in the original data collection would require the researcher
to re-type the measurement date and time several times and would result in the spreadsheet
being longer, and so harder to see at once when recording data. We’ll discuss this balance
in designing data collection templates more in the next module, when we create a tidier version
of this example data collection template.</p>
<p>There is a final element we’d like to highlight on this example template that
could make the data hard to integrate into a reproducible pipeline. There are
cases in the example template where either column names or cell values are
formatted in a way that would be hard to work with when the data is read into a
more advanced program like R or Python (Figure <a href="experimental-data-recording.html#fig:growthformatting">2.5</a>). For
example, the column names include spaces and parentheses (e.g., “Time (clock)”).
If left as-is, when the data are read into another program, the column names
will need to be cleaned up to take these characters out, so that the column
names are composed only of alphabetical characters, numbers, or underscores.
While this can be done in code like R or Python, it will add to the data
cleaning process and could be avoided by using simpler column names in the
original data collection template. Similarly, in the example template there are
recordings of time in a format that combines numbers with text indicators for
units (e.g., “4hr 5min”). While these could be parsed in a programming language,
it will take extra code and could be avoided with a better design for recording
the data. Also, some of the data in the template is recorded in a format that
Excel might try to automatically change into a date (e.g., “9-Jul”). Even if the
value is a date, it is better to avoid formats that Excel automatically
converts. Excel could, for example, convert the date incorrectly (e.g., convert
“12/3/2020” to December 3, 2020, when it was meant to represent March 12, 2020).
It is better to record the data in a format that will pass unchanged through to
the file that you read in later coding and analysis.</p>
<div class="figure"><span style="display:block;" id="fig:growthformatting"></span>
<img src="figures/growth_curve_formatting.png" alt="Examples of special characters and formatting in the example template that could cause problems later in a data analysis pipeline." width="\textwidth" />
<p class="caption">
Figure 2.5: Examples of special characters and formatting in the example template that could cause problems later in a data analysis pipeline.
</p>
</div>
</div>
<div id="converting-to-a-tidier-format-for-data-collection-templates" class="section level3" number="2.4.3">
<h3><span class="header-section-number">2.4.3</span> Converting to a “tidier” format for data collection templates</h3>
<p>Now that we’ve looked at characteristics that can make a data collection template “untidy,” let’s
go through some principles for creating “tidy” templates to record the same data. There are three
basic principles for designing “tidy” templates that will go a long way to creating ways to
collect data in a research group that can be easily used within a reproducible analysis pipeline.
These three principles are:</p>
<ol style="list-style-type: decimal">
<li>Limit the template to the collection of data.</li>
<li>Make sensible choices when dividing data collection into rows and columns.</li>
<li>Avoid characters or formatting that will make it hard for a computer program to process the data.</li>
</ol>
<p>The first principle in designing a tidier template for collecting laboratory
data is to <strong>limit the template to the collection of data</strong>. The key here is the
word “collection.” A tidy template will avoid any calculations done on the
original data and instead focus only on the initial data that the researcher
records for the experiment. This means that you should exclude from the template
any element that provides a calculation, summary, or plot based on the initial
recorded element. You should also exclude any special formatting that you are using
to encode information. For example, say that you are collecting data, and in some
cases you get a warning that the reading may be below the instrument’s detection
limit. It may be tempting to highlight the cells with measurements where this warning
was displayed as you record the data. However, you should avoid doing this, as any
color or other formatting information will be lost when you read the data in the
file into a statistical program. Instead, you could add a second column to indicate
if the measurement included a warning.</p>
<p>The second principle is to <strong>make sensible choices when dividing data collection
into rows and columns</strong>. There are many different ways that you could spread the
data collection into rows and columns. One decision is how (and whether) to
divide recorded information across columns. Figure <a href="experimental-data-recording.html#fig:arrangingcolumns">2.6</a>,
for example, shows several ways that you could divide data on a date and time
into one or more columns. In this example, it typically makes the most sense
to use a single column to record all the date and time elements (the top example
in Figure <a href="experimental-data-recording.html#fig:arrangingcolumns">2.6</a>). Most statistical programs have powerful
functions for parsing dates and times, after which they store these data in special
classes that allow time-related operations (for example, calculating the time difference
between two date-time measurements). It will be most efficient to record all date and
time elements in a single column.</p>
<div class="figure"><span style="display:block;" id="fig:arrangingcolumns"></span>
<img src="figures/arranging_columns.png" alt="Examples of special characters and formatting in the example template that could cause problems later in a data analysis pipeline." width="\textwidth" />
<p class="caption">
Figure 2.6: Examples of special characters and formatting in the example template that could cause problems later in a data analysis pipeline.
</p>
</div>
<p>Conversely if you have complex data with different elements
(for example, height in components of inches and feet), it may make
sense to use separate columns for each of the components. For
example, rather than using one column to record <code>5'7"</code>, you
could divide the information into one column with the component
that is in feet (<code>5</code>) and one with the component in inches (<code>7</code>).
In the first case, when you read the data into a program like
R you would need to use complex code to split the value into
its parts to be able to use it. In the second case, you could
easy work with the values in the two separate columns to calculate
a value to use in further work (e.g., use a formula like
<code>height_ft * 12 + height_in</code> to calculate the full height in inches).</p>
<p>Another decision at this stage is how “long” versus “wide” you make your
template. A “wide” design will include more columns, while a “long” design will
include more rows. Often, you can create different designs that allow you to
collect the same values but with different designs on this wide-versus-long
spectrum. Figure <a href="experimental-data-recording.html#fig:longversuswide">2.7</a> gives two examples of templates that
collect the same data, but one is using a wider design and the other is using a
longer design.</p>
<div class="figure"><span style="display:block;" id="fig:longversuswide"></span>
<img src="figures/growth_curve_long_vs_wide.png" alt="Examples of two ways arranging the same data in a data recording template. The format on the left records the optical density measurements for each test tube in a separate column, and the column header identifies the test tube. This is an example of a 'wider' format. The format on the right records the optical density for all test tupbes in a single column, using a separate column to record which test tube the measurement represents. This is an example of a 'longer' format." width="\textwidth" />
<p class="caption">
Figure 2.7: Examples of two ways arranging the same data in a data recording template. The format on the left records the optical density measurements for each test tube in a separate column, and the column header identifies the test tube. This is an example of a ‘wider’ format. The format on the right records the optical density for all test tupbes in a single column, using a separate column to record which test tube the measurement represents. This is an example of a ‘longer’ format.
</p>
</div>
<p>In module 2.3, we described the rules for the “tidy” format for dataframes.
If you record data directly into a “tidy” format, it will be very easy to
read into a programming language to analyze and visualize. However,
this tidy format can sometimes result in datasets that are very long. It
may be more convenient to record data into a wider format, especially if you
are recording the data in a laboratory setting where it is inconvenient to
scroll up and down within a longer-format file. Fortunately, there are some
convenient tools in programs like R and Python that can be used to take
data that are collected in a wider format and reformat them to the tidy
format as soon as they are read into the software program. While this will
require some extra code, it is usually code that is fairly simple and straightforward.
Therefore, when you design your data collection template, you can balance
any the pratical advantages of using a wider data collection format against
the advantages of a fully “tidy” format that apply once your input the data
into a statistical program for analysis and visualization. Often, the wider
format might win out in this balance, and that’s fine.</p>
<p>The third principle is to <strong>avoid characters or formatting that will make it
hard for a computer program to process the data</strong>. This principle is
particularly important for the column names for each column. When you read data
into a statistical program like R, these names will automatically be used as the
column names in the R data frame object, and the code will regularly use these
column names to refer to parts of the data when analyzing and visualizing it.
You will find it easiest to use the data in a reproducible pipeline if you
follow a couple rules for the column names. The reason that these rules will help
is that they replicate the rules for naming objects in programming languages,
and so will help in seamlessly transitioning between the stages of data
collection and data analysis. First, always start a column name with a letter.
Second, only use letters, numbers, or the underscore character ("_") for
the rest of the characters in the column name.</p>
<p>Based on these rules, then, you should avoid putting spaces in your column names
when you design a data collection template. It is tempting to include spaces to
make the names clearer for humans to read, and this is understandable. Often,
using an underscore in place of a space can allow for easy human comprehension
while still avoiding characters that are difficult for statistical programs.
For example, if you have a column named “Optical density,” you can change it
to “Optical_density” without making it much more difficult for a person to
understand. As with other choices in designing a data collection template,
these choices about column names can be a balance between making the template
easy for researchers to use in the laboratory and easy for the statistical
program to parse later in the pipeline. For example, statistical programs like
R have functions for working with character strings that can be used to
replace all the spaces in column names with another character. However, if
it isn’t unreasonable to follow the recommended rules in writing column names
for the data collection template, you can keep code later in the pipeline
much simpler, so it’s worth considering.</p>
<p>Beyond spaces, there are a number of other special characters that you might
be tempted to include in column names. These could include parentheses, dollar signs,
percent signs, hash marks (“#”), and so on. Any of these will require extra code
in later steps of an analysis pipeline, and some can cause more severe problems
because they have special functionality in the programming language. For example,
hash marks are used in the R programming language to add comments within code, while
dollar signs are used for subsetting elements of a list or data frame object.
It is worth the effort to avoid all these characters in column names in a data
collection template.</p>
<p>There are also considerations you can make in terms of how you record data within
cells of the data collection template, and these can make a big difference in terms
of how hard or easy it is to work with the data within a statistical program. While
statistical programs like R are very powerful in terms of being able to handle even
very “messy” input data, they require a lot of code to leverage this power. By being
thoughtful when you design the template to record the data, you can avoid having to
use a lot of code to input and clean the data in later stages of the pipeline.</p>
<p>Figure <a href="experimental-data-recording.html#fig:recordingtime">2.8</a> gives an example of a choice that you could make
in the format you use to record data. This figure shows two columns from the
original data collection template from the example experiment for this module.
This template includes two columns that record the time since the start of the
experiment, and they use different formats for doing this. In column B, time is
recorded in hours and minutes, with the characters “hr” and “min” used to
separate the two time components. In column D, the same information is recorded,
but in decimals of hours (e.g., 4.08 hours for 4 hours and 5 minutes). While the
format in column B is more similar to how humans think of time, it will take
more code to parse in a statistical program. When reading this data into a
program like R, you would need to use regular expressions to split apart the
different elements and then recombine them into a format that the program
understands. By contrast, the values recorded in column D could be easily read
in by a statistical program, with minimal code needed before they could be used
in analysis and visualizations.</p>
<div class="figure"><span style="display:block;" id="fig:recordingtime"></span>
<img src="figures/growth_curve_recording_time.png" alt="Examples of two ways of recording time in the original template from the example experiment. Column B uses hours and minutes, with characters embedded to separate hours from minutes, while column D uses hours in decimal degrees. The format in column D will be much easier to integrate into a larger data analysis pipeline." width="\textwidth" />
<p class="caption">
Figure 2.8: Examples of two ways of recording time in the original template from the example experiment. Column B uses hours and minutes, with characters embedded to separate hours from minutes, while column D uses hours in decimal degrees. The format in column D will be much easier to integrate into a larger data analysis pipeline.
</p>
</div>
<p>Finally, when you are designing the data collection template, you should try to
avoid using formats that may be “auto-converted” by the spreadsheet program. For
example, if you enter a value like “7-9-19” into a cell, the spreadsheet may try
to automatically convert it to a date. Perhaps it is a date, but even if it is,
the spreadsheet algorithm might make problematic assumptions in the conversion.
For example, it might assume that “7-9-19” means July 9, 2019, when you meant
for it to represent September 7, 1919. Further, there are cases where you might
enter a value that is not a date, but that the spreadsheet thinks is based on
its formatting. This was found to be a problem, for example, for some gene
names. To avoid potential autoconversion by the spreadsheet, consider putting
any character stings, including entries with dates and identifiers, inside
quotation marks when you enter them in the spreadsheet program. The spreadsheet
program will respect this as a sign to leave the entry as-is, rather than
attempting automatic formatting into a date or other special class of data.</p>
<p>These three principles are an excellent starting point for designing a “tidy”
template for collecting data. By using these, you will be well on your way to
collecting data in a way that is easy to integrate in a longer reproducible data
analysis pipeline. There are some additional steps that you could consider that
can help make it easier to do clever and interesting things with your data once
you read it into a statistical program.</p>
<p>For example, you could design column names and column entries so that you will
be able to take advantage of statistical programming tools based on something
called regular expressions. “Regular expressions” refers to patterns in
character strings (which are just strings of one or more characters, like
“aerated_1” or “mouseID”) that can be described and searched for using defined
patterns. For example, take the following set of character strings: “aerated1,”
“aerated3,” “low_oxygen1,” “low_oxygen2,” “low_oxygen3.” These strings current
include two pieces of information. First, they give the growth condition, which
is either “aerated” or “low_oxygen.” This information is given in each string
using only alphabetical characters (e.g., a, b, c) and the underscore character.
Next, the strings include information on the test tube of the sample for that
condition—for example, “aerated1” indicates the first test tube under the
aerated conditions. This test tube number is given using only numerical
characters (e.g., 1, 2, 3). Since these pieces of information are encoded
in the character strings using these patterns, you can use regular expressions
in a program like R to isolate only the non-numeric part of each string
(“aerated” versus “low_oxygen”) or only the number part (“1,” “2,” or “3”).
This functionality can be a very powerful way to use column names and cell values
to encode information that you can later separate or extract to use in things
like adding color to plots based on certain conditions. In the next module,
we’ll show an example of using regular expressions in this way to leverage
information taken when collecting the data. When designing a tidy data collection
template, it’s worthwhile to think of writing column names or otherwise recording
data in a way that uses these types of regular patterns in a meaningful way.</p>
<p>When you convert data collection templates to “tidier” formats, they will
typically look much simpler than the templates that your research group may have
been using. In the example experiment that we described earlier in this module,
this process of tidying the template results in a template like that shown in
Figure <a href="experimental-data-recording.html#fig:growthexcel1">2.1</a> (in the next module, we’ll walk through all the
steps to create this tidier template, using this principles we’ve covered in
this module). By comparison, the starting template for data collection for this
experiment is shown in Figure <a href="experimental-data-recording.html#fig:growthexcel1">2.1</a>.</p>
<div class="figure"><span style="display:block;" id="fig:growthsimple1"></span>
<img src="figures/growth_curve_simple.png" alt="Example of an simpler format that can be used to record and analyze data for the same laboratory experiment as the previous figure. Annotations highlight where data is entered by hand. No calculations are conducted or figures created---these are all done later, using a code script." width="\textwidth" />
<p class="caption">
Figure 2.9: Example of an simpler format that can be used to record and analyze data for the same laboratory experiment as the previous figure. Annotations highlight where data is entered by hand. No calculations are conducted or figures created—these are all done later, using a code script.
</p>
</div>
<p>By comparing these two templates, you can see that the simpler template does
not, by itself, provide immediate, real-time summaries of the collected data. The
simpler template has removed elements like plots and values calculated by embedded
formulas. At first glance, this might seem like a disadvantage of using a tidier
template to collect data. However, by combining other tools in a pipeline, it is
easy to connect the tidier raw data file to reporting tools. In this way, you can
quickly create real-time summaries of the data that are similar to those shown in
Figure <a href="experimental-data-recording.html#fig:growthexcel1">2.1</a>, but that are created and reported outside the file
used to originally record the data.</p>
<p>Figure <a href="experimental-data-recording.html#fig:growthreport1">2.10</a> shows an example of a simple report that could
be created for the example experiment. This report is generated using a
statistical program, R, which inputs the data from the simple template shown in
Figure <a href="experimental-data-recording.html#fig:growthsimple1">2.9</a>. The report then uses R code to generate a PDF
or Word file with the output shown below. The file for this report is created in
a way that the output can be quickly regenerated with a single button click, and
so it can be applied to other data saved using the same template. In fact, you can
create templates for reports that coordinate with each data collection template
that you create. In the next module, we’ll walk through how you could create
the generating file for this report, and in later modules (3.7–3.9), we provide
a thorough overview of creating these types of “knitted” documents.</p>
<div class="figure"><span style="display:block;" id="fig:growthreport1"></span>
<img src="figures/growth_curve_report.png" alt="Examples of an automated report that can be created to quickly generate summaries and estimates of the data collected in the simplified data collection template for the example experiment." width="\textwidth" />
<p class="caption">
Figure 2.10: Examples of an automated report that can be created to quickly generate summaries and estimates of the data collected in the simplified data collection template for the example experiment.
</p>
</div>
<p>The report shown in Figure <a href="experimental-data-recording.html#fig:growthreport1">2.10</a> repeats some of the same summaries
that were shown in the more complex original data collection template
(Figure <a href="experimental-data-recording.html#fig:growthexcel1">2.1</a>). There are a number of advantages, however, to using
separate steps and files for the processes of collecting versus analyzing the data.
The separate report (Figure <a href="experimental-data-recording.html#fig:growthexcel1">2.1</a>) provides a starting point that can
be easily adapted to make more complex figures and analysis, as well as to integrate
the collected data with data measured in other ways for the experiment.</p>
<p>For example, take a look at the graph in the top left corner on the second page
of the report shown in Figure <a href="experimental-data-recording.html#fig:growthreport1">2.10</a>. This figure shows the
growth curve from the collected data, and it adds a shaded area to show the time
range that was used to estimate doubling times for each sample. This provides a
helpful quality check for this experiment. Bacterial growth goes through several
phases, including an initial lag phase, an exponential growth phase (when the
bacteria are regularly doubling), a stationary phase (when growth starts to slow
down, because of exhaustion of nutrients or buildup of waste), and a dying
phase. The doubling time should be calculated only during the exponential phase
of growth, as the equation used to calculate it relies on describing growth during
a period of regular doubling. When the growth curve is plotted with a log scale on
the y-axis, the growth curve will look approximately linear in this exponential
growth region. By including the plot on the top left of the second page in
Figure <a href="experimental-data-recording.html#fig:growthreport1">2.10</a>, the researcher can quickly see that, in this
experiment, the selected time range for calculating doubling time might not be
appropriate—for the low oxygen condition, in particular, this time range looks
like it included some measurements made during the transition into the stationary
phase of growth. By quickly being able to assess this, the researcher
can reassess whether a different time range should be use to calculate the
doubling time for this experiment.</p>
<p>The report shown in Figure <a href="experimental-data-recording.html#fig:growthreport1">2.10</a> provides results that are
very similar to those calculated in the original spreadsheet, to show that
you don’t need to give up fast and clear summaries and visuals if you simplify
the template for collecting data. However, this report template could easily be
made more sophisticated. For example, you could add code into the report that
would perform quality control checks. In the example case, the cell growth is
measured using optical density, and while this measure is proportional to cell
density in many cases, the measurement can be prone to error once the optical
density is very high. Therefore, you could, for example, add a check into the report to
highlight any measures of optical density that are higher than a certain value.</p>
</div>
<div id="learning-more-about-tidy-data-collection-in-the-laboratory" class="section level3" number="2.4.4">
<h3><span class="header-section-number">2.4.4</span> Learning more about tidy data collection in the laboratory</h3>
<p>It may take some iteration to develop the data collection templates that are both
convenient and appropriate to input to more complex programs for pre-processing,
analysis, and visualization. This module and the next module provide guidance and
examples, but it can be helpful to see more examples. Two excellent resources on this
topic are articles by <span class="citation">Ellis and Leek (2018)</span> and <span class="citation">Broman and Woo (2018)</span>.</p>
<!-- ----------------------------------------------------------------------------- -->
<!-- **Older text** -->
<!-- It is usually very little work to record data in a structure -->
<!-- that follows the "tidy data" principles, especially if you are planning to record -->
<!-- the data in a two-dimensional, tabular format already, and following these  -->
<!-- principles can bring some big advantages. We explain these rules and provide  -->
<!-- examples of biomedical datasets that both comply and don't comply with these -->
<!-- principles, to help make it clearer how you could structure a "tidy-compliant"  -->
<!-- structure for recording experimental data for your own research.  -->
<!-- If the data is the same regardless of whether it's "tidy" or not, then why all -->
<!-- the fuss about following the "tidy" principles when you're designing the format -->
<!-- you'll use to record your data? The magic here ix this---if you follow these -->
<!-- principles, then your data can be immediately input into a collection of -->
<!-- powerful tools for visualizing and analyzing the data, without further cleaning -->
<!-- steps (as discussed in the previous module). What's more, all those tools (the -->
<!-- set of tools is calld the "tidyverse") will typically *output* your data in a -->
<!-- "tidy" format, as well. -->
<!-- Once you have tools that input and output data in the same way, it becomes very  -->
<!-- easy to model each of the tools as "small, sharp tools"---each one does one  -->
<!-- thing, and does it really well. That's because, if each tool needs the same -->
<!-- type of input and creates that same type of output, those tools can be chained -->
<!-- together to solve complex problems. The alternative is to create large software -->
<!-- tools, ones that do a lot to the input data before giving you some output.  -->
<!-- "Big" tools are harder to understand, and more importantly, they make it hard -->
<!-- to adapt your own solutions, and to go beyond the analysis or visualization that -->
<!-- the original tool creators were thinking of when they created it. Think of it this -->
<!-- way---if you were writing an essay, how much more can you say when you can mix and  -->
<!-- match words to create your own sentences versus if you were made to combine  -->
<!-- pre-set sentences?  -->
<!-- ### Creating the rules for collecting data in the same time each time -->
<!-- It is likely that there are certain types of experiments that you conduct  -->
<!-- regularly, and that they're often trying to answer the same type of  -->
<!-- question and generate data of a consistent type and structure. This is  -->
<!-- a perfect chance to lay down rules or a pattern for how members of  -->
<!-- your research group will record that data.  -->
<!-- These rules can include:  -->
<!-- 1. How many units of observation does the experiment typically have?  -->
<!-- Say, for example, that you are measuring the influence of two drugs on  -->
<!-- bactieral load in an animal at  -->
<!-- three time points. There may be some measurements taken at the unit of the drug  -->
<!-- (for example, measurements related to its chemical composition) and some -->
<!-- taken at the unit of animal and time point (for example, the concentration of drug in  -->
<!-- an animal's blood at a certain time point). This will help you define how many  -->
<!-- tables you should use to collect the data---one for each unit of observation. -->
<!-- 2. Which measurements will be recorded for each observation? In tidy data, the measurements -->
<!-- taken for an observation are recorded in rows, so you then specify what -->
<!-- column names should be used for each measurement (e.g., "sample_time",  -->
<!-- "animal_weight"). If data is being recorded using multiple tables (because there -->
<!-- are multiple units of observation), make sure that each table include -->
<!-- "ID" columns that can be used to link across the tables. For example, each  -->
<!-- table might have a column with a unique ID for each drug being tests, or tables -->
<!-- with measurements on animals might each have a column that uniquely identifies -->
<!-- the animal in an observation. -->
<!-- 3. What units will be used for recording each measurement? For timestamp-type  -->
<!-- measurments, like the date and time that an experiment started and the time of  -->
<!-- each sample measurement, what timezone will be used? (Even if it's always the  -->
<!-- same one, this can come in useful every now and then if you need to figure out  -->
<!-- something like whether that location's timezone followed Daylight Savings Time,  -->
<!-- for an experiment that spans the switch between Standard and Daylight Savings). -->
<!-- [Figure: Three tables---measurements on a drug (chemistry), measurements on an animal (weight), -->
<!-- measurements on an animal at time points (drug concentration)] -->
<!-- You can then take this information and design a *template* for collecting that -->
<!-- type of data. A template is, in this case, a file that gives the "skeleton" of -->
<!-- the table or tables. You will create this template file and save it somewhere -->
<!-- easy for lab members to access, with a filename that makes it clear that this is -->
<!-- a template. For example, you may create a folder with all the templates for -->
<!-- tables for your experiment, and name a template in it for collecting something -->
<!-- like animal weights at the start of the experiment something like -->
<!-- "animal_wt_table_template.csv" or "animal_wt_table_template.xlsx". -->
<!-- Each time someone starts an experiment collecting that type of data, he or she -->
<!-- can copy that template file, move it to the directory with files for that -->
<!-- experiment and rename it. When you open that copy of the file, you can record -->
<!-- observations directly into it. -->
<!-- [Figure: Example template file] -->
<!-- ``` -->
<!-- #################################################################################### -->
<!-- #################################################################################### -->
<!-- # -->
<!-- # Column names and meanings -->
<!-- # -->
<!-- # animal_id:        A unique identifier for each animal.  -->
<!-- # animal_wt_g:      The weight of the animal, recorded in grams.  -->
<!-- # date_wt_measured: The date that the animal's weight was measured, recorded as  -->
<!-- #                   "month day, year", e.g., "Jan 1, 2019" -->
<!-- # cage_id:          A unique identifier for the case in which the animal was housed -->
<!-- #  -->
<!-- # Other table templates for this experiment type:  -->
<!-- # drug_conc_by_time.csv: A template for recording drug concentrations in the animals -->
<!-- #                        by time point -->
<!-- #  -->
<!-- animal_id, animal_wt_g, date_wt_measured, cage_id -->
<!--    "A101",        50.2,   "Jan 1, 2019",      "B"     -->
<!-- ``` -->
<!-- Adding in one row of sample values, to be deleted each time the template is  -->
<!-- copied and used, can be a very helpful addition. This will help the user remember -->
<!-- the formats that are expected for each column (for example, the format the  -->
<!-- date should be recorded in), as well as small details like which columns should -->
<!-- include quotation marks. -->
<!-- These template tables can be created as flat files, like comma-separated value  -->
<!-- files. However, if this is too big of a jump, they can also be created as  -->
<!-- spreadsheet files. Many of the downsides of spreadsheet files are linked to  -->
<!-- the use of embedded macros, integration of raw and processed / calculated data,  -->
<!-- and other factors, rather than related to their use as a method to record data.  -->
<!-- However, do note that plain text files like flat files can be opened in RStudio -->
<!-- in a spreadsheet-like view in RStudio. Data can be recorded directly here, in  -->
<!-- a format that will feel comfortable for spreadsheet users, but without all the  -->
<!-- bells and whistles that we're aiming to avoid in spreadsheet programs like Excel. -->
<!-- [Figure---Opening a csv file with a spreadsheet like view] -->
<!-- There are some advantages to shifting to record data in flat files like CSVs, -->
<!-- rather than Excel files, and using the spreadsheet-style view in RStudio to work -->
<!-- with those files if you find it easier than working with the files in a text -->
<!-- editor (which can get tough, since the values in a column don't always visually -->
<!-- line up, and you have to remember to put in the right number of columns). By -->
<!-- recording the data in a plain text file, you can later move to tracking changes -->
<!-- that are made to the data using the version control tool *git*. This is a -->
<!-- powerful tool that can show who made changes to a file and when, with exact -->
<!-- details on the changes made and room for comments on why the change was made. -->
<!-- However, *git* does not provide useful views of changes made to binary files -->
<!-- (like Excel), only those made in plain text files. Further, plain text files are -->
<!-- guaranteed to not try to "outsmart" you---for example, they will not try to -->
<!-- convert something that looks like a date into a date. Instead, they will leave -->
<!-- things exactly as you typed them in. Finally, later in this book we will build -->
<!-- up to creating templates that do even more---for example, templates for reports -->
<!-- you need to write and presentations you need to give, as well as templates for -->
<!-- the whole structure of a project. Plain text files fit very nicely into this -->
<!-- developing framework, while files in complex binary formats like xlxs don't fit -->
<!-- as naturally. -->
<!-- Google Sheets is another tool that might come in useful. [More about using this  -->
<!-- with R.] -->
<!-- This idea of creating template files for data recording isn't -->
<!-- revolutionary---many laboratory groups have developed spreadsheet template files -->
<!-- that they share and copy to use across similar experiments that they conduct. -->
<!-- The difference here is in creating a table for recording data *that follows the -->
<!-- tidy data principles*, or at least comes close to them (any steps away from -->
<!-- characteristics like embedded macros and use of color to record information will -->
<!-- be helpful). -->
<!-- The next chapter will walk through two examples of changing from non-tidy table -->
<!-- templates to ones that record data in a way that follows the tidy data -->
<!-- principles. -->
<!-- ### Subsection 1 -->
<!-- > "Or maybe your goal is that your data is *usable* in a wide range of -->
<!-- applications? If so, consider adopting standard formats and metadata  -->
<!-- standards early on. At the very least, keep track of versions of data -->
<!-- and code, with associated dates." [@goodman2014ten] -->
<!-- > "Standards for data include, for example, data formats, data exchange -->
<!-- protocols, and meta-data controlled vocabularies." [@barga2011bioinformatics] -->
<!-- > "Software systems are transparent when they don't have murky corners or hidden -->
<!-- depths. Transparency is a passive quality. A program is passive when it is possible -->
<!-- to form a simple mental model of its behavior that is actuaally predictive for all -->
<!-- or most cases, because you can see through the machinery to what is actually going  -->
<!-- on." [@raymond2003art] -->
<!-- > "Software systems are discoverable when they include features that are designed  -->
<!-- to help you build in your mind a correct mental model of what they do and how they -->
<!-- work. So, for example, good documentation helps discoverability to a programmer. Discoverability -->
<!-- is an active quality. To achieve it in your software, you cannot merely fail to be obscure,  -->
<!-- you have to go out of your way to be helpful." [@raymond2003art] -->
<!-- > "Elegant code does much with little. Elegant code is not only correct but visibly,  -->
<!-- *transparently* correct. It does not merely communicate an algorithm to a computer,  -->
<!-- but also conveys insight and assurance to the mind of a human that reads it. By seeking -->
<!-- elegance in our code, we build better code. Learning to write transparent code is a first,  -->
<!-- long step toward learning how to write elegant code---and taking care to make code  -->
<!-- discoverable helps us learn how to make it transparent. Elegant code is both transparent and -->
<!-- discoverable." [@raymond2003art] -->
<!-- > "To design for transparency and discoverability, you need to apply every tactic for -->
<!-- keeping your code simple, and also concentrate on the ways in which your code is a  -->
<!-- communication to other human beings. The first questions to ask, after 'Will this design -->
<!-- work?' are 'Will it be reaadable to other people? Is it elegant?' We hope it is clear ... -->
<!-- that these questions are not fluff and that elegance is not a luxury. These qualities -->
<!-- in the human reaction to software are essential for reducing its bugginess and -->
<!-- increasing its long-term maintainability." [@raymond2003art] -->
<!-- > "Software is maintainable to the extent that people who are not its author can  -->
<!-- successfully understand and modify it. Maintainability demands more than code that -->
<!-- works; it demands code that follows the Rule of Clarity and communicates successfully  -->
<!-- to human beings as well as the computer." [@raymond2003art] -->
<!-- > "An equivalent to the laboratory notebook that is standard good practice in  -->
<!-- labwork, we advocate the use of a computational diary written in the R markdown -->
<!-- format. ... Together with a version control system, R markdown helps with  -->
<!-- tracking changes." [@holmes2018modern] -->
<!-- > "R.A. Fisher, one of the fathers of experimental design, is quoted as  -->
<!-- saying 'To consult the statistician after an experiment is finished is  -->
<!-- often merely to ask him to conduct a post mortem examination. He can  -->
<!-- perhaps say what the expierment died of.' So it is important to design an  -->
<!-- experiment with the analysis already in mind. Do not delay thinking about -->
<!-- how to analyze the data until after they have been acquired. ... -->
<!-- Dailies: start with the analysis as soon as you have acquired some data.  -->
<!-- Don't wait until everything is collected, as then it's too late to  -->
<!-- troubleshoot. ... Start writing the paper while you're analyzing the data.  -->
<!-- Only once you're writing and trying to present your results and  -->
<!-- conclusions will you realize what you should have done properly to support -->
<!-- them." [@holmes2018modern] -->
<!-- > "In the same way a file director will view daily takes to correct potential -->
<!-- lighting or shooting issues before they affect too much footage, it is a  -->
<!-- good idea not to wait until all runs of an experiment have been finished -->
<!-- before looking at the data. Intermediate data analyses and visualizations -->
<!-- will track unexpected sources of variation and enable you to adjust the -->
<!-- protocol. Much is known about the sequential design of experiments, but  -->
<!-- even in a more pragmatic setting it is important to be aware of your sources -->
<!-- of variation as they occur and adjust for them." [@holmes2018modern] -->
<!-- > "Analysis projects often begin with a simple script, perhaps to try out a  -->
<!-- few initial ideas and explore the quality of the pilot data. Then more ideas are -->
<!-- added, more data come in, other datasets are integrated, more people become  -->
<!-- involved. Eventually the paper need to be written, the figures need to be done -->
<!-- 'properly' and the analysis needs to be saved for the scientific record and  -->
<!-- to document its integrity." [@holmes2018modern] -->
<!-- > "**Use literate programming tools.** Examples are Rmarkdown and Jupyter. This -->
<!-- makes code more readable (for yourself and for others) than burying  -->
<!-- explanations and usage instructions in comments in the source code or in  -->
<!-- separate README files. In addition, you can directly embed figures and tables -->
<!-- in these documents. Such documents are good starting points for the supplementary  -->
<!-- material of your paper. Moreover, they're great for reporting analyses to your -->
<!-- collaborators." [@holmes2018modern] -->
<!-- ### Don't Repeat Yourself! -->
<!-- One of the core tenets of programming is the philosophy of "Don't Repeat -->
<!-- Yourself" (a.k.a., the "DRY Principle").[Source of "Don't Repeat -->
<!-- Yourself"---*The Pragmatic Programmer*] With programming, you can invest a -->
<!-- little bit of time to code your computer to do things that take a lot of your -->
<!-- time otherwise. In this way, you can automate repetitive tasks. -->
<!-- > "The DRY principle, for Don't Repeat Yourself, is one of the colloquial tenets -->
<!-- of programming. That is, you should name things once, do things once, create a -->
<!-- function once, and let the computer repeat itself." [ford2015code] -->
<!-- > "Code, in other words, is really good at making things *scale*. Computers -->
<!-- may require utterly precise instructions, but if you get the instructions -->
<!-- right, the machine will tirelessly do what you command over and over and -->
<!-- over again, for users around the world. ... Solve a problem once, and you've -->
<!-- solved it for everyone." [Coders, p. 20] -->
<!-- > "Since they have, at their beck and call, machines that can repeat -->
<!-- instructions with robotic perfection, coders take a dim view of doing  -->
<!-- things repetitively themselves. They have a dislike of inefficiency -->
<!-- that is almost aesthetic---they recoil from it as if from a disgusting -->
<!-- smell. Any opportunity they have to automate a process, to do something -->
<!-- more efficiently, they will." [Coders, p. 20] -->
<!-- > "Programmers are obsessed with efficiency. ... Removing the friction  -->
<!-- from a system is an aesthetic joy; [programmers'] eyes blaze when they -->
<!-- talk about making something run faster, or how they eliminated some  -->
<!-- bothersome human effort from a process." [Coders, p. 122] -->
<!-- > "Computers, in many ways, inspire dreams of efficiency greater than -->
<!-- any tool that came before. That's because they're remarkably good at -->
<!-- automating repetitive tasks. Write a script once, set it running, and -->
<!-- the computer will tirelessly execute it until it dies or the power -->
<!-- runs out. What's more, computers are strong in precisely the ways that -->
<!-- humans are weak. Give us a repetitive task, and our mind tends to  -->
<!-- wander, so we gradually perform it more and more irregularly. Ask us -->
<!-- to do something at a precise time or interval, and we space out and  -->
<!-- forget to do it. ... In contrast, computers are clock driven and superb -->
<!-- at doing the same thing at the same time, day in and day out." -->
<!-- [Coders, p. 124] -->
<!-- > "Larry Wall, the famous coder and linguist who created the Perl  -->
<!-- programming language, deeply intuited this coderly aversion to  -->
<!-- repetition. In his book on Perl, he and coauthors wrote that one of the -->
<!-- key virtues of a programmer is 'laziness'. It's not that you're too lazy -->
<!-- for coding. It's that you're too lazy to do routine things, so it  -->
<!-- inspires you to automate them." [Coders p. 126] -->
<!-- In scientific research, there are a lot of these repetitive tasks, and as tools for  -->
<!-- automation continue to develop, there are many opportunities to "automate away" busywork.  -->
<!-- > "Science often involves repetition of computational tasks such as processing  -->
<!-- large number of data files in the same way or regenerating figures each time -->
<!-- new data are added to an existing analysis. Computers were invented to do these -->
<!-- kinds of repetitive tasks but, even today, many scientists type the same  -->
<!-- commands in over and over again or click the same buttons repeatedly." [wilson2014best] -->
<!-- > "Whenever possible, rely on the execution of programs instead of manual procedures -->
<!-- to modify data. Such manual procedures are not only inefficient and error-prone,  -->
<!-- they are also difficult to reproduce." [sandve2013ten] -->
<!-- > "Other manual operations like the use of copy and paste between documents should -->
<!-- also be avoided. If manual operations cannot be avoided, you should as a minimum -->
<!-- note down which data fiels were modified or moved, and for what purpose." [sandve2013ten] -->
<!-- Statisticians have been doing this for a while for data cleaning analysis tasks. -->
<!-- For example, if you need to read in an Excel file into a statistical programming -->
<!-- language like R, you could write a few lines of code to do that anew each time -->
<!-- you get a new file. However, say you get Excel files over and over that follow -->
<!-- the same format--for example, files with the same number of columns, the same -->
<!-- names for those columns, and the same type of data. You can write a *script*---a -->
<!-- recorded file with a few lines of code, in this case---that reads in the file. -->
<!-- You can apply this script to each new file. -->
<!-- This saves you a little bit of time. It also ensures that you do the exact same thing -->
<!-- with every file you get. It also means that you can reproduce what you do now to a file -->
<!-- in the future. Say, for example, that you are working on a project and you read in a file -->
<!-- and conduct an analysis. Your laboratory group sends the paper out for review. Months -->
<!-- later, you get back comments from the reviewers, and they are wondering what would  -->
<!-- happen if you had analyzed the data a bit differently---say, used a different  -->
<!-- statistical test. If you use a script to read in the data file, then when you re-run -->
<!-- it to address the reviewers' comments, you can be sure that you are getting your  -->
<!-- data into the statistical program in the exact same way you did months ago, and so  -->
<!-- you're not unintentionally introducing differences in your results because you  -->
<!-- are doing some small things differently in processing the file. -->
<!-- This idea can extend across the full data analysis you do on a project. You are only  -->
<!-- saving a little bit of time and effort, maybe, by automating the step where you read -->
<!-- the data from a spreadsheet into the statistical program. And it takes some time to  -->
<!-- write that script the first time, so it can be tempting to do it fresh each time you  -->
<!-- need to do it. However, you can also write scripts that will automate cleaning your data.  -->
<!-- Maybe you want to identify data points with very high (maybe suspect) values for a certain -->
<!-- measurement, or remove observations with missing data. You can also write scripts that -->
<!-- will automating processing your data---doing things like calculating the time since the -->
<!-- start of an experiment based on the recorded sampling time for an observation. Each of  -->
<!-- these steps might be small, but the time saved really adds up since you typically  -->
<!-- need to perform many of these steps each time you run a new experiment.  -->
<!-- There are many cases in life where you'll need to make the choice between spending  -->
<!-- some time upfront to make something more efficient, versus doing it more quickly the first -->
<!-- time but then having to do it "from scratch" again each following time. For example,  -->
<!-- say that you're teaching a class, and you need to take attendance for each class period.  -->
<!-- You could write down the names of each student at the first class and save that, and  -->
<!-- then the next class write down the name of each student who shows up that day on a separate -->
<!-- sheet of paper, and so on for each class meeting. Conversely, you could take some extra -->
<!-- time before the first class and create a table or spreadsheet file with every student's -->
<!-- name and the date of each class, and then use that to mark attendance. The first method -->
<!-- will be quicker the first day, but more time consuming each following time. The second  -->
<!-- method requires a small initial investment, but with time-saving returns in the following -->
<!-- class meetings.  -->
<!-- For people who use scripts and computer programs to automate their data-related tasks,  -->
<!-- it quickly becomes very confusing how anyone who doesn't could argue that they don't because -->
<!-- they don't have time to learn how to. The amount of time you end up saving based on your  -->
<!-- initial investment is just so high if you're working with data, that it would have to take -->
<!-- a huge time investment to not be worth it. Plus---the thrill of running something that you've -->
<!-- automated! It's a very similar feeling to the feeling you get when a student or postdoc that  -->
<!-- you've spent a lot of time training has gotten to the point where you can just ask them  -->
<!-- to run something, and they do, and it means you don't have to. -->
<!-- Here are some of the problems that are solved by automating your small tasks:  -->
<!-- 1. **It gets done the same way every single time.** Even simple tasks can be done with  -->
<!-- numerous small modifications. You will probably remember some of those choices and  -->
<!-- settings and modifications the next time you need to do the same thing, but probably  -->
<!-- not all of them, and so those choices will not be exact from one time to the next.  -->
<!-- If the computer is doing it based on a clear set of instructions, it will be.  -->
<!-- 2. **It gets done more quickly.** Or if not more quickly (some large data might take  -->
<!-- some time to process), at least the spent time is the computers time, not yours. You  -->
<!-- can leave the computer to run the script while you get on with other things that  -->
<!-- a computer can't do.  -->
<!-- 3. **Anyone who does it can do it the same way.** Just as you might not do something -->
<!-- exactly the same way from one time to the next, one person in a laboratory group is  -->
<!-- likely to do things at least slightly different than other members of the group.  -->
<!-- Even with very detailed instructions, few instructions written for humans can be  -->
<!-- so detailed and precise to ensure that something is done exactly the same way by  -->
<!-- everyone who follows them. If everyone is given the same computer script to run,  -->
<!-- however, and they all instruct the computer to run that script, the task will be  -->
<!-- done in exactly the same way. -->
<!-- 4. **It is easier teach new people how to do the task.** Often, with a script to  -->
<!-- automate a task, you just need to teach someone new to the laboratory group how -->
<!-- to get the computer to run a script in a certain language. When you need them to  -->
<!-- run a new script, the process will be the same. The script encapsulates all the  -->
<!-- task-specific details, and so the user doesn't need to understand all of them to  -->
<!-- get something to run. What's more, once you want to teach a new lab member how  -->
<!-- everything it working, so they can understand the full process, the script provides -->
<!-- the exact recipe. You can teach them how to read and understand scripts in that -->
<!-- language, and then the scripts you've created to automate tasks serve as a recipe -->
<!-- book for everything going on in terms of data analysis for the lab. -->
<!-- 5. **You can create tools to share with others.** If you've written a script that's  -->
<!-- very useful, with a bit more work you can create it into a tool that you can share -->
<!-- with other research groups and perhaps publish a paper about. Papers about R  -->
<!-- software extensions (also called *packages*) and data analysis workflows and  -->
<!-- pipelines are becoming more and more common in biological contexts.  -->
<!-- 6. **It's more likely to be done correctly.** Boring, repetative tasks are easy -->
<!-- to mess up. We get so bored with them, that we shift our brains into a less-attentive -->
<!-- gear when we're working on them. This can lead to small, stupid mistakes, ones -->
<!-- at the level of typos but that, with data cleaning and analysis, can have much  -->
<!-- more serious ramifications. -->
<!-- > "We view workflows as a paradigm to: 1) expose non-experts to well-understood -->
<!-- end-to-end data analysis processes that have proven successful in challenging  -->
<!-- domains and represent the state-of-the-art, and 2) allow non-experts to easily  -->
<!-- experiment with different combinations of data analysis processes, represented -->
<!-- as workflows of computations that they can easily reconfigure and that the -->
<!-- underlying system can easily manage and execute." [hauder2011making] -->
<!-- > "While reuse [of workflows] by other expert scientists saves them time and  -->
<!-- effort, reuse by non-experts is an enabling matter as in practice they would  -->
<!-- not be able to carry out the analytical tasks without the help of  -->
<!-- workflows." [hauder2011making] -->
<!-- > "We observed that often steps that could be easily automated were performed manually  -->
<!-- in an error-prone fashion." [vidger2008supporting] -->
<!-- Biological research is quickly moving where a field where projects often required -->
<!-- only simple and straightforward data analysis once the experimental data was  -->
<!-- collected---with the raw data often published directly in a table in the manuscript---to  -->
<!-- a field with very complex and lengthy data analysis pipelines between the experiment -->
<!-- and the final manuscript. To ensure rigor and clarity in the final research results,  -->
<!-- as well as to allow others to reproduce the results exactly, the researcher must  -->
<!-- document all details of the computational data analysis, and this is often  -->
<!-- missing from papers. RMarkdown documents (and their analogues) can provide all these -->
<!-- details unambiguously---with RMarkdown documents, you can even run a command to  -->
<!-- pull out all the code used within the document, if you'd like to submit that  -->
<!-- code script as a stand-alone document as a supplement to a manuscript. -->
<!-- > "More recently, scientists who are not themselves computational experts are  -->
<!-- conducting data analysis with a wide range of modular software tools and packages.  -->
<!-- Users may often combine these tools in unusual or nove ways. In biology,  -->
<!-- scientists are now routinely able to acquire and explore data sets far beyond  -->
<!-- the scope of manual analysis, including billions of DNA bases, millions of genotypes, -->
<!-- and hundreds of thousands of RNA measurements. ... While propelling enormous  -->
<!-- progress, this increasing and sometimes 'indirect' use of computation poses -->
<!-- new challenges for scientific publication and replication. Large datasets are -->
<!-- often analyzed many times, with modifications to the methods and parameters, and -->
<!-- sometimes even updates of the data, until the final results are produced. The -->
<!-- resulting publication often gives only scant attendtion to the computations details. -->
<!-- Some papers have suggested these papers are 'merely the advertisement of  -->
<!-- scholarship whereas computer programs, input data, parameter values, etc., embody -->
<!-- the scholarship itself.' However, the actual code or software 'mashup' that -->
<!-- gave rise to the final analysis may be lost or unrecoverable." [mesirov2010accessible] -->
<!-- > "Bioinformatic analyses invariably involve shepherding files through a series  -->
<!-- of transformations, called a pipeline or workflow. Typically, these transformations -->
<!-- are done by third-part executable command line software written for Unix-compatible -->
<!-- operating systems. The advent of next-generation sequencing (NGS), in which millions -->
<!-- of short DNA sequences are used as the source input for interpreting a range of -->
<!-- biological phenomena, has intensified the need for robust pipelines. NGS analyses -->
<!-- tend to involve steps such as sequence alignment and genomic annotation that are -->
<!-- both time-intensive and parameter-heavy." [leipzig2017review] -->
<!-- > "**Rule 7: Always Store Raw Data behind Plots.** From the time a figure is first -->
<!-- generated to it being part of a published article, it is often modified several  -->
<!-- times. In some cases, such modifications are merely visual adjustments to -->
<!-- improve readability, or to ensure visual consistency between figures. If raw data -->
<!-- behind figures are stored in a systematic manner, so as to allow raw data for -->
<!-- a given figure to be easily retrieved, one can simply modify the plotting  -->
<!-- procedure, instead of having to redo the whole analysis. An additional  -->
<!-- advantage of this is that if one really wants to read fine values in a figure,  -->
<!-- one can consult the raw numbers. ... When plotting is performed using a  -->
<!-- command-based system like R, it is convenient to also store the code -->
<!-- used to make the plot. One can then apply slight modifications to these -->
<!-- commands, instead of having to specify the plot from scratch." [sandve2013ten] -->
<!-- ### Don't repeat your report-writing! -->
<!-- Until a few years ago, statisticians and data analysts frequently automated the -->
<!-- data cleaning, processing, and analysis tasks. But that still left the paper  -->
<!-- and report writing to be done by hand. This process is often repetitive. You would -->
<!-- do your analysis and create some tables or figures. You would save these from  -->
<!-- your statistical program and then paste them into your report or paper draft.  -->
<!-- If you decided that you needed to change your analysis a bit, or if you got a new -->
<!-- set of data to analyze in a similar way, you had to go back to the statistical -->
<!-- program, run things again there, save the tables and figure files again, and paste -->
<!-- them in the report or paper again to replace the outdated version. If there were -->
<!-- numbers from the analysis in the text of the paper, then you had to go back through -->
<!-- the text and update all of those with the newer numbers, too.  -->
<!-- Do you still write your papers and reports like this? I can tell you that there is  -->
<!-- now a *much* better way. Computer scientists and other programmers started thinking  -->
<!-- quite a while ago about how to create documents that combine computer code and  -->
<!-- text for humans, and to do it in a way where the computer code isn't just a static -->
<!-- copy of what someone once told the computer to do, but instead a living, working,  -->
<!-- *executable* set of instructions that the computer can run anytime you ask it to.  -->
<!-- These ideas first perculated with Donald Knuth, who many consider to be the greatest -->
<!-- computer programmer of all time [Bill Gates, for example, has told anyone who reads Dr. Knuth's  -->
<!-- magnum opus, *The Art of Computer Programming*, to come see him right away about a job]. -->
<!-- As Dr. Knuth was writing a book on computer programming, he became frustrated with the  -->
<!-- quality of the typesetting used in the final book. In a field that requires a lot of mathematical -->
<!-- and other symbols incorporated into the text, it takes a bit more to make an attractive -->
<!-- book than with simpler text. Dr. Knuth therefore took some time to create a programming -->
<!-- for typesetting. (You may have heard of it---if you ever notice that a journal's  -->
<!-- Instructions to Authors allow authors to submit articles in "LaTeX" or "TeX", that's -->
<!-- using a system built off of Donald Knuth's typesetting program.) -->
<!-- And *then*, once he had that typesetting program, he started thinking about how -->
<!-- programmers document their code. When one person does a very small code project, -->
<!-- and that one person is the only person who will ever go back to try to modify or -->
<!-- understand the code, that person might be able to get away with poor -->
<!-- documentation in the code. However, interesting code projects can become -->
<!-- enormous, with many collaborators, and it becomes impossible to understand and -->
<!-- improve the code if it doesn't include documentation explaining, in human terms, -->
<!-- what the code is doing at each step, as well as some overall documentation -->
<!-- explaining how different pieces of the code coordinate to get something big -->
<!-- done. -->
<!-- Traditionally, code was documented by including small comments within the code. These comments -->
<!-- are located near the code that they explain, and the order of the information in the  -->
<!-- code files are therefore dominated by the order of the instructions to the computer,  -->
<!-- not the order that you might explain what's going on to a human. To "read" the code and  -->
<!-- the documentation, you end up hopscotching through the code, following the code inside  -->
<!-- one function when it calls another function, for example, to where the code for that -->
<!-- second function is defined and then back to the first, and so on. You often follow paths as  -->
<!-- you get deeper and deeper into helper functions and the helper functions for those functions,  -->
<!-- that you feel like you're searching through a set of Russian dolls and then coming back up -->
<!-- to start on a new set of Russian dolls later down the line.  -->
<!-- Donald Knuth realized that, with a good typesetting program that could itself be programmed,  -->
<!-- you could write your code so that the documentation for humans took precedence, and could  -->
<!-- be presented in a very clear and attractive final document, rather than hard-to-read -->
<!-- computer code with some plain-text comments sprinkled in. Computers don't care what order  -->
<!-- the code is recorded in---as long as you give them some instructions on how to decipher -->
<!-- code in a certain format or order, they can figure out how to use it fine. But human brains -->
<!-- are a bit more finicky, and we need clear communication, laid out in a logical and helpful  -->
<!-- order. Donald Knuth created a paradigm of *literate programming* that interleaved  -->
<!-- executable code inside explanations written for humans; by making the code executable, it -->
<!-- meant that the document was a living guide. When someone changed the program, they did -->
<!-- it by changing the documentation---documentation wasn't left as the final, often neglected,  -->
<!-- step to refine once the "real code" was written (and the "real work" done). -->
<!-- > "Programs must be written for people to read, and only incidentally for machines  -->
<!-- to execute. A great program is a letter from current you to future you or  -->
<!-- the the person who inherits your code. A generous humanistic document." [ford2015what] -->
<!-- Well, this was a fantastic idea. It hasn't been universely leveraged, but the projects that -->
<!-- do leverage it are much stronger for it. But that's not where the story ends. If you are  -->
<!-- someone who does a little bit of coding (maybe small scripts to analyze and visualize your  -->
<!-- data, for example) and a lot of "documenting" of the results, and if you're not planning  -->
<!-- on doing a lot of large coding projects or creating software tools, it's not immediately  -->
<!-- how you'd use these literate programming ideas.  -->
<!-- Well, there are many people who do a little bit of programming in service to a larger -->
<!-- research project. While they are not creating software that needs classical software  -->
<!-- documentation, they do want to document the results that they get when they run their  -->
<!-- scripts, and they want to create reports and journal articles to share what they've  -->
<!-- found. Several people took the ideas behind literate programming---as it's used to  -->
<!-- document large software projects---and leveraged it to create tools to automate -->
<!-- writing in data-related fields.  -->
<!-- [F. Leisch?] was the first to do this with the R programming language, with a -->
<!-- tool called "Sweave" ("S"-"weave", as R builds off of another programming -->
<!-- language called "S" and Leisch's program would "weave" together S / R code and -->
<!-- writing). This used Donald Knuth's typesetting program. It allowed you to write -->
<!-- a document for humans (like a report or journal article) and to intersperse bits -->
<!-- of code in the paper. You'd put each code piece in the spot in the paper where -->
<!-- the text described what was going on or where you wanted the results that it -->
<!-- generated for example, if you had a section in the Methods where you talked -->
<!-- about removing observations that were outliers, you would add in the code that -->
<!-- took out those outliers right there in the paper. And if you had a placed in the -->
<!-- Results that talked about how your data were different between two experimental -->
<!-- groups, you would add the code that generated the plot to show that right there -->
<!-- in the paper. -->
<!-- To tell the computer how to tell between code and writing, you would add a little -->
<!-- weird combination of text each time that you wanted to "switch" into code and then  -->
<!-- another one each time you wanted to switch back into writing for humans. (These -->
<!-- combinations were so weird because that guaranteed that it was a combination you  -->
<!-- would probably never want to type otherwise, so you wouldn't have a lot of  -->
<!-- cases of the computer getting confused between whether the combo meant to switch -->
<!-- to code or whether it was just something that came up in the regular writing.)  -->
<!-- You'd send the document, code and writing and all, through R once you had it  -->
<!-- written up. R would ignore everything that wasn't code. When it got to the code  -->
<!-- pieces, it would run them, and if the code created output (like a figure or table),  -->
<!-- it would "write" that into the document at that point in the text. Then you'd run  -->
<!-- the document through Donald Knuth's typesetting program (or an extension of it),  -->
<!-- and the whole document would get typeset into an attractive final product (often  -->
<!-- a pdf, although you had some choices on the type of output). -->
<!-- This meant that you got very attractive final documents. It also meant that your -->
<!-- data analysis code was well documented---it was "documented" by the very article -->
<!-- or report you wrote based on it, because the code was embedded right there in the  -->
<!-- final product! It also meant that you could save a lot of time if you needed to  -->
<!-- go back and change some of your code later (or input a different or modified dataset). -->
<!-- You just had to change that small piece of code or data input, and then essentially  -->
<!-- press a button to put everything together again, and the computer would re-write the -->
<!-- whole report for you, with every figure and table updated. It even let you write  -->
<!-- small bits of computer code directly into the written text, in case you need to  -->
<!-- write something like "this study included 52 subjects", where the "52" came from  -->
<!-- you counting up the number of rows in one of your datasets---if you later added three -->
<!-- more subjects and re-ran the analysis with the updated dataset, the report would -->
<!-- automatically change to read "this study included 55 subjects".  -->
<!-- Leisch's system is still out there, but another has been adopted much more -->
<!-- widely, building on it. Yihui Xie started work on a program that tweaked and -->
<!-- improved Leisch's Sweave program, creating something called "knitr" -->
<!-- ("knit"-"R"---are you noticing a pattern in the names?). Xie's knitr program, -->
<!-- along with its extensions, is now widely used for data analysis projects. What's -->
<!-- more, it's grown to allow for larger or more diverse writing projects---this -->
<!-- book, for example, is written using an extension called "bookdown", and -->
<!-- extensions also exist for create blogs that include executable R code -->
<!-- ("blogdown") and websites with documentation for R packages ("packagedown"). -->
<!-- So now, let's put these two pieces together. We know that programmers love to  -->
<!-- automate small tasks, and we know that there are tools that can be used to  -->
<!-- "program" tasks that involve writing and reporting. So what does this mean if  -->
<!-- you frequently need to write reports that follow a similar pattern and start  -->
<!-- from similar types of data? If you are thinking like a code, it means that  -->
<!-- you can move towards automating the writing of those reports.  -->
<!-- One of us was once talking to someone who works in a data analysis-heavy field, -->
<!-- and she was talking about how much time she spends copying the figures that her -->
<!-- team creates, based on a similar analysis of new data that's regularly -->
<!-- generated, into PowerPoint presentations. So, for this weeks report, she's -->
<!-- creating a presentation that shows the same analysis she showed last week, just -->
<!-- with newer data. Cutting and pasting is an enormous waste of time---there are -->
<!-- tools to automate this.  -->
<!-- ### Automating reports -->
<!-- First---think through the types of written reports or presentations you've -->
<!-- created in the past year or two. Are there any that follow a similar pattern? -->
<!-- Any that input the same types of data, but from different experiments, and then -->
<!-- report the same types of statistics or plots for them? Are there Excel -->
<!-- spreadsheets your lab uses that generate specific tables or plots that you often -->
<!-- cut and paste for reports or presentations? Look through your computer file -->
<!-- folders or email attachments if you need to---many of these might be small -->
<!-- regular reports that are so regular that they don't pop right to mind. If you -->
<!-- are creating documents that match any of these conditions, you probably have -->
<!-- something ripe for converting to a reusable, automatable template. -->
<!-- > "Think like Henry Ford; he saw that building cars was a repeatable process and  -->
<!-- came up with the moving assembly line method, revolutionizing production. You  -->
<!-- may not be building a physical product, but chances are you are producing something. -->
<!-- ... Look for the steps that are nearly identical each time, so you can build your -->
<!-- own assembly line." [rose2018dont] -->
<!-- ... [Creating a framework for the report] -->
<!-- > "Odds are, if you're doing any kind of programming, especially Web programming, -->
<!-- you've adopted a framework. Whereas an SDK is an expression of a corporate  -->
<!-- philosophy, a framework is more like a product pitch. Want to save time? Tired  -->
<!-- of writing old code? Curious about the next new thing? You use a graphics  -->
<!-- framework to build graphical applications, a Web framework to build Web -->
<!-- applications, a network framework to build network servers. There are  -->
<!-- hundreds of frameworks out there; just about every language has one.  -->
<!-- A popular Web framework is Django, which is used for coding in Python. -->
<!-- Instagram was bootstrapped on it. When you sit down for the first time -->
<!-- with Django, you run the command 'startproject', and it makes a directory -->
<!-- with some files and configuration inside. This is your project directory. Now -->
<!-- you have access to libraries and services that add to and enhance the standard -->
<!-- library." [ford2015what] -->
<!-- One key advantage of creating a report template is that it optimizes the time of -->
<!-- statistical collaborators. It is reasonable for a scientists with a couple of -->
<!-- courses worth of statistical training to design and choose the statistical tools -->
<!-- for simple and straightforward data analysis. However, especially as the -->
<!-- biological data collected in experiments expands in complexity and size, a -->
<!-- statistician can recommend techniques and approaches to draw more knowledge -->
<!-- from the data and to appropriately handle non-standard features of the data. -->
<!-- There is substantial work involved in the design of any data analysis pipeline -->
<!-- that goes beyond the very basics. It waste time and resources to recreate this -->
<!-- with each new project, time that---in the case of statistical collaborators---could  -->
<!-- probably be better spent in extending data analysis goals beyond the simplest  -->
<!-- possible analysis to explore new hypotheses or to add exploratory analysis that  -->
<!-- could inform the design of future experiments. -->
<!-- > "Workflows effectively capture valuable expertise, as they represent how an  -->
<!-- expert has designed computational steps and combined them into an  -->
<!-- end-to-end process." [hauder2011making] -->
<!-- When collaborative work between scientists and statisticians can move towards -->
<!-- developing repeatable data analysis scripts and report templates, you will start -->
<!-- to think more about common patterns and common questions that you ask across -->
<!-- many experiments in your research program, rather than focusing on the immediate -->
<!-- needs for a specific project. You can start to think of the data analysis tools -->
<!-- that are general purpose for your research lab, develop those into clean, -->
<!-- well-running scripts or functions, and then start thinking about more -->
<!-- sophisticated questions you want to ask of your data. The statisticians you -->
<!-- collaborate will be able to see patterns across your work and help to develop -->
<!-- global, and perhaps novel, methods to apply within your research program, rather -->
<!-- than piecemeal small solutions to small problems. -->
<!-- > "Although foundational knowledge is taught in major universities and colleges,  -->
<!-- advanced data analytics can only be acquired through hands-on practical training. -->
<!-- Only exposure to real-world datasets allows students to learn the importance of  -->
<!-- preparing and cleansing the data, designing appropriate features, and  -->
<!-- formulating the data mining task so that the data reveals phenomena of interest.  -->
<!-- However, the effort required to implement such complex multi-step data analysis -->
<!-- systems and experiment with the tradeoffs of different algorithms and feature  -->
<!-- choices is daunting. For most practical domains, it can take weeks to months for -->
<!-- a student to setup the basic infrastructure, and only those who have access to  -->
<!-- experts to point them to the right high-level design choices will endeavor on  -->
<!-- this type of learning. As a result, acquiring practical data analytics skills  -->
<!-- is out of reach for many students and professionals, posing severe limitations  -->
<!-- to our ability as a society to take advantage of our vast digital data resources." -->
<!-- [hauder2011making] -->
<!-- > "In practice designing an appropriate end-to-end process to prepare and analyze the -->
<!-- data plays a much more influential role than using a novel classifier or  -->
<!-- statistical model." [hauder2011making] -->
<!-- It is neither quick nor simple to design the data analysis plan and framework -->
<!-- for a research experiment. It is not simply naming a statistical test or two. -->
<!-- Instead, the data analyst must start by making sure they understand the data, -->
<!-- how it was measured, how to decipher the format in which it's stored, what -->
<!-- questions the project is hoping to answer, where there might be problems in the -->
<!-- data (and what they would look like), and so on. If a data analyst is helping -->
<!-- with a lot of projects using similar types of data to answer similar questions, -->
<!-- then he or she should, in theory, need less time for these "framework" types of -->
<!-- questions and understanding. However, if data isn't shared in the same format -->
<!-- each time, it will still take overhead to figure out that this is indeed the -->
<!-- same type of data and that code from a previous project can be adapted or -->
<!-- repurposed. -->
<!-- Let's think about one area where you likely repeat very similar steps -->
<!-- frequently---writing up short reports or slide presentations to share your -->
<!-- to-date research results with your research group or colleagues. These  -->
<!-- probably often follow a similar structure. For example, they may start with  -->
<!-- a section describing the experimental conditions, and then have a slide  -->
<!-- showing a table with the raw data (or a simple summary of it, if there's a lot -->
<!-- of data), and then have a figure showing something like the difference in  -->
<!-- experimental measurements between to experimental groups.  -->
<!-- [Figure: Three simple slides for a research update---experimental conditions,  -->
<!-- table of raw data, boxplots with differences between groups.] -->
<!-- > "The cornerstone of using DRY in your work life is the humble template.  -->
<!-- Whenever you create something, whether it's an email, a business document,  -->
<!-- or an infographic, think if there's something there you could save for -->
<!-- future use. The time spend creating a template will save you exponentially  -->
<!-- more time down the road." [rose2018dont] -->
<!-- You could start very simply in turning this into a template. You could start by -->
<!-- creating a PowerPoint document called "lab_report_template.pptx". It could -->
<!-- include three slides, with the titles on each slide of "Experimental -->
<!-- conditions", "Raw data", and "Bacterial burden by group", and maybe with some -->
<!-- template set to provide general formatting that you like (font, background -->
<!-- color, etc.). That's it. When you need to write a new report, you copy this file,  -->
<!-- rename the copy, and open it up. Now instead of needing to start from a blank -->
<!-- PowerPoint file, you've shaved off those first few steps of setting up the  -->
<!-- pieces of the file you always use. -->
<!-- [Figure: Simplest possible template] -->
<!-- This very simple template won't save you much time---maybe just a minute or so for -->
<!-- each report. However, once you can identify other elements that you commonly use -->
<!-- in that type of report, you can add more and more of these "common elements" to the -->
<!-- template, so that you spend less time repeating yourself with each report. For  -->
<!-- example, say that you always report the raw data using the same number of columns  -->
<!-- and the same names for those columns. You could add a table to that slide in your -->
<!-- template, with the columns set with appropriate column names. You can always add or -->
<!-- delete rows in the table if you need to in your reports, but now each time you  -->
<!-- create a new report, you save yourself the time it takes to create the table  -->
<!-- structure and add the column names. Plus, now you've guaranteed that the first -->
<!-- table will use the exact same column names every time you give a report! You'll never -->
<!-- have to worry about someone wondering if you are using a different model animal -->
<!-- because you have a column named "Animal ID" in one report, while your last report -->
<!-- had "Mouse ID", for example. And because you're making a tool that you'll use many -->
<!-- times, it becomes worthwhile to take some time double-checking the clean-up, so  -->
<!-- you're more likely to avoid things like typos in the slide titles or in columns names -->
<!-- of tables.  -->
<!-- [Figure: Template with a table skeleton added.] -->
<!-- You can do the same thing for written reports or paper manuscripts. For example,  -->
<!-- most of the papers you like may have the classic scientific paper sections: "Introduction",  -->
<!-- "Data and Methods", "Results", and "Discussion". And then, you probably typically include -->
<!-- a couple of pages at the beginning for the title page and abstract, and then a section -->
<!-- at the end with references and figure captions. Again, you could create a file called -->
<!-- "article_template.docx" with section headings for each of the sections and with space for -->
<!-- the title page, abstract, and references. Presumably, you are always an author on papers you're  -->
<!-- writing, so go ahead and add your name, contact information, and affiliation in the right -->
<!-- place on the title page (I bet you have to take the time to do that every time you start -->
<!-- a paper---and if you're like me, you have to look up the fax number for your building  -->
<!-- every time you do). You probably need to mention funding sources on the title page for  -->
<!-- every paper, too. Do you need to look those grant numbers up every time? Nope! Just  -->
<!-- put all your current ones in the title page of your template, and then you can just  -->
<!-- delete those that don't apply when you start a new paper. -->
<!-- [Figure: Simple article template] -->
<!-- Again, you can build on this simple template. Look through the "Data and Methods"  -->
<!-- section of several of your recent papers. Are there certain elements that you commonly  -->
<!-- report there? For example, is there a mouse model you use in most of your experiments,  -->
<!-- that you need to describe? Put it in the template. Again, you can always delete or  -->
<!-- modify this information if it doesn't apply to a specific paper. But for any information -->
<!-- that you find yourself copying and pasting from one paper draft to another, add it to  -->
<!-- your template. It is so much more delightful to start work on a paper by *deleting* the -->
<!-- details that don't apply than by staring down a blank sheet of paper.  -->
<!-- [Quote---Taking away everything that isn't the statue.] -->
<!-- > "Most docs you work on will have some sort of repeatable process. For example,  -->
<!-- when I sit down to write a blog post, I go through the same repeatable steps when -->
<!-- setting up my file: Title, Subtitle, Focus Keywords, Links to relevant articles /  -->
<!-- inspiration, Outline of subheds, Intro / hook, etc. ... Even though it is a  -->
<!-- well-worn process, I can save time by creating a writing template with these -->
<!-- sections already pre-set. Not only does this save time, but it also saves mental -->
<!-- energy and helps push me into 'Writing' mode instead of 'Set-up' or 'Research' mode." -->
<!-- [mackay2019dry] -->
<!-- This template idea is so basic, and yet far fewer people use it than would seem -->
<!-- to make sense. Maybe it's because it does require some forward thinking, about -->
<!-- the elements of presentations, reports, and papers that are common across your -->
<!-- body of work, not just the details that are pertinent to a specific project. It -->
<!-- also does require some time investment, but not much more that adding all these -->
<!-- element to a single paper or presentation takes. If you can see the appeal of -->
<!-- having a template for the communication output that you create from your -->
<!-- research, and if you try it an like it, then you are well on your way to having -->
<!-- a programmers mindset. The joy of programming is exactly this kind of joy---a -->
<!-- little thinking and time at the start and you have these little tools that do -->
<!-- some of your work for you over and over again. In fact, a Python programmer has -->
<!-- even written a book whose title captures this intrinsic *esprit*: "[Automating -->
<!-- the Boring Stuff?]. -->
<!-- But wait. There's more. Do you always do the same calculations or statistical -->
<!-- tests with the data you're getting in? Or at least often enough that it would -->
<!-- save time to have a template? There is a way to add this into the template that -->
<!-- you create for your presentation, report, or paper. -->
<!-- > "Your templates are living documents. If you notice that you're making the same -->
<!-- change over and over, that means it's time to update the template itself." -->
<!-- [rose2018dont] -->
<!-- Researchers create and use Excel templates for this purpose. The template -->
<!-- may have macros embedded in it to make calculations or create basic graphs.  -->
<!-- However, spreadsheets---whether created from templates or not---share -->
<!-- the limitations discussed in an earlier chapter. What's more, they can't  -->
<!-- easily be worked into a template that creates a final document to  -->
<!-- communicate results, whether that's a slide presentation or a  -->
<!-- a written document. Finally, they are in a binary format that can't  -->
<!-- clearly be tracked with version control like git.  -->
<!-- [R Project templates? Can you create them? Clearly something like that is  -->
<!-- going on when you start a new package...] -->
<!-- ### Scripts and automated reports as simple pipelines -->
<!-- Scientific workflows or pipelines have become very popular in many biological  -->
<!-- research areas. These are meant to meet many of the DRY goals---create a  -->
<!-- recipe that can be repeated at different times and by different research groups,  -->
<!-- clearly record each step of an analysis, and automate steps or processes that -->
<!-- are repeated across different research projects so they can be completed  -->
<!-- more efficiently.  -->
<!-- There are very sophisticated tools now available for creating biological data -->
<!-- analysis pipelines and workflows,[leipzig2017review] including tools like Galaxy -->
<!-- and Taverna. Simple code scripts and tools that build on them (like makefiles, -->
<!-- RMarkdown documents, and Jupyter Notebooks), however, can be thought of as the -->
<!-- simpler (and arguably much more customizable) little sibling of these more -->
<!-- sophisticated tools. -->
<!-- > "Scripts, written in Unix shell or other scripting languages such as Perl, can -->
<!-- be seen as the most basic form of pipeline framework." [leipzig2017review] -->
<!-- > "Naive methods such as shell scripts or batch files can be used to describe -->
<!-- scientific workflows." [mishima2011agile] -->
<!-- Flexibility can be incorporated into scripts, and the tools that build directly off -->
<!-- them, through including *variables*, which can be set in different configurations  -->
<!-- each time the script is run [leipzig2017review]. -->
<!-- More complex pipeline systems do have some advantages (although generalizable tools -->
<!-- that can be applied to scripts are quickly catching up on most of these). For  -->
<!-- example, many complex data analysis or processing steps may use open-source  -->
<!-- software that is under continuing development. If the creators of that software -->
<!-- modify it between the time that you submit your first version of an article and  -->
<!-- the time that you need to submit revisions, and you have updated the version  -->
<!-- of the package on your computer, the code may no longer run the same way. The same -->
<!-- thing can happen if someone else tries to run your code---if they are trying to  -->
<!-- run it with a more recent version of some of the open-source software used in the -->
<!-- code, they may run into problems. -->
<!-- This problem of changes in *dependencies* of the code (software programs, packages,  -->
<!-- or extensions that the code loads as runs as part of its process) is an important  -->
<!-- challenge to reproducibility in many areas of science. Pipeline software can improve -->
<!-- on simpler scripts by helping limit dependency problems [by ...]. However,  -->
<!-- R extensions are rapidly being developed that also address this issue. For example,  -->
<!-- the `packrat` package ...., while [packrat update Nichole was talking about]. -->
<!-- > "Dependencies refer to upstream files (or tasks) that downstream transformation -->
<!-- steps require as input. When a dependency is updated, associated downstream files -->
<!-- should be updated as well." [leipzig2017review] -->
<!-- The tools that we've discussed for reproducable and automatable report writing---like -->
<!-- Rmarkdown and Jupyter Notebooks---build off of a tool for coordinating and  -->
<!-- conducting a process involving multiple scripts and input files, or a "build tool".  -->
<!-- Among computer programmers, perhaps the most popular build tool is called "make".  -->
<!-- This tool allows coders to write a "Makefile" that details the order that scripts  -->
<!-- should be run in a big process, and what other scripts and inputs they require.  -->
<!-- With these files, you can re-run a whole project, and do it in the right order,  -->
<!-- and the only steps that will be re-run are those where something will change based -->
<!-- on whatever change you just made to the code or input data.  -->
<!-- > "To avoid errors and inefficiencies from repeating commands manually, we recommend -->
<!-- that scientists use a build tool to automate workflows, e.g., specify the ways in  -->
<!-- which intermediate data files and final results depend on each other, and on the programs -->
<!-- that create them, so that a single command will regenerate anything that needs to  -->
<!-- be regenerated." [wilson2014best] -->
<!-- For example, say that you have a large project that starts by inputing data, cleans -->
<!-- or processes it using a step that takes a lot of time to run, analyzes the simpler  -->
<!-- processed data, and then creates some plots and tables based on this analysis. With  -->
<!-- a makefile, if you want to change the color of the labels on a plot, you can change that -->
<!-- code and re-run the Makefile, and the computer will re-make the plots, but not re-run -->
<!-- the time-intensive early data processing steps. However, if you update the raw data -->
<!-- for the project and re-run the Makefile, the computer will (correctly) run everything -->
<!-- from the very beginning, since the updated data needs to be reprocessed, all the way -->
<!-- through to creating the final plots and tables.  -->
<!-- > "A file containing commands for an interactive system is often called a script, though -->
<!-- there is really no difference between this and a program. When these scripts are  -->
<!-- repeatedly used in the same way, or in combination, a workflow management tool can  -->
<!-- be used. The most widely used tool for this task is probably Make, although many  -->
<!-- alternatives are now available. All of these allow people to express the dependencies -->
<!-- between files, i.e., to say that if A or B has changed, then C needs to be updated -->
<!-- using a specific set of commands. These tools have been successfully adopted for  -->
<!-- scientific workflows as well." [wilson2014best] -->
<!-- > "This experience motivated the creation of a way to encapsulate all aspects of  -->
<!-- our in silico analyses in a manner that would facilitate independent replication -->
<!-- by another scientist. Computer and computational scientists refer to this goal as -->
<!-- 'reproducible research', a coinage attributed to the geophysicist Jon Claerbout in 1990, -->
<!-- who imposed the standard of makefiles for construction of all the filgures and computational  -->
<!-- results in papers published by the Stanford Exploration Project. Since that time, other -->
<!-- approaches have been proposed, including the ability to insert active scripts -->
<!-- within a text document and the use of a markup language that can produce -->
<!-- all of the text, figures, code, algorithms, and settings used for the computational -->
<!-- research. Although these approaches may accomplish the goal, they are not practical -->
<!-- for many nonprogramming experimental scientists using other groups' or commercial  -->
<!-- software tools today." [mesirov2010accessible] -->
<!-- > "All science campaigns of sufficient complexity consist of numerous interconnected computational -->
<!-- tasks. A workflow in this context is the composition of several such computing -->
<!-- tasks." [deelman2018future] -->
<!-- > "Scientific applications can be very complex as software artifacts. They may contain -->
<!-- a diverse amalgam of legacy codes, compute-intensive parallel codes, data conversion routines, and remote  -->
<!-- data extraction and preparation. These individual codes are often stitched  -->
<!-- together using scripted languages that specify the data and software to be -->
<!-- executed, and orchestrate the allocation of computing resources and the  -->
<!-- movement of data across locations. To manage a particular set of codes, a number -->
<!-- of interdependent scripts may be used." [gil2008data] -->
<!-- [Disadvantages of more complex pipeline tools over starting from scripts] -->
<!-- > "Unlike command line-based pipeline frameworks ... workbenches allow  -->
<!-- end-users, typically scientists, to design analyses by linking preconfigured  -->
<!-- modular tools together, typically using a drag-and-drop graphical interface.  -->
<!-- Because they require exacting specifications of inputs and outputs,  -->
<!-- workbenches are intrinsically a subset of configuration-based pipelines." -->
<!-- [leipzip2017review] -->
<!-- > "Magnificent! Wonderful! So, what's the downside? Well, frameworks lock you -->
<!-- into a way of thinking. You can look at a website and, with a trained eye, go,  -->
<!-- 'Oh, that's a Ruby on Rails site.' Frameworks have an obvious influence on  -->
<!-- the kind of work developers can do. Some people feel that frameworks make things -->
<!-- too easy and that they become a crutch. It's pretty easy to code yourself into -->
<!-- a hole, to find yourself trying to force the framework to do something it  -->
<!-- doesn't want to do. Django, for example, isn't the right tool for building a giant -->
<!-- chat application, nor would you want to try competing with Google Docs using  -->
<!-- a Django backend. You pay a price in speed and control for all that convenience.  -->
<!-- The problem is really in knowing how much speed, control, and convenience you need." -->
<!-- [ford2015what] -->
<!-- > "Workbenches and class-based frameworks can be considered heavyweight. There -->
<!-- are costs in terms of flexibility and ease of development associated with making -->
<!-- a pipeline accessible or fast. Integrating new tools into workbenches clearly -->
<!-- increases their audience but, ironically, the developers who are most capable of -->
<!-- developing plug-ins for workbenches are the least likely to use them." -->
<!-- [leipzip2017review] -->
<!-- > "Business workflow management systems emerged in the 1990's and are well accepted  -->
<!-- in the business community. Scientific workflows differ from business workflows in that -->
<!-- rather than coordinating activities between individuals and systems, scientific -->
<!-- workflows coordinate data processing activities." [vigder2008supporting] -->
<!-- > "The concept of workflows has traditionally been used in the areas of process  -->
<!-- modelling and coordination in industries. Now the concept is being applied to -->
<!-- the computational process including the scientific domain." [mishima2011agile] -->
<!-- > "Although bioinformatics-specific pipelinessuch as bcbio-nextgen and Omics Pipe -->
<!-- offer high performance automated analysis, they are not frameworks in the sense they -->
<!-- are not easily extensible to integrate new user-defined tools." [leipzig2017review] -->
<!-- Writing a script-based pipeline does require that you or someone in your laboratory  -->
<!-- group develops some expertise in writing code in a "scripting language" like R or  -->
<!-- Python. However, the barriers to entry for these languages continues to come down, and -->
<!-- with tools that leverage the ideas of templating and literate programming, it is  -->
<!-- becoming easier and easier for new R or Python users to learn to use them quickly. -->
<!-- For example, one of us teaches a three-credit R Programming class, designed for researchers -->
<!-- who have never coded. The students in the class are regularly creating code projects -->
<!-- by the end of the class that integrate literate programming tools to weave together  -->
<!-- code and text and saving these documents within code project directories that include -->
<!-- raw data, processed data, and scripts with code definitions for commonly used pieces of  -->
<!-- code (saved as functions). These are all the skills you'd need to craft an R project -->
<!-- template for your research group that can serve as a starting point for each future -->
<!-- experiment or project. -->
<!-- > "Without an easy-to-use graphical editor, developing workflows requires some programming -->
<!-- knowledge." [vigder2008supporting] -->
<!-- > "Scripting languages are programming languages and as a result are inaccessible to  -->
<!-- any scientists without computing background. Given that a major aspect of scientific  -->
<!-- research is the assembly of scientific processes, the fact that scientists cannot  -->
<!-- assemble or modify the applications themselves results in a significant bottleneck." -->
<!-- [gil2008data] -->
<!-- > "As anyone who's ever shared a networked folder---or organized a physical -->
<!-- filing cabinent---knows, without a good shared filing system your office will -->
<!-- implode." [ford2015code] -->
<!-- > "You can tell how well code is organized from across the room. Or by squinting or  -->
<!-- zooming out. The shape of code from 20 feet away is incrediably informative. Clean -->
<!-- code is idiomatic, as brief as possible, obvious even if it's not heavily  -->
<!-- documented. Colloquial and friendly." [ford2015code] -->
<!-- > "[Wesley Clark] wanted to make the world's first 'personal computer', one that -->
<!-- could fit in a single office or laboratory room. No more waiting in line; one -->
<!-- scientist would have it all to himself (or, more rarely, herself). Clark wanted -->
<!-- specifically to target biologists, since he knew they often needed to crunch -->
<!-- data in the middle of an experiment. At that time, if they were using a huge IBM -->
<!-- machine, they'd need to stop and wait their turn. If they had a personal -->
<!-- computer in their own lab? They could do calculations on the fly, rejiggering -->
<!-- their experiment as they went. It would even have its own keyboard and screen, -->
<!-- so you could program more quickly: no clumsy punch cards or printouts. It would -->
<!-- be a symbiosis of human and machine intelligence. Or, as Wilkes put it, you'd -->
<!-- have 'conversational access' to the LINC: You type some code, you see the result -->
<!-- quickly. Clark knew he and his team could design the hardware. But he needed -->
<!-- Wilkes to help create the computers' operating system that would let the user -->
<!-- control the hardware in real time. And it would have to be simple enough that -->
<!-- biologists could pick it up with a day or two of training." [Coders, p. 32] -->
<!-- > "When they had a rough first prototype [of the LINC] working, Clark tested -->
<!-- it on a real-life problem of biological research. He and his colleague  -->
<!-- Charles Molnar dragged a LINC out to the lab of neurologist Arnold Starr, who -->
<!-- had been trying and failing to record the neuroelectric signals cats produce -->
<!-- in their brains when they heard a sound. Starr had put an electrode implant -->
<!-- into a cat's cortex, but he couldn't distinguish the precise neuroelectirc -->
<!-- signal he was looking for. In a few hours, Molnar wrote a program for the  -->
<!-- LINC that would play a clicking noise out of a speaker, record precisely  -->
<!-- when the electrode fired, and map on the LINC's screen the average response -->
<!-- of the cat to noises. It worked: As data scrolled across the screen, the -->
<!-- scientists 'danced a jig right around the equipment'." [Coders, p. 33] -->
<!-- If you have built a pipeline as an R or Python script, but there is an open source software -->
<!-- tool that you need to use that is written in another language, you can right a "wrapper"  -->
<!-- function that calls that software from within the R or Python process. And chances are good,  -->
<!-- if that software is a popular tool, that someone else has already written one, so you can  -->
<!-- just leverage that code or tool. Open-source scripting languages and R and Python "play  -->
<!-- well with others", and can communiciate and run just about anything that you could run at -->
<!-- a command line. -->
<!-- > "Our approach to dealing with software integration is to wrap applications with Python -->
<!-- wrappers." [vigder2008supporting] -->
<!-- The templating process can eventually extend to making small tools as software functions -->
<!-- and extensions. For example, if you regularly create a certain type of graph to show  -->
<!-- your results, you could write a small function in R that encapsulates the common code -->
<!-- for creating that. One research group I know of wanted to make sure their figures all  -->
<!-- had a similar style (font, color for points, etc.), but didn't like the default values,  -->
<!-- and so wrote a small function that applied their style choices to every plot they made.  -->
<!-- Once your research group has a collection of these small functions, you can in turn  -->
<!-- encapsulate them in a R package (which is really just a collection of R functions, plus -->
<!-- maybe some data and documentation). This package doesn't have to be shared outside your -->
<!-- research group---you can just share it internally, but then everyone can load and use  -->
<!-- it in their computational work. With the rise of larger datasets in many fields, and the  -->
<!-- accompanying need to do more and more work on the computer to clean, manage, and  -->
<!-- analyze the data, more scientists are getting into this mindset that they are not just -->
<!-- the "end users" of software tools, but they can dig in and become artisans of small tools -->
<!-- themselves, building on the larger structure and heavier lifting made available by the  -->
<!-- base software package.  -->
<!-- > "End-user software engineering refers to research dedicated to improving the  -->
<!-- capability of end-users who need to perform programming or engineering tasks. For many,  -->
<!-- if not all, of these end-users, the creation and maintenance of software is a -->
<!-- secondary activity performed only in service of their real work. This scenario -->
<!-- applies to many fields include science. However, there is little research specifically -->
<!-- focused on scientists as end-user software engineers." [vigder2008supporting] -->
<!-- John Chambers (one of the creators of R's precursor S, and heavily involved in R  -->
<!-- deveopment) defines programming as "a language and environment to turn ideas into new -->
<!-- tools." [Programming with Data, p. 2] -->
<!-- > "Sometimes, it seems that the software we use just sort of sprang into -->
<!-- existance, like grass growing on the lawn. But it didn't. It was created by -->
<!-- someone who wrote out---in code---a long, painstaking set of instructions -->
<!-- telling the computer precisely what to do, step-by-step, to get a job done. -->
<!-- There's a sort of priestly class mystery cultivated around the word *algorithm*, -->
<!-- but all they consist of are instructions: Do this, then do this, then do this. -->
<!-- News Feed [in Facebook] is now an extraordinarily *complicated* algorithm -->
<!-- involving some trained machine learning; but it's ultimately still just a list -->
<!-- of rules." [Coders, p. 10] -->
<!-- > "One of your nonnegotiable rules should be that every person in the lab must -->
<!-- keep a clear and detailed laboratory notebook. The business of the lab is  -->
<!-- results and the communication of those results, and the lab notebook is the  -->
<!-- all-important documentation of each person's research. There are dozens of  -->
<!-- reasons to keep a clear and detailed lab notebook and only one---laziness---for -->
<!-- not. Whether the work is on an esoteric branch of clam biology or is heading  -->
<!-- toward a potentially lucrative patent, it makes sense to keep data clear and retrievable for both present and future lab members." [@leips2010helm] -->
<!-- > "Paper lab notebooks are most commonly seen, valued for their versatility,  -->
<!-- low expense, and ease of use. The paper notebook type may be determined -->
<!-- by the department or institution. Especially at latge companies, there may  -->
<!-- also be a policy that dictates format, daily signatures by supervisors, and -->
<!-- lock-up at night. If there is no requirement, everyone in your lab should -->
<!-- use the same kind of bound lab notebook, as determined by you. It should have -->
<!-- numbered pages, gridlines, and a tough enough binding that it does not fall  -->
<!-- apart after a few months of rigorous use on the bench." [@leips2010helm] -->
<!-- > "Electronic lab notebooks (ELNs) may be used to enter, store, and analyze -->
<!-- data. Coupled with a sturdy notebook computer, they can be used at the bench -->
<!-- for notes and alterations to the protocol. Lab members and collaborators can  -->
<!-- share data and drawings. Reagents can be organized. Shareware versions are  -->
<!-- available as well as many stand-alone programs that can be tweaked for your -->
<!-- needs by the company. A lab that generates high-throughput, automated, or  -->
<!-- visual data; collaborates with other labs; or has a high personnel turnover -->
<!-- should consider using an ELN (Phillips 2006)." [@leips2010helm] -->
<!-- > "Laboratory Information Management Systems (LIMSs) are programs that are -->
<!-- coupled to a database as well as to lab equipment and facilitate the entry  -->
<!-- and storage of laboratory data. These systems are expensive and are designed -->
<!-- for more large-scale testing and production than for basic research labs. They -->
<!-- can be very useful tools: Some can manage protocols, schedule maintenance of  -->
<!-- lab instruments, receive and process data from multiple instruments, track -->
<!-- reagents and samples, print sample labels, do statistical analyses, and be -->
<!-- customized to your needs. But although LIMSs can handle a great deal of  -->
<!-- data analysis, they cannot yet substitute for a lab notebook." [@leips2010helm] -->
<!-- > "You should demand a level of care with lab notebooks. Everything in it should -->
<!-- be understandable not only to the owner, but to you." [@leips2010helm] -->
<!-- > "Whether you check notebooks, or have a lab member present to you with  -->
<!-- raw data, or stop at everyone's lab bench a few times a week, you must -->
<!-- have a feeling for the quality and results of each person's raw data. It is -->
<!-- very easy to make assumptions on the basis of the polished data you  -->
<!-- see at a research meeting, but many a lab member has gone astray with over- -->
<!-- or misinterpreted data. By keeping an eye on the raw data, you can be  -->
<!-- ready to comment on the number of repetitions, alternative experiments,  -->
<!-- or the implications of a minor result." [@leips2010helm] -->
<!-- ### Applied exercise -->

</div>
</div>
<div id="module5" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> Example: Creating a template for “tidy” data collection</h2>
<p>We will walk through an example of creating a template to collect data in a
“tidy” format for a laboratory-based research project, based on a research
project on drug efficacy in murine tuberculosis models. We will show the initial
“untidy” format for data recording and show how we converted it to a “tidy”
format. Finally, we will show how the data can then easily be analyzed and
visualized using reproducible tools.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Understand how the principles of “tidy” data can be applied for a real, complex research project;</li>
<li>List advantages of the “tidy” data format for the example project</li>
</ul>
<p>In the last module, we covered three principles for designing tidy templates for
data collection in a biomedical laboratory, motivated by an example dataset from
a real experiment. In this module, we’ll show you how to apply those principles
to create a tidier template for the example dataset from the last module.
As a reminder, those three principles are:</p>
<ol style="list-style-type: decimal">
<li>Limit the template to the collection of data.</li>
<li>Make sensible choices when dividing data collection into rows and columns.</li>
<li>Avoid characters or formatting that will make it hard for a computer program to process the data.</li>
</ol>
<p>It is important to note that there’s no reason that you can’t continue to use a
spreadsheet program like Excel or Google Sheets to collect data. The spreadsheet
program itself can easily be used to create a simple template to use as you
collect data. In fact, we’ll continue using a spreadsheet format in the rest of
this module and in the next one as we show how to redesign the data collection
for this example experiment. It is important, however, to think through how you
will arrange that template spreadsheet to make it most useful in the larger
context of reproducible research.</p>
<div id="example-datadata-on-rate-of-bacterial-growth" class="section level3" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> Example data—Data on rate of bacterial growth</h3>
<p>Here, we’ll walk through an example using real data collected in a laboratory
experiment. We described these data in detail in the previous module. As a
reminder, they were collected to measure the growth rate of <em>Mycobacteria
tuberculosis</em> under two conditions—high oxygen and low oxygen. They were
collected from five test tubes that were measured regularly over one week for
bacteria growth using a measure of optical density. Figure
<a href="experimental-data-recording.html#fig:growthexcel2">2.11</a> shows the original template that the research group used
to record these data.</p>
<div class="figure"><span style="display:block;" id="fig:growthexcel2"></span>
<img src="figures/growth_curve_example.png" alt="Example of an Excel spreadsheet used to record and analyze data for a laboratory experiment. Annotations highlight where data is entered by hand, where calculations are done by hand, and where embedded Excel formulas are used. The figures are created automatically using values in a specified column." width="\textwidth" />
<p class="caption">
Figure 2.11: Example of an Excel spreadsheet used to record and analyze data for a laboratory experiment. Annotations highlight where data is entered by hand, where calculations are done by hand, and where embedded Excel formulas are used. The figures are created automatically using values in a specified column.
</p>
</div>
<p>In the previous module, we described features that make this template “untidy”
and potentially problematic to include in a larger pipeline of reproducible
research. In the next few sections of this module, we’ll walk step-by-step
through changes that you could make to make this template tidier. We’ll finish
the module by showing how you could then easily design a further step of the
analysis pipeline to visualize and analyze the collected data, so that the
advantages of real-time plotting from the more complex spreadsheet are not
missed when moving to a tidier template.</p>
</div>
<div id="limiting-the-template-to-the-collection-of-data" class="section level3" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> Limiting the template to the collection of data</h3>
<p>The example template (Figure <a href="experimental-data-recording.html#fig:growthexcel2">2.11</a>) includes a number of
“extra” elements beyond simple data collection—all the elements outside rows
1–15 of columns A–I. Outside this area of the original spread, there are a
number of extra elements, including plots that visualize the data, summaries
generated based on the data (rows 16–18, for example), notes about the data,
and even a macro (top right) that wasn’t involved in data collection but instead
was used by the researcher to calculate the initial volume of inoculum to
include in each test tube. None of these “extras” can be easily read into a
statistical program like R or Python—at best, they will be ignored by the program.
They can even complicate reading in the cells with measurements (rows
1–15 of columns A–I), as most statistical programs will try to read in all the
non-empty cells of a spreadsheet unless directed otherwise.</p>
<p>A good starting point, then, would be to start designing a tidy data collection
template for this experiment by extracting only the content from the box in
Figure <a href="experimental-data-recording.html#fig:extractraw">2.2</a>. This would result in a template that looks like
Figure <a href="experimental-data-recording.html#fig:step1">2.12</a>.</p>
<div class="figure"><span style="display:block;" id="fig:step1"></span>
<img src="figures/growth_curve_step1.png" alt="First step in designing a tidy data collection template for the example project. A template has been created that focuses only on the raw data, removing all extra elements like plots, notes, macros, and summaries." width="\textwidth" />
<p class="caption">
Figure 2.12: First step in designing a tidy data collection template for the example project. A template has been created that focuses only on the raw data, removing all extra elements like plots, notes, macros, and summaries.
</p>
</div>
<p>Notice that we’ve also removed any of the color formatting from the spreadsheet. It is fine to
keep color in the spreadsheet if it will help the research to find the right spot to record data
while working in the laboratory, but you should make sure that you’re not using it to encode
information about the data—all color formatting will be ignored when the data are read by a
statistical program like R.</p>
<p>While the template shown in Figure <a href="experimental-data-recording.html#fig:step1">2.12</a> has removed a lot of the calculated values from the
original template, it has not removed all of them. Two of the columns are still values that were
determined by calculation after the original data were collected. Column B and column D both provide
measures of the length of time since the start of the experiment, and both are calculated by
comparing a measurement time to the time at the start of the experiment.</p>
<p>The time since the start of the experiment can easily be calculated later in the analysis pipeline,
once you read the data into a statistical program like R. By delaying this step, you can both
simplify the data collection template (requiring fewer columns for the research in the laboratory
to fill out) and also avoid the chance for mistakes, which could occur both in the hand calculations
of these values and in data entry, when the researcher enters the results of the calculations in the
spreadsheet cell. Figure <a href="experimental-data-recording.html#fig:step2">2.13</a> shows a new version of the template, where these calculated
columns have been removed. This template is now restricted to only data points originally collected
in the course of the experiment, and has removed all elements that are based on calculations or other
derivatives of those original, raw data points.</p>
<div class="figure"><span style="display:block;" id="fig:step2"></span>
<img src="figures/growth_curve_step2.png" alt="Second step in designing a tidy data collection template for the example project. This template started from the previous one, but removed columns that were hand-calculated and then entered by the researcher in the previous template. This version has removed all calculated values on the template, limiting it to only the original recorded values required for the experiment." width="\textwidth" />
<p class="caption">
Figure 2.13: Second step in designing a tidy data collection template for the example project. This template started from the previous one, but removed columns that were hand-calculated and then entered by the researcher in the previous template. This version has removed all calculated values on the template, limiting it to only the original recorded values required for the experiment.
</p>
</div>
</div>
<div id="making-sensible-choices-about-rows-and-columns" class="section level3" number="2.5.3">
<h3><span class="header-section-number">2.5.3</span> Making sensible choices about rows and columns</h3>
<p>The second principle is to <strong>make sensible choices when dividing data collection
into rows and columns</strong>. There are many different ways that you could spread the
data collection into rows and columns, and in this step, you can consider which
method would meet a reasonable balance between making the template easy for the
researcher in the laboratory to use to record data and also making the resulting
data file easy to incorporate in a reproducible data analysis pipeline.</p>
<p>For the example experiment, Figure <a href="experimental-data-recording.html#fig:extractraw">2.2</a> shows three examples
that we can consider for how to arrange data collection across rows and columns.
All three build on the changes we made in the earlier step of “tidying” the template,
which resulted in the template shown in Figure <a href="experimental-data-recording.html#fig:step2">2.13</a>.</p>
<div class="figure"><span style="display:block;" id="fig:columnoptions"></span>
<img src="figures/growth_curve_column_options.png" alt="Examples of ways that data collection could be divided into rows and columns in the example template. Panel A shows an example where date and time are recorded in different columns. Panel B is similar to Panel A, but in this case, date and time are recorded in a single column. Panel C shows a classically 'tidy' data format, where each measurement date-time is repeated for each of the five test tubes, and columns give the test tube ID and absorbance measurement at that time for that tube (only part of the data is shown for this format, while remaining rows are off the page). While Panel C provides the 'tidiest' format, it may have some practical constraints when used in a laboratory setting. For example, it would require more data entry during data collection (since date-time is entered five times at each measurement time), and its long format prevent it all from being seen at once without scrolling on a computer screen." width="\textwidth" />
<p class="caption">
Figure 2.14: Examples of ways that data collection could be divided into rows and columns in the example template. Panel A shows an example where date and time are recorded in different columns. Panel B is similar to Panel A, but in this case, date and time are recorded in a single column. Panel C shows a classically ‘tidy’ data format, where each measurement date-time is repeated for each of the five test tubes, and columns give the test tube ID and absorbance measurement at that time for that tube (only part of the data is shown for this format, while remaining rows are off the page). While Panel C provides the ‘tidiest’ format, it may have some practical constraints when used in a laboratory setting. For example, it would require more data entry during data collection (since date-time is entered five times at each measurement time), and its long format prevent it all from being seen at once without scrolling on a computer screen.
</p>
</div>
<p>Panel A (an exact repeat of the template shown in Figure <a href="experimental-data-recording.html#fig:step2">2.13</a>) shows
an example where date and time are recorded in different columns. Panel B is
similar to Panel A, but in this case, date and time are recorded in a single
column. Panel C shows a classically “tidy” data format, where each measurement’s
date-time is repeated for each of the five test tubes, and columns give the test
tube ID and absorbance measurement at that time for that tube (only part of the
data is shown for this format, while remaining rows are off the page).</p>
<p>In this example, the template that may be the most reasonable is the one shown
in Panel B. While Panel C provides the “tidiest” format, it has some practical
constraints when used in a laboratory setting. For example, it would require
more data entry during data collection (since date-time is entered five times at
each measurement time), and its long format prevent it all from being seen at
once without scrolling on a computer screen. When comparing Panels A and B, the
template in Panel B has an advantage. The information on date and time are
useful together, but not individually. For example, to calculate the time since
the start of the experiment, you cannot just calculate the difference in dates
or just the difference in times, but instead must consider both the date and
time of the measurement in comparison to the date and time of the start of the
experiment. As a result, at some point in the data analysis pipeline, you’ll
need to combine information about the date and the time to make use of the two
elements. While this combination of two columns can be easily done within a
statistical program like R, it can also be directly designed into the original
template for collecting the data. Therefore, unless there is a practical reason
why it would be easier for the researcher to enter date and time separately, the
template shown in Panel B is preferable to that shown in Panel A in terms of
allowing for the “tidy” collection of research data into a file that is easy to
include in a reproducible pipeline. Figure <a href="experimental-data-recording.html#fig:step3">2.15</a> shows the template
design at this stage in the process of tidying it, highlighting the column that
combines date and time elements in a single column. In this version of the
template, we’ve also been careful about how date and time are recorded, a
consideration that we’ll discuss more in the next section.</p>
<div class="figure"><span style="display:block;" id="fig:step3"></span>
<img src="figures/growth_curve_step3.png" alt="Third step in designing a tidy data collection template for the example project. This template started from the previous one, but combined collection of the date and time of the measurement into a single column and revised the format to include all date elements and to prevent automatic conversion by the spreadsheet program." width="\textwidth" />
<p class="caption">
Figure 2.15: Third step in designing a tidy data collection template for the example project. This template started from the previous one, but combined collection of the date and time of the measurement into a single column and revised the format to include all date elements and to prevent automatic conversion by the spreadsheet program.
</p>
</div>
</div>
<div id="avoiding-problematic-characters-or-formatting" class="section level3" number="2.5.4">
<h3><span class="header-section-number">2.5.4</span> Avoiding problematic characters or formatting</h3>
<p>The third principle is to <strong>avoid characters or formatting that will make it
hard for a computer program to process the data</strong>. There are a number of special
characters and formatting conventions that can be hard for a statistical program to
handle. In the example template shown in Figure <a href="experimental-data-recording.html#fig:step3">2.15</a>, for example,
the column names include spaces (for example, in “Date and time”), as well as
parenthese (for example, in “VA 001 (A1)”). While most statistical programs have
tools that allow you to handle and convert these characters once the data are
read in, it’s even simpler to use simpler column names in the original data collection
template, and this will save some extra coding further along in the analysis pipeline.
Two general rules for creating easy-to-use column names in a data collection template
are: (1) start each column name with a letter and (2) for the rest of the column
name, use only letters, numbers, or the underscore character ("_“). For example,”aerated1" would work well, but “1–aerated” would not.</p>
<p>Within the cell values below the column names, there is more flexibility. For example,
if you have a column that gives the IDs of different samples, it would be fine to include
spaces and other characters in those IDs. There are a few exceptions, however. A big one
is with values that record dates or date-time combinations. First, it is important to include
all elements of the date (or date and time, if both are recorded). For example, the year
should be included in the recorded date, even if the experiment only took a few days.
This is because statistical programs have excellent functions for working with data that
are dates or date-times, but to take advantage of these, the data must be converted into
a special class in the program, and conversion to that class requires specific elements
(for example, a date must include the year, month, and day of month). Second, it is
useful to avoid recording dates and date-times in a way that results in a spreadsheet
program automatically converting them. Surrounding the information about a date in
quotation marks when entering it (as shown in Figure <a href="experimental-data-recording.html#fig:step3">2.15</a>) can avoid this.
Finally, consider using a format to record the date that is unambiguous and so less likely
to have recording errors. Dates, for example, are sometimes recorded using only numbers—for
example, the first date of “July 9, 2019” in the example data could be recorded as
“7/9/2019” or “7/9/19,” to be even more concise. However, this format has some ambiguity.
It can be unclear if this refers to July 9 or to September 7, both of which could be
written as “7/9.” For the version that uses two digits for the year, it can be unclear
if the date is for 2019 or 1919 (or any other century). Using the format “July 9, 2019,”
as done in the latest version of the sample template, avoids this potential ambiguity.</p>
<p>Figure <a href="experimental-data-recording.html#fig:growthsimple2">2.16</a> shows the template for the example experiment after the
column names have been revised to avoid any problematic characters. This template is now in
a very useful format for a reproducible research pipeline—the data collected using this
template can be very easily read into and processed using further statistical programs like
R or Python.</p>
<div class="figure"><span style="display:block;" id="fig:growthsimple2"></span>
<img src="figures/growth_curve_simple.png" alt="Example of an simpler format that can be used to record and analyze data for the same laboratory experiment as the previous figure. Annotations highlight where data is entered by hand. No calculations are conducted or figures created---these are all done later, using a code script." width="\textwidth" />
<p class="caption">
Figure 2.16: Example of an simpler format that can be used to record and analyze data for the same laboratory experiment as the previous figure. Annotations highlight where data is entered by hand. No calculations are conducted or figures created—these are all done later, using a code script.
</p>
</div>
</div>
<div id="moving-further-data-analysis-to-later-in-the-pipeline" class="section level3" number="2.5.5">
<h3><span class="header-section-number">2.5.5</span> Moving further data analysis to later in the pipeline</h3>
<p>Once you have created a “tidy” template for collecting your data in the laboratory,
you can create a report template that will input that data and then provide summaries
and visualizations. This allows you to separate the steps (and files) for collecting data
from those for analyzing data. Figure <a href="experimental-data-recording.html#fig:growthreport2">2.17</a> shows an example of
a report template that could be created to pair with the data collection template shown
in Figure <a href="experimental-data-recording.html#fig:growthsimple2">2.16</a>.</p>
<div class="figure"><span style="display:block;" id="fig:growthreport2"></span>
<img src="figures/growth_curve_report.png" alt="Examples of an automated report that can be created to quickly generate summaries and estimates of the data collected in the simplified data collection template for the example experiment." width="\textwidth" />
<p class="caption">
Figure 2.17: Examples of an automated report that can be created to quickly generate summaries and estimates of the data collected in the simplified data collection template for the example experiment.
</p>
</div>
<p>To create a report template like this, you can use tools for reproducible
reports from statistical programs like R and Python. In this section, we’ll walk
through how you could create the report template shown in Figure
<a href="experimental-data-recording.html#fig:growthreport2">2.17</a>. We will be using the programming language R, including
RMarkdown, which is R’s main tool for creating reproducible reports. If you have
never used R or RMarkdown before, you’ll find it useful to learn some more about
their use to help in creating these types of reproducible reports from collected
data. Numerous excellent (and free) resources exist to help learn R. One of the
best is the book “R for Data Science” by Hadley Wickham and Garrett Grolemund.
It is available in print, as well as free online at <a href="https://r4ds.had.co.nz/" class="uri">https://r4ds.had.co.nz/</a>.</p>
<hr />
<p><strong>Older text</strong></p>
</div>
<div id="exampledata-on-rate-of-bacterial-growth-1" class="section level3" number="2.5.6">
<h3><span class="header-section-number">2.5.6</span> Example—Data on rate of bacterial growth</h3>
<p>The first set of data are from a study on the growth of <em>Mycobacterium
tuberculosis</em>. The goal of this study was to compare growth yield and doubling
time of <em>Mycobacterium tuberculosis</em> grown in rich medium under two assay
conditions. One set of cultures were grown in tubes with a low culture volume
relative to a large air head space to allow free oxygen exchange. A second set
of cultures were grown in tubes filled to near capacity, resulting in limited
air head space which has been shown elsewhere to limit oxygen availability over
time. The caps on both sets of cultures were sealed to restrict air exchange
during the study.</p>
<p>Some background information is helpful in understanding these example data,
especially if you have not conducted this type of experiment. The increase in
the cell size and cell mass during the development of an organism is termed
growth. It is the unique characteristics of all organisms. The organism must
require certain basic parameters for their energy generation and cellular
biosynthesis. The growth of the organism is affected by both physical and
nutritional factors. There are multiple methods by which growth can be measured,
but the use of closed tissue culture tubes and a spectrophotometer to track
increases in optical density (absorbance at 600 nm) over time offers several
advantages: 1) it is less subject to technical error and contamination, 2) read
out is fast and simple, 3) growth as measure by increased absorbance (turbidity)
is directly proportional to increases in cell mass. There are four distinct
phases of bacterial growth. Lag phase, log (exponential phase), stationary
phase, death phase. From these data, bacterial generation times (doubling time)
during the exponential growth phase can be calculated.</p>
<p><span class="math display">\[
\mbox{Doubling time} = \frac{log(2)(t_1 - t_2)}{log(OD_{t_1} - log(OD_{t_2}))}
\]</span>
where <span class="math inline">\(t_1\)</span> and <span class="math inline">\(t_2\)</span> are two time points and <span class="math inline">\(OD_{t_1}\)</span> and <span class="math inline">\(OD_{t_2}\)</span> are the
optical densities at the two time points (all <span class="math inline">\(log\)</span>s are natural in this case).</p>
<p>An excel-based workbook (Figure <a href="experimental-data-recording.html#fig:growthexcel2">2.11</a>) was created to allow the
student performing the work to (1) calculate the amount of initial inoculum
(cell culture) to add to each tube to begin the study, (2) record the raw data
absorbance measurements, (3) graph the data on both a log and linear scale, and
(4) calculate doubling time in two phases of growth using the equation listed
above. Columns were added to allow the student to track the time (column A), the
difference in time (hours) between each time point in which data were collected
(column B), the date on which data were gathered (column C), and the time in
hours for each data point from the start of the study for graphing purposes
(column D). Absorbance data for each sampling timepoint were listed in Columns
E-F (high oxygen conditions; VA001 A1, A3) or columns G-I (limited oxygen
conditions; VA001 L1, L2, L3).</p>
<p>What the researchers found appealing about the format of this Excel sheet was
the ease with which the student could accomplish the study goals. They also
cited transparency of the raw data and ease with which additional sampling data
points could be added. The data being graphed in real time and the inclusion of
a simple macro to calculate doubling time, allowed the student to see tangible
differences between the two assay conditions. This was also somewhat problematic
as the equation to calculate doubling time was based on anchored time points
built into the original spreadsheet resulting in two different results that were
not properly linked to the correct data time points.</p>
<p>Data that are saved in a format like that shown in Figure <a href="experimental-data-recording.html#fig:growthexcel2">2.11</a>,
however, are hard to read in for a statistical program like R, Perl, or Python.
In this format, the raw data (the time points each observation was collected and
the optical density for the sample at that time point) form only part of the
spreadsheet. The spreadsheet also includes notes, automated figures, and
cells where an embedded formula runs calculations behind the scenes.</p>
<p>Instead of this format, we can design a simpler format to collect the data. We’ll
remove all figures and calculations, and instead save those to perform in a
code script. Figure <a href="#fig:growthsimple"><strong>??</strong></a> shows an example of a simpler
format for collecting the same data. In this case, all the “extras” have been
stripped out—this only has spaces for recording times points and the observed
optical density at those time points. In later chapters, we’ll show how a code
script can be used to input these data into R and then perform calculations
and create figures. By separating out the steps of data recording from
data analysis, you can ensure that all steps of analysis are clearly spelled
out (and can be easily reproduced with other similar data) through a code
script. Note that you can still collect the data in this simpler format using
a spreadsheet program, if you’d like—Figure <a href="experimental-data-recording.html#fig:growthsimple2">2.16</a> shows
the data collection set up to be recorded in a spreadsheet program, for example.
Within the spreadsheet, you can choose to save the data in a plain text format
(a csv [comma-separated value] file, for example).</p>
<p>In this new data collection format, the data are not completely “tidy.” This is because
there is still some information included in the column names that we might want to use
for analysis and plotting—namely, the different experimental group names (e.g.,
“aerated1,” “low_oxygen1”). However, there is a balance in creating data collection
spreadsheets. They should be in a format that is easy to read into an interactive
programming environment like R, as well as in a format that will be easy to convert
to a truly “tidy” format once they are read in. However, it’s okay to balance these
needs with aims to make the data collection spreadsheet easy for a researcher to use.</p>
<p>The example shown in Figure <a href="experimental-data-recording.html#fig:growthsimple2">2.16</a> is designed to be easy to use
when collecting data. All data points for a single collection time are grouped together
on a single row. When a researchers collects data for one time point, he or she can
easy confirm visually that all the experimental groups have been measured for that
time point. This format still makes it easy to read the data into an interactive
programming environment, however, since they are in a clear two-dimensional format,
with column names in the first row and values in the remaining rows. The removal of
extraneous elements—like embedded formulas, the results of hand calculations or
automated calculations, and annotations through notes or colored highlighting—remove
barriers when reading the data into more sophisticated software. Once the data are
read into R, there can be converted into a truly tidy data format with just a few
command calls.</p>
<p>The following code shows an example of how easy it is to read data into R in the simplified
format shown in Figure <a href="experimental-data-recording.html#fig:growthsimple2">2.16</a>. It also shows how a few lines of code
can then be used to convert the data into a truly “tidy” format, and how easily
sophisticated plots can then be made with the data.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="experimental-data-recording.html#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;tidyverse&quot;</span>)</span>
<span id="cb21-2"><a href="experimental-data-recording.html#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;readxl&quot;</span>)</span>
<span id="cb21-3"><a href="experimental-data-recording.html#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="experimental-data-recording.html#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Read data into R from the simplified data collection template</span></span>
<span id="cb21-5"><a href="experimental-data-recording.html#cb21-5" aria-hidden="true" tabindex="-1"></a>growth_curve <span class="ot">&lt;-</span> <span class="fu">read_excel</span>(<span class="st">&quot;data/growth_curve_data_in_excel (1)/growth curve data_GR.xls&quot;</span>, </span>
<span id="cb21-6"><a href="experimental-data-recording.html#cb21-6" aria-hidden="true" tabindex="-1"></a>                           <span class="at">sheet =</span> <span class="st">&quot;simplified_template&quot;</span>)</span>
<span id="cb21-7"><a href="experimental-data-recording.html#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="experimental-data-recording.html#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of data</span></span>
<span id="cb21-9"><a href="experimental-data-recording.html#cb21-9" aria-hidden="true" tabindex="-1"></a>growth_curve</span></code></pre></div>
<pre><code>## # A tibble: 14 × 6
##    sampling_date_time  aerated1 aerated3 low_oxygen1 low_oxygen2 low_oxygen3
##    &lt;dttm&gt;                 &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;
##  1 2019-07-09 12:00:00    0.02     0.02        0.02        0.02        0.02 
##  2 2019-07-09 16:05:00    0.027    0.02        0.022       0.022       0.027
##  3 2019-07-10 09:50:00    0.087    0.087       0.057       0.082       0.086
##  4 2019-07-10 16:50:00    0.129    0.137       0.072       0.104       0.111
##  5 2019-07-11 10:03:00    0.247    0.258       0.102       0.114       0.126
##  6 2019-07-11 16:07:00    0.299    0.302       0.11        0.11        0.126
##  7 2019-07-12 09:48:00    0.41     0.428       0.137       0.111       0.131
##  8 2019-07-12 16:30:00    0.426    0.432       0.138       0.11        0.13 
##  9 2019-07-13 10:35:00    0.442    0.444       0.138       0.1         0.125
## 10 2019-07-13 17:52:00    0.448    0.45        0.143       0.103       0.131
## 11 2019-07-14 13:10:00    0.436    0.448       0.144       0.1         0.131
## 12 2019-07-15 10:27:00    0.434    0.45        0.147       0.097       0.129
## 13 2019-07-15 16:42:00    0.428    0.45        0.147       0.097       0.129
## 14 2019-07-16 09:28:00    0.42     0.448       0.145       0.095       0.128</code></pre>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="experimental-data-recording.html#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to a fully tidy format</span></span>
<span id="cb23-2"><a href="experimental-data-recording.html#cb23-2" aria-hidden="true" tabindex="-1"></a>growth_curve <span class="ot">&lt;-</span> growth_curve <span class="sc">%&gt;%</span> </span>
<span id="cb23-3"><a href="experimental-data-recording.html#cb23-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="sc">-</span>sampling_date_time, </span>
<span id="cb23-4"><a href="experimental-data-recording.html#cb23-4" aria-hidden="true" tabindex="-1"></a>               <span class="at">names_to =</span> <span class="st">&quot;experimental_group&quot;</span>, </span>
<span id="cb23-5"><a href="experimental-data-recording.html#cb23-5" aria-hidden="true" tabindex="-1"></a>               <span class="at">values_to =</span> <span class="st">&quot;optical_density&quot;</span>)</span>
<span id="cb23-6"><a href="experimental-data-recording.html#cb23-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-7"><a href="experimental-data-recording.html#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="co"># How the data look after this transformation</span></span>
<span id="cb23-8"><a href="experimental-data-recording.html#cb23-8" aria-hidden="true" tabindex="-1"></a>growth_curve</span></code></pre></div>
<pre><code>## # A tibble: 70 × 3
##    sampling_date_time  experimental_group optical_density
##    &lt;dttm&gt;              &lt;chr&gt;                        &lt;dbl&gt;
##  1 2019-07-09 12:00:00 aerated1                     0.02 
##  2 2019-07-09 12:00:00 aerated3                     0.02 
##  3 2019-07-09 12:00:00 low_oxygen1                  0.02 
##  4 2019-07-09 12:00:00 low_oxygen2                  0.02 
##  5 2019-07-09 12:00:00 low_oxygen3                  0.02 
##  6 2019-07-09 16:05:00 aerated1                     0.027
##  7 2019-07-09 16:05:00 aerated3                     0.02 
##  8 2019-07-09 16:05:00 low_oxygen1                  0.022
##  9 2019-07-09 16:05:00 low_oxygen2                  0.022
## 10 2019-07-09 16:05:00 low_oxygen3                  0.027
## # … with 60 more rows</code></pre>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="experimental-data-recording.html#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of how easily sophisticated plots can be created with data in this format</span></span>
<span id="cb25-2"><a href="experimental-data-recording.html#cb25-2" aria-hidden="true" tabindex="-1"></a>growth_curve <span class="sc">%&gt;%</span> </span>
<span id="cb25-3"><a href="experimental-data-recording.html#cb25-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> sampling_date_time, <span class="at">y =</span> optical_density)) <span class="sc">+</span> </span>
<span id="cb25-4"><a href="experimental-data-recording.html#cb25-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span> </span>
<span id="cb25-5"><a href="experimental-data-recording.html#cb25-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> experimental_group)</span></code></pre></div>
<p><img src="improve_repro_files/figure-html/unnamed-chunk-23-1.png" width="672" />
In later chapters, we’ll discuss R’s “tidyverse,” a collection of tools within R
that facilitate analyzing and visualizing data once they’ve been read into R. Here, we
only aim to give an example of how little R code is needed to create useful output from
the data, with the only requirement for gaining this power being that the data need to
be collected in a format that is “tidy” or close enough to easily read into R.</p>
</div>
<div id="exampledata-on-bacteria-colony-forming-units" class="section level3" number="2.5.7">
<h3><span class="header-section-number">2.5.7</span> Example—Data on bacteria colony forming units</h3>
</div>
<div id="exampledata-from-multiple-related-experiments" class="section level3" number="2.5.8">
<h3><span class="header-section-number">2.5.8</span> Example—Data from multiple related experiments</h3>
</div>
<div id="issues-with-these-data-sets" class="section level3" number="2.5.9">
<h3><span class="header-section-number">2.5.9</span> Issues with these data sets</h3>
<ol style="list-style-type: decimal">
<li>Issues related to using a spread sheet program
<ul>
<li>Embedded macros</li>
<li>Use of color to encode information</li>
</ul></li>
<li>Issues related to non-structured / non-two-dimensional data
<ul>
<li>Added summary row</li>
<li>Multiple tables in one sheet</li>
<li>One cell value is meant to represent values for all rows below, until next
non-missing row</li>
</ul></li>
<li>Issues with data being non-“tidy”</li>
</ol>
</div>
<div id="final-tidy-examples" class="section level3" number="2.5.10">
<h3><span class="header-section-number">2.5.10</span> Final “tidy” examples</h3>
</div>
<div id="options-for-recording-tidy-data" class="section level3" number="2.5.11">
<h3><span class="header-section-number">2.5.11</span> Options for recording tidy data</h3>
<p><strong>Spreadsheet program.</strong></p>
<p><strong>Spreadsheet-like interface in R.</strong></p>
</div>
<div id="examples-of-how-tidy-data-can-be-easily-analyzed-visualized" class="section level3" number="2.5.12">
<h3><span class="header-section-number">2.5.12</span> Examples of how “tidy” data can be easily analyzed / visualized</h3>
</div>
<div id="discussion-questions" class="section level3" number="2.5.13">
<h3><span class="header-section-number">2.5.13</span> Discussion questions</h3>

</div>
</div>
<div id="module6" class="section level2" number="2.6">
<h2><span class="header-section-number">2.6</span> Power of using a single structured ‘Project’ directory for storing and tracking research project files</h2>
<p>To improve the computational reproducibility of a research project, researchers
can use a single ‘Project’ directory to collectively store all research data,
meta-data, pre-processing code, and research products (e.g., paper drafts,
figures). We will explain how this practice improves the reproducibility and
list some of the common components and subdirectories to include in the
structure of a ‘Project’ directory, including subdirectories for raw and
pre-processed experimental data.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Describe a ‘Project’ directory, including common components and subdirectories</li>
<li>List how a single ‘Project’ directory improves reproducibility</li>
</ul>
<p>In previous modules, we’ve discussed how you can separate the steps of data
collection from steps of data cleaning, management, pre-processing, and
analysis. In the case of data recorded in a laboratory, this separation
will often result in moving from a single file (for example, a spreadsheet),
that combines all the steps to using separate files for the different
steps. As you move to separate data recording and analysis, then, it
is very important to store the set of files for a project in a way that
is clear and easy to manage. If the file directory for the project is
well-designed, it can even allow you to create software tools and report templates
that can be reused over many experiments. In the next few modules, we’ll
discuss how you can organize the files for an experiment using R’s “Project”
system. The modules will discuss the advantages of well-designed project
directories, tips for arranging files within a project directory,
and how to create templates for R Projects that allow you to use consistent
file organization across many experiments.</p>
<p>As a motivating example, for the next few modules, we’ll use an example based on
a set of real immunology experiments. This example highlights how a research
laboratory will often conduct a similar type of experiment many times, so it
lets us demonstrate how the design of the project’s files within a project
directory can be reused across similar experiments. It will allow us to show you
how you can move from designing a file directory for a single experiment to
designing one that can be used repeatedly, and then how you can take advantage
of consistency in the directory structure across projects to make tools and
templates that can be reused.</p>
<div id="example-project" class="section level3" number="2.6.1">
<h3><span class="header-section-number">2.6.1</span> Example project</h3>
<p>The examples for this and the next few modules are based on a collection of
studies that were conducted with similar designs and similar goals—all aimed
to test candidate treatments for tuberculosis. Most studies in this set tested
one or more treatments as well as one or more controls. The controls could
include negative controls, like saline solution, or positive controls, like a
drug already in use to treat the disease, isoniazid. A few of the studies tested
only controls, to help in developing baseline expectations for things like the
bacterial load in different mouse strains used in studies in the set. The
set of studies tested some treatments that were monotherapies (only one drug
given to the animal) as well as some that were combinations of two or three
different drugs. For many of the drugs that were tested, they were tested at
different doses and, in some cases, different methods of delivery or different
mouse models.</p>
<p>Each of the treatments were given to several mice that had been infected with
<em>Mycobacterium tuberculosis</em>. During the treatment, the mice were weighed
regularly. This weight measurement helps to determine if a particular treatment
is well-tolerated by the animals—if not, it may show through the treated mice
losing weight during treatment. For convenience, the mice were not weighed
individually. Instead, mice with the same treatment were kept in a single cage,
and the entire cage was weighed, the weight of the cage itself factored out, and
the average weight of mice for that treatment determined by dividing the weight
of all mice in the cage by the number of mice in the cage. After a period of
time, the mice were sacrificed and one lobe from their lungs was used to
determine each mouse’s bacterial load, through plating the material from the
lobe and counting the colony forming units (CFUs). One aim of the data analysis
is to compare the bacterial load of mice under various treatments to the
bacterial load of mice in the control group.</p>
<p>The full set of studies included 19 different studies. These were conducted at
different times, but the data for all of the studies can be collected using a
common format. In this module, as well as the following two, we’ll be exploring
how you can use RStudio’s Project functionality to organize data from one or
more studies. We’ll particularly focus on how, by using a common format for data
collection, you can create tools that can be used repeatedly for different
experiments to ensure that methods are the same across all studies of a similar
type, as well as to improve the reproducibility of the studies. Let’s walk
through the types of data that were collected for each study.</p>
<p>First, there was some metadata recorded for each study. Figure
<a href="experimental-data-recording.html#fig:metadata">2.18</a> gives an example. This includes information about the strain
of mouse that was used in the study, treatment details (including the method of
giving the drug or drugs, how often they were given each week, and for how many
weeks), how much bacteria the animals were exposed to (measured both in terms of
the inoculum they were given and their bacterial load one day after they were
given that inoculum, which was based on sacrificing one animal the day after
challenging all the animals with the bacteria), and, if the study included a
novel drug as part of the tested treatment, the batch number of that drug.</p>
<div class="figure"><span style="display:block;" id="fig:metadata"></span>
<img src="figures/project_metadata.png" alt="Example of recording metadata for a study in the set of example studies for this module." width="\textwidth" />
<p class="caption">
Figure 2.18: Example of recording metadata for a study in the set of example studies for this module.
</p>
</div>
<p>Next, the researchers recorded some information about each treatment group
within the experiment. This typically included at least one negative control. In
some cases, there was also a positive control, in which the animals were treated
with a drug that’s in standard use against tuberculosis already (e.g.,
isoniazid). Most studies would also test one or more treatments, which could
include monotherapies or combined therapies. Figure <a href="experimental-data-recording.html#fig:treatmentdetails">2.19</a>
shows an example of the data that were recorded on each treatment in the study.
These data include the names and doses of up to three drugs in each treatment,
as well as a column where the researcher can provide detailed specifications of
the treatment.</p>
<div class="figure"><span style="display:block;" id="fig:treatmentdetails"></span>
<img src="figures/project_treatment_details.png" alt="Example of recording treatment details for a study in the set of example studies for this module." width="\textwidth" />
<p class="caption">
Figure 2.19: Example of recording treatment details for a study in the set of example studies for this module.
</p>
</div>
<p>Once the animals were challenged with the bacteria, treatment began, and two
main types of data were measured and recorded. First, the mice were weighed once
a week. These weights were converted to a measure of the percent change in
weight since the start of treatment. If the animals’ weights decrease during the
treatment, it is a marker that the treatment is not well-tolerated by the
animals. Figure <a href="experimental-data-recording.html#fig:mouseweight">2.20</a> shows an example of how these data were
recorded. All animals within a treatment group were kept in the same cage, and
this cage was measured once a week. By dividing the weight of all animals in the
cage by the number of animals, the researchers could estimate the average weight
of animals in that treatment group, which is recorded as shown in Figure
<a href="#fig:mouseweights"><strong>??</strong></a>.</p>
<div class="figure"><span style="display:block;" id="fig:mouseweight"></span>
<img src="figures/project_metadata.png" alt="Example of recording weekly weights of mice in each treatment group for the example set of studies." width="\textwidth" />
<p class="caption">
Figure 2.20: Example of recording weekly weights of mice in each treatment group for the example set of studies.
</p>
</div>
<p>Finally, after the treatment period, the mice were sacrificed and a portion of
each mouse’s lung was used to estimate the bacterial load in that mouse. Figure
<a href="experimental-data-recording.html#fig:bacterialload">2.21</a> shows an example of how the data on the bacterial load
in each mouse was recorded.</p>
<div class="figure"><span style="display:block;" id="fig:bacterialload"></span>
<img src="figures/project_bacterial_load.png" alt="Example of recording the bacterial load in the lungs of each mouse at the end of treatment for the example set of studies." width="\textwidth" />
<p class="caption">
Figure 2.21: Example of recording the bacterial load in the lungs of each mouse at the end of treatment for the example set of studies.
</p>
</div>
<p>As you can see, these data were all recorded using templates that were designed
for the tidy collection of laboratory data (see modules 2.4 and 2.5). These
spreadsheets were used only to record the data, and then processing, analysis,
and visualization were done in a separate file. Specifically, for this set of
studies a preliminary report was designed, with an example shown in Figure [x].
This report uses the first page to provide a nicely format version of the
metadata for the study, including a table with overall details and a table with
details for each specific treatment that was tested. The second page provides a
graph that shows the percent weight change for mice in each treatment group
compared to the weight of that group at the start of treatment. The third page
provides a graph that shows the bacterial loads in each mouse, grouped by
treatment, as well as the results of running a statistical test to test, for
each treatment group, the hypothesis that the mean of a transformed version of
the measure of bacterial load (log-10) for the group was the same as for the
untreated control group.</p>
<div class="figure"><span style="display:block;" id="fig:prelimreport"></span>
<img src="figures/project_prelim_report.png" alt="Example of the preliminary report generated for each study in the set of example studies for this module. The first page includes metadata on the study, as well as details on each treatment that was tested. The second page shows how mouse weights in each treatment group changed over the course of treatment, to help identify if a treatment was well-tolerated. The third page graphs the bacterial load in each mouse, grouped by treatment, and gives the result of a statistical analysis to test which treatment groups had outcomes that were significantly different from the untreated control group." width="\textwidth" />
<p class="caption">
Figure 2.22: Example of the preliminary report generated for each study in the set of example studies for this module. The first page includes metadata on the study, as well as details on each treatment that was tested. The second page shows how mouse weights in each treatment group changed over the course of treatment, to help identify if a treatment was well-tolerated. The third page graphs the bacterial load in each mouse, grouped by treatment, and gives the result of a statistical analysis to test which treatment groups had outcomes that were significantly different from the untreated control group.
</p>
</div>
<p>Let’s take a closer look at a few of these elements. For example, Figure
<a href="experimental-data-recording.html#fig:studytable">2.23</a> shows the tables from the first page of the report shown
in Figure <a href="experimental-data-recording.html#fig:prelimreport">2.22</a>. If you look back to the data collection for
this study (e.g., Figures <a href="experimental-data-recording.html#fig:metadata">2.18</a> and <a href="experimental-data-recording.html#fig:treatmentdetails">2.19</a>),
you can see that all of the information in these tables was pulled from data
recorded at the start of the study.</p>
<div class="figure"><span style="display:block;" id="fig:studytable"></span>
<img src="figures/project_study_info_table.png" alt="Example of one element of the preliminary report generated for each study in the set of example studies for this module. The first page provides tables with metadata about the study and details about each treatment that was tested." width="\textwidth" />
<p class="caption">
Figure 2.23: Example of one element of the preliminary report generated for each study in the set of example studies for this module. The first page provides tables with metadata about the study and details about each treatment that was tested.
</p>
</div>
<p>Figure <a href="experimental-data-recording.html#fig:mouseweightsplot">2.24</a> shows the second page of the report. This
figure has taken the mouse weights—which were recorded in one of the data
collection templates for the project (Figure <a href="#fig:mouseweights"><strong>??</strong></a>)—and used
them to generate a plot of how average mouse weight in each treatment group
changed over the course of the treatment.</p>
<div class="figure"><span style="display:block;" id="fig:mouseweightsplot"></span>
<img src="figures/project_mouse_weights_graph.png" alt="Example of one element of the preliminary report generated for each study in the set of example studies for this module. The second page provides a plot of how the weights of mice in each treatment changed over the course of treatment." width="\textwidth" />
<p class="caption">
Figure 2.24: Example of one element of the preliminary report generated for each study in the set of example studies for this module. The second page provides a plot of how the weights of mice in each treatment changed over the course of treatment.
</p>
</div>
<p>Figure <a href="experimental-data-recording.html#fig:bactcompare">2.25</a> shows the last page of the report. This page
starts with a figure that shows the bacterial load in the lungs of each mouse in
the study at the end of the treatment period. In this figure, the measurement
for each mouse is shown with a point, and these points are grouped by the
treatment group of the mouse. Boxplots are added to show the distribution across
the mice in each group. The color is used to show whether the treatment was a
negative control, a positive control, a monotherapy, or a combined therapy. The
second part of the page gives a table with the results from running a
statistical analysis to compare the bacterial load for mice in each treatment
group to the bacterial load in the mice in the untreated control group. Color is
added to the table to highlight treatments that had a large difference in
bacterial load from the untreated control, as well as treatments for which the
difference from the untreated control was estimated to be statistically
significant. All the data for these results, including the labels for the plot,
are from the data collected in the data collection templates shown earlier.</p>
<div class="figure"><span style="display:block;" id="fig:bactcompare"></span>
<img src="figures/project_bact_compare_plot.png" alt="Example of one element of the preliminary report generated for each study in the set of example studies for this module. The third page provides results on how bacterial load in the lungs compares among treatments at the end of the treatment period." width="\textwidth" />
<p class="caption">
Figure 2.25: Example of one element of the preliminary report generated for each study in the set of example studies for this module. The third page provides results on how bacterial load in the lungs compares among treatments at the end of the treatment period.
</p>
</div>
</div>
<div id="creating-an-organized-directory-for-project-files" class="section level3" number="2.6.2">
<h3><span class="header-section-number">2.6.2</span> Creating an organized directory for project files</h3>
<p>To start, let’s look at how we can organize the files from this set of studies
into a directory in an efficient way. …</p>
<p>…</p>
<p>The full directory of files for this example can be found at [GitHub address],
where you can download them or explore them online. All files for this
project can be stored within a well-designed directory, and this directory
can be enhanced into something called an R Project very easily. In this
module, we’ll explore how to use an R Project and what advantages it
offers compared to other ways of organizing the files associated with
a study. In particular, we’ll build on ideas from earlier modules about
creating reproducible data collection templates, as in this example, the
use of a common template across many studies in a set makes it very
easy to create and apply a common reporting template to the data, easily
creating a reproducible report for each of the nineteen studies in the
example set of studies. Further, we’ll look at how this organization
allows not only for reporting on specific studies in a reproducible way,
but also makes it easier to create an overall report that combines
results and details from all studies in the set.</p>
</div>
<div id="making-the-project-directory-and-r-project" class="section level3" number="2.6.3">
<h3><span class="header-section-number">2.6.3</span> Making the project directory and R Project</h3>
<hr />
<p><strong>Old text</strong></p>
</div>
<div id="organizing-project-files-through-the-file-system" class="section level3" number="2.6.4">
<h3><span class="header-section-number">2.6.4</span> Organizing project files through the file system</h3>
<p>One of the most amazing parts of how modern computers work is their file
directory systems. [More on these.]</p>
<p>It is useful to leverage this system to organize all the files related to a
project. These include data files (both “raw” data—directly output from
measurement equipment or directly recorded from observations, as well as any
“cleaned” version of this data, after steps have been taken to preprocess the
data to prepare it for visualization and analysis in papers and reports). These
files also include the files with writing and presentations (posters and slides)
associated with the project, as well as code scripts for preprocessing data,
for conducting data analysis, and for creating and sharing final figures and
tables.</p>
<p>There are a number of advantages to keeping all files related to a single project
inside a dedicated file directory on your computer. First, this provides a clear
and obvious place to search for all project files throughout your work on the
project, including after lulls in activity (for example, while waiting for
reviews from a paper submission). By keeping all project files within a single
directory, you also make it easier to share the collection of files for the
project. There are several reasons you might want to share these files. An
obvious one is that you likely will want to share the project files across members
in your research team, so they can collaborate together on the project. However,
there are also other reasons you’d need to share files, and one that is growing
in popularity is that you may be asked to share files (data, code scripts, etc.)
when you publish a paper describing your results.</p>
<p>When files are all stored in one directory, the directory can be compressed and
shared as an email attachment or through a file sharing platform like Google Drive.
As you learn more tools for reproducibility, you can also share the directory through
some more dynamic platforms, that let all those sharing access continue to change
and contribute to the files in the directory in a way that is tracked and
reversible. In later modules in this book, we will introduce <code>git</code> version control
software and the GitHub platform for sharing files under this type of version
control—this is one example of this more dynamic way of sharing files within
a directory.</p>
</div>
<div id="organizing-files-within-a-project-directory" class="section level3" number="2.6.5">
<h3><span class="header-section-number">2.6.5</span> Organizing files within a project directory</h3>
<p>To gain the advantages of directory-based project file organization, all the
files need to be within a single directory, but they don’t all have to be within
the same “level” in that directory. Instead, you can use subdirectories to
structure and organize these files, while still retaining all the advantages of
directory-based file organization. This will help limit the number of files in
each “level” of the directory, so none becomes an overwhelming slew of files of
different types. It can help you navigate the files in the directory, and also
help someone you share the directory with figure out what’s in it and where
everything is.</p>
<p>Subdirectory organizations can also, it turns out, be used in clever ways within
code scripts applied to files in the directory. For example, there are functions
in all scripting languages that will list all the files in a specified subdirectory.
If you keep all your raw data files of a certain type (for example, all output from
running flow cytometry for the project) within a single subdirectory, you can
use this type of function with code scripts to list all the files in that directory
and then apply code that you’ve developed to preprocess or visualize the data
across all those files. This code would continue to work as you added files to that
directory, since it starts by looking in that subdirectory each time it runs and
working with all files there as of that moment.</p>
<p>It is worthwhile to take some time to think about the types of files that are
often generated by your research projects, because there are also big advantages
to creating a standard structure of subdirectories that you can use consistently
across the directories for all the projects in your research program. Of course,
some projects may not include certain files, and some might have a new or unusual
type of file, so you can customize the directory structure to some degree for these
types of cases, but it is still a big advantage to include as many common elements
as possible across all your projects.</p>
<p>For example, you may want to always include a subdirectory called “raw_data,” and
consistently call it “raw_data,” to store data directly from observations or
directly output from laboratory equipment. You may want to include subdirectories
in that “raw_data” subdirectory for each type of data—maybe a “cfu” subdirectory,
for example, with results from plating data to count colony forming units, and
another called “flow” for output from a flow cytometer. By using the same structure
and the same subdirectory names, you will find that code scripts are easier to
reuse from one project to another. Again, most scripting languages allow you to
leverage order in how you’ve arranged your files in the file system, and so using
the same order across different projects lets you repeat and reuse code scripts
more easily from one project to another.</p>
<p>Finally, if you create a clear and clean organization structure for your project
directories, you will find it is much easier to navigate your files in all
directories, and also that new lab members and others you share the directories
with will be able to quickly learn to navigate them. In other areas of science
and engineering, this idea of standardized directory structures has allowed the
development of powerful techniques for open-source software developers to work
together. For example, anyone may create their own extensions to the R
programming language and share these with others through GitHub or several large
repositories. This is coordinated by enforcing a common directory structure on
these extension “packages”—to create a new package, you must put certain types
of files in certain subdirectories within a project directory. With these
standardized rules of directory structure and content, each of these packages
can interact with the base version of R, since there are functions that can tap
into any of these new packages by assuming where each type of file will be
within the package’s directory of files. In a similar way, if you impose a
common directory structure across all the project directories in your research
lab, your collaborators will quickly be able to learn where to find each
element, even in projects they are new to, and you will all be able to write
code that can be easily applied across all project directories, allowing you to
improve reproducibility and comparability across all projects by assuring that
you are conducting the same preprocessing and analysis across all projects (or,
if you are conducting things differently for different projects, that you are
deliberate and aware that you are doing so).</p>
<p>Figure [x] gives an example of a project directory organization that might make
sense for a immunology research laboratory.</p>
<p>Once you have decided on a structure for your directory, you can create a
template of it—a file directory with all the subdirectories included, but
without any files (or only template files you’d want to use as a starting
point in each project). When you start a new project, you can then just
copy this template and rename it. If you are using R and begin to use
R Project (described in the next section), you can also create an R Studio
Project template to serve as this kind of starting point each time you
start a new project.</p>
</div>
<div id="using-rstudio-projects-with-project-file-directories" class="section level3" number="2.6.6">
<h3><span class="header-section-number">2.6.6</span> Using RStudio Projects with project file directories</h3>
<p>If you are using the R programming language for data preprocessing, analysis,
and visualization—as well as RMarkdown for writing reports and
presentations—then you can use RStudio’s “Project” functionality to make it
even more convenient to work with files within a research project’s directory.
You can make any file directory a “Project” in RStudio by chosing “File” -&gt;
“New Project” in RStudio’s menu. This gives you the option to create a
project from scratch or to make an existing directory and RStudio Project.</p>
<p>When you make a file directory an RStudio Project, it doesn’t change much in
the directory itself except adding a “.RProj” file. This file keeps track of
some things about the file directory for RStudio, includuing … Also, when you
open one of these Projects in RStudio, it will move your working directory
into that projects top-level directory. This makes it very easy and practical
to write code using relative pathnames that start from this top-level of the
project directory. This is very good practice, because these relative pathnames
will work equally well on someone else’s computer, whereas if you use file
pathnames that are absolute (i.e., giving directions to the file from the root
directory on your computer), then when someone else tries on run the code on their
own computer, it won’t work and they’ll need to change the filepaths in the code,
since everyone’s computer has its files organized differently. For example, if you,
on your personal computer, have the project directory stored in your “Documents”
folder, while a colleague has stored the project directory in his or her “Desktop”
directory, then the absolute filepaths for each file in the directory will be
different for each of you. The relative pathnames, starting from the top level of
the project directory, will be the same for both of you, though, regardless of
where you each stored the project directory on your computer.</p>
<p>There are some other advantages, as well, to turning each of your research
project directories into RStudio Projects. One is that it is very easy to
connect each of these Projects with GitHub, which facilitates collaborative work
on the project across multiple team members while tracking all changes under
version control. This functionality is described in a later module in this book.</p>
<p>As you continue to use R and RStudio’s Project functionality, you may want to
take the template directory for your project and create an RStudio Project
template based on its structure. Once you do, when you start a new research
project, you can create the full directory for your project’s files from within
RStudio by going to “File” -&gt; “New Project” and then choosing to create a new
project based on that template. The new project will already be set up with the
“.RProj” file that allows you to easily navigate into and out of that project,
to connect it to GitHub, and all the other advantages of setting a file
directory as an RStudio Project. The next module gives step-by-step directions
for making a directory an RStudio Project, and also how to create you own
RStudio Project template to quickly create a new directory for project files
each time you start a new research project.</p>
<p>[Visual—project directory as a <em>mise en place</em> for cooking—everything you
need for the analysis, plus the recipe for someone to repeat later.]</p>
<p>[Reference: The Usual Suspects—you’ll typically have the same types of data files,
analysis, types of figures, etc., come up again and again for different research
projects. Leverage tools to improve efficiency when working with these “usual
suspects.” The first time you follow a protocol that is new to you, or the first
time you cook a recipe, it takes much longer and much more thought than it should
as you do it over and over—there are some recipes where I only use the cookbook
now to figure out the oven temperature or the exact measurement of an ingredient.
These tools will help you streamline your project file organization and move towards
reuse of modular tools and ideas (e.g., remembering how to make a vinaigrette and
applying that regardless of the type of salad) across projects.]</p>
<p>[Analogies for moving to do things more programatically—Tom Sawyer outsourcing the
fence painting, sorcerer’s apprentice (all the mops, plus some difficulties when you
first start, before you get the hang of it).]</p>
<p>[File extensions give an idea of the power of consistent file names. While some
operating systems don’t require these, by naming all the files that should be
opened with, for example, Word “.docx,” the operating system can easily do a
targeted search that looks for files with certain key words in the name while
limiting the search only to Word files. You can leverage this same power yourself,
and in a way that’s more customized to your project or typical research approach,
by using consistent conventions to name your files.]</p>
</div>
<div id="subsection-1" class="section level3" number="2.6.7">
<h3><span class="header-section-number">2.6.7</span> Subsection 1</h3>
<p>One study surveyed over 250 biomedical researchers at the University of Washington.
They noted that, “a common theme surrounding data management and analysis was that
may researchers preferred to utilize their own individual methods to organize data.
The varied ways of managing data were accepted as functional for most present needs.
Some researchers admitted to having no organizational methodology at all, while others
used whatever method best suited their individual needs.” <span class="citation">(Anderson et al. 2007)</span>
One respondent answered, “They’re not organized in any way—they’re just thrown into
files under different projects,” while another said “I grab them when I need them, they’re
not organized in any decent way,” and another, “It’s not even organized—a file on a central
computer of protocols that we use, common lab protocols but those are just individual
Word files within a folder so it’s not searchable per se.” <span class="citation">(Anderson et al. 2007)</span></p>
<blockquote>
<p>“In general, data reuse is most possible when: 1) data; 2) metadata (information
describing the data); and 3) information about the process of generating those data,
such as code, are all provided.” <span class="citation">(Goodman et al. 2014)</span></p>
</blockquote>
<blockquote>
<p>“So far we have used filenames without ever saying what a legal name is, so it’s time for a couple
of rules. First, filenames are limited to 14 characters. Second, although you can use almost any
character in a filename, common sense says you should stick to ones that are visible, and that you
should avoid characters that might be used with other meanings. … To avoid pitfalls, you would
do well to use only letters, numbers, the period and the underscore until you’re familiar with the
situation [i.e., characters with pitfalls]. (The period and the underscore are conventionally used
to divide filenames into chunks…) Finally, don’t forget that case distinctions matter—junk, Junk,
and JUNK are three different names.” <span class="citation">(Kernighan and Pike 1984)</span></p>
</blockquote>
<blockquote>
<p>“The [Unix] system distinguishes your file called ‘junk’ from anyone else’s of the same name. The
distinction is made by grouping files into <em>directories</em>, rather in the way that books are placed om
shelves in a library, so files in different directories can have the same name without any conflict.
Generally, each user haas a personal or <em>home directory</em>, sometimes called login directory, that
contains only the files that belong to him or her. When you log in, you are ‘in’ your home directory.
You may change the directory you are working in—often called your working or <em>current directory</em>—but
your home directory is always the same. Unless you take special action, when you create a new file it is
made in your current directory. Since this is initially your home directory, the file is unrelated
to a file of the same name that might exist in someone else’s directory. A directory can contain
other directories as well as ordinary files … The natural way to picture this organization is as a
tree of directories and files. It is possible to move around within this tree, and to find any file in the system
by starting at the root of the tree and moving along the proper branches. Conversely, you can start where
you are and move toward the root.” <span class="citation">(Kernighan and Pike 1984)</span></p>
</blockquote>
<blockquote>
<p>“The name ‘/usr/you/junk’ is called the <em>pathname</em> of the file. ‘Pathname’ has an intuitive meaning:
it represents the full name of the path from the root through the tree of directories to a particular
file. It is a universal rule in the Unix system that wherever you can use an ordinary filename, you can
use a pathname.” <span class="citation">(Kernighan and Pike 1984)</span></p>
</blockquote>
<blockquote>
<p>“If you work regularly with Mary on information in her directory, you can say ‘I want to work on Mary’s
files instead of my own.’ This is done by changing your current directory with the <code>cd</code> command…
Now when you use a filename (without the /’s) as an argument to <code>cat</code> or <code>pr</code>, it refers to the file
in Mary’s directory. Changing directories doesn’t affect any permissions associated with a file—if you
couldn’t access a file from your own directory, changing to another directory won’t alter that fact.” <span class="citation">(Kernighan and Pike 1984)</span></p>
</blockquote>
<blockquote>
<p>“It is usually convenient to arrange your own files so that all the files related to one thing are in a
directory separate from other projects. For example, if you want to write a book, you might want to
keep all the text in a directory called ‘book.’” <span class="citation">(Kernighan and Pike 1984)</span></p>
</blockquote>
<blockquote>
<p>“Suppose you’re typing a large document like a book. Logically this divides into many small pieces,
like chapters and perhaps sections. Physically it should be divided too, because it is cumbersome
to edit large files. Thus you should type the document as a number of files. You might have separate
files for each chapter, called ‘ch1,’ ‘ch2,’ etc. … With a systematic naming convention, you can tell at
a glance where a particular file fits into the whole. What if you want to print the whole book? You could
say <code>$ pr ch1.1 ch1.2 ch 1.3 ...</code>, but you would soon get bored typing filenames and start to make mistakes.
This is where filename shorthand comes in. If you say <code>$ pr ch*</code> the shell takes the <code>*</code> to mean ‘any
string of characters,’ so ch* is a pattern that matches all filenames in the current directory that
begin with ch. The shell creates the list, in alphabetical order, and passes the list to <code>pr</code>. The
<code>pr</code> command never sees the <code>*</code>; the pattern match that the shell does in the current directory
generates aa list of strings that are passed to <code>pr</code>.” <span class="citation">(Kernighan and Pike 1984)</span></p>
</blockquote>
<blockquote>
<p>“The current directory is an attribute of a process, not a person or a program. … The notion of a
current directory is certainly a notational convenience, because it can save a lot of typing, but
its real purpose is organizational. Related files belong together in the same directory. ‘/usr’ is
often the top directory of a user file system… ‘/usr/you’ is your login directory, your current
directory when you first log in. … Whenever you embark on a new project, or whenever you have
a set of related files … you could create a new directory with <code>mkdir</code> and put the files there.” <span class="citation">(Kernighan and Pike 1984)</span></p>
</blockquote>
<blockquote>
<p>“Despite their fundamental properties inside the kernel, directories sit in the file system as
ordinary files. They can be read as ordinary files. But they can’t be created or written as
ordinary files—to preserve its sanity and the users’ files, the kernel reserves to itself all
control over the contents of directories.” <span class="citation">(Kernighan and Pike 1984)</span></p>
</blockquote>
<blockquote>
<p>“A file has several components: a name, contents, and administrative information such as
permissions and modifications times. The administrative information is stored in the inode
(over the years, the hyphen fell out of ‘i-node’), along with essential system data such as
how long it is, where on the disc the contents of the file are stored, and so on. …
It is important to understand inodes, not only to appreciate the options on <code>ls</code>, but because
in a strong sense the inodes <em>are</em> the files. All the directory hierarchy does is provide
convenient names for files. The system’s name for a file is its <em>i-number</em>: the number of the
inode holding the file’s information. … It is the i-number that is stored in the first two bytes
of a directory, before the name. …
The first two bytes in each directory entry are the only connection between the name of a file and its
contents. A filename in a directory is therefore called a <em>link</em>, because it links a name in the
directory hierarchy to the inode, and hence to the data. The same i-number can appear in more than
one directory. The <code>rm</code> command does not actually remove the inodes; it removes directory entries
or links. Only when the last link to a file disappears does the system remove the inode, and hence
the file itself. If the i-number in a directory entry is zero, it means that the link has been
removed, but not necessarily the contents of the file—there may still be a link somewhere else.” <span class="citation">(Kernighan and Pike 1984)</span></p>
</blockquote>
</div>
<div id="subsection-2" class="section level3" number="2.6.8">
<h3><span class="header-section-number">2.6.8</span> Subsection 2</h3>
<blockquote>
<p>“The file system is the part of the operating system that makes physical storage media
like disks, CDs and DVDs, removable memory devices, and other gadgets look like hierarchies
of files and folders. The file system is a great example of the distinction between
logical organization and physical implementation; file systems organize and store
information on many differet kinds of devices, but the operating system presents the
same interface for all of them.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>" A <em>folder</em> contains the names of other folders and files; examining a folder will
reveal more folders and files. (Unix systems traditionally use the word <em>directory</em>
instead of folder.) The folders provide the organizational structure, while the files
hold the actual contents of documents, pictures, music, spreadsheets, web pages, and
so on. All the information that you computer holds is stored in the file system and
is accessible through it if you poke around. This includes not only your data, but the
executable forms of programs (a browser, for example), libraries, device drivers, and the
files that make up the operating system itself. … The file system manages all this
information, making it accessible for reading and writing by applications and the rest of
the operating system. It coordinates accesses so they are performed efficiently and
don’t interfere with each other, it keeps track of where data is physically located,
and it ensures that the pieces are kept separate so that parts of your email don’t
mysteriously wind up in your spreadsheets or tax returns." <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“File system services are available through system calls at the lowest level,
usually supplemented by libraries to make common operations easy to program.”
<span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“The file system is a wonderful example of how a wide variety of physical
systems can be made to present a uniform logical appearance, a hierarchy of folders
and files.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“A folder is a file that contains information about where folders and files are
located. Because information about file contents and organization must be perfectly
accurate and consistent, the file system reserves to itself the right to manage and
maintain the contents of folders. Users and application programs can only change the
folder contents implicitly, by making requests of the file system.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“In fact, folders <em>are</em> files; there’s no difference in how they are stored except
that the file system is totally responsible for folder contents, and application
programs have no direct way to change them. But otherwise, it’s just blocks on the disk,
all managed by the same mechanisms.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“A folder entry for this [example] file would contain its name, its size of 2,500 bytes,
the date and time it was created or changed, and other miscellaneous facts about it
(permissions, type, etc., depending on the operating system). All of that information
is visible through a program like Explorer or Finder. The folder entry also contains
information about where the file is stored on disk—which of the 100 million blocks
[on the example computer’s hard disk] contain its bytes. There are different ways to
manage that location information. The folder entry could contain a list of block numbers;
it could refer to a block that itself contains a list of block numbers; or it could
contain the number of the first block, which in turn gives the second block, and so
on. … Blocks need not be physically adjacent on disk, and in fact they typically
won’t be, at least for large files. A megabyte file will occupy a thousand blocks, and
those are likely to be scattered to some degree. The folders and the block lists are
themselves stored in blocks…” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“When a program wants to access an existing file, the file system has to search for
the file starting at the root of the file system hierarchy, looking for each component
of the file path name in the corresponding folder. That is, if the file is
<code>/Users/bwk/book/book.txt</code> on a Mac, the file system will search the root of the file
system for <code>Users</code>, then search within that folder for <code>bwk</code>, then within that folder
for <code>book</code>, then within that for <code>book.txt</code>. … This is a divide-and-conquer strategy,
since each component of the path narrows the search to files and folders that lie within
that folder; all others are eliminated. Thus multiple files can have the same name for
some component; the only requirement is that the full path name be unique. In practice,
programs and the operating system keep track of the folder that is currenlty in use
so searches need not start from the root each time, and the system is likely to
cache frequently-used folders to speed up operations.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“When quitting R, the option is given to save the ‘workspace image.’ The workspace
consists of all values that have been created during a session—all of the data values
that have been stored in RAM. The workspace is saved as a file called <code>.Rdata</code> and then
R starts up, it checks for such a file in the current working directory and loads it
automatically. This provides a simple way of retaining the results of calculations from
one R session to the next. However, saving the entire R workspace is not the recommended
approach. It is better to save the original data set and R code and re-create results by
running the code again.” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“Just as a well-organized laboratory makes a scientist’s life easier, a well-organized
and well-documented project makes a bioinformatician’s life easier. Regardless of the
particular project you’re working on, your project directory should be laid out in a
consistent and understandable fashion. Clear project organization makes it easier
for both you and collaborators to figure out exactly where and what everything is.
Additionally, it’s much easier to automate tasks when files are organized and
clearly named. For example, processing 300 gene sequences stored in separate FASTA
files with a script is trivial if these files are organized in a single directory and
are consistently named.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Project directory organization isn’t just about being tidy, but is essential to the
way by which tasks are automated across large numbers of files” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“All files and directories used in your project should live in a single project directory
with a clear name. During the course of a project, you’ll have amassed data files, notes,
scripts, and so on—if these were scattered all over your hard drive (or worse, across
many computers’ hard drives), it would be a nightmare to keep track of everything. Even
worse, such a disordered project would later make your research nearly impossible to
reproduce.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Naming files and directories on a computer matters more than you may think. In
transitioning from a graphical user interface (GUI) based operating system to
the Unix command line, many folks bring the bad habit of using spaces in
file and directory names. This isn’t appropriate in a Unix-based environment, because
spaces are used to separate arguments in commands. … Although Unix doesn’t require
file extensions, including extensions in file names helps indicate the type of each
file. For example, a file named <em>osativa-genes.fasta</em> makes it clear that this is
a file of sequences in FASTA format. In contrast, a file named <em>osativa-genes</em> could
be a file of gene models, notes on where these <em>Oryza sativa</em> genes came from, or
sequence data. When in doubt, explicit is always better than implicit when it comes to
filenames, documentation, and writing code.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Scripts and analyses often need to refer to other files (such as data) in your
project hierarchy. This may require referring to parent directories in you directory’s
hierarcy … In these cases, it’s important to always use <em>relative paths</em> … rather
than <em>absolute paths</em> … As long as your internal project directory structure remains the
same, these relative paths will always work. In contrast, absolute paths rely on you particular
user account and directory structures details <em>above</em> the project directory level
(not good). Using absolute paths leaves your work less portable between collaborators and
decreases reproducibility.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“<em>Document the origin of all data in your project directory.</em> You need to keep track of
where data was downloaded from, who gave it to you, and any other relevant information.
‘Data’ doesn’t just refer to your project’s experimental data—it’s any data that
programs use to create output. This includes files your collaborators send you from their
separate analyses, gene annotation tracks, reference genomes, and so on. It’s critical
to record this important data about you’re data, or <em>metadata</em>. For example, if you downloaded
a set of genic regions, record the website’s URL. This seems like an obvious recommendation,
but ocuntless times I’ve encountered an analysis step that couldn’t be easily reproduced
because someone forgot to record the data’s source.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“<em>Record data version information.</em> Many databases have explicit release numbers,
version numbers, or names (e.g., TAIR10 version of genome annotation for <em>Arabidopsis
thaliana</em>, or Wormbase release WS231 for <em>Caenorhabditis elegans</em>). It’s important to
record all version information in your documentation, including minor version numbers.”
<span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“<em>Describe how you downloaded the data.</em> For example, did you use MySQL to download a
set of genes? Or the USCS Genome Browser? THese details can be useful in tracking down
issues like when data is different between collaborators.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Bioinformatics projects involve many subprojects and subanalyses. For example, the
quality of raw experimental data should be assessed and poor quality regions removed
before running it through bioinformatics tools like aligners or assemblers. … Even
before you get to actually analyzing the sequences, your project directory can get
cluttered with intermediate files. Creating directories to logically separate subprojects
(e.g., sequencing data quality improvement, aligning, analyzing alignment results, etc.)
can simplify complex projects and help keep files organized. It also helps reduce the
risk of accidentally clobbering a file with a buggy script, as subdirectories help
isolate mishaps. Breaking a project down into subprojects and keeping these in separate
subdirectories also makes documenting your work easier; each README pertains to the
directory it resides in. Ultimately, you’ll arrive at your own project organization
system that works for you; the take-home point is: leverage directories to help stay
organized.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Because automating file processing tasks is an integral part of bioinformatics,
organizing our projects to facilitate this is essential. Organizing data into subdirectories
and using clear and consistent file naming schemes is imperative—both of these practices
allow us to <em>programmatically</em> refer to files, the first step to automating a task.
Doing something programatically means doing it through code rather than manually, using
a method that can effortlessly scale to multiple objects (e.g., files). Programatically
referring to multiple files is easier and safer than typing them all out (because it’s
less error prone.)” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Organizing data files into a single directory with consistent filenames prepares us to
iterate over <em>all</em> of our data, whether it’s the four example files used in this example,
or 40,000 files in a real project. Think of it this way: remember when you discovered you
could select many files with your mouse cursor? With this trick, you could move 60 files
as easily as six files. You could also select certain file types (e.g., photos) and attach
them all to an email with one movement. By using consistent file naming and directory
organization, you can do the same programatically using the Unix shell and other
programming languages.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Because lots of daily bioinformatics work involves file processing, programmatically
accessing files makes our job easier and eliminates mistakes from mistyping a filename
or forgetting a sample. However, our ability to programmatically access files with
wildcards (or other methods in R or Python) is only possible when our filenames are
consistent. While wildcards are powerful, they’re useless if files are inconsistently
named. … Unfortunately, inconsistent naming is widespread across biology, and is
the source of bioinformaticians everywhere. Collectively, bioinformaticians have
probably wasted thousands of hours fighting others’ poor naming schemes of files,
genes, and in code.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Another useful trick is to use leading zeros … when naming files. This is useful
because lexicographically sorting files (as <code>ls</code> does) leads to correct ordering. …
Using leading zeros isn’t just useful when naming filenames; this is also the best
way to name genes, transcripts, and so on. Projects like Ensembl use this naming
scheme in naming their genes (e.g., ENSG00000164256).” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“In addition to simplifying working with files, consistent naming is an often overlooked
component of robust bioinformatics. Bad naming schemes can easily lead to switched samples.
Poorly chosen filenames can also cause serious errors when you or collaborators think you’re
working with the correct data, but it’s actually outdate or the wrong file. I guarantee
that out of all the papers published in the past decade, at least a few and likely many
more contain erroneous results because of a file naming issue.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“In order to read or write a file, the first thing we need to be able to do is specify
<em>which</em> file we want to work with. Any function that works with a file requires a
precise description of the name of the file and the location of the file. A filename
is just a character value…, but identifying the location of a file can involve a
<strong>path</strong>, which describes a location on a persistent storage medium, such as a hard drive.”
<span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“A regular expression consists of a mixture of <strong>literal</strong> characters, which have their
normal meaning, and <strong>metacharacters</strong>, which have a special meaning. The combination
describes a <strong>pattern</strong> that can be used to find matches amongst text values.” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“A regular expression may be as simple as a literal word, such as <code>cat</code>, but regular
expressions can also be quite complex and express sophisticated ideas, such as
<code>[a-z]{3,4}[0-9]{3}</code>, which describes a pattern consisting of either three or four
lowercase letters followed by any three digits.” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“… it’s important to mind R’s working directory. Scripts should <em>not</em> use
<code>setwd()</code> to set their working directory, as this is not portable to other
systems (which won’t have the same directory structure). For the same reason,
use <em>relative</em> paths … when loading in data, and <em>not</em> absolute pathers…
Also, it’s a good idea to indicate (either in comments or a README file)
which directory the user should set as their working directory.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“<strong>Centralize the location of the raw data files and automate the derivation of
intermediate data.</strong> Store the input data on a centralized file server that is
profesionally backed up. Mark the files as read-only. Have a clear and linear
workflow for computing the derived data (e.g., normalized, summarized, transformed,
etc.) from the raw files, and store these in a separate directory. Anticipate that
this workflow will need to be run several times, and version it. Use the
<code>BiocFileCache</code> package to mirror these files on your personal computer.
[footnote: A more basic alternative is the rsync utility. A popular solution offered
by some organizations is based on ownCloud. Commercial options are Dropbox,
Google Drive and the like].” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
</div>
<div id="practice-quiz-1" class="section level3" number="2.6.9">
<h3><span class="header-section-number">2.6.9</span> Practice quiz</h3>

</div>
</div>
<div id="module7" class="section level2" number="2.7">
<h2><span class="header-section-number">2.7</span> Creating ‘Project’ templates</h2>
<p>Researchers can use RStudio’s ‘Projects’ can facilitate collecting research
files in a single, structured directory, with the added benefit of easy use of
version control. Researchers can gain even more benefits by consistently
structuring all their ‘Project’ directories. We will demonstrate how to
implement structured project directories through RStudio, as well as how RStudio
enables the creation of a ‘Project’ for initializing consistently-structured
directories for all of a research group’s projects.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Be able to create a structured <code>Project</code> directory within RStudio</li>
<li>Understand how RStudio can be used to create ‘Project’ templates</li>
</ul>
<p>The last module describe the advantages of organizing all the files for a
research project within a single file directory, as well as the added
advantages of making that file directory an RStudio “Project.” In this
module, we’ll walk through the steps required to do that, as well as
how to navigate and use the “Project” structure and functionality to
make it easier to integrate project files, code, and final output like
reports and presentation slides. If you have created a standardized
file directory structure that you will use as a starting point for all of
your research projects, then it can save time to create an RStudio Project
Template with this structure. This way, you can set up a new directory,
including all the subdirectories you want to use and any code or report
templates that might be useful, by just opening a new project with this
template through RStudio.</p>
<div id="making-an-existing-file-directory-an-rstudio-project" class="section level3" number="2.7.1">
<h3><span class="header-section-number">2.7.1</span> Making an existing file directory an RStudio Project</h3>
<p>It’s very easy to turn an existing file directory into an RStudio Project.
Open RStudio, and then in its menu go to “File” and then “New Project.”
This will open a pop-up window with several options, including the option to
create a new RStudio Project from an existing directory. Choose this option,
and then in the window that is opened, navigate through your file directories
to the directory with your project files.</p>
<p>This will create a new RStudio Project with the same name as the file directory
name of the directory you selected [check this]. Once you have created the
directory, RStudio will automatically move you into that Project. When you
close RStudio and reopen it, it will automatically open in the last Project
you had open. There is a small tab in the top right hand corner of the RStudio
window that lists the project you are currently in. To move to a different Project,
you can click on the down arrow beside this project name. There will be a list
of your most recent projects, as well as options to open any Project on your
computer. If you want to work in RStudio, but not in any of the Projects, you can
choose to “Close Project.”</p>
<p>When you are working in an RStudio Project, RStudio will automatically move your
working directory to be the top-level directory of the Project directory. This
makes it easy to write code that uses this directory as the presumed working
directory, using relative file paths to identify and files within the directory.
For example … . Now if you share the project directory with someone else, they
can similarly open the RStudio Project in their own version of RStudio, and all
the relative pathnames to files should work on their system without any problems.
This feature helps make code in an RStudio Project directory reproducible across
different people’s computers.</p>
<p>The RStudio Project environment has some other features, as well, that may be
useful for some projects. For example, if you are tracking the project directory
under the git version control system, then when you open the RStudio Project,
there will be a special tab in one of the panes to help in using git with the
project. This tab provides a visual interface for you to commit changes you’ve made,
so they are tracked and can be reversed if needed, and also so you can easily
push and pull these committed changes to and from a remote repository, like a
GitHub repository, if you are collaborating with others.</p>
<p>For certain types of projects, you may also want to include a “Makefile.” [More
about Makefiles]. If you add this to an RStudio Project, you will get a new
“Build” tab that allows you to run the Makefile for the project with the click of
a button. For some more complex RStudio Projects, like projects that use RMarkdown
to create online books or websites using <code>bookdown</code> and <code>blogdown</code>, this “Build”
pane will allow you to render the whole book.</p>
</div>
<div id="making-an-rstudio-project-template" class="section level3" number="2.7.2">
<h3><span class="header-section-number">2.7.2</span> Making an RStudio Project Template</h3>
<p>If you have created a template directory for research projects for your group,
you can create an RStudio Project template to make it easy to set up a new
project directory every time you start a project. At its most basic, this can be
a directory that includes the subdirectories (with standardized names for each
subdirectory) that you want to include—for example, you may know that you will
always want the project directory to include subdirectories for “raw_data” (with
its own subdirectories for different types of data, for example for “cfus” and
“flow”), “data” (with clean versions of the data, after conducting and needed
preprocessing, like calculating CFUs in a sample based on data from plating at
different dilutions, or the output from gating flow cytometry data), “reports”
(for writing, posters, and presentation slides), and “R” (for common scripts
that you use for preprocessing, visualization, and data analysis).</p>
<p>As you progress, you may also want to add templates that serve as a starting point
for files within this project. For example, if you always want to collect
observed data in a standard way, you could create a template for data collection,
for example as a CSV file. Each researcher in your lab could copy and rename this
file each time they collect a new set of data—by ensuring a common structure
when collecting the data, including file format, column names, and so on, you can
build code scripts that will work on data collected for all your experiments. You
may also have some standard reports that you want to create with types of data
you commonly collect, and so you could include templates for those reports in your
R Project template. Again, these can be copied and adapted within the project—the
template serves as a starting point so you don’t have to start with a blank slate
with every project, but it is not restrictive and can be adapted to each project
as you work on that project.</p>
<p>[How to create a template]</p>
<p>You can have different templates to use when you create a new project. A
template will provide a basic set-up for the project. For example, R packages
are created within a directory, and package developers now often use an R
Project for that directory. Since R packages have a consistent set of usual
subdirectories—including subdirectories for R code, data, and documentation
and a file for metadata—there is now a Project Template specificially for R
package development, and this sets up a directory with some of the typical
subdirectories and files needed for these projects.</p>
<p>You may find, similarly, that there are typical structures and files that your
research laboratory includes in the R Projects it creates for research projects.
In this case, someone in your lab can create an R Project template that can
be used each time a new project is initialized for the lab. This will help
create consistency across projects in the directory structure, which can
facilitate the use and re-use of automated tools like code scripts across
different experiments.</p>
<p>When you create a new project in R, you will have the option to use any of
the available project templates currently downloaded to your copy of R
<span class="citation">(<span>“<span>RStudio Project Templates</span>”</span> 2021)</span>. To create a new project, go to the “File” menu
in the top menu bar in RStudio, and then choose “New Project.” This will open
a pop-up box like the one shown in Figure <a href="experimental-data-recording.html#fig:createnewproject">2.26</a>.</p>
<div class="figure"><span style="display:block;" id="fig:createnewproject"></span>
<img src="figures/create_new_project.png" alt="Creating a new project in RStudio. When you chose 'File' then 'New Project' from the RStudio menu, it opens the New Project Wizard shown here. You have the option to create a new project that is not based on a project template by selecting 'New Project'. You also have the chance to create a project using a template by selecting one of the templates. The listed templates will depend on which packages you have downloaded for your copy of R. For example, here the `bookdown` package has been installed for the local copy of R, and so a template is available for 'Book Project using bookdown'." width="\textwidth" />
<p class="caption">
Figure 2.26: Creating a new project in RStudio. When you chose ‘File’ then ‘New Project’ from the RStudio menu, it opens the New Project Wizard shown here. You have the option to create a new project that is not based on a project template by selecting ‘New Project.’ You also have the chance to create a project using a template by selecting one of the templates. The listed templates will depend on which packages you have downloaded for your copy of R. For example, here the <code>bookdown</code> package has been installed for the local copy of R, and so a template is available for ‘Book Project using bookdown.’
</p>
</div>
<p>This pop-up contains the New Project Wizard in RStudio. Here, you can either
create a new Project without using a template (click on “New Project”) or you
can create a Project starting from a template. The templates available in your
copy of R will be listed below the “New Project” listing. Depending on which
packages you’ve installed for your copy of R, you will have different choices of
project templates available, as project templates tend to be created and shared
within R packages <span class="citation">(<span>“<span>RStudio Project Templates</span>”</span> 2021)</span>. In the example shown in
Figure <a href="experimental-data-recording.html#fig:createnewproject">2.26</a>, for example, one of the template options is
for a “Book Project using bookdown,” available because the <code>bookdown</code> R package
has been installed locally.</p>
</div>
</div>
<div id="creating-project-templates-in-rstudio" class="section level2" number="2.8">
<h2><span class="header-section-number">2.8</span> Creating ‘Project’ templates in RStudio</h2>
<p>Your research group can create your own Project templates. You will need to
create them within an R package, but this package does not need to be posted to
a public site like CRAN. Instead, it can be shared exclusively among the
research group as a zipped file that can be installed directly from source onto
each person’s computer. Alternatively, you can post the package code as a GitHub
repository, and there are straightforward tools for installing R package code
from GitHub onto each team member’s computer. RStudio has provided a
detailed guide to creating your own project template at
<a href="https://rstudio.github.io/rstudio-extensions/rstudio_project_templates.html" class="uri">https://rstudio.github.io/rstudio-extensions/rstudio_project_templates.html</a>.
This topic has also been discussed through a short talk at the
yearly RStudio::conf: <a href="https://rstudio.com/resources/rstudioconf-2020/rproject-templates-to-automate-and-standardize-your-workflow/" class="uri">https://rstudio.com/resources/rstudioconf-2020/rproject-templates-to-automate-and-standardize-your-workflow/</a>.</p>
<blockquote>
<p>“RStudio v1.1 introduces support for custom, user-defined project templates.
Project templates can be used to create new projects with a pre-specified
structure.” <span class="citation">(<span>“<span>RStudio Project Templates</span>”</span> 2021)</span></p>
</blockquote>
<blockquote>
<p>“R packages are the primary vehicle through which RStudio project templates
are distributed. Package authors can provide a small bit of metadata describing
the template functions available in their package—RStudio will discover these
project templates on start up, and make them available in the New Project…
dialog.” <span class="citation">(<span>“<span>RStudio Project Templates</span>”</span> 2021)</span></p>
</blockquote>
<blockquote>
<p>“R experts keep all the files associated with a project together—input data,
R scripts, analytical results, figures. This is such a wise and common practice
that RStudio has built-in support for this via <strong>projects</strong>.” <span class="citation">(Wickham and Grolemund 2016)</span></p>
</blockquote>
<div id="discussion-questions-1" class="section level3" number="2.8.1">
<h3><span class="header-section-number">2.8.1</span> Discussion questions</h3>

</div>
</div>
<div id="module8" class="section level2" number="2.9">
<h2><span class="header-section-number">2.9</span> Example: Creating a ‘Project’ template</h2>
<p>We will walk through a real example, based on the experiences of one of our
Co-Is, of establishing the format for a research group’s ‘Project’ template,
creating that template using RStudio, and initializing a new research project
directory using the created template. This example will be from a
laboratory-based research group that studies the efficacy of tuberculosis drugs
in a murine model.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Create a ‘Project’ template in RStudio to initialize consistently-formatted
‘Project’ directories</li>
<li>Initialize a new ‘Project’ directory using this template</li>
</ul>
<div id="subsection-1-1" class="section level3" number="2.9.1">
<h3><span class="header-section-number">2.9.1</span> Subsection 1</h3>
</div>
<div id="subsection-2-1" class="section level3" number="2.9.2">
<h3><span class="header-section-number">2.9.2</span> Subsection 2</h3>
</div>
<div id="applied-exercise-1" class="section level3" number="2.9.3">
<h3><span class="header-section-number">2.9.3</span> Applied exercise</h3>

</div>
</div>
<div id="module9" class="section level2" number="2.10">
<h2><span class="header-section-number">2.10</span> Harnessing version control for transparent data recording</h2>
<p>As a research project progresses, a typical practice in many experimental
research groups is to save new versions of files (e.g., ‘draft1.doc,’
‘draft2.doc’), so that changes can be reverted. However, this practice leads to
an explosion of files, and it becomes hard to track which files represent the
‘current’ state of a project. Version control allows researchers to edit and
change research project files more cleanly, while maintaining the power to
‘backtrack’ to previous versions, messages included to explain changes. We will
explain what version control is and how it can be used in research projects to
improve the transparency and reproducibility of research, particularly for data
recording.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Describe version control<br />
</li>
<li>Explain how version control can be used to improve reproducibility
for data recording</li>
</ul>
<div id="what-is-version-control" class="section level3" number="2.10.1">
<h3><span class="header-section-number">2.10.1</span> What is version control?</h3>
<p>Version control developed as a way to coordinate collaborative work on software
programming projects. The term “version” here refers to the current state of a
document or set of documents, for example the source code for a computer
program. The idea of “control” is to allow for safe changes and updates to this
version while more than one person is working on it. The general term “version
control” can refer to any method of syncing contributions from several people to
a file or set of files, and very early on it was done by people rather than
through a computer program. While version control of computer files can be done
by people, and originally was <span class="citation">(Irving 2011)</span>, it’s much more
efficient to use a computer program to handle this tracking of the history of a
set of files as they evolve.</p>
<blockquote>
<p>“Tracking all that detail is just the sort of thing computers
are good at and humans are not.” <span class="citation">(E. S. Raymond 2003)</span></p>
</blockquote>
<p>While the very earliest version control systems tracked single files, these
systems quickly moved to tracking sets of files, called <em>repositories</em>. You can
think of a repository as a computer file directory with some extra overhead
added to record how the files in the directory have changed over time. In a
repository of files that is under version control, you take regular “snapshots”
of how the files look during your work on them. Each snapshot is called a
<em>commit</em>, and it provides a record of which lines in each file changed from one
snapshot to another, as well as exactly how they changed. The idea behind these
commits—recording the differences, line-by-line, between an older and newer
version of each file derives from a longstanding Unix command line tool called
<code>diff</code>. This tool, developed early in the history of Unix at AT&amp;T
<span class="citation">(E. S. Raymond 2003)</span>, is and extremely solid and well-tested tool that did the
simple but important job of generating a list of all the differences between two
plain text files.</p>
<p>When you are working with a directory under version control, you can also
explain your changes as you make them—in other words, it allows for
<em>annotation</em> of the developing and editing process <span class="citation">(E. Raymond 2009)</span>. Each
commit requires you to enter a <em>commit message</em> describing why the change was
made. The commit messages can serve as a powerful tool for explaining changes to
other team members or for reminding yourself in the future about why certain
changes were made. As a result, a repository under version control includes a
complete history of how files in a project directory have changed over the
timecourse of the project and why. Further, each of the commits is given its own
ID tag (a unique SHA-1 hash), and version control systems have a number of
commands that let you “roll back” to earlier versions, by going back to the
version as it was when a certain commit was made, provided <em>reversability</em>
within the project files <span class="citation">(E. Raymond 2009)</span>.</p>
<p>It turns out that this functionality—of being able to “roll back” to earlier
versions—has a wonderful side benefit when it comes to working on a large
project. It means that you <em>don’t</em> need to save earlier versions of each file.
You can maintain one and only one version of each project file in the project’s
directory, with the confidence that you never “lose” old versions of the file
<span class="citation">(J. Perkel 2018; Blischak, Davenport, and Wilson 2016)</span>. This allows you to maintain a clean and
simple version of the project files, with only one copy of each, ensuring it’s
always clear which version of a file is the “current” one (since there’s only
one version). This also provides the reassurance that you can try new directions
in a project, and always roll back to the old version if that direction doesn’t
work well.</p>
<blockquote>
<p>“Early in his graduate career, John Blischak found himself creating figures
for his advisor’s grant application. Blischak was using the programming language
R to generate the figures, and as he iterated and optimized his code, he ran
into a familiar problem: Determined not to lose his work, he gave each new
version a different filename—analysis_1, analysis_2, and so on, for
instance—but failed to document how they had evolved. ‘I had no idea what had
changed between them,’ says Blischak… Using Git, Blischak says, he no longer
needed to maintain multiple copies of his files. ‘I just keep overwriting it and
changing it and saving the snapshots. And if the professor comes back and says,
’oh, you sent me an email back in March with this figure,’ I can say, ‘okay,
well, I’ll just bo back to the March version of my code and I can recreate
it.’” <span class="citation">(J. Perkel 2018)</span></p>
</blockquote>
<p>Finally, most current version control systems operate under a <em>distributed</em>
framework. In earlier types of version control programs, there was one central
(“main”) repository for the file or set of files the team was working on
<span class="citation">(E. Raymond 2009; Target 2018)</span>. Very early on, this was kept on one
computer <span class="citation">(Irving 2011)</span>. A team member who wanted to make a change
would “check out” the file he or she wanted to work on, make changes, and then
check it back in as the newest main version <span class="citation">(E. S. Raymond 2003)</span>. While one team
member had this file checked out, other members would often be “locked” out of
making any changes to that file—they could look at it, but couldn’t make any
edits <span class="citation">(E. Raymond 2009; Target 2018)</span>. This meant that there was no
chance of two people trying to change the same part of a file at the same time.
In spirit, this early system is pretty similar to the idea of sending a file
around the team by email, with the understanding that only one person works on
it at a time. While the “main” version is in different people’s hands at
different times, to do work, you all agree that only one person will work on it
at a time. A slightly more modern analogy is the idea of having a single version
of a file in Dropbox or Google Docs, and avoiding working on the file when you
see that another team member is working on it.</p>
<p>This system is pretty clunky, though. In particular, it usually increases the
amount of time that it takes the team to finish the project, because only one
person can work on a file at a time. Later types of version control programs
moved toward a different style, allowing for <em>distributed</em> rather than
<em>centralized</em> collaborative work on a file or a set of files
<span class="citation">(E. Raymond 2009; Irving 2011)</span>. Under the distributed model,
all team members can have their own version of all the files, work on them and
make records of changes they make to the files, and then occassionally sync with
everyone else to share your changes with them and bring their changes into your
copy of the files. This distributed model also means there is a copy of the full
repository on every team members computer, which has the side benefit of
provided natural backup of the project files. Remote repositories—which may be
on a server in a different location—can be added with another copy of the
project, which can similarly be synced regularly to update with any changes made
to project files.</p>
<p>While there are a number of software systems for version control, by far the
most common currently used for scienctific projects is <em>git</em>. This program was
created by Linus Torvalds, who also created the Linux operating system, in 2005
as a way to facilitate the team working on Linux development. This program for
version control thrives in large collaborative projects, for example open-source
software development projects that include numerous contributors, both regular
and occasional <span class="citation">(Brown 2018)</span>.</p>
<p>In recent years, some complementary tools have been developed that make the
process of collaborating together using version control software easier. Other
tools can helps in collaborating on file-based projects, including <em>bug
trackers</em> or <em>issue trackers</em>, which allow the team to keep a running “to-do”
list of what needs to be done to complete the project, all of which are
discussed in the next chapter as tools that can be used to improve collaboration
on scientific projects spread across teams. GitHub, a very popular version
control platform with these additional tools, was created in 2008 as a web-based
platform to facilitate collaborating on projects running under git version
control. It can provide an easier entry to using git for version control than
trying to learn to use git from the command line <span class="citation">(Perez-Riverol et al. 2016)</span>. It also plays
well with RStudio, making it easy to integrate a collaborative workflow through
GitHub from the same RStudio window on your computer where you are otherwise
doing your analysis <span class="citation">(Perez-Riverol et al. 2016)</span>.</p>
<blockquote>
<p>“If your software engineering career, like mine, is no older than GitHub, then
git may be the only version control software you have ever used. While people
sometimes grouse about its steep learning curve or unintuitive interface, git has
become everyone’s go-to for version control.” <span class="citation">(Target 2018)</span></p>
</blockquote>
</div>
<div id="recording-data-in-the-laboratoryfrom-paper-to-computers" class="section level3" number="2.10.2">
<h3><span class="header-section-number">2.10.2</span> Recording data in the laboratory—from paper to computers</h3>
<p>Traditionally, experimental data collected in a laboratory was recorded in a
paper laboratory notebook. These laboratory notebooks played a role not only as
the initial recording of data, but also can serve as, for example, a legal
record of the data recorded in the lab <span class="citation">(Mascarelli 2014)</span>. They were also
a resource for collaborating across a team and for passing on a research project
from one lab member to another <span class="citation">(Butler 2005)</span>.</p>
<p>However, paper laboratory notebooks have a number of limitations. First, they
can be very inefficient. In a time when almost all data analyses—even simple
calculations—are done on a computer, recording research data on paper rather
than directly entering it into a computer is inefficient. Also, any stage of
copying data from one format to another, especially when done by a human rather
than a machine, introduces the chance to copying errors. Handwritten laboratory
notebooks can be hard to read <span class="citation">(Butler 2005; J. M. Perkel 2011)</span>, and
may lack adequate flexibility and expandability to handle the complex
experiments often conducted. Further, electronic alternatives can also be easier
to search, allowing for deeper and more comprehensive investigations of the data
collected across multiple experiments <span class="citation">(Giles 2012; Butler 2005; J. M. Perkel 2011)</span>.</p>
<blockquote>
<p>“Handwritten lab notebooks are usually chaotic and always unsearchable.”
<span class="citation">(J. M. Perkel 2011)</span></p>
</blockquote>
<p>Given a widespread recognition of the limitations of paper laboratory notebooks,
in the past couple of decades, there have been a number of efforts, both formal
and informal, to move from paper laboratory notebooks to electronic
alternatives. In some fields that rely heavily on computational analysis, there
are very few research labs (if any) that use paper laboratory notebooks
<span class="citation">(Butler 2005)</span>. In other fields, where researchers have traditionally
used paper lab notebooks, companies have been working for a while to develop
electronic laboratory notebooks specifically tailored to scientific research
needs <span class="citation">(Giles 2012)</span>. These were adopted more early in pharmaceutical
industrial labs, where companies had the budgets to get customized versions and
the authority to require their use, but have taken longer to be adapted in
academic laboratories <span class="citation">(Giles 2012; Butler 2005)</span>. A widely
adopted platform for electronic laboratory notebooks has yet to be taken up
by the scientific community, despite clear advantages of recording data directly
into a computer rather than first using a paper notebook.</p>
<blockquote>
<p>“Since at least the 1990s, articles on technology have predicted the imminent,
widespread adoption of electronic laboratory notebooks (ELNs) by researchers. It has
yet to happen—but more and more scientists are taking the plunge.” <span class="citation">(Kwok 2018)</span></p>
</blockquote>
<p>Instead of using customized electronic laboratory notebook software, some
academics are moving their data recording online, but are using more generalized
electronic alternatives, like Dropbox, Google applications, OneNote, and
Evernote <span class="citation">(J. M. Perkel 2011; Kwok 2018; Giles 2012; K. Powell 2012)</span>.
Some scientists have started using version control tools, especially the
combination of git and GitHub, as a way to improve laboratory data recording,
and in particular to improve transparency and reproducibility standards.
These pieces of software share the same pattern as Google tools or
Dropbox—they are generalized tools that have been honed and optimized for ease
of use through their role outside of scientific research, but can be harnessed
as a powerful tool in a scientific laboratory, as well. They are also free—at
least, for GitHub, at the entry and academic levels—and, even better, one
(git) is open source.</p>
<blockquote>
<p>“The purpose of a lab notebook is to provide a lasting record of events in a
laboratory. In the same way that a chemistry experiment would be nearly
impossible without a lab notebook, scientific computing would be a nightmare of
inefficiency and uncertainty without version-control systems.”
<span class="citation">(Tippmann 2014)</span></p>
</blockquote>
<p>While some generalized tools like Google tools and Dropbox might be simpler to
initially learn, version control tools offer some key advantages for recording
scientific data and are worth the effort to adopt. A key advantage is their
ability to track the full history of files as they evolve, including not only
the history of changes to each file, but also a record of why each change was
made. Git excels in tracking changes made to plain text
files. For these files, whether they record code, data, or text, git can show
line-by-line differences between two versions of the file. This makes it very
easy to go through the history of “commits” to a plain text file in a
git-tracked repository and see what change was made at each time point, and then
read through the commit messages associated with those commits to see why a
change was made. For example, if a value was entered in the wrong row of a csv,
and the researcher then made a commit to correct that data entry mistake, the
researcher could explain the problem and its resolution in the commit message
for that change.</p>
<p>Platforms for using git often include nice tools for visualizing differences
between two files, providing a more visual way to look at the “diffs” between
files across time points in the project. For example, GitHub automatically shows
these using colors to highlight addititions and substractions of plain text for
one file compared to another version of it when you look through a repository’s
commit history. Similarly, RStudio provides a new “Commit” window that can be
used to compare differences between the original and revised version of plain
text files at a particular stage in the commit history.</p>
<!-- [Maybe include a figure with an example of how the difference between two  -->
<!-- text files can be seen, along with a commit message explaining the change?] -->
<p>The use of version control tools and platforms, like git and GitHub, not only
helps in transparent and trackable recording of data, but it also brings some
additional advantages in the research project. First, this combination of tools
aids in collaboration across a research group, as we discuss in depth in the next
chapter.</p>
<p>Second, if a project uses these tools, it is very easy to share data recorded
for the project publicly. In a project that uses git and GitHub version control
tools, it is easy to share the project data online once an associated manuscript
is published, an increasingly common request or requirement from journals and
funding agencies <span class="citation">(Blischak, Davenport, and Wilson 2016)</span>. Sharing data allows a more complete
assessment of the research by reviewers and readers and makes it easier for
other researchers to build off the published results in their own work,
extending and adapting the code to explore their own datasets or ask their own
research questions <span class="citation">(Perez-Riverol et al. 2016)</span>. On GitHub, you can set the access to a
project to be either public or private, and can be converted easily from one
form to the other over the course of the project <span class="citation">(Metz 2015)</span>. A private
project can be viewed only by fellow team members, while a public project can be
viewed by anyone. Further, because git tracks the full history of changes to
these documents, it includes functionality that let’s you tag the code and data
at a specific point (for example, the date when a paper was submitted) so that
viewers can look at that specific “version” of the repository files, even while
the project team continues to move forward in improving files in the directory.
At the more advanced end of functionality, there are even ways to assign a DOI
to a specific version of a GitHub repository <span class="citation">(Perez-Riverol et al. 2016)</span>.</p>
<p>Third, the combination of git and GitHub can help as a way to backup study data
<span class="citation">(Blischak, Davenport, and Wilson 2016; Perez-Riverol et al. 2016; J. Perkel 2018)</span>. Together, git and GitHub
provide a structure where the project directory (repository) is copied on
multiple computers, both the users’ laptop or desktop computers and on a remote
server hosted by GitHub or a similar organization. This set-up makes it easy to
bring all the project files onto a new computer—all you have to do is clone
the project repository. It also ensures that there are copies of the full
project directory, including all its files, in multiple places
<span class="citation">(Blischak, Davenport, and Wilson 2016)</span>. Further, not only is the data backed up across multiple
computers, but so is the full history of all changes made to that data and the
recorded messages explaining those changes, through the repositories commit
messages <span class="citation">(Perez-Riverol et al. 2016)</span>.</p>
<p>There are, of course, some limitations to using version control tools when
recording experimental data. First, while ideally laboratory data is recorded in
a plain text format (see the module in section 2.2 for a deeper discussion of
why), some data may be recorded in a binary file format. Some version control
tools, including git, can be used to track changes in binary files. However, git
does not take to these types of files naturally. In particular, git typically
will not be able to show users a useful comparison of the differences between
two versions of a binary file. More problems can arise if the binary file is
very large <span class="citation">(Perez-Riverol et al. 2016; Blischak, Davenport, and Wilson 2016)</span>, as some experimental research
data files are (e.g., if they are high-throughput output of laboratory equipment
like a mass spectrometer). However, there are emerging tools and strategies for
improving the ability to include and track large binary files when using git and
GitHub <span class="citation">(Blischak, Davenport, and Wilson 2016)</span></p>
<blockquote>
<p>“You can version control any file that you put in a Git repository, whether it is
text-based, an image, or a giant data file. However, just because you <em>can</em> version
control something, does not mean that you <em>should</em>.” <span class="citation">(Blischak, Davenport, and Wilson 2016)</span></p>
</blockquote>
<!-- [Add something about whether version control timestamps would meet legal standards,  -->
<!-- e.g., for patent claims? Mike had a comment about using locks with data that might -->
<!-- go with that.] -->
<p>Finally, as with other tools and techniques described in this book, there is an
investment required to learn how to use git and GitHub <span class="citation">(Perez-Riverol et al. 2016)</span>, as well
as a bit of extra overhead when using version control tools in a project
<span class="citation">(E. S. Raymond 2003)</span>. However, both can bring dramatic gains to efficiency,
transparency, and organization of research projects, even if you only use a
small subset of its basic functionality <span class="citation">(Perez-Riverol et al. 2016)</span>. In Chapter 11 we
provide guidance on getting started with using git and Github to track a
scientific research project.</p>
<blockquote>
<p>“Although Git has a complex set of commands and can be used for rather complex
operations, learning to apply the basics requires only a handful of new concepts
and commands and will provide a solid ground to efficiently track code and related
content for research projects.” <span class="citation">(Perez-Riverol et al. 2016)</span></p>
</blockquote>
</div>
<div id="discussion-questions-2" class="section level3" number="2.10.3">
<h3><span class="header-section-number">2.10.3</span> Discussion questions</h3>
<hr />
<blockquote>
<p>“Using an RCS [revision control system] has changed how I work. … a day’s
work is no longer a featureless slog toward the summit, but a sequence of small
steps. What one feature could I add? What one problem could I fix? Once a step is
made and you are sure your code base is in a safe and clean state, commit a revision,
and if your next step turns out disastrously, you can fall back to the revision you
just committed instead of starting from the beginning.” <span class="citation">(Klemens 2014)</span></p>
</blockquote>
<blockquote>
<p>With version control, “Our filesystem now has a time dimension. We can query the
RCS’s repository of file information to see what a file looked like last week and
how it changed from then to now. Even without the other powers, I have found that
this alone makes me a more confident writer.” <span class="citation">(Klemens 2014)</span></p>
</blockquote>
<blockquote>
<p>“The most rudimentary means of revision control is via <code>diff</code> and <code>patch</code>, which
are POSIX-standard and therefore most certainly on your system.” <span class="citation">(Klemens 2014)</span></p>
</blockquote>
<blockquote>
<p>“Git is a C program like any other, and is based on a small set of objects.
The key object is the commit object, which is akin to a unified diff file.
Given a previous commit object and some changes from that baseline, a new commit
object encapsulates the information. It gets some support from the <em>index</em>, which
is a list of the changes registered since the last commit object, the primary
use of which will be in generating the next commit object. The commit objects
link together to form a tree much like any other tree. Each commit object will
have (at least) one parent commit object. Stepping up and down the tree is akin to
using <code>patch</code> and <code>patch -R</code> to step among versions.” <span class="citation">(Klemens 2014)</span></p>
</blockquote>
<blockquote>
<p>“Having a backup system organized enough that you can delete code with confidence
and recover as needed will already make you a better writer.” <span class="citation">(Klemens 2014)</span></p>
</blockquote>

</div>
</div>
<div id="module10" class="section level2" number="2.11">
<h2><span class="header-section-number">2.11</span> Enhance the reproducibility of collaborative research with version control platforms</h2>
<p>Once a researcher has learned to use <em>git</em> on their own computer for
local version control, they can begin using version control platforms (e.g.,
<em>GitLab</em>, <em>GitHub</em>) to collaborate with others under version
control. We will describe how a research team can benefit from using a version
control platform to work collaboratively.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>List benefits of using a version control platform to collaborate
on research projects, particularly for reproducibility</li>
<li>Describe the difference between version control (e.g., <em>git</em>) and
a version control platform (e.g., <em>GitLab</em>)</li>
</ul>
<div id="what-are-version-control-platforms" class="section level3" number="2.11.1">
<h3><span class="header-section-number">2.11.1</span> What are version control platforms?</h3>
<p>The last module introduced the idea of version control, including the
popular software tool often used for version control, <em>git</em>. In this
module, we’ll go a step further, telling you about how you can expand
the idea of version control to leverage it when collaborating across
your research team, using <strong>version control platforms</strong>.</p>
<p>When research groups—or any other professional teams—collaborate on
publications and research, the process can be a bit haphazard. Teams often use
emails and email attachments to share updates on the project, and email
attachments to pass around the latest version of a document for others to review
and edit. For example, one group of researchers investigated a large collection
of emails from Enron <span class="citation">(Hermans and Murphy-Hill 2015)</span>. They found that passing Excel files
through email attachements was a common practice, and that messages within
emails suggested that spreadsheets were stored locally, rather than in a
location that was accessible to all team members <span class="citation">(Hermans and Murphy-Hill 2015)</span>, which
meant that team members might often be working on different versions of the same
spreadsheet file. They note that “the practice of emailing spreadsheets is known
to result in serious problems in terms of accountability and errors, as people
do not have access to the latest version of a spreadsheet, but need to be
updated of changes via email.” <span class="citation">(Hermans and Murphy-Hill 2015)</span> The same process for
collaboration is often used in scientific research, as well: one study found,
“Team members regularly pass data files back and forth by hand, by email, and by
using shared lab or project servers, websites, and databases.”
<span class="citation">(Edwards et al. 2011)</span></p>
<blockquote>
<p>“The most primitive (but still very common) method [of version control] is all
hand-hacking. You snapshot the project periodically by manually copying
everything in it to a backup. You include history comments in source files. You
make verbal or email arrangements with other developers to keep their hands off
certain files while you hack them. … The hidden costs of this hand-hacking
method are high, especially when (as frequently happens) it breaks down. The
procedures take time and concentration; they’re prone to error, and tend to get
slipped under pressure or when the project is in trouble—that is exactly when
they are needed.” <span class="citation">(E. S. Raymond 2003)</span></p>
</blockquote>
<p>These practices make it very difficult to keep track of all project files,
and in particular, to track which version of each file is the most current.
Further, this process constrains patterns of collaboration—it requires
each team member to take turns in editing each file, or for one team member
to attempt to merge in changes that were made by separate team members at
the same time when all versions are collected. Further, this process makes
it difficult to keep track of why changes were made, and often requires
one team member to approve the changes of other team members. While the
“Track changes” and comment features can help the team communicate with
each other, but these features often lead to a very messy document at
stages in the editing, where it is hard to pick out the current versus
suggested wording, and once a change is accepted or a comment deleted,
these conversations are typically lost forever. Finally, word processing
tools are poorly suited to track changes or add suggestions directly to
data or code, as both data and code are usually saved in formats that
aren’t native to word processing programs, and copying them into a format
like Word can introduce problematic hidden formatting that can cause the
data or code to malfunction.</p>
<p>A version control platform allows you to share project files across a group of
collaborators while keeping track of what changes are made, who made each
change, and why each change was made. It therefore combines the strengths of a
“Track changes” feature with those of a file sharing platform like Dropbox. To
some extent, Google Docs or Google Drive also combine these features, and some
spreadsheet programs are moving toward some rudimentary functionality for
version control <span class="citation">(Birch, Lyford-Smith, and Guo 2018)</span>. However, there are added advantages of
version control platforms. Since open-source version control platforms like
GitHub can be set up on a server that you own, they can be used to collaborate
on projects with sensitive data, and also can store data directly on the server
you would like to use to store large project datasets or to run
computationally-intensive pre-processing or analysis. Finally, most version
control platforms include tools that help you manage and track the project.
These include “Issue Trackers,” tools for exploring the history of each file
and each change, and features to assign project tasks to specific team members.
The next section will describe the features of version control platforms that
make them helpful as a tool for collaborating on scientific research. These
systems are being leveraged by some scientists, both to manage research projects
and also to collaborate on writing scientific manuscripts and grant proposals
<span class="citation">(Perez-Riverol et al. 2016)</span>.</p>
<blockquote>
<p>“Using GitHub or any similar versioning / tracking system is not a replacement
for good project management; it is an extension, an improvement for good
project and file management.” <span class="citation">(Perez-Riverol et al. 2016)</span></p>
</blockquote>
<p>Version control platforms are always used in conjunction with version control
software, like the <em>git</em> software described in the last module. Version control
itself has been described as “a suite of programs that automates away most of
the drudgery involved in keeping an annotated history of your project and
avoiding modification conflicts,” <span class="citation">(E. S. Raymond 2003)</span>. The version control
platform leverages the history of commits that were made to the project, as well
as the version control software’s capabilities for merging changes made by
different people at different times. On top of these facilities, a version
control platform also adds attractive visual interfaces for working with the
project, free or low-cost online hosting of project files, and team management
tools for each project. You can think of <em>git</em> as the engine, in other words,
and the version control platform as the driver’s seat, with dashboard, steering
wheel, and gears to leverage the power of the underlying <em>git</em> software.</p>
<p>A number of version control platforms are available. Two that are currently very
popular for scientific research are GitHub (<a href="https://github.com/" class="uri">https://github.com/</a>) and GitLab
(<a href="https://about.gitlab.com/" class="uri">https://about.gitlab.com/</a>). Both provide free options for scientific
researchers, including the capabilities for using both public and private
repositories in collaboration with other researchers.</p>
<blockquote>
<p>Resources like GitHub are “essential for collaborative software projects
because they enable the organization and sharing of programming tasks between
different remote contributors.” <span class="citation">(Perez-Riverol et al. 2016)</span></p>
</blockquote>
</div>
<div id="why-use-version-control-platforms" class="section level3" number="2.11.2">
<h3><span class="header-section-number">2.11.2</span> Why use version control platforms?</h3>
<p>Version control platforms offer a number of advantages when collaborating
on a research project that can help to improve your efficiency, rigor, and
reproducibility. Further, there are several high-quality free versions
of version control platforms that are available for researchers, and as
their use becomes more popular, resources for learning the details of how
to use these platforms effectively. Open-source versions, like GitLab,
even allow you to set up a version control platform on a server you own,
rather than needing to post data or code on an outside platform, and so
you can use these tools even in cases of sensitive data.</p>
<p>Some of the key advantages of using a version control platform like GitHub
to collaborate on research projects include:</p>
<ul>
<li>Ability to track and merge changes that different collaborators made to the
document</li>
<li>Ability to create alternative versions of project files (<em>branches</em>), and merge them into the main project as desired</li>
<li>Tools for project management, including Issue Trackers</li>
<li>Default backup of project files</li>
<li>Ability to share project information online, including through hosting websites related to the project or supplemental files related to a manuscript</li>
</ul>
<p>Many of these strengths draw directly on the functions provided by the
underlying version control software (e.g., <em>git</em>). However, the version control
platform will typically allow team members to explore and work with these
functions in an easier way than if they try to use the barebones version control
software. In earlier years, the use of version control often required users to
be familiar with the command line, and to send arcane commands to track the
project files through that interface. With the rising popularity of version
control platforms, version control for project management can be taught
relatively quickly to students with a few months—or even weeks—of coding
experience. In fact, version control is beginning to be used as a method of
turning in and grading homework in beginning programming classes, with students
learning these techniques in the first few weeks of class. This would be
practically unimaginable without the user-friendly interface of a version
control platform as a wrapper for the power of the version control software
itself.</p>
<blockquote>
<p>“One reason for GitHub’s success is that it offers more than a simple source
code hosting service. It provides developers and researchers with a dynamic and
collaborative environment, often referred to as a social coding platform, that
supports peer review, commenting, and discussion. A diverse range of efforts,
ranging from individual to large bioinformatics projects, laboratory
repositories, as well as global collaborations, have found GitHub to be a
productive place to share code and ideas and collaborate.” <span class="citation">(Perez-Riverol et al. 2016)</span></p>
</blockquote>
<p>The first strength of using version control—and a version control
platform—to collaborate on scientific projects is its ability to track every
change made to files in the project, why the change was made, and who made it.
Version control creates a full history of the evolution of each file in the
project. When a change is committed, the history records the exact change made,
including the previous version of the file. No change is ever fully lost,
therefore, unless a great deal of extra work is taken to erase something from
the project’s commit history. Version control also requires a user to provide a
<em>commit message</em> describing each change that is made. If this feature is used
thoughtfully, then the commit history of the project provides a well-documented
description of the project’s full evolution. If you’re working on a manuscript,
for example, when it’s time to edit, you can cut whole paragraphs, and if you
ever need to get them back, they’ll be right there in the commit history for
your project, with their own commit message about why they were cut (hopefully a
nice clear one that will make it easy to find that commit if you ever need those
paragraphs again).</p>
<blockquote>
<p>“[Version control systems] are a huge boon to productivity and code quality in
many ways, even for small single-developer projects. They automate away many
procedures that are just tedious work. They help a lot in recovering from
mistakes. Perhaps most importantly, they free programmers to experiment by
guarnateeing that reversion to a known-good state will always be easy.”
<span class="citation">(E. S. Raymond 2003)</span></p>
</blockquote>
<p>These capacities to track changes and histories of project files becomes even
more important when working in collaboration on a project. As the proverb about
too many cooks in the kitchen captures, any time you have multiple people
working on a project, it introduces the chance for conflicts. While higher-level
conflicts, like about what you want the final product to look like or who should
do which jobs, can’t be easily managed by a computer program, now the
complications of integrating everyone’s contributions—and letting people work
in their own space and then bring together their individual work into one final
joint project—can be. While these programs for version control were originally
created to help with programmers developing code, they can be used now to
coordinate group work on numerous types of file-based projects, including
scientific manuscripts, books, and websites <span class="citation">(E. Raymond 2009)</span>. And
although they can work with projects that include binary code, they thrive in
projects with a heavier concentration of text-based files, and so they fit in
nicely in a scientific research / data analysis workflow that is based on data
stored in plain text formats and data analysis scripts written in plain text
files, tools we discuss in other parts of this book.</p>
<blockquote>
<p>“In a medium-sized project, it often happens that a (relatively small) number
of people work simultaneously on a single set of files, the ‘program’ or the
‘project.’ Often these people have additional tasks, causing their working
speeds to differ greatly. One person may be working a steady ten hours a day on
the project, a second may have barely time to dabble in the project enough to
keep current, while a third participant may be sent off on an urgent temporary
assignment just before finishing a modification. It would be nice if each
participant could be abstracted from the vicissitudes of the lives of the
others.” <span class="citation">(Grune 1986)</span></p>
</blockquote>
<p>Modern version control systems like <em>git</em> take a distributed approach to
collaboration on project files. Under the distributed model, all team members
can have their own version of all the files, work on them and make records of
changes they make to the files, and then occassionally sync with everyone else
to share your changes with them and bring their changes into your copy of the
files. This functionality is called <em>concurrency</em>, since it allows team members
to concurrently work on the same set of files <span class="citation">(E. Raymond 2009)</span>. This idea
allowed for the development of other useful features and styles of working,
including <em>branching</em> to try out new ideas that you’re not sure you’ll
ultimately want to go with and <em>forking</em>, a key tool used in open-source
software development, which among other things facilitates someone who isn’t
part of the original team getting a copy of the files they can work with and
suggesting some changes that might be helpful. So, this is the basic idea of
modern version control—for a project that involves a set of computer files,
everyone on the team (even if that’s just one person) has their own copy of a
directory with those files on their own computer, makes changes at the time and
in the spots in the files that they want, and then regularly re-syncs their
local directory with everyone else’s to share changes and updates.</p>
<p>There is one key feature of modern version control that’s critical to making
this work—merging files that started the same but were edited in different
ways and now need to be put back together, bringing any changes made from the
original version. This step is called <em>merging</em> the files. While this is
typically described using the plural, “files,” at a higher-level, you can thing
of this as just merging the <em>changes</em> that two people have made as they edited a
single file, a file where they both started out with identical copies.</p>
<p>Think of the file broken up into each of its separate lines. There will be some
lines that neither person changed. Those are easy to handle in the
“merge”—they stay the same as in the original copy of the file. Next, there
will be some lines that one person changed, but that the other person didn’t. It
turns out that these are pretty easy to handle, too. If only one person changed
the line, then you use their version—it’s the most up-to-date, since if both
people started out with the same version, it means that the other person didn’t
make any changes to that part of the file. Finally, there may be a few lines
that both people changed. These are called <em>merge conflicts</em>. They’re places in
the file where there’s not a clear, easy-to-automate way that the computer can
know which version to put into the integrated, latest version of the file.
Different version control programs handle these merge conflicts in different
ways. For the most common version control program used today, <em>git</em>, these spots
in the file are flagged with a special set of symbols when you try to integrate
the two updated versions of the file. Along with the special symbols to denote a
conflict, there will also be <em>both</em> versions of the conflicting lines of the
file. Whoever is integrating the files must go in and pick the version of those
lines to use in the integrated version of the file, or write in some compromise
version of those lines that brings in elements from both people’s changes, and
then delete all the symbols denoting that was a conflict and save this latest
version of the file.</p>
<blockquote>
<p>“You will likely share your code with multiple lab mates or collaborators,
and they may have suggestions on how to improve it. If you email the code
to multiple people, you will have to manually incorporate all the changes
each of them sends.” <span class="citation">(Blischak, Davenport, and Wilson 2016)</span></p>
</blockquote>
<p>There are a number of other features of version control that make it useful for
collaborating on file-based projects with teams. First, these systems allow you
to explain your changes as you make them—in other words, it allows for
<em>annotation</em> of the developing and editing process <span class="citation">(E. Raymond 2009)</span>. This
provides the team with a full history of why the files evolved in the way they
did across the team. It also provides a way to communicate across the team
members.</p>
<p>For example, if one person is the key person working on a certain file,
but has run into a problem with one spot and asks another team member to take a
go, then the second team member isn’t limited to just looking at the file and
then emailing some suggestions. Instead, the second person can make sure he or
she has the latest version of that file, make the changes they think will help,
<em>commit</em> those changes with a message (a <em>commit message</em>) about why they think
this change will fix the problem, and then push that latest version of the file
back to the first person. If there are several places where it would help to
change the file, then these can be fixed through several separate commits, each
with their own message. The first person, who originally asked for help, can
read through the updates in the file (most platforms for using version control
will now highlight where all these changes are in the file) and read the second
person’s message or messages about why each change might help. Even better, days
or months later, when team members are trying to figure out why a certain change
was made in that part of the file, can go back and read these messages to get an
explanation.</p>
<blockquote>
<p>“You know your code has changed; do you know why? It’s easy to forget the
reasons for changes, and step on them later. If you have collaborators on a
project, how do you know what they have changed while you weren’t looking, and
who was responsible for each change?” <span class="citation">(E. S. Raymond 2003)</span></p>
</blockquote>
<p>In recent years, some complementary tools have been developed that make the
process of collaborating together using version control software easier. Other
tools can helps in collaborating on file-based projects, including <em>bug
trackers</em> or <em>issue trackers</em>, which allow the team to keep a running “to-do”
list of what needs to be done to complete the project, all of which are
discussed in the next chapter <span class="citation">(Perez-Riverol et al. 2016)</span>.</p>
<!-- > "GitHub issues are a great way to keep track of bugs, tasks, feature requests, -->
<!-- and enhancements. While classical issue trackers are primarily intended to be  -->
<!-- used as bug trackers, in contrast, GitHub issue trackers follow a different  -->
<!-- philosophy: each tracker has its own section in every repository and can be used -->
<!-- to trace bugs, new ideas, and enhancements by using a powerful tagging system. -->
<!-- The main objective of issues in GitHub is promoting collaboration and providing  -->
<!-- context using cross-references. Raising an issue does not require lengthy forms -->
<!-- to be completed. It only requires a title and, preferably, at least a short description. -->
<!-- Issues have very clear formatting and provide space for anyone with a GitHub account -->
<!-- to provide feedback. ... Additional elements of issues are (i) color-coded labels -->
<!-- that help to categorize and filter issues, (ii) milestones, and (iii) one assignee  -->
<!-- responsible for working on the issue." [@perez2016ten] -->
<!-- > "As another illustration of issues and their generic and wide application, we -->
<!-- and others used GitHub issues to discuss and comment on changes in manuscripts -->
<!-- and address reviewers' comments." [@perez2016ten] -->
<p>Finally, version control platforms like GitHub can be used for a number
of supplementary tasks for your research project. These include publishing
webpages or other web resources linked to the project and otherwise improving
public engagement with the project, including by allowing other researchers
to copy and adapt your project through a process called <em>forking</em>. Version
control platforms also provide a supplemental backup to project files.</p>
<p>First, GitHub can be used to collaborate on, host, and publish websites and
other online content <span class="citation">(Perez-Riverol et al. 2016)</span>. Version control systems have been used by
some for a long time to help in writing longform materials like books (e.g.,
<span class="citation">(E. S. Raymond 2003)</span>); new tools are making the process even easier. Thethe GitHub
Pages functionality, for example, is now being used to host a number of books
created in R using the <code>bookdown</code> package, including the online version of this
book. The <code>blogdown</code> package similarly can be used to create websites, either
for individual researchers, for research labs, or for specific projects or
collaborations. Further, if a project includes the creation of scientific
software, it can be used to share that software—as well as associated
documentation—in a format that is easy for others to work with. The platform
can also be used to share supplemental material for a manuscript, including the
code used for preprocessing and analyzing data. The most popular version control
platforms, GitHub and GitLab, both allow users to toggle projects between
“public” and “private” modes, which can be used to work privately on a
project prior to peer review and publication, and then switch to a public mode
after publication. This functionality will allow those who access the code to
see not only the final product, but also the history of the development of the
code and data for the project, providing more transparency in the development
process, but without jeopardizing the novelty of the research results prior to
publication.</p>
<blockquote>
<p>“The traditional way to promote scientific software is by publishing an
associated paper in the peer-reviewed scientific literature, though, as pointed
out by Buckheir and Donoho, this is just advertising. Additional steps can boost
the visibility of an organization. For example, GitHub Pages are simple websites
freely hosted by GitHub. Users can create and host blog websites, help pages,
manuals, tutorials, and websites related to specific projects.” <span class="citation">(Perez-Riverol et al. 2016)</span></p>
</blockquote>
<p>With GitHub, while only collaborators on a public project can directly change
the code, anyone else can <em>suggest</em> changes through a process of copying a
version of the project (<em>forking</em> it). This allows someone to make the changes
they would like to suggest directly to a copy of the code, and then ask the
project’s owners to consider integrating the changes back into the main version
of the project through a <em>pull request</em>. GitHub therefore creates a platform
where people can explore, adapt, and add to other people’s coding projects,
enabling a community of coders <span class="citation">(Perez-Riverol et al. 2016)</span>, and because of this
functionality it has been described as “a social network for software
development” <span class="citation">(J. Perkel 2018)</span> and as “a kind of bazaar that offers just about
any piece of code you might want—and so much of it free.” <span class="citation">(Metz 2015)</span>.
This same process can be leveraged for others to copy and adapt code—this is
particularly helpful in ensuring that a software or research project won’t be
“orphaned” if its main developer is unavailable (e.g., retires, dies), but
instead can be picked up and continued by other interested researchers.
Copyright statements and licenses within code projects help to clarify
attribution and rights in these cases.</p>
<blockquote>
<p>“The astonishment was that you might want to make even your tiny hacks to
other people’s code public. Before GitHub, we tended to keep those on our own
computer. Nowadays, it is so each to make a fork, or even edit the code directly
in your browser, that potentially anyone can find even your least polished
bug fixes immediately.” <span class="citation">(Irving 2011)</span></p>
</blockquote>
<p>Finally, version control platforms help in providing additional back-up for
project files. As you collaborate with others using version control under
a distributed model, each collaborator will have their own copy of all
project files on their local computer. All project files are also
stored on the remote repository to which you all push and pull commits. If you
are using the GitHub platform, this will be GitHub’s servers; if you use
GitLab, you can set up the system on your own server. Each time you push
or pull from the remote copy of the project repository, you are syncing your
copy of the project files with those on other computers.</p>
<!-- > "A good approach is to store at least three copies in at least two -->
<!-- geographically distributed locations (e.g., original location such as a desktop -->
<!-- computer, an external hard drive, and one or more remote sites) and to adopt a -->
<!-- regular schedule for duplicating the data (i.e., backup)." [@michener2015ten] -->
<!-- > "One study surveyed neuroscience researchers at a UK institute. "The backup 'rule -->
<!-- of three' states that for a file to be sufficiently backed up it should be kept -->
<!-- in three separate locations using two different types of media with one offsite -->
<!-- backup. A lack of an adequate backup solution could mean permanently lost data, -->
<!-- effort and time. In this research, more than 82% of the respondents seemed to be -->
<!-- unaware of suitable backup procedures to protect their data. Some respondents -->
<!-- kept a single backup of work on external hard disks. Others used the -->
<!-- Universities local networked servers as their means of backup." -->
<!-- [@altarawneh2017pilot] -->
<blockquote>
<p>“Backup, backup, backup—this is the main action you can take to care for your
computers and your data. Many PIs assume that backup systems are inherently
permanent and foolproof, and it often takes a loss to remind one that
materials break, systems fail, and humans make mistakes. Even if your data
are backed up at work, have at least one other backup system. Keep at least
one backup off site, in case of a diaster in the lab (yes, fires and floods
do happen). It doesn’t make much sense to have two separate backup systems stored
next to each other in a drawer.” <span class="citation">(LEIPS 2010)</span></p>
</blockquote>
<!-- > "Departmental or institutional servers provide an area to store large files -->
<!-- such as graphics files as well as e-mail and documents. Such systems will  -->
<!-- usually have frequent routine backups of all data, often onto optical  -->
<!-- disks. They might also encrypt the data, which makes it less able to be  -->
<!-- hacked. This is the most dependable form of long-term storage." [@leips2010helm] -->
</div>
<div id="how-to-use-github" class="section level3" number="2.11.3">
<h3><span class="header-section-number">2.11.3</span> How to use GitHub</h3>
<p>In the next module, we describe practical ways to leverage these resources
within your research group. We include instructions both for team leaders—who
may not code but may want to use GitHub within projects to help manage the
projects—as well as researchers who work directly with data and code for the
research team. There are also a number of excellent resources that are now
available that walk users through how to set up and use a version control
platform. The process is particularly straightforward when the research project
files are collected in an RStudio Project format, as described in earlier
modules.</p>
</div>
<div id="discussion-questions-3" class="section level3" number="2.11.4">
<h3><span class="header-section-number">2.11.4</span> Discussion questions</h3>

</div>
</div>
<div id="module11" class="section level2" number="2.12">
<h2><span class="header-section-number">2.12</span> Using git and GitLab to implement version control</h2>
<p>For many years, use of version control required use of the command line,
limiting its accessibility to researchers with limited programming experience.
However, graphical interfaces have removed this barrier, and RStudio has
particularly user-friendly tools for implementing version control. In this
module, we will show how to use <em>git</em> through RStudio’s user-friendly
interface and how to connect from a local computer to <em>GitLab</em> through
RStudio.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Understand how to set up and use <em>git</em> through RStudio’s interface</li>
<li>Understand how to connect with <em>GitLab</em> through RStudio to collaborate on<br />
research projects while maintaining version control</li>
</ul>
<div id="how-to-use-version-control" class="section level3" number="2.12.1">
<h3><span class="header-section-number">2.12.1</span> How to use version control</h3>
<p>In this chapter, we will give you an overview of how to use git and GitHub
for your laboratory research projects. In this chapter, we’ll address two
separate groups, in separate sections. First, we’ll provide an overview of how
you can leverage and use these tools as the director or manager of a project,
without knowing how to code in a lanugage like R. GitHub provides a number of
useful tools that can be used by anyone, providing a common space for managing
the data recording, analysis and reporting for a scientific research project.
In this case, there would need to be at least one member of your team who is
comfortable with a programming language, but all team members can participate
in many features of the GitHub repository regardless of programming skill.</p>
<p>Second, we’ll provide some details on the “how-tos” of setting up and using
git and GitHub for scientists who <em>are</em> programmers or learning to program in
a language like R or Python. We will not be exhaustive in this section, as there
are a number of excellent resources that already go into depth on these topics.
Instead, we provide an overview of getting starting, and what tools you might
want to try within projects, and then provide advice on more references to follow
up with to learn more and fully develop these skills.</p>
<p>As an example, we’ll show different elements from a real GitHub repository, used
for scientific projects and papers. The first repository is available at
<a href="https://github.com/aef1004/cyto-feature_engineering" class="uri">https://github.com/aef1004/cyto-feature_engineering</a>. It provides example data
and code to accompany a published article on a pipeline for flow cytometry
analysis <span class="citation">(Fox et al. 2020)</span>.</p>
</div>
<div id="leveraging-git-and-github-as-a-project-director" class="section level3" number="2.12.2">
<h3><span class="header-section-number">2.12.2</span> Leveraging git and GitHub as a project director</h3>
<p>Because <code>git</code> has a history in software development, and because most
introductions to it quickly present arcane-looking code commands, you may have
hesitations about whether it would be useful in your scientific research group
if you, and many in your research group, do not have experience programming.
This is not at all the case, and in fact, the combination of git and GitHub can
become a secret weapon for your research group if you are willing to encourage
those in your group who do know some programming (or are willing to learn a bit)
and to take them time to try out this environment for project managemment.</p>
<p>As mentioned in the previous two chapters, repositories that are tracked with
git and shared through GitHub provide a number of tools that are useful in
managing a project, both in terms of keeping track of what’s been done in the
project and also for planning what needs to be done next, breaking those goals
into discrete tasks, assigning those tasks to team members, and maintaining a
discussion as you tackle those tasks.</p>
<p>While <code>git</code> itself traditionally has been used with a command-line interface
(think of the black and green computer screens shown when movies portray
hackers), GitHub has wrapped <code>git</code>’s functionality with an attractive and easy
to understand graphical user interface. This is how you will interact with a
project repository if you are online and logged into GitHub, rather than
exploring it on your own computer (although there are also graphical user
interfaces you can use to more easily explore git repositories locally, on your
computer).</p>
<p>Key project management tools for GitHub that you can leverage, all covered in
subsections below, are:</p>
<ul>
<li>Commits and commit history</li>
<li>Issues</li>
<li>Repository access and ownership</li>
<li>Insights</li>
</ul>
<p>Successfully using GitHub to help track and manage a research project does not
require using all of these tools, and in fact you can go a long way by just starting
with a subset. The first four covered (Commits, Issues, Commit history, and Repository access
and ownership) would be a great set to try out in a first project.</p>
<p><strong>Commits</strong></p>
<p>Each time a team member makes a change to files in a GitHub repository, the change is
recorded as a <strong>commit</strong>, and the team member must include a short <strong>commit message</strong>
describing the change. Each file in the project will have its own page on GitHub
(Figure <a href="experimental-data-recording.html#fig:githubcommits1">2.27</a>), and you can see the history of changes to that
files by clicking the “History” link on that page.</p>
<div class="figure"><span style="display:block;" id="fig:githubcommits1"></span>
<img src="figures/github_commits1.png" alt="Example of a file page within a GitHub repository. Each file in a repository has its own page. On this page, you can see the history of changes made to the file by looking at 'History'. You can also make a commit an edit directly in GitHub by clicking on the 'Edit' icon." width="\textwidth" />
<p class="caption">
Figure 2.27: Example of a file page within a GitHub repository. Each file in a repository has its own page. On this page, you can see the history of changes made to the file by looking at ‘History.’ You can also make a commit an edit directly in GitHub by clicking on the ‘Edit’ icon.
</p>
</div>
<p>You can make changes to a file locally, on the repository copy on your own computer.
For team members who are working a lot on coding, this will likely be the primary
method they use to make commits, as this allows you to test the code locally before
you commit it.</p>
<p>However, it is also possible to make a commit directly on GitHub, and this may be a
useful option for team members who are not coding and would like to make small changes
to the writing files. On the file’s page on GitHub, there is an “Edit” icon
(Figure <a href="experimental-data-recording.html#fig:githubcommits1">2.27</a>). By clicking on this, you will get to a page where you
can directly edit the file (Figure <a href="experimental-data-recording.html#fig:githubcommits2">2.28</a>). Once you have made your
edits, you will need to commit them, and a short description of the commit is required.
If you would like to include a longer explanation of your changes, there is space for
that, as well, when you make the commit (Figure <a href="experimental-data-recording.html#fig:githubcommits2">2.28</a>).</p>
<div class="figure"><span style="display:block;" id="fig:githubcommits2"></span>
<img src="figures/github_commits1.png" alt="Committing changes directly in GitHub. When you click on the 'Edit' button in a file's GitHub page (see previous figure), it will take you to a page where you can edit the file directly. You save the changes with a 'commit', including a commit message describing why you made the change. The change will be tagged with the message and your name." width="\textwidth" />
<p class="caption">
Figure 2.28: Committing changes directly in GitHub. When you click on the ‘Edit’ button in a file’s GitHub page (see previous figure), it will take you to a page where you can edit the file directly. You save the changes with a ‘commit,’ including a commit message describing why you made the change. The change will be tagged with the message and your name.
</p>
</div>
<p>You can see the full history of changes that have been made to each file in the
project (Figure <a href="experimental-data-recording.html#fig:githubcommithistory">2.29</a>). Each change is tracked through
a commit, which includes markers of who made the change and a message describing
the change. Further, this history page for the file provides a line-by-line
history of when each line in the file was last changed and what that change
is—this allows you to quickly pinpoint changes in a particular part of the
file.</p>
<div class="figure"><span style="display:block;" id="fig:githubcommithistory"></span>
<img src="figures/github_commit_history.png" alt="Commit history in GitHub. Each file in a repository has a 'History' page, where you can explore each change commited for the file. Each commit has a unique identifier and commit message describing the change. You can click on the entry for any of these commits to see the changes made to the file with the commit (see next figure)." width="\textwidth" />
<p class="caption">
Figure 2.29: Commit history in GitHub. Each file in a repository has a ‘History’ page, where you can explore each change commited for the file. Each commit has a unique identifier and commit message describing the change. You can click on the entry for any of these commits to see the changes made to the file with the commit (see next figure).
</p>
</div>
<p>If you click on one of the commits listed on a file’s History page (Figure
<a href="experimental-data-recording.html#fig:githubcommithistory">2.29</a>, it will take you to a page providing information
on the changes made with that commit (Figure <a href="experimental-data-recording.html#fig:githubcommithistory2">2.30</a>).
This page provides a line-by-line view of each change that was made to project
files with that commit. This page includes the commit message for that commit.
If the person committing the change included a longer description or commentary
on the commit, this information will also be included on this page. Near the
commit message are listings of which team member made the commit and when it was
made. Within the body of the page, you can see the changes made with the commit.
Added lines will be highlighted in green while deleted lines are highlighted in
red. If only part of a line was changed, it will be shown twice, once in red as
its version before the commit, and once in green showing its version following
the commit.</p>
<div class="figure"><span style="display:block;" id="fig:githubcommithistory2"></span>
<img src="figures/github_commit_history2.png" alt="Commit history in GitHub. Each commit has its own page, where you can explore what changes were made with the commit, who made them, and when they were committed." width="\textwidth" />
<p class="caption">
Figure 2.30: Commit history in GitHub. Each commit has its own page, where you can explore what changes were made with the commit, who made them, and when they were committed.
</p>
</div>
<p><strong>Issues</strong></p>
<p>GitHub, as well as other version control platforms, includes functionality
that will help your team collaborate on a project. A key tool is the
“Issues” tracker. Each repository includes this type of tracker, and it
can be easily used by all team members, whether they are comfortable
coding or not.</p>
<p>Figure <a href="experimental-data-recording.html#fig:githubissues1">2.31</a> gives an example of <a href="https://github.com/aef1004/cyto-feature_engineering/issues">the Issues tracker page</a> for the
repository we are using as an example.</p>
<div class="figure"><span style="display:block;" id="fig:githubissues1"></span>
<img src="figures/github_issues.png" alt="Issues tracker page for an example GitHub repository. Arrows highlight the tab to click to get to the Issues tracker page in a repository, as well as where to go to find open and closed Issues for the repository." width="\textwidth" />
<p class="caption">
Figure 2.31: Issues tracker page for an example GitHub repository. Arrows highlight the tab to click to get to the Issues tracker page in a repository, as well as where to go to find open and closed Issues for the repository.
</p>
</div>
<p>The main Issues tracker page provides clickable links to all open issues for
the repository. You can open a new issue using the “New Issue” on this main
page or on the specific page of any of the repository’s issues (see Figure
<a href="experimental-data-recording.html#fig:githubissues2">2.32</a> for an example of this button).</p>
<div class="figure"><span style="display:block;" id="fig:githubissues2"></span>
<img src="figures/github_issues2.png" alt="Conversation about an Issue on Issues tracker page of an example GitHub repository. In this example, you can see how GitHub Issues trackers allow you to discuss how to resolve an issue across your team. From this page, you can read the current conversation about Issue \#1 of the repository and add your own comments. Once the Issue is resolved, you can 'Close' the Issue, which moves it off the list of active issues, but allows you to still re-read the conversation and, if necessary, re-open the issue later. You can also open a new issue from this page, using the button highlighted at the top right." width="\textwidth" />
<p class="caption">
Figure 2.32: Conversation about an Issue on Issues tracker page of an example GitHub repository. In this example, you can see how GitHub Issues trackers allow you to discuss how to resolve an issue across your team. From this page, you can read the current conversation about Issue #1 of the repository and add your own comments. Once the Issue is resolved, you can ‘Close’ the Issue, which moves it off the list of active issues, but allows you to still re-read the conversation and, if necessary, re-open the issue later. You can also open a new issue from this page, using the button highlighted at the top right.
</p>
</div>
<p>On the page for a specific issue (e.g., Figure <a href="experimental-data-recording.html#fig:githubissues2">2.32</a>), you
can have a conversation with your team to determine how to resolve the issue.
This conversation can include web links, figures, and “To-do” check boxes, to
help you discuss and plan how to resolve the issue. Each issue is numbered,
which allows you to track each individually as you work on the project.</p>
<p>Once you have resolved an issue, you can “Close” it. This moves the issue
from the active list into a “Closed” list. Each closed issue still has its
own page, where you can read through the conversation describing how it
was resolved. If you need to, you can re-open a closed issue later, if you
determine that it was not fully resolved.</p>
<div class="figure"><span style="display:block;" id="fig:githubissues3"></span>
<img src="figures/github_issues3.png" alt="Labeling and assigning Issues. The GitHub Issues tracker allows you to assign each issue to one or more team members, clarifying that they will take the lead in resolving the issue. It also allows you to tag each issue with one or more labels, so you can easily navigate to issues of a specific type or identify the category of a specific issue." width="\textwidth" />
<p class="caption">
Figure 2.33: Labeling and assigning Issues. The GitHub Issues tracker allows you to assign each issue to one or more team members, clarifying that they will take the lead in resolving the issue. It also allows you to tag each issue with one or more labels, so you can easily navigate to issues of a specific type or identify the category of a specific issue.
</p>
</div>
<p>The Issues tracker page includes some more advanced functionality, as well
(Figure <a href="experimental-data-recording.html#fig:githubissues3">2.33</a>). For example, you can “assign” an issue to one
of more team members, indicating that they are responsible for resolving that
issue. You can also tag each issue with one of more labels, allowing you to
group issues into common categories. For example, you could tag all issues that
cover questions about pre-processing the data using a “pre-processing” label,
and all that are related to creating figures for the final manuscript with a
“figures” label.</p>
<p><strong>Repository access and ownership</strong></p>
<p>Repositories include functionality for inviting team members, assigning
roles, and otherwise managing access to the repository. First, a repository
can be either public or private. For a public repository, anyone will be
able to see the full contents of the repository through GitHub. You can
also set a repository to be private. In this case, the repository can only
be seen by those who have been invited to collaborate on the repository, and
only when they are logged in to their GitHub accounts. The private / public
status of a repository can be changed at any time, so if you want you can
maintain a repository for a project as private until you publish the results,
and then switch it to be public, to allow others to explore the code and data
that are linked to your published results.</p>
<p>You can invite team members to collaborate on a repository, as long as they
have GitHub accounts (these are free to sign up for). While public repositories
can be seen by anyone, the only people who can add to or change the contents
of the repository are people who have been invited to collaborate on the
repository. The person who creates the repository can invite other collaborators
through the “Settings” tab of the repository, which will have a “Manage access”
function for the repositories maintainer. On this page, you can invite other
collaborators by searching using their GitHub “handle” (the short name they
chose to be identified by in GitHub). You can also change access rights, for
example, allowing some team members to be able to make major changes to the
repository—like deleting it—while others can make only smaller modifications.</p>
<p><strong>Insights</strong></p>
<p>Each GitHub repository also provides an “Insights” page, which lets you see who is
contributing to the project and, as well when and how much they have contributed, as
tracked by the commits they’ve made.</p>
<p>First, this page provides some repository-wide summaries, regardless of who was
contributing. The figure below shows an example of the “Code frequency” graph,
showing the number of additions and deletions to the code each week (here, “code” means
any data in the tracked files, so it would include data recorded for the project or
text written up for a project report or presentation [double-check that this is the
case]).</p>
<p><img src="figures/github_code_frequency.png" width="864" /></p>
<p>During periods when the research team is collecting data, you would expect
a lot more additions that deletions, and you could check this plot to ensure that the
team is committing data soon after it’s recorded (i.e., that there are lots of additions
on weeks with major data collection for the experiment, not several weeks after).
Periods with a lot of deletions, aren’t bad, but instead likely indicate that a lot of
work is being done in editing reports and manuscripts. For example, if a paper is being
prepared for publication, you’d expect a lot of delections as the team edits it to meet
word count requirements.</p>
<p>The “Insights” page on a GitHub repository also lets you track the frequency of commits
to the project, where each commit could be something small (like fixing a typo) or
large (adding new data files for all data recorded for a timepoint for the experiment).
However, the frequency of these commits can help identify periods when the team is working
on the project. For example, the commit history graph shown below is for the GitHub
repository for a website for a spring semester course in 2020. It’s clear to see the
dates when the course was in session, as well as how the project required a lot of
initial set up (shown by the number of commits early in the project period compared
to later). You can even see spring break in mid-March (the week in the middle with no
commits).</p>
<p><img src="figures/github_commit_frequency.png" width="739" /></p>
<p>This window also allows you to track the number and timing of commits of each contributor
to the project.</p>
</div>
<div id="leveraging-git-and-github-as-a-scientist-who-programs" class="section level3" number="2.12.3">
<h3><span class="header-section-number">2.12.3</span> Leveraging git and GitHub as a scientist who programs</h3>
<p>To be able to leverage GitHub to manage projects and share data, you will
need to have at least one person in the research group who can set up the
initial repository. GitHub repositories can be created very easily starting
from an RStudio Project, a format for organizing project files that was
described in module [x]. In this section, we’ll give some advice on how you
can use an RStudio Project to create and update a GitHub repository, and how
this can allow separate team members to maintain identical copies of the
RStudio Project on their own computers, while continually evolving files in
the project as data pre-processing, data analysis, and manuscript
preparation are done for the project. We will keep this advice limited,
as there are excellent existing resources that provide more thorough
instructions in this area, but we will introduce the methods and then
point to more thorough resources for more detail.</p>
<ul>
<li>Interfacing with RStudio</li>
<li>Initiating a repository and first commit</li>
<li>Subsequent commits and commit messages</li>
<li>Fixing merge conflicts when team members make concurrent changes</li>
<li>Using branches and forks / pull requests to try out new things</li>
<li>Using GitHub Actions for automation (e.g., automatic testing?)</li>
<li>[Odds and ends—.DS_Store, $Word_doc]</li>
<li>More resources for learning to use git and GitHub</li>
</ul>
<!-- ### Notes -->
<!-- While this method of merging two sets of changes to a file generally works very well in  -->
<!-- integrating what members of the team have typed into the file in their edits (i.e., the -->
<!-- actual text), it doesn't do *anything* to check the logic. If that text is computer -->
<!-- code, then the latest version, with code integrated from two people, could be "broken",  -->
<!-- even if each person confirmed that the code worked on their own computers before merging,  -->
<!-- because a change in one spot in code could break something somewhere else in the code.  -->
<!-- One way to try to quickly identify this kind of a problem is to create small pieces of  -->
<!-- code that can be run whenever you'd like to check that your code is still behaving like you -->
<!-- want it to. These pieces of code are called "unit tests". They allow you to define how -->
<!-- you *expect* your code to behave---for example, if you have created a function in your code that -->
<!-- counts the number of letters in a word, you expect it to always give back "5" for "fever" -->
<!-- and "0" for "". You can write a bit of code that runs the function, using inputs of  -->
<!-- "fever" and "", and then tests to see if what it gets in return is "5" and "0". If it does,  -->
<!-- great! You won't hear anything else. If it doesn't though, it will print out a message  -->
<!-- to warn you that one of the functions in your code didn't return what you were expecting -->
<!-- it to.  -->
<!-- You can collect all of these unit tests in one part of the project directory, and you  -->
<!-- can even set up a "hook" with your version control program to run all of those tests -->
<!-- every time you merge in new changes from team members.  -->
<!-- If everything still works fine after -->
<!-- merging in new changes, it will be a silent success, but if a merge breaks something,  -->
<!-- you'll get a noisy failure which, while certainly worse that a silent success, is much,  -->
<!-- much better than a silent failure. As Mark Twain (supposedly) said, "What gets us into  -->
<!-- trouble is not what we don't know. It's what we know for sure that just ain't so." -->
<!-- > "It is possible to put hook scripts in [some version control program's] repository -->
<!-- that will fire at various interesting times---notably before each commit, or after.  -->
<!-- Hook scripts can be used for many purposes, including enforcing fine-grained access  -->
<!-- control and sending automated commit notifications to mailing lists, bug trackers,  -->
<!-- or even IRC channels." [@raymondunderstanding] -->
<!-- > "When the system prints the prompt `$` and you type commands that get  -->
<!-- executed, it's not the kernel that is talking to you, but a go-between called -->
<!-- the command interpreter or *shell*. The shell is just an ordinary program like -->
<!-- `date` or `who`, although it can do some remarkable things. The fact that the shell -->
<!-- sits between you and the facilities of the kernel has real benefits, some of which -->
<!-- we'll talk about here. There are three main ones: (1) Filename shorthands: you can  -->
<!-- pick up a whole set of filenames as arguments to a program by specifying a  -->
<!-- pattern for the names---the shell will find the filenames that fit your pattern;  -->
<!-- (2) Input-output redirection: you can arrange for the output of any program to  -->
<!-- go into a file instead of onto the terminal, and for the input to come from  -->
<!-- a file instead of the terminal. Input and output can even be connected to  -->
<!-- other programs. (3) Personalizing the environment: you can define your own  -->
<!-- commands and shorthands." [@kernighan1984unix] -->
<!-- > "Suppose you're typing a large document like a book. Logically this divides into many small pieces,  -->
<!-- like chapters and perhaps sections. Physically it should be divided too, because it is cumbersome -->
<!-- to edit large files. Thus you should type the document as a number of files. You might have separate -->
<!-- files for each chapter, called 'ch1', 'ch2', etc. ... With a systematic naming convention, you can tell at -->
<!-- a glance where a particular file fits into the whole. What if you want to print the whole book? You could  -->
<!-- say `$ pr ch1.1 ch1.2 ch 1.3 ...`, but you would soon get bored typing filenames and start to make mistakes. -->
<!-- This is where filename shorthand comes in. If you say `$ pr ch*` the shell takes the `*` to mean 'any -->
<!-- string of characters,' so ch* is a pattern that matches all filenames in the current directory that  -->
<!-- begin with ch. The shell creates the list, in alphabetical order, and passes the list to `pr`. The -->
<!-- `pr` command never sees the `*`; the pattern match that the shell does in the current directory  -->
<!-- generates aa list of strings that are passed to `pr`. The crucial point is that filename shorthand -->
<!-- is not a property of the `pr` command, but a service of the shell. Thus you can use it to generate -->
<!-- a sequence of filenames for *any* command." [@kernighan1984unix] -->
<!-- > "One of the virtues of the Unix system is that there are several ways to bring it closer to  -->
<!-- your personal taste or the conventions of your local computing environmentl. ... If there is a file -->
<!-- named '.profile' in your login directory, the shell will execute the commands in it when you log in, -->
<!-- before printing the first prompt. So you can put commands into '.profile' to set up your environment -->
<!-- as you like it, and they will be executed every time you log in. ... -->
<!-- Some of the properties of the shell are actually controlled by so-called *shell variables*, with  -->
<!-- values that you can access and set yourself. For example, the prompt string, which we have been showing -->
<!-- as `$`, is acually stored in a shell variable called 'PS1', and you can set it to anything you like, like -->
<!-- this `PS1='Yes dear?'`. ... Probably the most useful shell variable is the one that controls where the shell -->
<!-- looks for commands. Recall that when you type the name of a command, the shell normally looks for it  -->
<!-- first in the current directory, then in '/bin', and then in '/usr/bin'. This sequence of directories -->
<!-- is called the *search path*, and is stored in a shell variable called 'PATH'. If the default search  -->
<!-- path isn't what you want, you can change it, again usually in your '.profile'. ... It is also possible -->
<!-- to use variables for abbreviation. If you find yourself frequently referring to some directory with  -->
<!-- a long name, it might be worthwhile adding a line like `d=/horribly/long/directory/name` to your profile, -->
<!-- so that you can say things like `$cd $d`. Personal variables like `d` are conventionally spelled  -->
<!-- in lower case to distinguish them from those used by the shell itself, like `PATH`." [@kernighan1984unix] -->
<!-- > "The culmination of your login efforts is a *prompt*, usually a single character, indicating that the  -->
<!-- system is ready to accept commands from you. The prompt is most likely to be a dolloar sign or a percent -->
<!-- sign, but you can change it to anything you like... The prompt is actually printed by a program called -->
<!-- the *command interpreter* or *shell*, which is your main interface to the system. ... Once you receive  -->
<!-- the prompt, you can type *commands*, which are requests that the system do something. We will use *program* -->
<!-- as a synonym for command." [@kernighan1984unix] -->
<!-- > "While checksums are a great method to check if files are different, they don't tell -->
<!-- us *how* the files differ. One approach to this is to compute the *diff* between  -->
<!-- two files using the Unix tool *diff*. Unix's *diff* works line by line, and outputs -->
<!-- blocks (called *hunks*) that differ between files (resembling Git's *git diff* command)." -->
<!-- [@buffalo2015bioinformatics] -->
<!-- > "**Use version control.** An example is git. This takes time to learn, but the -->
<!-- time is well invested. In the long run, it will be infinitely better than all your  -->
<!-- self-grown attempts at managing evolving code with version numbers, switches,  -->
<!-- and the like. Moreover, this is the sanest option for collaborative work on code,  -->
<!-- and it provides an extra backup of your codebase, especially if the server is  -->
<!-- distinct from your personal computer." [@holmes2018modern] -->
<blockquote>
<p>“Get a new repository in the directory you are working in via: <code>git init</code>. Ok, you
now have a revision control system in place. You might not see it, because Git stores
all its files in a directory names <code>.git</code>, where the dot means that all the usual
utilities like <code>ls</code> will take it to be hidden. You can look for it via, e.g.,
<code>ls -a</code> or via a show hidden files option in your favorite file manager. …
Given that all the data about a repository is in the <code>.git\ subdirectory of your project directory, the analog to freeing a repository is simple:</code>rm -rf .git`.”
<span class="citation">(Klemens 2014)</span></p>
</blockquote>
<blockquote>
<p>“Calling <code>git commit -a</code> writes a new commit object to the repostiory based on
all the changes the index was able to track, and clears the index. Having saved your
work, you can now continue to add more. Further—and this is the real, major
benefit of revision control so far—you can delete whatever you want, confident
that it can be recovered if you need it back. Don’t clutter up the code with large
blocks of commented-out obsolete routines—delete!” <span class="citation">(Klemens 2014)</span></p>
</blockquote>
<blockquote>
<p>“Having generated a commit object, your interactions with it will mostly consist of
looking at its contents… The key metadata is the name of the object, which is
assigned via an unpleasant but sensible naming convention: the SHA1 has, a
40-digit hexadecimal number that can be assigned to an object, in a manner that
lets us assume that no two objects will have the same hash, and that the same object
will have the same name in every copy of the repository. When you commit your files,
you’ll see the first few digits of the hash on the screen… Fortunately, you
need only as much of the hash as will uniquely identify your commit.”
<span class="citation">(Klemens 2014)</span></p>
</blockquote>
</div>
<div id="applied-exercise-2" class="section level3" number="2.12.4">
<h3><span class="header-section-number">2.12.4</span> Applied exercise</h3>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="experimental-data-preprocessing.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/01-separating.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["improve_repro.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
