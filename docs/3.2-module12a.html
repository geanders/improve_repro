<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="3.2 Selecting software options for pre-processing | Improving the Reproducibility of Experimental Data Recording and Pre-Processing" />
<meta property="og:type" content="book" />

<meta property="og:description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing." />
<meta name="github-repo" content="rstudio/bookdown-demo" />

<meta name="author" content="Brooke Anderson, Michael Lyons, Mercedes Gonzalez-Juarrero, Marcela Henao-Tamayo, and Gregory Robertson" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing.">

<title>3.2 Selecting software options for pre-processing | Improving the Reproducibility of Experimental Data Recording and Pre-Processing</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#rigor-and-reproducibility-in-computation" id="toc-rigor-and-reproducibility-in-computation"><span class="toc-section-number">1</span> Rigor and reproducibility in computation</a></li>
<li><a href="2-experimental-data-recording.html#experimental-data-recording" id="toc-experimental-data-recording"><span class="toc-section-number">2</span> Experimental Data Recording</a></li>
<li><a href="3-experimental-data-preprocessing.html#experimental-data-preprocessing" id="toc-experimental-data-preprocessing"><span class="toc-section-number">3</span> Experimental Data Preprocessing</a></li>
<li><a href="4-references.html#references" id="toc-references"><span class="toc-section-number">4</span> References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="module12a" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Selecting software options for pre-processing</h2>
<p>[Intro]</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Describe software approaches for pre-processing data</li>
<li>Compare the advantages and disadvantages of Graphical User Interface–based
versus scripted approaches and of open-source versus proprietary approaches
to pre-processing</li>
</ul>
<p>The previous module described some common themes and processes in
preprocessing biomedical data. These are often combined together into a pipeline
(also called a workflow). These pipelines can become fairly long and complex
when you need to preprocess data that are complex.</p>
<p>While we’ve covered some key processes of preprocessing, we haven’t talked yet
about the tools you can use to implement it. Most preprocessing will be done on
the computer, with software tools. An exception might be for very simple
preprocessing tasks—one example is generating the average cage weight for a
group of mice based on the total cage weight and the number of mice. However,
even simple processes like this, which can be done by hand, can also be done
with a computer, and doing so can help avoid errors and to provide a record of
the calculation that was used for the preprocessing.</p>
<p>You will have a choice about which type of software you use for preprocessing.
There are two key dimensions that separate these choices—first, whether the
software is point-and-click versus script-based, and, second, whether the
software is proprietary versus open-source. It is important to note
that, in some cases, it may make sense to develop a pipeline that chains
together a few different software programs to complete the required
preprocessing.</p>
<p>In this module, we’ll talk about the advantages and disadvantages of these
different types of software. For reproducibility and rigor, there are many
advantages to using software that is script-based and open source for data
preprocessing, and so in later modules, we’ll provide more information on how
you can use this type of software for preprocessing biomedical data. We also
recognize, however, that there are some cases where such software may not be a
viable option for some or all of the data preprocessing for a project.</p>
<p><strong>GUI versus code script</strong></p>
<p>When you pick software for preprocessing, the first key dimension to consider
is whether the software is “point-and-click” or script-based.
Let’s start with a definition of each.</p>
<p>Point-and-click software is more formally known as GUI software, where GUI stand
for “graphical user interface”. These are programs where your hand is on the
mouse most of the time, and you use the mouse to select actions and options from
buttons and other widgets that are shown by the software on the screen. This
type of software is also sometimes called “widget-based”, as it is built around
widgets like drop-down menus and slider bars <span class="citation">(J. M. Perkel 2018)</span>.</p>
<p>A basic example of GUI-based software is your computer’s calendar application
(“application” is a common synonym for “software”). To navigate across dates on
your calendar, you use your mouse to click on arrows or dates on the calendar.
The software includes some text entry—for example, if you add something to
your calendar, you can click on a textbox and enter a description of the
activity using your keyboard. However, the basic way that you navigate and use
the software is via your computer mouse.</p>
<p>Script-based software uses a script, rather than clickable buttons and graphics,
as its main interface. A script, in this case, is a line-by-line set of
instructions describing what actions you want the software to perform. With
script-based software, you typically keep your keys on the keyboard more often
than on the mouse. Many script-based software programs will also allow you to
also send the lines of instructions one at a time in an area referred to as a
“console”, which will then return the result from each line after you run it.
Script-based software is also sometimes called software that is “used
programatically” <span class="citation">(J. M. Perkel 2018)</span>. Several script-based software programs are
commonly used with biomedical data including R, Python, and Unix bash scripts,
as well as some less common but emerging software programs like Julia.</p>
<p>When comparing point-and-click software to script-based software for
preprocessing, there are a few advantages to point-and-click software, but
many more to script-based software. In terms of code rigor and reproducibility,
script-based software comes out well ahead, especially when used to its
full advantage.</p>
<p>While script-based software has many advantages when it comes to rigor and
reproducibility, there are some appealing features of point-and-click software.
These features likely contribute to its wide popularity and to the fact that the
vase majority of software that you use in your day-to-day life outside of
research is probably point-and-click.</p>
<p>First, point-and-click software is often easier to learn to use, at least in
terms of basic use. The visual icons help you navigate choices and actions in
the software. Most GUIs are designed to take the underlying processes and
make them easier for a new user to access and use. They do this through
an interface that is visual, rather than language- and script-based.
Further, many people are most familiar with point-and-click
software, since so many everyday applications are of this type, and so its
interface can feel more familiar to users.
They also are easier for a new user to pick up because they typically
provide a much smaller set of options than a full programming language does.</p>
<p>By contrast, coding languages take more investment of time and energy to
initially learn how to use. This is
because a coding language is just that—a language. It is built on a set (often
large) of vocabulary that you must learn to be proficient in it, as you must
learn the names and options for a large set of functions within the language.
Further, it has rules and logic you must learn in terms of options for how to
structure and access data and how the inputs and outputs of different functions
can be chained together to build pipelines for preprocessing and analysis.</p>
<p>However, while there is a higher investment required to learn script-based
software versus point-and-click software, there is a very high payoff from
that effort. Script-based software creates a full framework for you to
combine tools in interesting ways and to build new tools when you need them.
With point-and-click software, there’s always a layer between the user and
the computer logic, and you are constrained to only use tools that were
designed by the person who programmed the point-and-click software. By
contrast, with script-based software, you have more direct access to the
underlying computer logic, and with many popular script-based languages
(R, Python), you have extraordinary power and flexibility in what you can
ask the program to do.</p>
<p>As an analogy, think about traveling to a country where you don’t yet speak the
language. You have a few choices in how you could communicate. You could
memorize a few key phrases that you think you’ll need, or get a phrase book that
lists these key phrases. Another choice is to try to learn the language,
including learning the grammar of the language, and how thoughts are put
together into phrases. Learning the language, even at a basic level, will take
much more time. However, it will allow you much greater ability to express
yourself. If you only know set phrases, then you may know how to ask someone at
a bakery for loaf of bread, if the person who wrote the phrase book decided to
include that, but not how to ask at a hotel for an extra blanket, if that wasn’t
included. By contrast, if you’ve learned the language, you have learned how to
form a question, and so you can extrapolate to express a great variety of
things.</p>
<p>Point-and-click software is often like using a phrase book for a foreign
language—if the person who developed the tool didn’t imagine something that
you need, you’re stuck. Scripted software is more like learning a language—you
have to learn the rules (grammar) and vocabulary (names of functions and
their parameters), but once you do, you can combine them to address a wide
variety of tasks, including things no one else has yet thought of.</p>
<p>In the late 1990s, a famous computer scientist named Richard Hamming wrote
a book called, “The Art and Science of Engineering”, in which he talks
a lot about the process of building things and the role that programming
can play in this process. He predicts that by 2020, it will be the experts in a particular field that do programming for that field, rather than experts in
computer programming trying to build tools for other fields <span class="citation">(Hamming 1997)</span>.
He notes:</p>
<blockquote>
<p>“What is wanted in the long run, of course, is that the man with the problem
does the actual writing of the code with no human interface, as we all to often
have these days, between the person who knows the problem and the person who
knows the programming language. This date is unfortunately too far off to
do much good immediately, but I would think by the year 2020 it would be
fairly universal practice for the expert in the field of application to do
the actual program preparation rather than have experts in computers (and
ignorant in the field of application) do the program preparation.” <span class="citation">(Hamming 1997)</span></p>
</blockquote>
<p>The rise of open-source, scripted programs like Python and R has helped to
achieve this vision—scientists in a variety of fields now write their own
small software programs and tools, building on the framework of larger
open-source languages. Training programs in many scientific fields recommend
or require at least one course in programming in these languages, often
taught in conjunction with data analysis and data management.</p>
<p>Another element that has helped make script-based software more accessible is
the development of programming languages that are easier to learn and use.
Very early programming languages required the programmer to understand a lot
about how the computer was built and organized, including thinking about where
and how data were stored in the computer’s memory. As programming languages
have developed, such “low-level” languages have remained in use, as they
often allow for unmatched speed in processing. However, more “higher-level”
programming languages have also developed, and while these might be
somewhat slower in processing, they are much faster for humans to learn
and create tools with, as they abstract away many of the details that make
low-level programming more difficult.</p>
<p>Because of the development of easier-to-learn high-level programming languages
like R and Python, it is possible for a scientist to become proficient in using
one of these script-based programs in about a year. Often, we find that one
semester of a dedicated course or serious self-study, followed with several
months of regularly applying the software to research data, is enough for a
scientist to become productive in using a script-based software like R or Python
for research. With another year or so of regular use, scientists can often start
making their own small software extensions to the language. However, in a 2017
article on analyzing scRNA-seq data, the author noted that “relatively few
biologists are comfortable working in those environments”, referring to Unix and
R <span class="citation">(J. M. Perkel 2017)</span>, and noted that this was a barrier to using many of the
available tools for working with scRNA-seq data at the time. We would
argue that becoming comfortable with these tools requires an investment, but is
not an insurmountable barrier for biologists.</p>
<p>Scripted-based approaches encourage the user to learn how the underlying process
works. The approach encourages the user to think more like a car owner who gets
under the hood from time to time than like one who only drives the car. This
approach does take more time to learn and develop, but with the upside that the
user will often have a much deeper understanding of what is happenening in each
step, as well as how to fix or adjust different steps to fix a pipeline or
adapt one pipeline to meet another analysis need.</p>
<p>It is true that this is a substantially larger investment in training than a
short course or workshop, which might be adequate for learning the basics of
many point-and-click software programs. There is a particularly high time
investment required to learn to use scripted software well. This can be a
critical barrier, especially for scientists who are advanced in their career and
may have minimal time for further training. Further, it’s more of a barrier in
analyzing some types of biomedical data, due to the extreme size and complexity
of the data <span class="citation">(Nekrutenko and Taylor 2012)</span>.</p>
<p>However, it is much less of a time investment than it takes to become an expert
in a scientific field. It takes years of training to become an expert in
cellular biology or immunology, for example. Richard Hamming’s vision was that
the experts can ask the best and most creative questions of the data, and that
it is best to remove the barrier of a different computer programmer, so that the
expert can directly create the program and leverage the full capabilities of the
computer. Higher-level programming languages now are accessible enough that this
vision is playing out across scientific fields.</p>
<blockquote>
<p>“That a language is easy for the computer expert does not mean it is
necessarily easy for the non-expert, and it is likely non-experts who will do
the bulk of the programming (coding, if you wish) in the near future.”
<span class="citation">(Hamming 1997)</span></p>
</blockquote>
<p>Another advantage of script-based software—and one that is related to the idea
of experts in a scientific field directly programming—is that often the most
cutting edge algorithms and pipelines will be available first in scripted
languages, and only later be added into point-and-click software programs. This
means that you may have earlier access to new algorithms and approaches if you
are comfortable coding in a script-based language.</p>
<p>In some cases, biologists aim to analyze data that represents the cutting edge
of equipment and measurement technology, or that is very specialized for a
particular field. In these cases, scripted, open-source packages will often be
the first place where algorithms working with the data are available. For
example, an article about scRNA-seq from 2017 noted that, at the time, there
were “very few, if any, ‘plug-and-play’ packages” for working with scRNA-seq
data, and of those available, they were “user-friendly but have the drawback
that they are to some extent a ‘black box’, with little transparency as to the
precise algorithmic details and parameters employed.” <span class="citation">(Haque et al. 2017)</span>
Similarly, another article in the same year noted that at the time, “most
scRNA-seq tools exist as Unix programs or packages in the programming language
R”, although “some ready-to-use pipelines have been developed”
<span class="citation">(J. M. Perkel 2017)</span>.</p>
<p>Another key advantage of script-based software is that, in writing the
script, you are thoroughly documenting the steps you took to preprocess the
data. When you create a code script, the script itself includes all
the steps and details of the process. In combination with information about the
version of all software used and the raw data input to the pipeline, it creates
a fully reproducible record of the data preprocessing and analysis.</p>
<p>By contrast, while you could write down the steps that you took and the buttons
you pressed when using point-and-click software, it’s very easy to forget to
record a step. When you use a code script, it will not run if you forget a step
or a detail of that step. Some GUI-based programs are taking steps to try to
ameliorate this, allowing a user to save or download a full record that records
the steps taken in a given pipeline or allow the user to develop a full,
recorded workflow (one example is FlowJo Envoy’s workflow model for analyzing
data from flow cytometry). As a note, there are some movements towards
“integrative frameworks”, which can help improve reproducibility for pipelines
that span different types of software (Galaxy, Gene Prof) <span class="citation">(Nekrutenko and Taylor 2012)</span>.</p>
<p>The final key advantage of scripted software is that—and this is often a gain
that fully pays back the investment in learning the software—it can make data
preprocessing and analysis much more efficient over the long term. Code scripts
often are easier to automate than a workflow through a point-and-click system. For
example, if you need to process a number of files that all follow the same
format, you can often develop a script using one of those files, check that
script very carefully, and the apply it with minimal modifications to each of
the files in the full set. This allows you to spend more time on the template
script, making sure that it works as you expect, and then apply it quickly,
whereas working through multiple files with point-and-click software may
essentially boil down to a lot of time spent in repetition. This
kind of automation can also help in limiting errors from human mistakes
in by-hand processing <span class="citation">(Gibb 2014)</span>.</p>
<p>The other dimension to consider for software for preprocessing is whether it is
open-source or proprietary. Open-source software is software where you can
access, explore, and build on all the underlying code for the software. It also
most often is free. By contrast, the code that powers proprietary software is
typically kept private, so you can use the product but cannot explore the way
that it is built or extend it in the same way that you can open-source software.
In biomedical research, many script-based languages are open-source, while many
point-and-click programs are proprietary. However, this is not a hard and fast
rule, and there are examples of open-source point-and-click software (for
example, the Inkscape program for vector graphic design) as well as proprietary
script-based software (for example, Matlab and SAS). There are advantages and
disadvantages to both types of software, but in terms of rigor and
reproducibility, open-source software often has the advantage.</p>
<p>Transparency is a key element of reproducibility <span class="citation">(Neff 2021)</span>. If the
algorithms of software can be investigated, then scientists who are using two
different programs (for example, one program in Python and one in R) can
determine if their choice of program is causing differences in their results. By
contrast, if two research groups use two different types of proprietary
software, the algorithms that underlie the processing are often kept secret and
so cannot be compared. In that case, if the two groups conduct the same
experiment and get different results, it’s impossible to rule out whether the
difference was caused by the choice of software.</p>
<blockquote>
<p>“Improved reproducibility comes from pinning down methods.” <span class="citation">(Lithgow, Driscoll, and Phillips 2017)</span></p>
</blockquote>
<blockquote>
<p>“As chemists, we have to be able to go to the literature, take a procedure,
and carry out a similar or identical transformation with our own hands. Frustratingly,
this doesn’t always happen, and the next-to-worst-case scenario possible is
when it’s one of your own reactions that can’t be reproduced by a lab
elsewhere. Unsurprisingly, one step worse than this is when you can’t even
reproduce one of your own reactions in your own lab!” <span class="citation">(Gibb 2014)</span></p>
</blockquote>
<blockquote>
<p>“If there is nothing wrong with the reagents and reproducibility is still an
issue, then as I like to tell students, there are two options: (1) the physical
constants of the universe and hence the laws of physics are in a state of flux
in their round-bottomed flask, or (2) the researcher is doing something wrong
and either doesn’t know it or doesn’t want to know it. Then I ask them which
explanation they think I’m leaning towards.” <span class="citation">(Gibb 2014)</span></p>
</blockquote>
<p>Another facet where proprietary software has an advantage is that it will often have
more comprehensive company-based user support than open-source software. The
companies that make and sell proprietary software will usually have a user
support team to answer questions and help develop pipelines and may also offer
training programs or materials.</p>
<p>Some open-source software also has robust user support, although sometimes a bit
less organized under a common source. In some cases, this has developed as a
result of a large community of users who help each other. Message boards like
StackOverflow provides a forum for users to ask and respond to questions. Some
companies also exist that provide, as their business model, user support for
open-source software. While open-source software is usually free, these
companies make money by providing support for that software.</p>
<p>User support is sparser for some of smaller software packages that are developed
as extensions of open-source software. For example, many of the packages for
preprocessing types of biomedical data are built by small bioinformatics teams
or individuals at academic or research institutions. Often this software is
developed by a single person or very small team as one part of their job
profile, with limited resources for user support and for providing training.
These extensions build on larger, more supported open-source software (e.g., R
or Python), but the extension itself is built and maintained by a very small
team that may not have the capacity to respond quickly to user questions. Many
open-source software developers try to create helpful documentation in the form
of help files and package vignettes (tutorials on how to use the software they
created), but from a practical point of view it is difficult for small
open-source developers to provide the same level of user support that a large
proprietary software company can.</p>
<p>This is often the case with cutting-edge open-source software for biomedical
preprocessing. These just-developed software packages are less likely to be
comprehensively documented than longer-established software. Further, it can
take a while for the community of software users to develop once software is
available, and while this is a limitation of new software for both open source
and proprietary languages, it can represent more of a problem for open-source
software, where there is typically not a company-based helpline and so the
community of users often represents one of the main sources for help and
troubleshooting.</p>
<p>For other facets, open-source software has important advantages over proprietary
software. One key advantage is that all code is open in open-source software, so
you can dig down to figure out exactly how each step of the program works.
Futher, in many cases for open-source scientific software, the algorithms and
their principles have gone through peer review as part of the academic
publication process.</p>
<p>With proprietary software, on the other hand, details of algorithms may be
considered protected intellectual property, and so it may be hard to find out
the details of how the underlying algorithms work <span class="citation">(Nekrutenko and Taylor 2012)</span>. Also,
the algorithms may not have gone through peer-review, especially if they are
considered private intellectual property.</p>
<p>Another advantage of open-source software is that older versions of the software
are often well-archived and easily available to reinstall and use if needed to
reproduce an analysis that was done using an earlier version of the software
than the current main version at the time of the replication.</p>
<p>As a final advantage, open-source software is often free. This makes it
economical to test out, and it means that trainees from a lab will have no
problem continuing to use the software as they move to new positions.
The cost with open-source software, then, comes not with the price to buy
the software, but with the investment that is required to learn it.</p>
<div id="discussion-questions-2" class="section level3" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Discussion questions</h3>
<hr />
<p>It can be helpful to walk through some examples some of these preprocessing
steps, in the context of specific types of biomedical experiments. In this
section, we’ll give an overview of the types of preprocessing steps you might
need in two example experiments. In later modules, we’ll go more in depth
into how this could be done in coded scripts using open-source software.</p>
<p><strong>Simpler example: Measuring bacterial growth rates</strong></p>
<p>The first example is when you have collected data to measure bacterial
growth rates. For example, you may be interested in how different the
growth rate of <em>Mycobacterium tuberculosis</em> is in an environment with
ample oxygen compared to low-oxygen environments, as this may have
important implications for how fast the bacteria grow in certain parts
of the human body. You can collect this data by …, saving data on
… in a spreadsheet or other file. Once you have collected the
data, you would then need to take a set of steps to preprocess it
and then analyze it, to allow you to answer your original research
question (how much the growth rate differs in the low-oxygen
environment).</p>
<p>[Module 2.5]</p>
<p>Preprocessing and analysis steps:</p>
<ul>
<li>Input data from spreadsheet in which it was recorded in the laboratory
(preprocessing)</li>
<li>For each sampling time, use the date and time of the sampling to calculate
the time since the start of the experiment (preprocessing)</li>
<li>Identify the period of exponential growth (preprocessing)</li>
<li>Calculate growth rate for each sample during this period of exponential
growth (analysis)</li>
</ul>
<p><strong>More complex example: Characterizing lung cell populations</strong></p>
<p>The second example is when you have collected single-cell RNA-seq
data from a sample with the aim of characterizing the cell populations
in the animal’s lungs.</p>
<p>One example of a pipeline of pre-processing and analysis might include
<span class="citation">(Luecken and Theis 2019)</span>:</p>
<ul>
<li>Read in raw sequencing data</li>
<li>Generate count matrices from the raw sequencing data [what count matrices are]</li>
<li>Quality control to identify and remove low quality cells from further
analysis, as well as to remove some transcripts (e.g., those that appear in
only a few cells)</li>
<li>Normalization to allow meaningful comparisons across cells, accounting for differences across cells in capture efficiency, amplification (reverse transcription?), sequencing, etc. This may involve processes that assume the
number of mRNA molecules in each cell is the same, or alternatively that
half or fewer of the genes are differentially expressed across cells.</li>
<li>Batch collection</li>
<li>Dimension reduction, to focus only on highly variable genes and exclude
from further analysis genes that have a similar expression across cells
(e.g., “housekeeping” genes)</li>
<li>Clustering and cluster annotation, to identify different cell populations</li>
<li>Differential expression analysis, to determine which genes have different
levels of expression in different cell populations [?]</li>
<li>Comparing cell populations across samples from different conditions (e.g.,
treated versus control)</li>
</ul>
</div>
<div id="example-dataset" class="section level3" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Example dataset</h3>
<p>In this module and several of the following modules, we’ll use two example
datasets that are based on real data collected from an immunology experiment.
These data will help us to describe and motivate the steps of pre-processing
data using code scripts, as well as explaining the use of tidyverse tools
and the discussion of complex versus tidy data formats in R.</p>
<p>The first dataset was introduced in module [x]. …</p>
<p>The second dataset have not yet been introduced. They come from an experiment to
test a novel vaccine for COVID-19, and we’ll be focusing on data collected
during this experiment that measured single-cell transcriptomics—in other
words, it characterizes the levels of messenger RNA with each of thousands of
cells collected from animals in different experimental groups. In this section,
we’ll give you more details to help you understand this example dataset, as well
as instructions on how to download the data on your own computer, if you would
like to follow along with examples.</p>
<p>The results of this experiment have been published [ref], and so you can read
the full details in the published paper, but we’ll provide an overview here.
This experiment tested a potential vaccine called SolaVAX. There are numerous
ways to make vaccines; this one uses an attenuated (in other words, weakened)
version of the full virus, and it is novel in that it attenuates the virus using
a light-activated … (hence the “Sola” in “SolaVAX”).</p>
<p>This experiment tested how well this vaccine worked, not only by itself, but
also when it was given in conjunction with something called an adjuvant. In the
context of vaccines, an adjuvant is a substance that is meant to shape the
body’s immune response as it “learns” from a vaccine. For example, an adjuvant
can be something as simple as a substance that triggers a larger immune response
than the vaccine by itself, to ensure that the immune system responds at a
sufficient scale to the core components of the vaccine—that is, the components
of the vaccine that the immune system needs to recognize in the future to mount
a fast defense against that pathogen. This experiment tested two adjuvants
in conjunction with the SolaVAX vaccine, both of which the researchers were
hoping might help in switching which type of T helper immune cells would
drive the later response to COVID after vaccination. Specifically, they
hypothesized that the adjuvants would bring about a later response that was
driven more by a type of T helper cell called Th2 rather than one called
Th1. [Why this would be good.]</p>
<p>To run the experiment, the researchers used Golden Hamsters as a model
animal. [Why?] They created four experimental groups: one control group,
one group vaccinated with only SolaVAX, one vaccinated with SolaVAX plus
an adjuvant called [x], and one vaccinated with SolaVAX plus an adjuvant
called [y]. There were eight hamsters in each of these groups, and these
were further divided into two groups of four, so that the vaccine could
be tested using two routes of administration: [the two routes].</p>
<p>The hamsters were vaccinated, and then after a period of time, they were
challenged with COVID. This allowed the researchers to see how successful
each vaccination type was, in terms of how well the animals could limit
the replication of COVID in their bodies, and also to explore how the
animals’ immune systems responded as they tried to limit COVID replication
after exposure. Therefore, this experiment could help not only in seeing
which vaccine strategies were successful, but also to explore how and why
they did or didn’t work, at the level of the immune response.</p>
<p>[x] days after the animals were exposed to COVID, they were sacrificed,
and the researchers took samples from several areas to use to measure
levels of COVID as well as the immune response to the challenge. In tissue
samples from the lungs of the animals, they measured things like the
[viral numbers?] to see the extent of COVID replication in that animal,
and [histopathology], to see the extent of damage that the infection had
done to the animal’s lungs. If the vaccine were successful, it would have
spurred a fast and powerful immune response, which you’d see through
lower [viral numbers] and less damage to the animal’s lungs.
They also used part of the lung sample to measure immune cell populations.
These measures can help to determine things like whether the immune
response was driven more by the innate immune response (which you’d
expect in an unvaccinated animal) or the adaptive immune response
(which you hope to see in a vaccinated animal, as a sign that the vaccine
helped in allowing for the immediate immune response to be much more
specific than that achieved by the innate immune system).</p>
<p>Out of the many types of data that were collected for this experiment, we’ll
focus on one type in our examples in this and following modules—data that were
collected that measured the single-cell transcriptomics of samples collected
from the lungs of each animal. While every cell in a body has the same genome,
the cells differ in which of those genes they express at a given time. The
expression of different genes in the genome can be measured based on the number
of messenger RNAs that are in the cell from each gene. This is what single cell
RNA sequencing aims to measure—the number of mRNAs from each of a large number
of genes, measuring the number in each cell in the sample separately. Because
different types of cells express different patterns and levels of genes, these
data can be used to help sort the cells into different types. For example, the
data can be used to help identify cells that are from the innate immune system
versus those that are hallmarks of an adaptive immune response. The data can
also help in categorizing cells within very specific cell categories—for
example, identifying Th1 versus Th2 cells out of the group of helper T cells,
which can help to address the hypothesis that these researchers had about how
the adjuvants might work. Finally, the data can help to identify how certain
types of cells work differently under different experimental conditions. For
example, do adaptive immune cells tend to express different genes (or different
levels of a gene) when the vaccination included an adjuvant versus when only the
vaccine was given?</p>
<p>These single cell RNA sequencing data were shared by the researchers through
one of NIH’s [?] databases for biological data. To get a copy of these
data, go to the study’s page on the Gene Expression Omnibus (GEO) database:
<a href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE165190" class="uri">https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE165190</a> (you can also
search for the study on the database by its accession number, GSE165190).
This page includes data from single cell
RNA sequencing for twelve samples from this study, three from each of the
experimental conditions. You can download data for all the samples by clicking
the link on the page to download the file named “GSE165190_RAW.tar”. This is
a compressed file, so once you download it, you will need to uncompress it,
which will give you a folder with a number of data files. On a Mac, you can
decompress the file by double-clicking on the file named “GSE165190_RAW.tar”
in the Finder program. On a Windows computer, [how to do it. From R?]</p>
<p>Among the data files, there are three files for each sample. One includes
“barcodes” in the file name, one includes “features” in the file name, and one
includes “matrix” in the file name. For each sample, the “barcodes” file gives
…, the “features” file gives …, and the “matrix” file gives … .</p>
<p>[More about working with these files?]</p>
<p>Collectively, these data provide a count that is related to the number of
messanger RNA particles [?] from each of approximately [x] genes within each
cell in the sample. These counts can be used to group the cells into
groups of similar cell types, identify those cell types, and explore how
the gene expression within a cell type varies across experimental conditions.</p>
<p>These data can therefore be used to answer interesting scientific questions.
However, before they can, it is important to do some pre-processing on the raw
data. This preprocessing serves several purposes. First, there are patterns
in the data that can be introduced as a result of the data collection process.
These need to be corrected or otherwise accounted for to move the data into
a format where it can be meaningfully compared, for example across different
cells in a sample or across different samples. Second, some of the pre-processing
will help identify and resolve any issues related to quality control. For
example, while the process of collecting these data will normally result in
isolating single cells, in some cases the values for a cell might be for a
poor-quality cell (for example, a dying cell) or might be a case where two
cells were captured together [true for this platform?]. Pre-processing can
help to identify and exclude these low-quality data points, so that the
analysis can focus on the higher-quality data collected for the sample.
Finally, some of the pre-processing will help us to prepare the data to be
used in analysis algorithms. For example, since the data is high dimensional
(that is, measurements are included for many genes [?]), we will often include
a step of dimension reduction in our analysis, to help pull out key patterns
in the data. Many of the dimension reduction algorithms will need to data to
be scaled, so that the mean value of each measurement in a type of measurement
has an average of zero and a standard deviation [? variance?] of one.</p>
<blockquote>
<p>“As a proxy for studying the proteome, many researchers have turned to
protein-encoding, mRNA molecules (collectively termed the ‘transcriptome’),
whose expression correlates well with cellular traits and changes in
cellular state. Transcriptomics was initially conducted on ensembles of
millions of cells, firstly with hybridization-based microarrays, and later
with next-generation sequencing (NGS) techniques referred to as RNA-seq.
RNA-seq on pooled cells has yielded a vast amount of information that
continues to fuel discovery and innovation in biomedicine. …
Nevertheless, the averaging that occurs in pooling large numbers of
cells does not allow detailed assessment of the fundamental biological
unit—the cell—or the individual nuclei that package the genome.
Since the first scRNA-seq study was published in 2009, there has been
increasing interest in conducting such studies. Perhaps one of the most
compelling reasons for doing so is that scRNA-seq can describe RNA molecules
in individual cells with high resolution and on a genomic scale.”
<span class="citation">(Haque et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“scRNA-seq permits comparison of the transcriptomes of individual cells.
Therefore, a major use of scRNA-seq has been to assess transcriptional
similarities and differences within a population of cells, with early reports
revealing unappreciated levels of heterogeneity… Thus, heterogeneity analysis
remains a core reason for embarking on scRNA-seq studies. Similarly, assessments
of transcriptional differences between individual cells have been used to
identiy rare cell populations that would otherwise go undetected in analyses of
pooled cells, for example malignant tumour cells within a seemingly homogeneous
group. … In addition to resolving cellular heterogeneity, scRNA-seq can also
provide important informaiton about fundamental characteristics of gene
expression… Importantly, studying gene co-expression patterns at the
single-cell level might allow identification of co-regulated gene modules and
even inference of gene-regulatory networks that underlie functional
heterogeneity and cell-type specification.” <span class="citation">(Haque et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“Single-cell RNA sequencing (scRNA-seq) is a recent and powerful technology
developed as an alternative to previously existing bulk RNA sequencing methods.
Bulk sequencing methods analyzed the average genetic content for individual
genes across a large population of input cells within a sample (e.g., a tissue),
potentially obscuring transcriptional features and other differences among
individual cells. Conversely, scRNA-seq is able to discern such heterogeneous properties
within a sample and has great potential to reveal novel subpopulations and
cell types.” <span class="citation">(Lytal, Ran, and An 2020)</span></p>
</blockquote>
<blockquote>
<p>“Single-cell RNA sequencing is widely used for high-resolution gene expression
studies investigating the behavior of individual cells.” <span class="citation">(McCarthy et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“While scRNA-seq data can provide substantial biological insights, the complexity
and noise of the data is also much greater than that of conventional bulk RNA-seq.
Thus, rigorous analysis of scRNA-seq data requires careful quality control to remove
low-quality cells and genes, as well as normalization to adjust for biases and
batch effects in the expression data. Failure to carry out these procedures
correctly is likely to compromise the validity of all downstream analyses.”
<span class="citation">(McCarthy et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“Conventional ‘bulk’ methods of RNA sequencing (RNA-seq) process hundreds of
thousands of cells at a time and average out the differences. But no two cells
are exactly alike, and scRNA-seq can reveal the subtle changes that make each
one unique. It can even reveal entirely new cell types.” <span class="citation">(J. M. Perkel 2017)</span></p>
</blockquote>
<blockquote>
<p>“It’s much more difficult to manipulate individual cells than large populations,
and because each cell yields only a tiny amount of RNA, there’s no room for
error. Another problem is analyzing the enormous amounts of data that result—not
least because the tools can be unintuitive.” <span class="citation">(J. M. Perkel 2017)</span></p>
</blockquote>
</div>
<div id="what-is-pre-processing" class="section level3" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> What is pre-processing?</h3>
<blockquote>
<p>“Assessing the structure of the data must also take account of the prior
knowledge of the system in regard to such matters as the design of th experiment,
the known sources of systemic variation (e.g., any blocking factors or known
groupings of the experimental units) and so on.” <span class="citation">(Chatfield 1995)</span></p>
</blockquote>
<blockquote>
<p>“The three main types of problem data are errors, outliers, and missing
observations. … An error is an observation which is incorrect, perhaps
because it has been copied or typed incorrectly at some stage. An outlier is a ‘wild’
or extreme observation which does not appear to be consistent with the rest of
the data. Outliers arise for a variety of reasons and can create severe problems.
… Errors and outliers are often confused. An error may or may not be an outlier,
while an outlier may or may not be an error. … An outlier may be caused by an error,
but it is important to consider the alternative possibility that the observation
is a genuine extreme result from the ‘tail’ of the distribution. This usually happens
when the distribution is skewed and the outlier comes from the long ‘tail’.”
<span class="citation">(Chatfield 1995)</span></p>
</blockquote>
<blockquote>
<p>“There are several types of error…, including the following: 1. A recording error
arises, for example, when an instrument is misread. 2. A typing error arises when an
observation is typed incorrectly. 3. A transcription error arises when an observation
is copied incorrectly, and so it is advisable to keep the amount of copying to a
minimum. …” <span class="citation">(Chatfield 1995)</span></p>
</blockquote>
<blockquote>
<p>“Extreme observations which, while large, could still be correct, are more difficult
to handle [than errors]. The tests for deciding whether an outlier is significant
provide little information as to whether an observation is actually an error. Rather
external subject-matter considerations become paramount. It is essential to get advice
from people in the field as to which suspect values are obviously silly or impossible,
and which, while physically possible, are extremely unlikely and should be viewed with
caution. … It is sometimes sensible to remove an outlier, or treat it as a missing
observation, but this outright rejection of an observation is rather drastic, particularly
if there is evidence of a long tail in the distribution. Sometimes the outliers are the
most interesting observations. … An alternative approach is to use robust methods of
examination, which automatically downweight extreme observations… My recommended
procedure for dealing with outliers, when there is no evidence that they are errors,
is to repeat the analysis with and without suspect values… If the conclusions are
similar, then the suspect values do not matter. However, if the conclusions differ
substantially, then the values do matter and additional effort should be expended
to check them. If the matter still cannot be resolved, then it may be necessary to present
two lots of results or a least point out that it may be unwise to make judgements
from a set of data where the results depend crucially on just one or two
observations (called influential observations).” <span class="citation">(Chatfield 1995)</span></p>
</blockquote>
<blockquote>
<p>“Missing observations arise for a variety of reasons. … It is important to find out
why an observation is missing. This is best done by asking ‘people in the field’. In
particular, there is a world of difference between observations that are lost through
random events, and situations where missing observations due to damage or loss are
more likely to arise under certain conditions.” <span class="citation">(Chatfield 1995)</span></p>
</blockquote>
<p>Some data collected through laboratory experiments is very straightforward and
requires little or no pre-processing before it’s used in analysis. For example,
if you are regularly weighing the animals in an experiment, then the data may
require no pre-processing (in other words, you’ll directly use the weight
recorded from the scale) or very minimal pre-processing (for example, if you are
keeping all animals for a treatment group in the same cage, you may weigh the
cage as a whole, in which case you could divide that weight by the number of
animals in the cage as a pre-processing step to estimate the average weight per
animal).</p>
<p>Other data collected in the laboratory may require some pre-processing that
takes a few more steps, but is still fairly straightforward. For example, if you
plate bacteria from a sample at a variety of dilutions, you might count each
plate and determine a measure of Colony Forming Units from the set of plates
with different dilutions by deciding which dilution provides the clearest count
and then back-calculating based on its dilution to get the total number of
colony-forming units in the original sample. Pre-processing these data typically
will also involve transforming data, to get them in a format that is easier to
visualize or more appropriate for statistical analysis. For example, when
bacterial loads are counted as colony forming units (CFUs), it is common to
transform these values using a log-10 [?] transform before plotting the data or
using them to test a hypothesis. [Why do a log-10 transform with CFUs?]</p>
<p>This step of pre-processing data can become much more complex with data that was
collected using complex equipment, like a flow cytometer, mass spectrometer, or
sequencer. In these cases, there are often steps required to extract from the
machine’s readings a biologically-relevant measurement. For example, the data
output from a mass spectrometer must be processed to move from measurements of
mass and retention time to estimates of concentrations of different molecules in
the sample. If you want to compare across multiple samples, then the
preprocessing will also involve steps to align the different samples (in terms
of …), as well as to standardize the measurements for each sample, to make the
measurements from the different samples comparable.</p>
<blockquote>
<p>“Scientists can now study all kinds of cell components and processes—from all
the proteins in a cell (a discipline known as proteomics) to the amount of messenger
RNA (the templates from which proteins are made) made from every gene (‘transcriptomics’)
to the intermediate and final products of cell metabolism (‘metabolomics’).”
<span class="citation">(Barry and Cheung 2009)</span></p>
</blockquote>
<blockquote>
<p>“To reap the full benefits of the omics revolution, we need information technology
tools capable of making sense of the vast data sets generated by omics experiments.
In fact, the development of such tools has become a discipline unto itself, called
bioinformatics. And only with those tools can researchers hope to clear another
obstacle to drug development: that posed by so-called emergent properties—behaviors
of biological systems that cannot be predicted from the basic biochemical properties
of their components.” <span class="citation">(Barry and Cheung 2009)</span></p>
</blockquote>
<p>For data collected from a flow cytometer, preprocessing may include steps to
disentangle the florescence from different markers to ensure that the read for
one marker isn’t inflated by spillover florescence from a different marker. …</p>
<p>For data from a sequencing machine [? sequencer?], the pre-processing will
include a series of steps. We’ll use an example of data from single-cell RNA
sequencing. First, the sequencing process results in small fragments of
complementary DNA [? cDNA], complementary to the original RNA in each cell in
the sample, for which the sequence of nucleotide bases along each small fragment
have been determined [?]. These raw read data can be used to answer more
interesting scientific questions—for example, how do gene expression levels
vary across cells in the samples, or what mixture of cell types comprise the
sample—but first the raw data require multiple steps of pre-processing. For
example, the small fragments of cNDA sequences must be aligned to a reference
genome or transcriptome, so that their counts can be used to estimate the
number of RNA molecules transcribed from different genes, and the counts for
each fragment must also be linked with the cell it originally came from, so
that these counts can be determined cell-by-cell.</p>
<blockquote>
<p>“Although each [scRNA-seq] experiment is unique …, most analysis pipelines follow
the same steps to clean up and filter the sequencing data, work out which transcripts
are expressed and correct for differences in amplification efficiency. Researchers
then run one or more secondary analyses to detect subpopulations and other
functions.” <span class="citation">(J. M. Perkel 2017)</span></p>
</blockquote>
<blockquote>
<p>“In many cases… the tools used in bulk RNA-seq can be applied to scRNA-seq. But
fundamental differences in the data mean that this is not always possible. For one
thing, single-cell data are noisier… With so little RNA to work with, small
changes in amplification and capture efficiencies can produce large differences from
cell to cell and day to day and have nothing to do with biology. Researchers must
therefore be vigilant for ‘batch effects’, in which seemingly identical cells
prepared on different days differ for purely technical reasons, and for
‘dropouts’—genes that are expressed in the cell but not picked up in the sequence
data. Another challenge is the scale… A typical bulk RNA-seq experiment involves
a handful of samples, but scRNA-seq studies can involve thousands. Tools that can
handle a dozen samples often slow to a crawl when confronted with ten or a hundred
times as many.” <span class="citation">(J. M. Perkel 2017)</span></p>
</blockquote>
<blockquote>
<p>“Even the seemingly simple question of what constitutes a good cell preparation is
complicated in the world of scRNA-seq. Lun’s workflow assumes that most of the
cells have approximately equivalent RNA abundances. But ‘that assumption isn’t
necessarily true’, he says. For instance, he says, naive T cells, which have never
been activated by an antigen and are relatively quiescent, tend to have less
messenger RNA than other immune cells and could end up being removed during analysis
because a program thinks there is insufficient RNA for processing.” <span class="citation">(J. M. Perkel 2017)</span></p>
</blockquote>
<blockquote>
<p>“Perhaps most significantly, researchers performing scRNA-seq tend to ask different
questions from those analysing bulk RNA. Bulk analyses typically investigate how
gene expression differs between two or more treatment conditions. But researchers
working with single cells are often aiming to identify new cell types or states or
reconstruct developmental cellular pathways. ‘Because the aims are different, that
necessarily requires a differnt set of tools to analyse the data,’ says Lun.”
<span class="citation">(J. M. Perkel 2017)</span></p>
</blockquote>
<blockquote>
<p>“Most scRNA-seq tools exist as Unix programs or packages in the programming
language R. But relatively few biologists are comfortable working in those
environments… Even if they are, they may lack the time to download and configure
everything to make such tools work. Some ready-to-use pipelines have been
developed. And there are end-to-end graphical tools too, including the
commercial SeqGeq package from FlowJo, as well as a pair of open-source
web tools: Granatum … and ASAP (the Automated Single-cell Analysis Pipeline)…
ASAP and Granatum use a web browser to provide relatively simple, interactive
workflows that allow researchers to explore their data graphically. Users upload
their data and the software walks them through the steps one by one. For ASAP,
that means taking data through preprocessing, visualization, clustering, and
differential gene-expression analysis; Granatum allows pseudo-time analyses
and the integration of protein-interaction data as well.” <span class="citation">(J. M. Perkel 2017)</span></p>
</blockquote>
<blockquote>
<p>“Appropriate methods are ‘very data-set dependent’… The methods and tuning
parameters may need to be adjusted to account for variable such as sequencing
length. But John Marioni at Cancer Research UK in Cambridge says it’s important not
to put complete faith in the pipeline. ‘Just because the satellite navigation
tells you to drive into the river, you don’t drive into the river,’ he says.”
<span class="citation">(J. M. Perkel 2017)</span></p>
</blockquote>
<blockquote>
<p>“For beginners, caution is warrented. Bioinformatics tools can almost always yield
an answer; the question is, does that answer mean anything? Dudoit’s advice is do
some exploratory analysis, and verify that the assumptions underlying your chosen
algorithms make sense.” <span class="citation">(J. M. Perkel 2017)</span></p>
</blockquote>
<ul>
<li>cDNA libraries</li>
<li>Mapping sequence fragments to a reference (BowTies, TopHats) / alignment of reads
(GSNAP) / pseudoalignment (Salmon)</li>
<li>Quantification / counting reads for each gene (htseq-count)</li>
</ul>
<blockquote>
<p>“Next, poly[T]-primed mRNA is converted to complementary DNA (cDNA) by
a reverse transcriptase. Depending on the scRNA-seq protocol, the reverse-transcription
primers will also have other nucelotide sequences added to them, such as
adaptor sequences for detection on NGS platforms, unique molecular identifiers
(UMIs) to mark unequivocally a single mRNA molecule, as well as sequences to
preserve information on cellular origin.” <span class="citation">(Haque et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“The minute amounts of cDNA are then amplified either by PCR or, in some instances,
in vitro transcription followed by another round of reverse transcriptions—some
protocols opt for nucleotide barcode-tagging at this stage to preserve information on
cellular origin.” <span class="citation">(Haque et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“Then, amplified and tagged cDNA from every cell is pooled and sequenced by NGS,
using library preparation techniques, sequencing platforms and genomic-alignment
tools similar to those used for bulk samples.” <span class="citation">(Haque et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“More recently, droplet-based platforms … have become commercially available…
Droplet-based instruments can encapsulate thousands of single cells in individual
partitions, each containing all the necessary reagents for cell lysis, reverse
transcription and molecular tagging, thus eliminating the need for single-cell
isolation through flow-cytometric sorting or micro-dissection.” <span class="citation">(Haque et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“An important initial step in scRNA-seq data processing is to quantify the
expression level of genomic features such as transcripts or genes from the
raw sequencing data.” <span class="citation">(McCarthy et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“Read counts can be obtained from conventional quantification methods such
as HTSeq and featureCounts … another option is to use computationally efficient
pseudoalignment methods such as kallisto and Salmon. This is especially appealing
for large scRNA-seq datasets containing hundreds to tens of thousands of cells.
To this end, scater also provides wrapper functions for kallisto and Salmon so
that fast quantification of transcript-level expression can be managed completely
within an R programming environment.” <span class="citation">(McCarthy et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“A common subsequent step for these methods is to collapse transcript-level
expression to gene-level expression. Exploiting the biomaRt R/Bioconductor
package, scater provides a convenient function for using Ensembl annotations
to obtain gene-level expression values and gene or transcript annotations.”
<span class="citation">(McCarthy et al. 2017)</span></p>
</blockquote>
<p>These early steps of pre-processing create a count matrix for each sample,
with a row for each feature of the data (for example, a gene), a column for
each cell in the sample, and matrix cell values that estimate the number of
messenger RNA molecules from each gene in each cell. However, more pre-processing
is required before these count matrices can be used to answer scientific
questions.</p>
<p>First, it is necessary to perform quality control on the count matrices, to
identify and remove problematic data points. For example, one method for
single cell sequencing involves isolating each cell within a droplet. While
most of the droplets that are processed will indeed have one and only one cell,
a few droplets will end up having two cells or no cells, and these must be
identified and removed during a quality control step of pre-processing. For
all methods of single cell sequencing, there will also be some poor-quality
cells. For example, if a cell’s membrane has broken [?] during the process of
measurement, then much of its messenger RNA may have leaked out [?] of the
cell. These cells can be identified based on a combination of characteristics:
they typically will have a combination of low total counts of RNA molecules,
low numbers of total genes expressed, and a high percentage of the RNA molecules
that are captured will be from mitochondrial RNA [because?]. Using these
characteristics, many of these cells can be identified and removed from the
count matrix in a quality control step of pre-processing. Other, more sophisticated
approaches are also available, based on machine learning approaches to
identify these low quality cells <span class="citation">(Ilicic et al. 2016)</span>.</p>
<blockquote>
<p>“Another important challenge is that existing available scRNA-seq protocols
often result in the captured cells (whether chambers in microfluidic systems,
microwell plates, or droplets) being stressed, broken, or killed. Moreover,
some capture sites can be empty and some may contain multiple cells. We refer
to all such cells as ‘low quality’. These cells can lead to misinterpretation
of the data and therefore need to be excluded.” <span class="citation">(Ilicic et al. 2016)</span></p>
</blockquote>
<blockquote>
<p>“We first tested whether each type of low quality cell (broken, empty, multiple)
has higher average gene expression in specific functional categories (Gene
Ontology terms) compared to high quality cells. Second, we calculated whether
gene expressio in these functional categories is noisier for low versus high
quality cells… Our results suggest that there are indeed several top-level
biological processes and components that are significantly different. Specifically,
genes related to Cytoplasm …, Metabolism …, Mitochondrion …, Membrane …,
and a few other categories … are strongly downregulated … in broken cells.
… Furthermore, broken cells have transcriptome-wide
increased noise levels compared to high quality cells. Interestingly, wells
containing multiple cells (multiples) show similar expression and noise patterns
to broken cells … This suggest that multiple cells contain a mixture of broken
and high quality cells.” <span class="citation">(Ilicic et al. 2016)</span></p>
</blockquote>
<blockquote>
<p>“Genes related to mtDNA were upregulated in low quality cells… This suggested
that these cells are broken and thus of low quality.” <span class="citation">(Ilicic et al. 2016)</span></p>
</blockquote>
<blockquote>
<p>“We have shown that there are biological and technical features within the
sequencing data that allow automatic identification of the majority of low
quality cells.” <span class="citation">(Ilicic et al. 2016)</span></p>
</blockquote>
<blockquote>
<p>“At least a subset of the features considered are cell type specific. …
Somewhat surprisingly, the levels of Membrane, Ribosomes, Metabolism, Apoptosis,
and Housekeeping genes are highly cell type specific. In contrast, Mitochondrial
(localized or encoded) and Cytoplasmic genes are more generic features. …
Interestingly, only moderately and strongly expressed genes seem to be similar
between the datasets [of different cell types]. Geners that are very strong or
lowly expressed are highly cell type specific.” <span class="citation">(Ilicic et al. 2016)</span></p>
</blockquote>
<blockquote>
<p>“There is an extensive literature on the relationship between mtDNA, mitochondrially
localized proteins, and cell death … However, upregulation of RNA levels of mtDNA
in broken cells suggests losses in cytoplasmic content. In a situation where cell
membrane is broken, cytoplasmic RNA will be lost, but RNAs enclosed in the
mitochondria will be retained, thus explaining our observation.” <span class="citation">(Ilicic et al. 2016)</span></p>
</blockquote>
<blockquote>
<p>“Overall, our analysis suggests that empty wells can be remarkably clearly distinct
from the remainder, while broken cells and multiples are distinct in most but not all
of the categories.” <span class="citation">(Ilicic et al. 2016)</span></p>
</blockquote>
<blockquote>
<p>“We designed three features based on the assumption that broken cells contain a
lower and multiple cells a higher number of transcripts compared to a typical
high quality single cell. For the first feature we calculated the number of
highly expressed and highly variable genes. For the second feature we calculated the
variance across genes. Lastly, we hypothesized that the number of genes expressed
at a particular level would differ between cells. Thus we binned normalized
read counts into intervals (very low to very high) and counted the number of genes
in each interval. … Overall, our results show that technical features [like the
number of detected genes and the percent of mapped reads] can help distinguish
low and high quality cells.” <span class="citation">(Ilicic et al. 2016)</span></p>
</blockquote>
<blockquote>
<p>“Before further analyses, scRNA-seq data typically require a number of bio-informatic
QC checks, where poor-quality data from single cells (arising as a result of many
possible reasons, including poor cell viability at the time of lysis, poor mRNA
recovery and low efficiency of cDNA production) can be justifiably excluded from
subsequent analysis. Currently, there is no consensus on exact filtering
strategies, but most widely used criteria include relative library size, number
of detected genes and fraction of reads mapped to mitochondria-encoded genes or
synthetic spike-in RNAs. … Other considerations are whether single cells have
actually been isolated or whether indeed two or more cells have been mistakenly
assessed in a particular sample.” <span class="citation">(Haque et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“Barcoding: Tagging single cells or sequencing libraries with unigue
oligonucleotide sequences (that is, ‘barcodes’), allowing sample multiplexing.
Sequencing reads corresponding to each sample are subsequently deconvoluted
using barcode sequence information.” <span class="citation">(Haque et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“Dropout: An event in which a transcript is not detected in the sequencing
data owing to a failure to capture or amplify it.” <span class="citation">(Haque et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“Sequencing depth: A measure of the sequencing capacity spent on a single sample,
reported for example as the number of raw reads per cell.” <span class="citation">(Haque et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“Spike-in: A molecule or set of molecules introduced to the sample in order to
calibrate measurements and account for technical variation” <span class="citation">(Haque et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“Transcriptional bursting: A phenomenon, also known as ‘transcriptional pulsing’,
of relatively short transcriptionally active periods being followed by longer silent
periods, resulting in temporal fluctuation of transcript levels.” <span class="citation">(Haque et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“Unique molecular identifier: A variation of barcoding, in which the RNA molecules
to be amplified are tagged with random n-mer oligonucleotides. The number of distinct
tags is designed to significantly exceed the number of copies of each transcript
species to be amplified, resulting in uniquely tagged molecules, and allowing control
for amplification biases.” <span class="citation">(Haque et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“A high percentage of counts mapping to spike-ins typically indicates that a small
amount of RNA was captured for the cell, suggesting protocol failure or death of the
cell in processing that renders it unsuitable for downstream analyses.” <span class="citation">(McCarthy et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“For each gene, QC metrics such as the average expression level and the proportion of
cells in which the gene is expressed are computed. This can be used to identify
low-abundance genes or genes with high dropout rates that should be filtered out
prior to downstream analyses.” <span class="citation">(McCarthy et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“Typical scRNA-seq datasets will show a broadly sigmoidal relationship between average
expression level and frequency of expression across cells. This is consistent with
expected behaviour where genes with greater average expression are more readily
captured during library preparation and are detected at greater frequency.” <span class="citation">(McCarthy et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“It is common to see ERCC spike-ins (if used), mitochondrial and ribosomal genes among
the highly expressed genes, while datasets consisting of healthy cells will also show high
levels of constitutively expressed genes like ACTB.” <span class="citation">(McCarthy et al. 2017)</span></p>
</blockquote>
<p>Second, the count matrix will require normalization before the data in it can be
used to test scientific hypotheses. To be able to count the small amount of mRNA
within single cells, the single-cell RNA-sequencing approach relies on
amplification (that is, once cDNA have been created from each RNA molecule, the
cDNA are replicated many times through … [?]). …</p>
<ul>
<li>size factor (approach earlier for bulk RNA-seq)</li>
<li>technical size factor with spike-ins <span class="citation">(Brennecke et al. 2013)</span>, plus a
biological size factor [?]</li>
</ul>
<blockquote>
<p>“Progress in gene expression analysis using minute amounts of starting
material has made single-cell transcriptomics accessible.” <span class="citation">(Brennecke et al. 2013)</span></p>
</blockquote>
<blockquote>
<p>“The low amount of RNA present in a single cell represents the main challenge in
single-cell RNA-seq experiments. … We demonstrated the relationship between
technical noise and the amount of starting material with a dilution series,
using technical replicates of decreasing amounts of total RNA taken from the
same pool of total RNA. … For 5,000 pg of input material, the noise pattern
was comparable to that of technical replicates from bulk RNA-seq experiments, in
which the spread can be accounted for by the Poisson distribution. However, the
number of genes affected by high levels of technical noise increased notably
at lower amounts of starting material (for example, a transcript could have 100–1,000
read counts in one technical replicate but 0 counts in another). … Nevertheless,
genes with a high read showed very good agreement between replicates even for the
10-pg data point—meaning that low-read count genes show strong noise and high-read
count genes show weak noise; what changes across the differing amounts of starting
material is the read-count range in which noise strength transitions from
weak to strong.” <span class="citation">(Brennecke et al. 2013)</span></p>
</blockquote>
<blockquote>
<p>“The crux of the issue is how to examine tens of thousands of genes possibly being
expressed in one cell, and provide a meaningful comparison to another cell expressing
the same large number of genes, but in a very different matter.” <span class="citation">(Haque et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“A major issue common to all protocols is how to account for technical variation
in the scRNA-seq process from cell to cell. Some protocols ‘spike-in’ a
commercially available, well-characterized mix of polyadenylated mRNA species …
The data from spike-ins can be used for assessing the level of technical
variability and for identifying genes with a hgih degree of biological variability.
In addition, spike-ins are valuable when computationally correcting for batch
effects between samples. However, the use of spike-ins is itself not without
problems… An increasingly popular method involves the use of UMIs, which
effectively tags every mRNA species recovered from one cell with a unique
barcode. Theoretically, this allows estimation of absolute molecule counts,
although the UMIs can be subject to saturation at high expression levels.
Nevertheless, the use of UMIs can significantly reduce amplification bias and
therefore improve precision.” <span class="citation">(Haque et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“It is as this point that the ‘zero or dropout problem’ of scRNA-seq should be
raised. The efficiency with which poly-adenylated mRNA are captured, converted
into cDNA and amplified is currently unclear, and, depending on the study,
can range between 10 and 40%. This means that, even if a gene is being
expressed, perhaps at a low level, there is a certain probability that it
will not be detected by current scRNA-seq methods. A partial solution to this
issue is to increase read depth. However, beyond a certain point, this
strategy leads to diminishing returns as the fraction of PCR duplicates increases
with deeper sequencing. Current data suggest that single-cell libraries from all common
protocols are very close to saturation when sequenced to a depth of 1,000,000 reads,
and a large majority of genes are detected already with 500,000 reads, although the
exact relatinoships are protocol specific.” <span class="citation">(Haque et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“While scRNA-seq workflows are conceptually closely related to population-level
transcriptomics protocols, data from scRNA-seq experiments have several
features that require specific bioinformatics approaches. First, even with the
most sensitive platforms, the data are relatively sparse owing to a high frequency
of dropout events (lack of detection of specific transcripts). Moreover, owing to
the digital nature of gene expression at the single-cell level, and the related
phenomenon of transcriptional bursting (in which pulses of transcriptional
activity are followed by inactive refractory periods), transcript levels are subject
to temporal fluctuation, further contributing to the high frequency of zero observations
in scRNA-seq data. Therefore, the numbers of expressed genes detected from single cells
are typically lower compared with population-level ensemble measurements. Because of
this imperfect coverage, the commonly used unit of normalized transcript levels used
for bulk RNA-seq, expressed as ‘reads per kilobase per million’ (RPKM), is biased on a
single-cell level, and instead the related unit ’transcripts per million (TPM) should
be used for scRNA-seq. Second, scRNA-seq data, in general, are much more variable
than bulk data. scRNA-seq data typically include a higher level of technical noise
(such as dropout events), but also reveal much of the biological variability that is
missed by RNA-seq on pooled cells. Biological variability that is present on many levels,
and which of these are considered as nuisance variation, depends on the underlying
biological question being asked. For example, at the gene level, transcriptional
bursting causes variation in transcript quantities, whereas as the global level, the
physical size of individual cells can vary substantially, affecting absolute
transcript numbers and reflected in the number of detected genes per cell. Cell-size
variation can also be closely related to proliferative status and cell-cycle phase. …
Finally, distributions of transcript quantities are often more complex in single-cell
datasets than in bulk RNA-seq. In general, single-cell expression measurements follow
a negative binomial distribution, and, in heterogeneous populations, multimodal
distributoins are also observed. As a consequence, statistical tests that assume
normally distributed data (used for example for detecting differentially expressed
genes) are likely to perform suboptimally on scRNA-seq data.” <span class="citation">(Haque et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“Methods to quantify mRNA abundance introduce systematic sources of variation
that can obscure signals of interest. Consequently, an essential first step in
most mRNA-expression analyses is normalization, whereby systemic variations
are adjusted to make expression counts comparable across genes and / or samples.
Within-sample normalization methods adjust for gene-specific features, such
as GC content and gene length, to facility comparisons of a gene’s expression
within an individual sample; whereas between-sample normalization methods adjust
for sample-specific features, such as sequencing depth, to allow for comparisons of
a gene’s expression across samples.” <span class="citation">(Bacher et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“A number of methods are available for between-sample normalization in bulk RNA-seq
experiments. Most of these methods calculate global scale factors (one factor is
applied to each sample, and this same factor is applied to all genes in the sample)
to adjust for sequencing depth. These methods demonstrate excellent performance in
bulk RNA-seq, but they are compromised in the single-cell setting because of an
abundance of zero-expression values and increased technical variability.” <span class="citation">(Bacher et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“scRNA-seq data show systematic variation in the relationship between transcript-specific
expression and sequencing depth (which we refer to as the count-depth relationship)
that is not accommodated by a single scale factor common to all genes in a cell.
Global scale factors adjust for a count-depth relatinoship that is assumed to be
common across genes. When this relationship is not common across genes, normalization
via global scale factors leads to overcorrection for weakly and moderately expressed
genes and, in some cases, undernormalization of highly expressed genes. To address this,
SCnorm uses quantile regression to estimate the dependence of transcript expression
on sequencing depth for every gene. Genes with similar dependence are then grouped, and
a second quantile regression is used to estimate scale factors within each group. Within-group
adjustment for sequencing depth is then performed using the estimated scale factors to
provide normalized estimates of expression.” <span class="citation">(Bacher et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“To further evaluate SCnorm, we conducted an experiment that, similar to the simulations,
sequenced cells at very different depths. … Prior to normalization, counts in the
second group will appear four times higher on average given the increased sequencing
depth. However, if normalization for depth is effective, fold-change estimates should
be near one… since the cells between the two groups are identical.” <span class="citation">(Bacher et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“Single-cell RNA-seq technology offers an unprecedented opportunity to address
important biological questions, but accurate data normalization is required to
ensure that results are meaningful.” <span class="citation">(Bacher et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“However, individual cells have extremely tiny amounts of input material available,
typically on the scale of picograms. The small scale of scRNA-seq input material means
that some level of inaccuracy is inevitable even with the most precise instruments,
resulting in an additional layer of stochasticity known as technical noise.
During the sequencing process, reverse transcription is necessary to convert RNA
to cDNA for use in amplification, but this introduces positional bias
regardless of where the polymerization begins. The following amplification process
counteracts low input material, though this in turn leads to additional bias as some
genes may experience preferential amplification, leading to uneven representation in
the data. The amplification process also runs the risk of producing dropout events,
in which either genes know to be present in a sample are completely absent from the
observed gene counts or genes are observed with lower values than their true expression.
These events frequently lead to excessive zeros, and in many cases, more than half of
all counts. Traditional bulk approaches do not naturally accommodate these differences,
and therefore lose their effectiveness when applied to scRNA-seq.” <span class="citation">(Lytal, Ran, and An 2020)</span></p>
</blockquote>
<blockquote>
<p>“Normalization is critical to the development of analysis techniques on scRNA-seq and
to counteract technical noise or bias. Before observed data can be used to identify
differentially expressed genes or potential subpopulations, it must undergo these
corrections, for what is observed is seldom exactly what is present within the data set.”
<span class="citation">(Lytal, Ran, and An 2020)</span></p>
</blockquote>
<blockquote>
<p>“Some scRNA-seq studies involve the use of spike-in molecules for the purpose of
normalization. The spike-in RNA set is artificially added to each cell’s lysate in
the same volume under the assumption that spike-ins and endogenous transcripts will
experience similar variation among the cells during the capture process. Since spike-in
gene concentrations are knows, normalization methods model existing technical variation
by utilizing the difference between these known values and the values observed after
processing.” <span class="citation">(Lytal, Ran, and An 2020)</span>
There are other steps of pre-processing that can be considered for single cell
RNA-seq data, but that might not be applied in every case. For example, a
technique called scaling could be used to help constrain the influence of
highly-expressed genes when conducted further analysis steps, like grouping
the cells into clusters based on cell type (with cell type in this case
being determined based on similar patterns of gene expression). [More on
scaling]</p>
</blockquote>
<blockquote>
<p>“Scaling normalization is typically required in RNA-seq data analysis to
remove biases caused by differences in sequencing depth, capture efficiency
or composition effects between samples.” <span class="citation">(McCarthy et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“In the analysis and interpretation of single-cell RNA-seq (scRNA-seq)
data, effective pre-processing and normalization represent key challenges.
While unsupervised analysis of single-cell data has transformative
potential to uncover heterogeneous cell types and states, cell-to-cell
variation in technical factors can also confound these results. In particular,
the observed sequencing depth (number of genes or molecules detected per
cell) can vary significantly between cells, with variation in molecular
counts potentially spanning an order of magnitude, even within the same
cell type. Importantly, while the now widespread use of unique molecular
identifiers (UMI) in scRNA-seq removes technical variation associated
with PCR, differences in cell lysis, reverse transcription efficiency,
and stochastic molecular sampling during sequencing also contribution
significantly, necessitating technical correction. These same challenges
apply to bulk RNA-seq workflows, but are exacerbated due to the extreme
comparative sparsity of scRNA-seq data.” <span class="citation">(Hafemeister and Satija 2019)</span></p>
</blockquote>
<blockquote>
<p>“The primary goal of single-cell normalization is to remove the influence
of technical effects in the underlying molecular counts, while preserving
true biological variation.” <span class="citation">(Hafemeister and Satija 2019)</span></p>
</blockquote>
<blockquote>
<p>“In general, the normalized expression level of a gene should not be
correlated with the total sequencing depth of a cell. Downstream analytical
tasks (dimension reduction, differential expression) should also not
be influenced by variation in sequencing depth.” <span class="citation">(Hafemeister and Satija 2019)</span></p>
</blockquote>
<blockquote>
<p>“The variance of a normalized gene (across cells) should primarily reflect
biological heterogeneity, independent of gene abundance or sequencing depth. For
example, genes with high variance after normalization should be differentially
expressed across cell types, while housekeeping genes should exhibit low
variance. Additionally, the variance of a gene should be similar when
considering either deeply sequenced cells, or shallowly sequenced cells.”
<span class="citation">(Hafemeister and Satija 2019)</span></p>
</blockquote>
<blockquote>
<p>“Sequencing detph variation across single cells represents a substantial
technical confounder in the analysis and interpretation of scRNA-seq data.”
<span class="citation">(Hafemeister and Satija 2019)</span></p>
</blockquote>
<p>There are also cases where pre-processing steps could be used to
remove patterns from technical noise or even from biological patterns that
are unrelated to the scientific question of interest. In terms of technical
noise, for example, there are cases where pre-processing steps could be used
to help remove variation that’s introduced by running the experimental samples
in different batches. In terms of biological patterns, one pattern that may
be desirable to remove through pre-processing is gene expression related to
a cell’s phase in the cell cycle. [More on this.]</p>
<blockquote>
<p>“[A] promising application [of scRNA-seq] is the study of transcriptional
heterogeneity within supposedly homogeneous cell types, a phenomenon of
physiological importance, which can now be studied in a transcriptome-wide
manner in single cells. In such analyses, which should be distinguished from
the more common two-group comparison setting, it is necessary to account for
strong technical noise. Technical noise is unavoidable owing to the low amount
of starting material, and it must be quantified in order to avoid mistaking it
for genuine differences in biological expression levels.” <span class="citation">(Brennecke et al. 2013)</span></p>
</blockquote>
<blockquote>
<p>“After scaling normalization, further correction is typically required to
ameliorate or remove batch effects. For example, in the case study dataset,
cells from two patients were each processed on two C1 machines. Although C1
machine is not one of the most important explanatory variables on a per-gene
level, this factor is correlated with the first principal component of the
log-expression data. This effect cannot be removed by scaling normalization
methods, which target cell-specific biases and are not sufficient for removing
large-scale batch effects that vary on a gene-by-gene basis. … For the
dataset here, we fit a linear model to the scran normalized log-expression
values with the C1 machine as an explanatory factor. (We also use the log-total
counts from the endogenous genes, percentage of counts from the top 100 most
highly-expressed genes and percentage of counts from control genes as
additional covariates to control for these other unwanted technical effects.)
We then use the residuals from the fitted model for further analysis. This approach
successfully removes the C1 machine effect as a major source of variation between
cells.” <span class="citation">(McCarthy et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“We emphasize that it is generally preferable to incorporate batch effects or
latent variables into statistical models used for inference. When this is not
possible (e.g., for visualization), directly regressing out these uninteresting
factors is required to obtain ‘corrected’ expression values for further analysis.
Furthermore, a general risk of removing latent factors is that interesting
biological variation may be removed along with the presumed unwanted variation.
Users should therefore apply such methods with appropriate caution, particularly
when an analysis aims to discover biological conditions, such as new cell types.”
<span class="citation">(McCarthy et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“Once identified, important covariates and latent variables can be flagged for
inclusion in downstream statistical models or their effects regressed out of
normalized expression values.” <span class="citation">(McCarthy et al. 2017)</span></p>
</blockquote>
<p>[Identifying highly-variable genes in terms of expression]</p>
<blockquote>
<p>“All genes will display some biological variability in expression from cell
to cell, but a high level of variance (exceeding the specified threshold) will
indicate genes important in explaining heterogeneity within the cell
population under study.” <span class="citation">(Brennecke et al. 2013)</span></p>
</blockquote>
<p>[Principal component analysis]</p>
<blockquote>
<p>“Most approaches seek to reduce these ‘multi-dimensional data’, with each
dimension being the expression of one gene, into a very small number of dimensions
that can be more easily visualised and interpreted. Principal component analysis
(PCA) is a mathematical algorithm that reduces the dimensionality of data, and
is a basic and very useful tool for examining heterogeneity in scRNA-seq data.”
<span class="citation">(Haque et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“Dimensionality reduction and visualization are, in many cases, followed by
clustering of cells into subpopulations that represent biologically meaningful
trends in the data, such as functional similarity or developmental
relationship. Owing to the high dimensionality of scRNA-seq data, clustering
often requires special consideration… Likewise, a variety of methods exist for
identifying differentially expressed genes across cell populations.” <span class="citation">(Haque et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“One common type of single-cell analysis, for instance, is dimension reduction.
This process simplifies data sets to facilitate the identification of similar
cells. … scRNA-seq data represent each cell as ‘a list of 20,000 gene-expression
values.’ Dimensionality-reduction algorithms such as principal components analysis
(PCA) and t-distributed stochastic neighbour embedding (t-SNE) effectively project
those shapes into two or three dimensions, making clusters of similar cells apparent.”
<span class="citation">(J. M. Perkel 2017)</span></p>
</blockquote>
<blockquote>
<p>“Multivariate techniques may be used to reduce the dimensionality… It is potentially
dangerous to allow the number of variables to exceed the number of observations
because of non-uniqueness and singularity problems. Put simply, the unwary analyst
may try to estimate more parameters than there are observations.” <span class="citation">(Chatfield 1995)</span></p>
</blockquote>
<blockquote>
<p>“It is also possible to take a wider view of IDA by allowing the use, when
necessary, of a group of more complicated, multivariate techniques which are
data-analytic in character. The adjective ‘data-analytic’ could reasonably be
applied to any statistical technique, but I follow modern usage in applying it
to techniques which do not depend on a formal probability model except perhaps
in a secondary way. Their role is to explore multivariate data, to provide
information-rich summaries, to generate hypotheses (rather than test them) and
to help generally in the search for structure, both between variables and between
individuals. In particular, they can be helpful in reducing dimensionality and
in providing two-dimensional plots of the data. The techniques include principal
component analysis, multi-dimensional scaling and other forms of cluster analysis…
They are generally much more sophisticated than earlier data descriptive techniques
and should not be undertaken lightly. However, they are occasionally very
fruitful.” <span class="citation">(Chatfield 1995)</span></p>
</blockquote>
<blockquote>
<p>“Principal component analysis rotates the p observed values to p new orthogonal
variables, called principal components, which are linear combinations of the original
variables and are chosen in turn to explain as much of the variation as possible.
It is sometimes possible to confine attention to the first two or three components,
which reduces the effective dimensionality of the problem. In particular, a scatter
diagram of the first two components is often helpful in detecting clusters of
individuals or outliers.” <span class="citation">(Chatfield 1995)</span></p>
</blockquote>
<blockquote>
<p>“Cluster analysis aims to partition a group of individuals into groups or clusters
which are in some sense ‘close together’. There is a wide variety of possible
procedures. In my experience the clusters obtained depend to a large extent on the methods
used (except where the clusters are really clear-cut) and users are now aware of
the drawbacks and the precautions which need to be taken to avoid irrelevant or misleading
results.” <span class="citation">(Chatfield 1995)</span></p>
</blockquote>
<blockquote>
<p>“Another popular application is pseudo-time analysis. Trapnell developed the first
such tool, called Monocle, in 2014. The software uses machine learning to infer from
an scRNA-seq experiment the sequence of gene-expression changes that accompany
cellular differentiation, much like inferring the path of a foot race by photographing
runners from the air, Trapnell says.” <span class="citation">(J. M. Perkel 2017)</span></p>
</blockquote>
<blockquote>
<p>“Other tools address subpopulation detection … and spatial positioning, which uses
data on the distribution of gene expression in tissues to determine where in a
tissue each transcriptome arose.” <span class="citation">(J. M. Perkel 2017)</span></p>
</blockquote>
<ul>
<li>Polymerase chain reaction (PCR): <strong>Amplifies</strong> DNA</li>
<li>Microarrays: allowed the analysis of multiple genes at once, previously
one gene at a time</li>
<li>Genomic profiles can differ from transcriptional profiles</li>
</ul>
<p>Proteomics:</p>
<blockquote>
<p>“Protein molecules, rather than DNA or RNA, carry out most cellular functions. The
direct measurement of protein levels and activity within the cell is therefore the
best determinant of overall cell function.” <span class="citation">(Lakhani and Ashworth 2001)</span></p>
</blockquote>
<blockquote>
<p>“To sequence a protein ten years ago, a substantial amount had to be purified and
a technique known as Edman degradation had to be used. … During the 1990s, mass
spectrometry (MS), in which biomolecules are ionized and their mass is measured by
following their specific trajectories in a vacuum system, deplaced Edman degradation,
because it is more sensitive and can fragment the peptides in seconds instead of hours
or days. Furthermore, MS does not require proteins or peptides to be purified to
homogeneity and has no problem identifying blocked or otherwise modified proteins.
In the last few years, further breathtaking tecnological advances have established MS
not only as the definitive tool to study the primary structure of proteins, but also
as a central technology for the field of proteomics.” <span class="citation">(Steen and Mann 2004)</span></p>
</blockquote>
<blockquote>
<p>“After protein purification, the first step is to convert proteins to a set of peptides
using a sequence-specific protease. Even though mass spectrometers can measure the mass
of intact proteins, there are a number of reasons why peptides, and not proteins,
are analysed in proteomics. Proteins can be difficult to handle and might not all
be soluble under the same conditions… In addition, the sensitivity of the mass
spectrometer for proteins is much lower than for peptides, and the protein may be
processed and modified such that the combinatorial effect makes determining the
masses of the numerous resulting isoforms impossible. … Most importantly, if the
purpose is to identify the protein, sequence information is needed and the mass spectrometer
is most efficient at obtaining sequence information from peptides that are up to
~20 residues long, rather than whole proteins. Nevertheless, with very specialized
equipment, it is becoming possible to derive partial sequence information from intact
proteins, which can then be used for identification purposes or the analysis of protein
modifications in an approach called ‘top-down’ protein sequencing.” <span class="citation">(Steen and Mann 2004)</span></p>
</blockquote>
<blockquote>
<p>“Digesting the protein into a set of peptides also means that the physico-chemical
properties of the protein, such as solubility and ‘stickiness’, become irrelevant.
As long as the protein generates a sequence of peptides, at least some of them can
be sequenced by the mass spectrometer, even if the protein itself would have been
unstable or insoluble under the conditions used.” <span class="citation">(Steen and Mann 2004)</span></p>
</blockquote>
<blockquote>
<p>“The peptides that are generated by protein digestion are not introduced to the
mass spectrometer all at once. Instead, they are injected onto a microscale
capillary high-performance liquid chromatography (HPLC) column that is
directly coupled to, or is ‘on-line’ with, the mass spectrometer. The peptides are
eluted from these columns using a solvent gradient of increasing organic content,
so that the peptide species elute in order of their hydrophobicity.” <span class="citation">(Steen and Mann 2004)</span></p>
</blockquote>
<blockquote>
<p>“When a peptide species arrives at the end of the column, it flows through a
needle. At the needle tip, the liquid is vaporized and the peptide is subsequently
ionized by the action of a strong electric potential. This process is called
‘electrospray ionization’.” <span class="citation">(Steen and Mann 2004)</span></p>
</blockquote>
<blockquote>
<p>“Electrosprayed peptide ions enter the mass spectrometer through a small hole
or a transfer capillary. Once inside the vacuum system, they are guided and
manipulated by electric fields. There are diverse types of mass spectrometer,
which differ in how they determine the mass-to-charge (m/z) ratios of the peptides.
Three main types of mass spectrometers are used in proteomics: quadrupole mass
spectometers, time of flight (TOF) mass spectrometers, and quadrupole ‘ion traps’.
… Each of these instruments generates a mass spectrum, which is a recording of the
signal intensity of the ion at each value of the m/z scale (which has units of daltons
(Da) per charge).”</p>
</blockquote>
<blockquote>
<p>“At the beginning of the 1990s, researchers realized that the
peptide-sequencing problem could be converted to a database-matching problem,
which would be much simpler to solve. The reason database searching is easier
than de novo sequencing is that only an infinitesimal fraction of the possible
peptide amino-acid sequences actually occur in nature. A peptide-fragmentation
spectrum might therefore not contain sufficient information to unambiguously
derive the complete amino-acid sequence, but it might still have sufficient
information to match it uniquely to a peptide sequence in the database on the
basis of the observed and expected fragment ions. There are several different
algorithms that are used to search sequence databases with tandem MS-spectra
data, and they have names such as PeptideSearch, Sequest, Mascot, Sonar ms/ms,
and ProteinProspector.” <span class="citation">(Steen and Mann 2004)</span></p>
</blockquote>
<blockquote>
<p>“As a result of rapidly improving technology, the identification of hundreds of
proteins is not unusual, even in a single project, and determining the reliability
of these protein hits is especially challenging. This is partly because even small
error rates for each of the corresponding proteins can quickly add up when many
thousands of peptides are being identified.” <span class="citation">(Steen and Mann 2004)</span></p>
</blockquote>
<blockquote>
<p>“After all the peptides have been identified, they have to be grouped into protein
identifications. Usually, the peptide scores are added up to yield protein scores in
a straightforward manner. However, the confidence in the accuracy of a particular
peptide identification increases if other peptides identify the same protein and
decreases if no other peptides do so.” <span class="citation">(Steen and Mann 2004)</span></p>
</blockquote>
<blockquote>
<p>“Often, we are interested not only in the identity of a peptide, but also in its
quantity. Unfortunately, the intensity of the signal of a peptide ion does not
directly indicate the amount of protein present. For example, when digesting a
protein, the peptides that are produced should all be equimolar and might be
expected to give peaks of equal height in the mass spectrum. However, accessibility
to the protease, the solubility of the peptide and the ionization efficiency of
the peptide combine to make these signals orders of magnitude different. Fortunately,
these factors are reproducible, so the peak height of the same peptides can be
a good indicator of the relative amount of the related protein from one experiment
to the next.” <span class="citation">(Steen and Mann 2004)</span></p>
</blockquote>
<blockquote>
<p>“Experiments that are aimed at determining protein expression in whole-cell lysates
or tissues (expression proteomics) have been less successful so far. However,
intense research efforts are underway at present, because such a strategy would
enable the detection/identification of disease-related biomarkers. Such a
measurement is essentially the equivalent of a microarray experiment, with the
difference being that protein, instead of mRNA, levels are compared. MS experiments
that compare protein-expression levels are much more laborious than microarray
experiments, but are attractive because proteins are the active agents of the cell,
whereas the mRNA population is often a poor indicator of protein levels. However,
it is still difficult to identify and quantify all the low-abundance proteins,
especially in the presence of highly abundant proteins. Furthermore, as in microarray
experiments, the results are ‘noisy’, because of the extremely large amounts of
data, and it can be difficult to distil functional and mechanistic hypotheses
from such global experiments.” <span class="citation">(Steen and Mann 2004)</span></p>
</blockquote>
<blockquote>
<p>“m/z ratio (mass-to-charge ratio): Mass spectrometry does not measure the mass of
molecules, but instead measures their m/z value. Electrospray ionization, in particular,
generates ions with multiple charges, such that the observed m/z value has to be
multiplied by z and corrected for the number of attached protons (which equals z) to
calculate the molecular weight of a particular peptide.” <span class="citation">(Steen and Mann 2004)</span>
<span class="citation">(Steen and Mann 2004)</span></p>
</blockquote>
<p>Flow cytometry:</p>
<blockquote>
<p>“In multicellular organisms, cells carry out a diverse array of complex, specialized
functions. This specialization occurs mostly through the expression of cell type–specific
genes and proteins that generate the appropriate structures and molecular networks.
A central challenge in the biomedical sciences, however, has been to identify the
distinct lineages and phenotypes of the specialized cells in organ systems, and track
their molecular evolution during differentiation.” <span class="citation">(Benoist and Hacohen 2011)</span></p>
</blockquote>
<blockquote>
<p>“Since the 1970s, fluoresence-based flow cytometry has been the leading technique
for studying and sorting cell populations. It involves passing cells through
flow chambers at high rates (&gt; 20,000 cells/s) and using lasers to excite fluorescent
tags (‘fluorochromes’) that are usually attached to antibodies; different antibodies
are tagged with different colors, enabling researchers to quantify molecules that
define cell subtypes or reflect activation of specific pathways. Progess in instrument
design, multi-laser combinations, and novel fluorochromes has led to experimental
configurations that simultaneously measure up to 15 markers. This has enabled very
detailed description of cell subtypes, perhaps most extensively in the immune
system, where the Immunological Genome Project is profiling &gt;200 distinct cell typles.
Fluorescence cytometry seems to have reached a technical plateau, however: In practice,
researchers typically measure only 6 to 10 cell markers because they are limited by
the specral overlap between fluorochromes.” <span class="citation">(Benoist and Hacohen 2011)</span></p>
</blockquote>
<blockquote>
<p>“In mass cytometry, fluorochrome tags are replaced by a series of rare earth elements
(e.g., lanthanides), which are attached to antibodies through metal-chelator coupling
reagents. Cells are labeled by incubation in a cocktail of tagged antibodies; as the
cells flow through the instrument, they are vaporized at 5500 K, and the released
tags are identified and quantified by time-of-flight mass spectrometry (MS). … The
beauty of this approach stems from three factors: the precision of MS detection, which
eliminates overlap between tags (a dream for any investigator who has battled this
problem, known as fluorescence compensation); the number of detectable markers
(34 here, but easily more); and the absence of background noise (because rare earth
elements are essentially absent from biological materials, there is no equivalent of
‘autofluorescence’).”
<span class="citation">(Benoist and Hacohen 2011)</span></p>
</blockquote>
<blockquote>
<p>“Because the software tools commonly used for flow cytometry data would be woefully
inadequate for analyzing dozens of dimensions [as you have for mass cytometry data],
Bendall et al. used software that clusters cell populations into ‘minimum-spanning trees’
that reproduce known hematopoietic differentiation, but with much finer granularity.
As a result, cells that once would have been grouped into one population are now
divided into many more; for example, naive CD4+ T lymphocytes, a priori considered
a homogeneous population, are now split into more than 10 subsets.” <span class="citation">(Benoist and Hacohen 2011)</span></p>
</blockquote>
<blockquote>
<p>“The secret is that, in flow cytometry, every cell acts as an independent ‘test tube’.”
<span class="citation">(Benoist and Hacohen 2011)</span></p>
</blockquote>
<blockquote>
<p>“Over the past 25 years there have been major technological advances in the ways that
CD4+ T cells are enumerated, with flow cytometry now generally regarded as the
predicate technique. During this period, flow cytometry has progressed from large,
expensive and complex fluorescence-activated cell sorting (FACS) machines that
require highly specialized operating personnel, to smaller, more affordable
bench-top instruments that can be used by individuals with minimal training. This
shift was made possible in part by including multicolour analysis coupled with
simplified gating technologies.” <span class="citation">(Barnett et al. 2008)</span></p>
</blockquote>
<blockquote>
<p>“In 1954, Wallace Coulter developed an instrument that could measure cell size and
count the absolute number of cells, and thus the discipline of flow cytometry was
born. Further developments enabled the production of instruments that could measure
cell size and nucleic acid content using a two-dimensional approach that compared
light scatter and light absorption. These instruments were cumbersome and required
specialist operators, but immunologists began to use them to investigate the
functions of immune cells. … By the mid-1980s, bench-top flow cytometers were
available and as the technology advanced the instruments became progressively
smaller. Coupled with the availability of monoclonal antibodies, the increasing
number of available fluorochromes (compounds that emit light at a greater wavelength
than the light source they are excited with) and computer improvements, flow
cytometers are now accessible for almost every clinical laboratory.” <span class="citation">(Barnett et al. 2008)</span></p>
</blockquote>
<blockquote>
<p>“Flow cytometry enables the examination of microscopic particles (such as cells) that
are suspended in a stream of fluid which is termed sheath fluid. This fluid is
focused hydrodynamically such that the cells flow in single file through a flow cell
in which a beam of light (usually a laser) is focused. As the cells pass through the
laser beam they scatter the light so that forward scatter (FSC) and side scatter (SSC)
light is captured; this enables the size and granularity of the cells to be determined.
In addition, if a cell is labelled with antibodies that carry a fluorochrome, as the
cell passed in front of the laser beam the fluorochrome emits light at a wavelength
that is higher than the single wavelength light source and which can be detected by
fluorescence detectors. The flow cytometers that are in clinical use can analyse at
least four fluorochromes simultaneously, in addition to the FSC and SSC. This is known
as multiparametric analysis. The information that is generated is computer analysed
so that specific analysis regions (known as gates) can be created, which allows
the user to build up a profile of the size, granularity and antigen profile of
the target cell population.” <span class="citation">(Barnett et al. 2008)</span></p>
</blockquote>
<blockquote>
<p>“The fundamental basis of CD4 counting and identification relies on the specific
use of electronic gating approaches. One of the earliest and most popular approaches
that was used from the mid-1980s to early-1990s relied on using light scatter and
anti-CD45 and anti-CD14 monoclonal antibodies. This gating approach was termed the
two-colour approach.”
<span class="citation">(Barnett et al. 2008)</span></p>
</blockquote>
<blockquote>
<p>“In the early 1990s researchers realized that because of these logistic problems
[with the two-colour approach], new technologies would need to be developed to
reduce processing time while providing accurate CD4 counts. Two new gating strategies
were developed almost simultaneously, one termed CD45 gating and the other T gating,
both of which helped to more accurately identify the lymphocyte analysis regions.”
<span class="citation">(Barnett et al. 2008)</span></p>
</blockquote>
<blockquote>
<p>“Flow cytometry is a powerful technology that is capable of quantifying individual
functions independently and simulataneously on a single-cell basis (and, in some cases,
under conditions that can preserve the viability of the cell for future use). Quantification
of multiple functions of T cells requires at least six-colour technology: three colors
to identify the T-cell lineage (typically, these label CD3, CD4, and CD8), one color for
viability and/or as a dump channel to remove unwanted cell populations, and two or
more colors devoted to the effector function of interest. Furthermore, delineation of
memory T-cell subsets requires an additional two or more colors to distinguish
differentiation stage.” <span class="citation">(Seder, Darrah, and Roederer 2008)</span></p>
</blockquote>
<blockquote>
<p>“Of note, the number of possible functionally unique subsets increases geometrically
with the number of measurements made. By applying Boolean gating, all possible
combinations of T cells that produce the cytokines measured can be determined. …
In general, not all possible combinations are represented in any given antigen
response; rather, the overall response is typically limited to a subset of the
individual combinations of the functions. Based on the frequency of each distinct
functional subset, a pie chart can be assembled to show the composition of a total
cytokine response.” <span class="citation">(Seder, Darrah, and Roederer 2008)</span></p>
</blockquote>
<blockquote>
<p>“Boolean gating: A flow cytometric data analysis technique in which cells are
divided into all possible combinations of the functions measured by using the
Boolean operators ‘and’ and ‘not’ on analysis gates applied to those measurements.
These populations of cells can be expressed as absolute frequencies or as a
fraction of the total response.” <span class="citation">(Seder, Darrah, and Roederer 2008)</span></p>
</blockquote>
<blockquote>
<p>“T cells have an essential role in protection against a variety of infections. Indeed,
the development of successful vaccines against HIV, malaria or tuberculosis will
require the generation of potent and durable T-cell responses. … As T cells are
functionally heterogeneous and mediate their effects through a variety of mechanisms,
a major hurdle in quantifying protective T-cell responses has been the technical
limitations in assessing the complexity of such responses. Methods to define the
full characteristics of T cells are crucial for developing preventative and
therapeutic vaccines for infections and cancer.” <span class="citation">(Seder, Darrah, and Roederer 2008)</span></p>
</blockquote>
<blockquote>
<p>“A typical ‘gating tree’ hierarchically identifies unique functional subsets of
CD4+ and CD8+ T cells based on the fluorescence staining of live cells and on the
expression of other relevant markers, such as CD3, CD4, CD8, CD45RA, CC-chemokine
receptor (CCR7), interferon gamma (INFgamma), interleukin-2 (IL-2) and tumour-necrosis
factor (TNF), following antigenic stimulation. The viability marker (ViViD) excludes
dead cells that can often show non-specific binding to other reagents. Live, CD3+
T cells are separated into CD4+ and CD8+ T-cell lineages; within each lineage,
memory T cells are seleced based on the expression of (for example) CCR7 and CD45RA.
Other combinations of markers can similarly exclude naive T cells from functional
analysis.” <span class="citation">(Seder, Darrah, and Roederer 2008)</span></p>
</blockquote>
<blockquote>
<p>“Flow cytometry has increasingly become a tool of choice for the analysis of
cellular phenotype and function in the immune system. It is arguably the most
powerful technology available for probing human immune phenotypes, because it
measures multiple parameters on many individual cells. Flow cytometry thus
allows for the characterization of many subsets of cells, including rare subsets,
in a complex mixture such as blood. And because of the wide array of antibody
reagents and protocols available, flow cytometry can be used to assess not only
the expression of cell-surface proteins, but also that of intracellular
phosphoproteins and cytokines, as well as other functional readouts.”
<span class="citation">(Maecker, McCoy, and Nussenblatt 2012)</span></p>
</blockquote>
<blockquote>
<p>“This diversity of reagents and applications has led to a large variety of
ways in which flow cytometry is being used to monitor the immune systems of
humans and model organisms (mostly mice). These uses include the identification
of antigen-specific T cells using tetramers or intracellular cytokine staining;
the measurement of T cell proliferation using dyes such as 5,6-CFSE; and the use
of immunophenotyping assays to identify lymphocyte, monocyte, and/or granulocyte
subsets.” <span class="citation">(Maecker, McCoy, and Nussenblatt 2012)</span></p>
</blockquote>
<blockquote>
<p>“Animal studies tend to be relatively small and self-contained, as the mice used
are inbred (genetically identical) and their environment and treatments are
carefully controlled. Human studies, by contrast, are often larger, to account for
genetic and environmental variables. They may require longitudinal assays over a
considerable length of time, owing to the difficulty of recruiting available
subjects and/or the need to follow those subjects over time. There may also be a
need to aggregate data from multiple sites, or even across multiple studies, to
achieve sufficient sample sizes for statistical analysis or to compare different
treatments. In these situations, the standardization of reagents and protocols
becomes crucial.” <span class="citation">(Maecker, McCoy, and Nussenblatt 2012)</span></p>
</blockquote>
<blockquote>
<p>“Immune phenotypes: Measurable aspects of the immune system, such as the
proportions of various cell subsets or measures of cellular immune function.”
<span class="citation">(Maecker, McCoy, and Nussenblatt 2012)</span></p>
</blockquote>
<blockquote>
<p>“The steps in a typical flow cytometry experiment … present several
variables that need to be controlled for effective standardization. These variables
include the general areas of reagents, sample handling, instrument setup
and data analysis… The effects of changes in these variables are largely
known. For example, the stabilization and control of staining reagents
through the use of pre-configured lyophilized-reagent plates, and centralized
data analysis, have both been shown to decrease variability in a multi-site
study. However, the wide-spread adoption of standards for controlling such
variables has not taken place. This is in contrast to other technologies, such
as gene expression microarrays, which have achieved a reasonable degree of
standardization in recent years. … Of course, microarray data are less complex
than flow cytometry data, which are based on many hierarchical gates. Still,
a reasonable degree of standardization of flow cytometry assays should be
possible to achieve.” <span class="citation">(Maecker, McCoy, and Nussenblatt 2012)</span></p>
</blockquote>
<blockquote>
<p>“Gates: Sequential filters that are applied to a set of flow cytometry data to
focus the analysis on particular cell subsets of interest.” <span class="citation">(Maecker, McCoy, and Nussenblatt 2012)</span></p>
</blockquote>
<blockquote>
<p>“The study of cells moving in suspension through an image plane—flow cytometry—is
a potent tool for immunology research and immune monitoring. Its main advantages
are that is makes multiparameter measurements and this it does so on a single-cell
basis. The result is that this technique can dissect the phenotypes and functions
of cell subsets in ways that are not possible using bulk assays, such as Western
blots, microarrays or enzyme-linked immunosorbent assays (ELISAs). Nowhere has this proven
more useful than in a mixed suspension of immune cells, such as the blood. Newer
instrumentation allows for the analysis of eight or more parameters at flow rates of
thousands of cells per second; the resulting rich data sets are unparalleled for
the knowledge of immune function that they have contributed.” <span class="citation">(Maecker, McCoy, and Nussenblatt 2012)</span></p>
</blockquote>
<blockquote>
<p>“In this context [Human Immunology Project], an ‘immune phenotype’ would encompass
not only the proportions of major immune cell subsets, but also, for example, their
activation state, responsiveness to stimuli, and T cell receptor or B cell receptor
reperotoires, among other parameters.” <span class="citation">(Maecker, McCoy, and Nussenblatt 2012)</span></p>
</blockquote>
<blockquote>
<p>“A typical flow cytometry experiment. Sample preparation from blood often involves
Ficoll gradient separation of mononuclear cells, and sometimes cryopreservation,
before staining with fluorescent antibody conjugates. Each of these steps can
introduce variability in the assay results. Instrument setup involves setting
voltage gains for the photomultiplier tubes (PMTs) so as to achieve optimal sensitivity.
To the extent that this is not standardized, it becomes a source of variability
as well. Data acquisition involves passing the stained cells through a laser beam and
recording the fluorescence emission from all of the bound antibody conjugates. Here,
the main variable is the type of instrument, including the lasers and optical filters
used. This is followed by data analysis, in which cell populations of interest are
defined and reported on, which is another significant source of variation.”
<span class="citation">(Maecker, McCoy, and Nussenblatt 2012)</span></p>
</blockquote>
<blockquote>
<p>“The definition of particular subsets of immune cells using cell-surface markers continues
to evolve, particularly for cell types that are the subject of intense current
research. These include regulatory T (TReg) cells, interleukin-17 (IL-17)-secreting T helper
(TH17) cells, dendritic cells (DCs) and natural killer (NK) cells. However, even
well-characterized subsets, such as naive and memory T cells, are defined differently
in various studies. For example, the classical T cell subsets of naive, central memory,
effector memory, and effector T cells were first defined on the basis of their expression
of CC-chemokine receptor 7 (CCR7; also known as CD197) and CD45RA … Other investigators
have since used different markers, such as CD62L or CD27 in place of CCR7, and CD45RO in
place of CD45RA. Although these different combinations of markers generally define
similar cell subsets, they introduce an unknown amount of heterogeneity that makes
comparisons between studies difficult. It is reasonable to think that the discovery
of new markers and new cell subsets will continue for quite some time in the future.
However, we propose that there is sufficient maturity in the field of cellular
immunology to reach consensus working definitions for most of the commonly studies
subsets of immune cells. … In other words, it should be possible to define a stable
set of markers that delineate the mojor classes og B, T and NK cells, monocytes and DCs.”
<span class="citation">(Maecker, McCoy, and Nussenblatt 2012)</span></p>
</blockquote>
<blockquote>
<p><span class="citation">(Maecker, McCoy, and Nussenblatt 2012)</span> recommend, as ways to minimize the effects of technical
variation in flow cytometry data analysis, “Central analysis by one or a few
coordinated experts” and “Use of automated gating algorithms”</p>
</blockquote>
<blockquote>
<p>“Activation markers for these cell types are of course of equal interest to the
differentiation markders discussed above, as the activated subsets can provide clues to
an individual’s response to infection, vaccination, cancer, or autoimmune disease. For
this purpose, one can assess intracellular markers of recent cell division (such as
Ki67) or surface markers of activation that have varying kinetics of expression
(for example, CD69 is a transient marker, whereas HLA-DR, CD38 and CD71 are
expressed for longer periods).”
<span class="citation">(Maecker, McCoy, and Nussenblatt 2012)</span></p>
</blockquote>
<blockquote>
<p>“How important is it to define the actual antibody cocktails used for
immunophenotyping? As described above, the choice of markers is clearly
important, but different antibody clones and fluorochromes can also greatly
influence results.” <span class="citation">(Maecker, McCoy, and Nussenblatt 2012)</span></p>
</blockquote>
<blockquote>
<p>“In addition to optimizing certain settings, such as laser time delays, instrument
setup is mostly concerned with the setting of the voltage gains applied to
each fluorescence detector, which influences the sensitivity of the detector
to dim versus bright signals. This used to be entirely a matter of user preference, but
some guiding principles have now emerged based on measuring the variance of a population
of dim particles over a series of voltage gains.” <span class="citation">(Maecker, McCoy, and Nussenblatt 2012)</span></p>
</blockquote>
<blockquote>
<p>“Data analysis is one of the largest variables in flow cytometry, as shown by
studies analyzing data at multiple sites versus centrally. It is also one of the
easiest variables to address, as re-analysis of existing data is always possible,
whereas choices made about sample handling and instrument setup are irrevocable.
Central analysis is simple in concept, but it can be overwhelming in terms of the
demands placed on one or a few coordinated personnel who must review all of the
data to achieve consistent gating. Fortunately, automated gating algorithms have
proliferated and improved, such that they now compete favorably with expert
manual gating.” <span class="citation">(Maecker, McCoy, and Nussenblatt 2012)</span></p>
</blockquote>
<blockquote>
<p>“With the major exception of CD4+ T cell counting for immune monitoring in
patients with HIV and the immunophenotyping of leukemias and lymphomas, flow
cytometry is mostly carried out in a research setting. Particular assays may be
internally vailidated for a given clinical study, but comprehensive standardization
is rare to non-existent. However, there is a clear and growing interest in the
standardization of human immunophenotyping. This is evidenced by standardization
efforts, such as those of the European ENTIRE and United States CTOC networks. The
NIH have also invested in a series of awards focused on human immunophenotyping, and
both FOCIS and the NIH are working to create standard immunophenotyping panels.”
<span class="citation">(Maecker, McCoy, and Nussenblatt 2012)</span></p>
</blockquote>
<p>Metabolomics:</p>
<p>Bulk transcriptomics:</p>
<blockquote>
<p>“Much of biochemistry and molecular biology rests on the assumption that the
behavior of cells in a population (in a culture, in an organ) can be reliably
approximated by the population average that results when cells are lysed and
their molecules analyzed as a pool. Increasingly, however, investigators realize
that stochastic fluctuations in gene or protein expression, between cells of
an otherwise identical group, can lead to major differences in their behavior.
This ‘noise’ in gene expression can have profound consequences for cell differentiation,
the response to apoptosis-inducing stimuli, or T lymphocyte triggering at the initiation
of immune responses.” <span class="citation">(Benoist and Hacohen 2011)</span></p>
</blockquote>
<blockquote>
<p>“The challenges of deriving statistically robust conclusions from high-throughput
technologies such as microarrays are well known. Yet the exploration of similar
challenges for data from sequencers, which can generate orders of magnitude more
data than an array, is largely at the beginning stages [as of 2011].” <span class="citation">(Mak 2011)</span></p>
</blockquote>
<blockquote>
<p>“Statistics becomes particularly important when sequencing technology is being
used quantitatively. In terms of traditional applications of just sequencing a
genome, I think that there is an abundance of methods already available that
addressed many of the major questions. But as soon as any sort of quantification comes
into play, such as using read depth to quantify copy number variation or
RNA transcript abundances (RNA-Seq), then statistical modeling—and particularly
normalization—becomes much more important.” <span class="citation">(Mak 2011)</span></p>
</blockquote>
<blockquote>
<p>“One of the main roles that statistics play in science is explaining variation—variation
of observed data. That variation can actually be true signal that you’re interested in,
but there can also be variations due to noise or confounding signal. So I think of
statistical modeling as the process of explaining variation in the data according
to concrete variables that have been measured.” <span class="citation">(Mak 2011)</span></p>
</blockquote>
<blockquote>
<p>“This process of accounting for, and possibly removing, sources of variation that
are not of biological interest is called normalization. There are two distinct
approaches to normalization. One of them I would call ‘unsupervised’ in that it
does not taken into account the study design. These are the most popular methods
because they require the least amount of statistical modeling and knowledge of
statistics. The other approach, which is one I strongly favor, is what I would
call ‘supervised normalization’. This approach directly takes into account the
study design. I find this appealing because is one is trying to parse sources of
variation, then it seems all sources of variation should be considered. If I
perform an experiment with 20 microarrays, say 10 treated and 10 control, then
I want to utilize this information when separating and removing technical
sources of variation. Another component of normalization, which is gaining
popularity, is normalizing by principal components. Again, I think this should
be done in the context of the study design, which was the goal of a recent
method I worked on called ‘surrogate variable analysis.’” <span class="citation">(Mak 2011)</span></p>
</blockquote>
<blockquote>
<p>“Let’s say you are doing an RNA-Seq experiment. The sequencer may produce a
different number of total reads from lane to lane, and that is more likely
driven by technology, not biology. And so, normalization would be about
accounting for those differences. Unsupervised normalization might involve
simply just dividing by the number of lanes and taking each gene as a percentage
of the reads in the lane. Why is that less than ideal? Suppose you have two batches
of data, one flow cell that was done in November and another flow cell that was
done in December. If you’re actually accounting for this variation in the
total number of reads per lane, my inclination would be to take into account the
fact that these two flow cells were processed in different months. And it can
be more complicated than that, too. Maybe you’ve taken clinical samples and there
were some differences in the clinical conditions under which they were taken.
In supervised normalization, you would take that information into account. For
example, the adjustment made to the raw reads may be based on a model that includes
the total number of reads per lane as well as the information about the study
design, such as batch and biological variables.” <span class="citation">(Mak 2011)</span></p>
</blockquote>
<blockquote>
<p>“The biggest, the easiest way [for a biologist doing RNA-Seq to tell that
better normalization of the data is needed]—the way that I discovered the
importance of normalization in the microarray context—is the lack of
reproducibility across different studies. You can have three studies that are
all designed to study the same thing, and you just see basically no reproducibility,
in terms of differentially expressed genes. And every time I encountered that, it
could always be traced back to normalization. So, I’d say that the biggest sign and
the biggest reason why you want to use normalization is to have a clear signal
that’s reproducible.” <span class="citation">(Mak 2011)</span></p>
</blockquote>
<blockquote>
<p>“Typically, RNA-seq data is analysed by laboriously typing commands into a Unix
operating system. Data files are passed from one software package to the next, with
each tool tackling one step in the process: genome alignment, quality control, variant
calling, and so on. The process is complicated. But for bulk RNA-seq, at least,
a consensus has emerged as to which algorithms work best for each step and how
they should be run. As a result, ‘pipelines’ now exist that are, if not exactly
plug-and-play, at least tractable for non-experts. To analyze differences in gene
expression, says Aaron Lun, a computational biologist at Cancer Research UK in
Cambridge, bulk RNA-seq is ‘pretty much a solved problem’.” <span class="citation">(J. M. Perkel 2017)</span></p>
</blockquote>
<p>Reproducibility of pre-processing:</p>
<blockquote>
<p>“Today, one often hears that life sciences are faced with the ‘big data problem.’
However, data are just a small facet of a much bigger challenge. The true
difficulty is that most biomedical researchers have no capacity to carry out
analyses of modern data sets using appropriate tools and computational infrastructure
in a way that can be fully understood and reused by others. This struggle began with
the introduction of microarray technology, which, for the first time, introduced
life sciences to truly large amounts of data and the need for quantitative training.
What is new, however, is that next-generation sequencing (NGS) has made this problem
vastly more challenging. Today’s sequencing-based experiments generate substantially
more data and are more broadly applicable than microarray technology, allowing for
various novel functional assays, including quantification of protein–DNA binding or
histone modifications (using chromatin immunoprecipitation followed by high-throughput
sequencing (ChIP-seq)), transcript levels (using RNA sequencing (RNA-seq)), spatial
interactions (using Hi-C) and others. These individual applications can be combined
into larger studies, such as the recently published genomic profiling of a human
individual whose genome was sequenced and gene expression tracked over an extended
period in a series of RNA-seq experiments. As a result, meaningful interpretation
of sequencing data has become particularly important. Yet such interpretation
relies heavily on complex computation—a new and unfamiliar domain to many of our
biomedical colleagues—which, unlike data generation, is not univerally
accessible to everyone.” <span class="citation">(Nekrutenko and Taylor 2012)</span></p>
</blockquote>
<blockquote>
<p>“The entire field of NGS analysis is in constant flux, and there is little agreement
on what is considered to be the ‘best practice’. In this situation, it is especially
important to be able to reuse and to adopt various analytical approaches reported
in the literature. Unfortunately, this is often difficult owing to the the lack of
necessary details. Let us look at the first and most straightforward of the analyses
[for genome variant discovery]: read mapping. To repeat a mapping experiment, it is
ncessary to have access to the primary data and to know the software and its version,
parameter settings and name of the reference genome build. From the 19 papers listed
in Box 1 …, only six satisfy all of these criteria. To investigate this further,
we surveyed 50 papers (Box 2) that use the Burrows-Wheeler Aligner (BWA) for
mapping (the BWA is one of the most popular mappers for Illumina data). More than
half do not provide primary data and list neither the version onr the parameters
used and neither do they list the exact version of the genomic reference sequence. If
these numbers are representative, then most results reported in today’s publications
using NGS data cannot be accurately verified, reproduced, adopted, or used to educate
others, creating an alarming reproducibility crisis.” <span class="citation">(Nekrutenko and Taylor 2012)</span></p>
</blockquote>
<blockquote>
<p>“We note that this discussion thus far has dealt only with technical reproducibility
challenges or with the ability to repeat published analyses using the original data
to verify the results. Most biomedical researchers are much more acquainted with
biological reproducibility, in which conceptual results are verified by an
alternative analysis of different samples. However, we argue that the computational
nature of modern biology blurs the distinction between technical and biological
reproducibility.” <span class="citation">(Nekrutenko and Taylor 2012)</span></p>
</blockquote>
<blockquote>
<p>“The 1000 Genomes Project is an international effort aimed at uncovering human
genetic variation with the ultimate goal of understanding the complex relationship
between genotype and phenotype. … The crucial importance of the pilot project
[phase of this project] was the establishment of a series of best practices that
are used for the analysis of the data from the main phase and are broadly applicable
for analysis of resequencing data in general. … Despite the fact that analytical
procedures developed by the project are well documented and rely on freely accessible
open-source software tools, the community still uses a mix of heterogeneous
approaches for polymorphism detection: we surveyed 299 articles published in 2011 that
explicitly cite the 1000 Genomes pilot publication. Nineteen of these were in fact
resequencing studies with an experimental design similar to that of the 1000 Genomes
Consortium (that is, sequencing a number of individuals for polymorphism discovery …).
Only ten studies used tools recommended by the consortium for mapping and variant
discovery, and just four studies used the full workflow involving realignment
and quality score recalibration. Interestingly, three of the four studies were
co-authored by at least one member of the 1000 Genomes Consortium.” <span class="citation">(Nekrutenko and Taylor 2012)</span></p>
</blockquote>
<blockquote>
<p>“The above challenges can be distilled into two main issues. First, most biomedical
researchers experience great difficulty carrying out computationally complex analyses
on large data sets. Second, there is a lack of mechanism for documenting analytical
steps in detail. Recently, a number of solutions have been developed that, if widely
adopted, would solve the bulk of these challenges. These solutions can collectively
be called integrative frameworks, as they bring together diverse tools under the
umbrella of a unified interface. These include BioExtract, Galaxy, GenePattern, Gene
Prof, Mobyle, and others. These tools record all analysis metadata, including tools,
versions and parameter settings used, ultimately transforming research provenance for
a task that requires active tracking by the analyst to one that is completely automatic.”
<span class="citation">(Nekrutenko and Taylor 2012)</span></p>
</blockquote>
<blockquote>
<p>“The overwhelming majority of currently published papers using NGS technologies include
analyses that are not detailed … Moreover, the computational approaches used in these
publications cannot be readily reused by others.” <span class="citation">(Nekrutenko and Taylor 2012)</span></p>
</blockquote>
<blockquote>
<p>“Many classical publications in life sciences have become influential because they
provide complete information on how to repeat reported analyses so others can adopt
these approaches in their own research, such as for chain termination sequencing
technology that was developed by Sanger and colleagues and for PCR. Today’s publications
that include computational analyses are very different. Next-generation sequencing (NGS)
technologies are undoubtedly as transformative as DNA sequencing and PCR were more than
30 years abgo. As more and more researchers use high-throughput sequencing in their
research, they consult other publications for examples of how to carry out computational
analyses. Unfortunately, they often find that the extensive informatics component that
is required to analyse NGS data makes it much more difficult to repeat studies published
today. Note that the lax standards of computational reproducibility are not unique to
life sciences; the importance of being able to repeat computational experiments was
first brought up in geosciences and became relevant in life sciences following the
establishment of microarray technology and high-throughput sequencing. Replication
of computational experiments requires access to input data sets, source code or binaries
of exact versions of software used to carry out the initial analysis (this includes
all helper scripts that are used to convert formats, groom data, and so on) and
knowing all parameter settings exactly as they were used. In our experience, …
publications rarely provide such a level of detail, making biomedical computational
analyses almost irreproducible.” <span class="citation">(Nekrutenko and Taylor 2012)</span></p>
</blockquote>
<p>Reproducibility in general:</p>
<blockquote>
<p>“Engineers expect their work to be subject to an [independent validation and
verification] IV&amp;V process, in which the organization conducting the research
uses a separate set of engineers to test, for example, whether microprocessors
or navigation software work as expected. NASA’s IV&amp;V facility was established
more than 25 years ago and has around 300 employees testing code and satellite
components.” <span class="citation">(Raphael, Sheehan, and Vora 2020)</span></p>
</blockquote>
<blockquote>
<p>“The biological sciences have depended on other, less-reliable techniques
for reproducbility. The most long-standing is the assumption that reproducibility
studies will occur organically as different researchers work on related problems.
In the past five years or so, funding agencies and journals have implemented
more-stringent experimental-reporting and data-availablility requirements for
grant proposals and submitted manuscripts. A handful of initiatives have attempted
to replicate published studies. The peer-reviewed ‘Journal of Visualized Experiments’
creates videos to disseminate details that are hard to convey in conventional
methods sections. Yet pitfalls persist. Scientists might waste resources trying
to build on unproven techniques. And real discoveries can be labelled irreproducible
because too few resources are available to conduct a validation.” <span class="citation">(Raphael, Sheehan, and Vora 2020)</span></p>
</blockquote>
<blockquote>
<p>“Data-analysis pipelines are replete with configuration decisions, assumptions,
dependencies and contingencies that move quickly beyond documentation, making
troubleshooting incrediably difficult. … Teams had to visit each others’ labs more
than once to understand and fully implement computational-analysis pipelines for
large microscopy datasets.” <span class="citation">(Raphael, Sheehan, and Vora 2020)</span></p>
</blockquote>
<blockquote>
<p>“The scientific community has lost the connection with the original culture of
skepticism which existed in the 17th century with the scientists of the
Royal Society who pioneered the scientific method as captured in their motton
nullius in verba (‘take nobody’s word’). They regarded the ability to replicate
results in independent studies as a fundamental criterion for the establishment
of a scientific fact. Modern scientific practice presents single experiments
as proofs. When work is published, it is typically presented without self-criticism.”
<span class="citation">(Neff 2021)</span></p>
</blockquote>
<blockquote>
<p>“Science requires that we constantly question what we know—and that should include
looking for advancements in study design or statistical analysis. For instance, the
T-test was a great tool when we didn’t have powerful computers to do complex
computations. Now that we do, it seems logical to rethink this.” <span class="citation">(Neff 2021)</span></p>
</blockquote>
<blockquote>
<p>“Currently, experimental design and data analysis expertise are not considered
critical skills and yet research fundamentally relies on the experimental process
and data is now an abundant output of the process. Statisticians are useful
support resources, but demand exceeds supply. The scientific process needs
quality, hands-on training that steps beyond theoretical concepts to give application
skills. I personally find researchers are opne; the resistance is arising as
scientists are overwhelmed by resisting forces (resources, time, skill, knowledge).”
<span class="citation">(Neff 2021)</span></p>
</blockquote>
<blockquote>
<p>“Transparency and quality management are key to improving scientific rigor and
reproducibility. It is, therefore, good to see that the Open Science movement
is gaining traction, and that research institutions increasingly view poor
science as a reputation risk.” <span class="citation">(Neff 2021)</span></p>
</blockquote>
<blockquote>
<p>“Robust findings become established over time as multiple lines of evidence
emerge. Achieving robustness takes rigour and reproducibility, plus patience
and judicious attention to the big picture.” <span class="citation">(Garraway 2017)</span></p>
</blockquote>
<blockquote>
<p>“Diverse data that converge on the same observations in aggregate provide
robustness, even though any single approach or model system has limitations.”
<span class="citation">(Garraway 2017)</span></p>
</blockquote>
<blockquote>
<p>“We scientists search tenaciously for information about how nature works through
reason and experimentation. Who can deny the magnitude of knowledge we have
gleaned, its acceleration over time, and its expanding positive impact on society?
Of course, some data and models are fragile, and our understanding remains
punctuated by false premises. Holding fast to the three Rs [rigor, reproducibility,
and robustness] ensures that the path—although tortuous and treacherous at times—remains
well lit.” <span class="citation">(Garraway 2017)</span></p>
</blockquote>
<blockquote>
<p>“Improved reproducibility comes from pinning down methods.” <span class="citation">(Lithgow, Driscoll, and Phillips 2017)</span></p>
</blockquote>
<blockquote>
<p>“Sources of variation include the quality and purity of reagents, daily fluctuations
in microenvironment and the idiosyncratic techniques of investigators. With so many
ways of getting it wrong, perhaps we should be surprised at how often experimental
findings are reproducible.” <span class="citation">(Lithgow, Driscoll, and Phillips 2017)</span></p>
</blockquote>
<blockquote>
<p>“Our first task, to develop a protocol, seemed straightforward. But subtle disparities
were endless. In one particularly painful teleconference, we spent an hour debating
the proper procedure for picking up worms and placing them on new agar plates. Some
batches of worms lived a full day longer with gentler techniques. Because a worm’s
lifespan is only about 20 days, this is a big deal. Hundreds of e-mails and many
teleconferences later, we converged on a technique but still had a stupendous three-day
difference in lifespan between labs. The problem, it turned out, was notation—one
lab determined age on the basis of when an egg hatched, others on when it was laid.”
<span class="citation">(Lithgow, Driscoll, and Phillips 2017)</span></p>
</blockquote>
<blockquote>
<p>“It is a rare project that specifies methods with such precision. In fact, several
mouse researchers have argued that standardization is counterproductive—better to
focus on results that persist across a wide range of conditions than to chase
fragile findings that occur only within narrow parameters. We argue that another way
forward is to chase down the variability and try to understand it within a common
environment.” <span class="citation">(Lithgow, Driscoll, and Phillips 2017)</span></p>
</blockquote>
<blockquote>
<p>“We have learnt that to understand how life works, describing how the research was
done is as important as describing what was observed.” <span class="citation">(Lithgow, Driscoll, and Phillips 2017)</span></p>
</blockquote>
<blockquote>
<p>“From time to time over the past few years, I’ve politely refused requests to referee
an article on the grounds that it lacks enough information for me to check the work.
This can be a hard thing to explain. Our lack of a precise vocabulary—in particular
the fact that we don’t have a word for ‘you didn’t tell me what you did in sufficient
detail for me to check it’—contributes to the crisis of scientific reproducibility.”
<span class="citation">(Stark 2018)</span></p>
</blockquote>
<blockquote>
<p>“In computational science, ‘reproducible’ often means that enough information
is provided to allow a dedicated reader to repeat the calculations in the paper for
herself. In biomedical disciplines, ‘reproducible’ often means that a different lab,
starting the experiment from scratch, would get roughly the same experimental
result.” <span class="citation">(Stark 2018)</span></p>
</blockquote>
<blockquote>
<p>“Results that generalize to all universes (or perhaps do not even require a
universe) are part of mathematics. Results that generalize to our Universe belong
to physics. Results that generalize to all life on Earth underpin molecular
biology. Results that generalize to all mice are murine biology. And results that
hold only for a particular mouse in a particular lab in a particular experiment
are arguably not science.” <span class="citation">(Stark 2018)</span></p>
</blockquote>
<blockquote>
<p>“Ushering in the Enlightenment era in the late seventeenth century, chemist
Robert Boyle put forth his controversial idea of a vacuum and tasked himself
with providing descriptions of his work sufficient ‘that the person I addressed
them to might, without mistake, and with as little trouble as possible, be able
to repeat such unusual experiments’. Much modern scientific communication falls
short of this standard. Most papers fail to report many aspects of the
experiment and analysis that we may not with advantage omit—things that are
crucial to understanding the result and its limitations, and to repeating the
work. We have no common language to describe this shortcoming. I’ve been in
conferences where scientists argued about whether work was reproducible,
replicable, repeatable, generalizable and other ‘-bles’, and clearly meant quite
different things by identical terms. Contradictory meanings across disciplines
are deeply entrenched.” <span class="citation">(Stark 2018)</span></p>
</blockquote>
<blockquote>
<p>“The lack of standard terminology means that we do not clearly distinguish
between situations in which there is not enough information to attempt
repetition, and those in which attempts do not yield substantially the same
outcome. To reduce confusion, I propose an intuitive, unambiguous neologism:
‘preproducibility’. An experiment or analysis is preproducible if it has been
described in adequate detail for others to undertake it. Preproducibility is a
prerequisite for reproducibility, and the idea makes sense across disciplines.”
<span class="citation">(Stark 2018)</span></p>
</blockquote>
<blockquote>
<p>“The distinction between a preproducible scientific report and current common
practice is like the difference between a partial list of ingredients and a
recipe. To bake a good loaf of bread, it isn’t enough to know that it contains
flour. It isn’t even enough to know that it contains flour, water, salt and
yeast. The brand of flour might be omitted from the recipe with advantage, as
might the day of the week on which the loaf was baked. But the ratio of
ingredients, the operations, their timing and the temperature of the oven
cannot. Given preproducibility—a ‘scientific recipe’—we can attempt to make
a similar loaf of scientific bread. If we follow the recipe but do not get the
same result, either the result is sensitive to small details that cannot be
controlled, the result is incorrect or the recipe was not precise enough (things
were omitted to disadvantage). Depending on the discipline, preproducibility
might require information about materials (including organisms and their care),
instruments and procedures; experimental design; raw data at the instrument
level; algorithms used to process the raw data; computational tools used in
analyses, including any parameter settings or ad hoc choices; code, processed
data and software build environments; or analyses that were tried and
abandoned.” <span class="citation">(Stark 2018)</span></p>
</blockquote>
<blockquote>
<p>“Just as I have pledged not to review papers that are not preproducible, I
have also pledged not to submit papers without providing the software I used,
and—to the extent permitted by law and ethics—the underlying data. I urge
you to do the same.” <span class="citation">(Stark 2018)</span></p>
</blockquote>
<blockquote>
<p>“If I publish an advertisement for my work (that is, a paper long on results
but short on methods) and it’s wrong, that makes me untrustworthy. If I say:
‘here’s my work’ and it’s wrong, I might have erred, but at least I am honest.
If you and I get different results, preproducibility can help us to identify
why—and the answer might be fascinating.” <span class="citation">(Stark 2018)</span></p>
</blockquote>
<blockquote>
<p>“It should not need to be stated, but here goes. Reproducibility is the key
underlying principle of science.” <span class="citation">(Gibb 2014)</span></p>
</blockquote>
<blockquote>
<p>“As chemists, we have to be able to go to the literature, take a procedure,
and carry out a similar or identical transformation with our own hands. Frustratingly,
this doesn’t always happen, and the next-to-worst-case scenario possible is
when it’s one of your own reactions that can’t be reproduced by a lab
elsewhere. Unsurprisingly, one step worse than this is when you can’t even
reproduce one of your own reactions in your own lab!” <span class="citation">(Gibb 2014)</span></p>
</blockquote>
<blockquote>
<p>“If there is nothing wrong with the reagents and reproducibility is still an
issue, then as I like to tell students, there are two options: (1) the physical
constants of the universe and hence the laws of physics are in a state of flux
in their round-bottomed flask, or (2) the researcher is doing something wrong
and either doesn’t know it or doesn’t want to know it. Then I ask them which
explanation they think I’m leaning towards.” <span class="citation">(Gibb 2014)</span></p>
</blockquote>
<blockquote>
<p>“Independent of what chemical companies do, it’s useful to have a good set of
standard operating procedures to maximize molecular hygiene (a phrase I like
because it emphasizes that we should treat our molecules in the same way as we
treat, say, our teeth). All established labs typically have such procedures, but
the problem is that newcomers often end up learning them the hard way.”
<span class="citation">(Gibb 2014)</span></p>
</blockquote>
<blockquote>
<p>“Another approach to maximizing reproducibility is to use automation to cut
out sloppy, all-too-human mistakes.” <span class="citation">(Gibb 2014)</span></p>
</blockquote>
<blockquote>
<p>“Quality systems are an integral part of most commercial goods and services,
used in manufacturing everything from planes to paint. Some labs that focus on
clinical applications implement certified QA systems such as Good Clinical
Practice, Good Manufacturing Practice and Good Laboratory Practice for data
submitted to regulatory bodies. There have also been efforts to guide research
practices outside these schemes. In 2001, the World Health Organization
published guidelines for QA in basic research. And in 2006, the British
Association of Research Quality Assurance (now simply the RQA) in Ipswich issued
guidelines for basic biomedical research. But few academic researchers know that
these standards exist … Instead, QA tends to be ad hoc in academic settings.
Many scientists are taught how to keep lab notebooks by their mentors,
supplemented perhaps by a perfunctory training course. Investigators often
improvise ways to safeguard data, maintain equipment or catalogue and care for
experimental materials. Too often, data quality is as likely to be assumed as
assured.” <span class="citation">(Baker 2016)</span></p>
</blockquote>
<blockquote>
<p>“Many think that antibodies are a major driver of what has been deemed a
‘reproducibility crisis’, a growing realization that the results of many
biomedical experiments cannot be reproduced and that the conclusions based on
them may be unfounded. Poorly characterized antibodies probably contribute more
to the problem than any other laboratory tool, says Glenn Begley, chief
scientific officer at TetraLogic Pharmaceuticals in Malvern, Pennsylvania, and
author of a controversial analysis1 showing that results in 47 of 53 landmark
cancer research papers could not be reproduced.” <span class="citation">(Baker 2015)</span></p>
</blockquote>
<blockquote>
<p>“Antibodies are produced by the immune systems of most vertebrates to target
an invader such as a bacterium. Since the 1970s, scientists have exploited
antibodies for research. If a researcher injects a protein of interest into a
rabbit, white blood cells known as B cells will start producing antibodies
against the protein, which can be collected from the animal’s blood. For a more
consistent product, the B cells can be retrieved, fused with an ‘immortalized’
cell and cultured to provide a theoretically unlimited supply.”
<span class="citation">(Baker 2015)</span></p>
</blockquote>
<blockquote>
<p>“The sandbox of the budding builder is not <em>making</em> as much as it is
modification: taking something that exists and making it better, either
functionally or aesthetically or both. Often that involves attaching and
securing parts that were not originally intended to go together.” <span class="citation">(Savage 2020)</span></p>
</blockquote>
<p>Thinking about the data-generating process—to figure out how to preprocess
your data, you need to think about all the processes that influenced it to
have the characteristics that it does. [Example of a very stressed mouse, or
recalibrating equipment between runs of samples]</p>
<blockquote>
<p>“That a language is easy for the computer expert does not mean it is
necessarily easy for the non-expert, and it is likely non-experts who will do the
bulk of the programming (coding, if you wish) in the near future.” <span class="citation">(Hamming 1997)</span></p>
</blockquote>
<blockquote>
<p>“What is wanted in the long run, of course, is that the man with the problem
does the actual writing of the code with no human interface, as we all to often
have these days, between the person who knows the problem and the person who
knows the programming language. This date is unfortunately too far off to
do much good immediately, but I would think by the year 2020 it would be
fairly universal practice for the expert in the field of application to do
the actual program preparation rather than have expers in computers (and
ignorant in the field of application) do the program preparation.” <span class="citation">(Hamming 1997)</span></p>
</blockquote>
<blockquote>
<p>“Rule 1: Do not attempt to analyze the data until you understand what is being
measured and why. … You may have to ask a lot of questions in order to clarify the
objectives, the meaning of each variable, the units of measurement, the meaning
of special symbols, whether similar experiments have been carried out before, and
so on.” <span class="citation">(Chatfield 1995)</span></p>
</blockquote>
<blockquote>
<p>“Statistical models usually contain one or more ssytematic components as well as
a random (or stochastic) component. The random component, sometimes called the noise,
arises for a variety of reasons, and it is sometimes helpful to distinguish between
(i) measurement error and (ii) natural random variability arising from differences
between experimental units and from changes in experimental conditions which cannot
be controlled. The systematic component, sometimes called the signal, may be deterministic,
but there is increasing interest in the case where the signal evolves through time
according to probabilistic laws. In engineering parlance, a statistical analysis
can be regarded as extracting information about the signal in the presence of noise.”
<span class="citation">(Chatfield 1995)</span></p>
</blockquote>
<hr />
<p><strong>Data pre-processing</strong></p>
<p>When we take measurements of experimental samples, we do so with the goal of
using the data we collect to gain scientific knowledge. The data are direct
measurement of something, but need to be interpreted to gain knowledge.
Sometimes direct measurements line up very closely with a research
question—for example if you are conducting a study that investigates the
mortality status of each test subject then whether or not each subject to dies
is a data point that is directly related to the research question you are aiming
to answer. In this case these data may go directly into a statistical analysis
model without extensive pre-processing. However, there are often cases where we
collect data that are not as immediately linked to the scientific question.
Instead, these data may require pre-processing before they can be used to test
meaningful scientific hypotheses. This is often the case for data extracted
using complex equipment. Equipment like mass spectrometers and flow cytometers
leverage physics, chemistry, and biology in clever ways to help us derive more
information from samples, but one tradeoff is that the data from such equipment
often require a bit of work to move into a format that is useful for answering
scientific questions.</p>
<p>One example if the data collected through liquid chromatography-mass
spectrometry (LC-MS). This is a powerful and useful technique for chemical
analysis, including analysis of biochemical molecules like metabolites and
proteins. However, when using this technique, the raw data require extensive
pre-processing before they can be used to answer scientific questions.</p>
<p>First, the data that are output by the mass spectrometer are often stored in a
specialized file format, like a netCDF or mzML file format. While these file
formats are standardized, they are likely formats you don’t regularly use in
other contexts, and so you may need to find special tools to read the data into
programs to analyze it. In some cases, the data are very large, and so it may be
necessary to use analysis tools that allow most of the data to stay “on disk”
while you analyze it, bringing only small parts into your analysis software at a
time.</p>
<p>Once the data are read in, they must be pre-processed in a number of ways. For
example, these data can be translated into features that are linked to the
chemical composition of the sample, with each feature showing up as a “peak” in
the data that are output from the mass spectrometer. A peak can be linked to a
specific metabolite feature based on its mass-to-charge ratio (m/z) and its
retention time. However, the exact retention time for a metabolite feature may
vary a bit from sample to sample. Pre-processing is required both to identify
peaks in the data and also to align the peaks from the same metabolite feature
across all samples from your experiment. There may also be technical bias across
samples, resulting in differences in the typical expression levels of all peaks
from one sample to the next. For example it may be the case that all intensities
measured for one sample tend to be higher than for another sample because of
technical bias in terms of the settings used for the equipment when the two
samples were run. These biases must also be corrected through pre-processing
before you can use the data within statistical tests or models to explore scientific hypotheses.</p>
<p>[Image of identifying and aligning peaks in LC-MS data]</p>
<p>In the research process, these pre-processing steps should be done before
the data are used for further analysis. There are the first step in working
with the data after they are collected by the equipment (or by laboratory
personal, in the case of data from simpler process, like plating samples
and counting colony-forming units). After the data are appropriately
pre-processed, you can use them for statistical tests—for example, to
determine if metabolite profiles are different between experimental groups—and
also combine them with other data collected from the experiment—for example,
to see whether certain metabolite levels are correlated with the bacterial
load in a sample.</p>
<p><strong>Approaches for pre-processing data.</strong></p>
<p>There are two main approaches for pre-processing experimental data in this
way. First, when data are the output of complex laboratory equipment, there
will often be proprietary software that is available for this pre-processing.
This software may be created by the same company that made the equipment, or
it may be created and sold by other companies. The interface will typically
be a graphical-user interface (GUI), where you will use pull-down menus and
point-and-click interfaces to work through the pre-processing steps. You
often will be able to export a pre-processed version of the data in a
common file format, like a delimited file or an Excel file, and that version
of the data can then be read into more general data analysis software, like
Excel or R.</p>
<p>[Include a screenshot of this type of software in action.]</p>
<p>In the simplest case, the point-and-click interface that is used for this
approach to pre-processing could be Excel or another version of spreadsheet
software. An Excel spreadsheet might be used with data recorded “by hand” in the
laboratory, with the researcher using embedded equations (or calculating and
entering new values by hand) for steps like calculating values based on the raw
data (for example, determining the time since the start of an experiment based
on the time stamp of each data collection timepoint). A spreadsheet may also be
used in steps of quality control. For example, if a recorded data point is known
to be problematic (for example, …), then the data point may be highlighted in
some way in the spreadsheet (e.g., with color) or removed entirely by deleting
or clearing the cell in the spreadsheet.</p>
<blockquote>
<p>“There are currently [2017] very few, if any, ‘plug-and-play’ packages that
allow researchers to quality control (QC), analyse and interpret scRNA-seq
data, although companies that sell the wet-lab hardware and reagents for
scRNA-seq are increasingly offering free software (for example, Loupe from
10x Genomics, and Singular from Fluidigm). These are user-friendly but have the
drawback that they are to some extent a ‘black box’, with little transparency
as to the precise algorithmic details and parameters employed.” <span class="citation">(Haque et al. 2017)</span></p>
</blockquote>
<p>The second approach is to conduct the pre-processing directly within general
data analysis software like R or Python. These programs are both open-source,
and include extensions that were created and shared by users around the world.
Through these extensions, there are often powerful tools that you can use to
pre-process complex experimental data. In fact, the algorithms used in
proprietary software are sometimes extended from algorithms first shared through
R or Python. With this approach, you will read the data into the program (R,
for example) directly from the file output from the equipment. You can
record all the code that you use to read in and pre-process the data in a
code script, allowing you to reproduce this pre-processing work. You can
also go a step further, and incorporate your code into a pre-processing
protocol, which combines nicely formatted text with executable code, and
which we’ll describe in much more detail later in this module and in the
following two modules.</p>
<p>There are advantages to taking the second approach—using scripted code in an
open-source program—rather than the first—using proprietary software with a
GUI interface. The use of codes scripts ensures that the steps of pre-processing
are reproducible. This means both that you will be able to re-do all the steps
yourself in the future, if you need to, but that also that other researchers can
explore and replicate what you do. You may want to share your process with
others in your laboratory group, for example, so they can understand the choices
you made and steps you took in pre-processing the data. You may also want to
share the process with readers of the articles you publish, and this may in fact
be required by the journal. Further, the use of a code script encourages you to
document this code and this process, even moreso when you move beyond a script
and include the code in a reproducible pre-processing protocol. Well-documented
code makes it much easier to write up the method section later in manuscripts
that leveraged the data collected in the experiment.</p>
<p>Also, when you use scripted code to pre-process biomedical data, you will find
that the same script can often be easily adapted and re-used in later projects
that use the same type of data. You may need to change small elements, like the
file names of files with data you want to use, or some details about the methods
used for certain pre-processing steps. However, often almost all of the
pre-processing steps will repeat over different experiments that you do. By
extending to write a pre-processing protocol, you can further support the
ease of adapting and re-using the pre-processing steps you take with one
experiment when you run later experiments that are similar.</p>
</div>
<div id="approaches-to-simple-preprocessing-tasks" class="section level3" number="3.2.4">
<h3><span class="header-section-number">3.2.4</span> Approaches to simple preprocessing tasks</h3>
<p>There are several approaches for tackling this type of data preprocessing, to
get from the data that you initial observe (or that is measured by a piece of
laboratory equipment) to meaningful biological measurements that can be analyzed
and presented to inform explorations of a scientific hypothesis. While there are
a number of approaches that don’t involve writing code scripts for this
preprocessing, there are some large advantages to scripting preprocessing any
time you are preprocessing experimental data prior to including it in figures or
further analysis. In this section, we’ll describe some common non-scripted
approaches and discuss the advantages that would be brought by instead using a
code script. In the next module, we’ll walk through an example of how scripts
for preprocessing can be created and applied in laboratory research.</p>
<p>In cases where the pre-processing is mathematically straightforward and the
dataset is relatively small, many researchers do the preprocessing by hand in a
laboratory notebook or through an equation or macro embedded in a spreadsheet.
For example, if you have plated samples at different dilutions and are trying to
calculate from these the CFUs in the original sample, this calculation is simple
enough that it could be done by hand. However, there are advantages to instead
writing a code script to do this simple preprocessing.</p>
<p>When you write a script to do a task with data, it is like writing a recipe that
can be applied again and again. By writing a script, you encode the process a
single time, so you can take the time to check and recheck to make sure that
you’ve encoded the process correctly. This helps in avoiding small errors when
you do the preprocessing—if you are punching numbers into a calculator over
and over, it’s easy to mistype a number or forget a step every now and then,
while the code will ensure that the same process is run every time and that it
faithfully uses the numbers saved in the data for each step, rather than relying
on a person correctly entering each number in the calculation.</p>
<p>Scripts can be used across projects, as well, and so they can ensure consistency
in the calculation across projects. If different people do the calculation in
the lab for different projects or experiments, and they are doing the
calculations by hand, they might each do the calculation slightly differently,
even if it’s only in small details like how they report rounded numbers. A
script will do the exact same thing every time it is applied. You can even share
your script with colleagues at other labs, if you want to ensure that your data
preprocessing is comparable for experiments conducted in different research
groups, and many scientific journals will allow supplemental material with
code used for data preprocessing and analysis, or links within the manuscript
to a repository of this code posted online.</p>
<p>There are also gains in efficiency when you use a script. For small
pre-processing steps, these might seem small for each experiment, and certainly
when you first write the script, it will likely take longer to write and test
the script than it would to just do the calculation by hand (even more if
you’re just starting to learn how to write code scripts). However, since the
script can be applied again and again, with very little extra work to apply it
to new data, you’ll save yourself time in the future, and over a lot of
experiments and projects, this can add up. This makes it particularly useful to
write scripts for preprocessing tasks that you find yourself doing again and
again in the lab.</p>
</div>
<div id="approaches-to-more-complex-preprocessing-tasks" class="section level3" number="3.2.5">
<h3><span class="header-section-number">3.2.5</span> Approaches to more complex preprocessing tasks</h3>
<p>Other preprocessing tasks can be much more complex, particularly those that need
to conduct a number of steps to extract biologically meaningful measurements
from the measurements made by a complex piece of laboratory equipment, as well
as steps to make sure these measurements can be meaningfully compared across
samples.</p>
<p>For these more complex tasks, the equipment manufacturer will often provide
software that can be used for the preprocessing. This software might conduct
some steps using defaults, and others based on the user’s specifications. These
are often provided through “GUIs” (graphical user interfaces), where the user
does a series of point-and-click steps to process the data. In some software,
this series of point-and-click steps is recorded as the user does them, so that
these steps can be “re-run” later or on a different dataset.</p>
<p>For many types of biological data, including output from equipment like flow
cytometers and mass spectrometers, open-source software has been developed
that can be used for this preprocessing. Often, the most cutting edge methods
for data preprocessing are first available through open-source software packages,
if the methods are developed by researchers rather than by the companies, and
often many of the algorithms that are made available through the equipment
manufacturer’s proprietary software are encoded versions of an algorithm
first shared by researchers as open-source software.</p>
<p>It can take a while to develop a code script for preprocessing the raw data from
a piece of complex equipment like a mass spectrometer. However, the process of
developing this script requires a thoughtful consideration of the steps of
preprocessing, and so this is often time well-spent. Again, this initial time
investment will pay off later, as the script can then be efficiently applied to
future data you collect from the equipment, saving you time in pointing and
clicking through the GUI software. Further, it’s easier to teach someone else
how to conduct the preprocessing that you’ve done, and apply it to future
experiments, because the script serves as a recipe.</p>
<p>When you conduct data preprocessing in a script, this also gives you access to
all the other tools in the scripting language. For example, as you work through
preprocessing steps for a dataset, if you are doing it through an R script, you
can use any of the many visualization tools that are available through R. By
contrast, in GUI software, you are restricted to the visualization and other
tools included in that particular set of software, and those software developers
may not have thought of something that you’d like to do. Open-source scripting
languages like R, Python, and Julia include a huge variety of tools, and once
you have loaded your data in any of these platforms, you can use any of these
tools.</p>
<p>If you have developed a script for preprocessing your raw data, it also becomes
much easier to see how changes in choices in preprocessing might influence your
final results. It can be tricky to guess whether your final results are sensitive,
for example, to what choice you make for a particular tranform for part of your
data, or in how you standardize data in one sample to make different samples
easier to compare. If the preprocessing is in a script, then you can test making
these changes and running all preprocessing and analysis scripts, to see if it
makes a difference in the final conclusions. If it does, then it helps you
identify parts of preprocessing that need to be deeply thought through for the
type of data you’re collecting, and you may want to explore the documentation on
that particular step of preprocessing to determine what choice is best for your
data, rather than relying on defaults.</p>
</div>
<div id="scripting-preprocessing-tasks" class="section level3" number="3.2.6">
<h3><span class="header-section-number">3.2.6</span> Scripting preprocessing tasks</h3>
<p>Code scripts can be developed for any open-source scripting languages, including
Python, R, and Julia. These can be embedded in or called from literate programming
documents, like RMarkdown and Julia, which are described in other modules. The
word “script” is a good one here—it really is as if you are providing the script
for a play. In an interactive mode, you can send requests to run in the programming
language step by step using a console, while in a script you provide the whole list
of all of your “lines” in that conversation, and the programming language will run
them all in order without you needing to interact from the console.</p>
<p>For preprocessing the data, the script will have a few predictible parts. First,
you’ll need to read the data in. There are different functions that can be used
to read in data from different file formats. For example, data that is stored in
an Excel spreadsheet can be loaded into R using functions in a package called
<code>readxl</code>. Data that is stored in a plain-text delimited format (like a csv file)
can be loaded into R using functions in the <code>readr</code> package.</p>
<p>When preprocessing data from complex equipment, you can determine how to read the
data into R by investigating the file type that is output by the equipment.
Fortunately, many types of scientific equipment follow standardized file formats.
This means that open-source developers can develop a single package that can
load data from equipment from multiple manufacturers. For example, flow cytometry
data is often stored in [file format]. Other biological datasets use file
formats that are appropriate for very large datasets and that allow R to work
with parts of the data at a time, without loading the full data in. [netCDF?]
In these cases, the first step in a script might not be to load in all the data,
but rather to provide R with a connection to the larger datafile, so it can
pull in data as it needs it.</p>
<p>Once the data is loaded or linked in the script, the script can proceed through
steps required to preprocess this data. These steps will often depend on the type
of data, especially the methods and equipment used to collect it. For example, for
mass spectrometry data, these steps will include … . For flow cytometry data,
these steps would include … .</p>
<p>The functions for doing these steps will often come from extensions that
different researchers have made for R. Base R is a simpler collection of data
processing and statistics tools, but the open-source framework of R has allowed
users to make and share their own extensions. In R, these are often referred to
as “packages”. Many of these are shared through the Comprehensive R Archive
Network (CRAN), and packages on CRAN can be directly installed using the
<code>install.packages</code> function in R, along with the package’s names. While CRAN
is the common spot for sharing general-purpose packages, there is a specialized
repository that is used for many genomics and other biology-related R packages
called Bioconductor. These packages can also be easily installed through a call
in R, but in this case it requires an installation function from the <code>BiocManager</code>
package. Many of the functions that are useful for preprocessing biological
data from laboratory experiments are available through Bioconductor.</p>
<p>Table [x] includes some of the primary R packages on Bioconductor that can be
used in preprocessing different types of biological data. There are often
multiple choices, developed by different research groups, but this list provides
a starting point of several of the standard choices that you may want to
consider as you start developing code.</p>
<p>Much of the initial preprocessing might use very specific functions that are
tailored to the format that the data takes once it is loaded. Later in the
script, there will often be a transfer to using more general-purpose tools in
that coding language. For example, once data is stored in a “dataframe” format
in R, it can be processed using a powerful set of general purpose tools
collected in a suite of packages called the “tidyverse”. This set of packages
includes functions for filtering to specific subsets of the data, merging
separate datasets, adding new measurements for each observation that are
functions of the initial measurements, summarizing, and visualizing. The
tidyverse suite of R tools is very popular in general R use and is widely
taught, including through numerous free online resources. By moving from
specific tools to these more general tools as soon as possible in the script, a
researcher can focus his or her time in learning these general purpose tools
well, as these can be widely applied across many types of data.</p>
<p>By the end of the script, data will be in a format that has extracted
biologically relevant measurements. Ideally, this data will be in a general
purpose format, like a dataframe, to make it easier to work with using general
purpose tools in the scripting language when the data is used in further data
analysis or to create figures for reports, papers, and presentations. Often, you
will want to save a version of this preprocessed version of the data in your
project files, and so the last step of the script might be to write out the
cleaned data in a file that can be loaded in later scripts for analysis and
visualization. This is especially useful if these data preprocessing steps are
time consuming, as is often the case for the large raw datasets output by
laboratory equipment like flow cytometers and mass spectrometers.</p>
<p>Figure [x] gives an example of a data preprocessing script, highlighting these
different common areas that often show up in these scripts.</p>
<p>[Some data may be incorporated into the preprocessing by downloading it from
databases or other online sources. These data downloads can be automated and
recorded by using scripted code for the download in many cases, as long as the
database or online source offers web services or another API for this type of
scripted data access. In this case, you can incorporate the script in a
RMarkdown document to record the date the data was downloaded, as well as the
code used to download it. R is able to run system calls, and one of these
will provide the current date, so this can be included in an RMarkdown file
to record the date the file is run. Further, there may be a call that can
be made to the online data source’s API that returns the working version of
the database or source, and if so this can also be included in the RMarkdown
code used to access the data.]</p>
<p>RMarkdown files can be used to combine both code and more manual document
(for example, a record of which collaborator provided each type of data file).
While traditionally this more manual documentation was recommended to be
recorded in plain-text README files in a project’s directory and subdirectories
<span class="citation">(Buffalo 2015)</span>, RMarkdown files provide some advantages over
this traditional approach. First, RMarkdown files are themselves in plain
text, and so they offer the advantages of simple plain text documentation
files (e.g., ones never rendered to another format) in terms of being able
to use script-based tools to search them. Further, they can be rendered into
attractive formatted documents that may be easier to share with project
team members who do not code.</p>
<p>[Example of a function: recipe for making a vinaigrette. There will be a
“basic” way that the function can run, which uses its default parameters.
However, you can also specify and customize certain inputs (for example,
using walnut oil instead of olive oil, or adding mustard) to tweak the
recipe in slight ways each time you use it, and to get customized outputs.]</p>
<p>[History of the mouse—enable GUIs, before everything was from the terminal.]</p>
</div>
<div id="potential-quotes" class="section level3" number="3.2.7">
<h3><span class="header-section-number">3.2.7</span> Potential quotes</h3>
<blockquote>
<p>For bioinformatics, “all too often the software is developed without
thought toward future interoperability with other software products. As a
result, the bioinformatics software landscape is currently characterized
by fragmentation and silos, in which each research group develops and uses
only the tools created within their lab.” <span class="citation">(Barga et al. 2011)</span></p>
</blockquote>
<blockquote>
<p>“The group also noted the lack of agility. Although they may be aware of
a new or better algorithm they cannot easily integrate it into their
analysis pipelines given the lack of standards across both data formats
and tools. It typically requires a complete rewrite of the code in order
to take advntge of a new technique or algorithm, requiring time and often
funding to hire developers.” <span class="citation">(Barga et al. 2011)</span></p>
</blockquote>
<blockquote>
<p>“The benefit of working with a programming language is that you have the code in
a file. This means that you can easily reuse that code. If the code has
parameters it can even be applied to problems that follow a similar pattern.”
<span class="citation">(Janssens 2014)</span></p>
</blockquote>
<blockquote>
<p>“Data exploration in spreadsheet software is typically conducted via menus and
dialog boxes, which leaves no record of the steps taken.” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“One reason Unix developers have been cool toward GUI interfaces is that, in their
designers’ haste to make them ‘user-friendly’ each one often becomes frustratingly
opaque to anyone who has to solve user problems—or, indeed, interact with it anywhere
outside the narrow range predicted by the user-interface designer.” <span class="citation">(E. S. Raymond 2003)</span></p>
</blockquote>
<blockquote>
<p>“Many operating systems touted as more ‘modern’ or ‘user friendly’ than Unix achieve their
surface glossiness by locking users and developers into one interface policy, and offer an
application-programming interface that for all its elaborateness is rather narrow and rigid.
On such systems, tasks the designers have anticipated are very easy—but tasks they have
not anticipated are often impossible or at best extremely painful. Unix, on the other hand, has
flexibility in depth. The many ways Unix provides to glue together programs means that components
of its basic toolkit can be combined to produce useful effects that the designers of the individual
toolkit parts never anticipated.” <span class="citation">(E. S. Raymond 2003)</span></p>
</blockquote>
<blockquote>
<p>“The good news is that a computer is a general-purpose machine, capable of performing
any computation. Although it only has a few kinds of instructions to work with, it can
do them blazingly fast, and it can largely control its own operation. The bad news is
that it doesn’t do anything itself unless someone tells it what to do, in excruciating
detail. A computer is the ultimate sorcere’s apprentice, able to follow instructions
tirelessly and without error, but requiring painstaking accuracy in the
specification of what to do.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“<em>Software</em> is the general term for sequences of instructions that make a computer
do something useful. It’s ‘soft’ in contrast with ‘hard’ hardware, because it’s
intangible, not easy to put your hands on. Hardware is quite tangible: if you drop
a computer on your foot, you’ll notice. Not true for software.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“Modern system increasingly use general purpose hardware—a processor, some memory,
and connections to the environment—and create specific behaviors by software. The
conventional wisdom is that software is cheaper, more flexible, and easier to change than
hardware is (especially once some device has left the factory).” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“An algorithm is a precise and unambiguous recipe. It’s expressed in terms of a fixed
set of basic operations whose meanings are completely known and specified; it spells out
a sequence of steps using those operations, with all possible situations covered; it’s
guaranteed to stop eventually. On the other hand, a <em>program</em> is the opposite of
abstract—it’s a concrete statement of the steps that a real computer must perform to
accomplish a task. The distinction between an algorithm and a program is like the difference
between a blueprint and a building; one is an idealization and the other is the real thing.”
<span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“One way to view a program is as one or more algorithms expressed in a form that a computer
can process directly. A program has to worry about practical problems like inadequate memory,
limited processor speed, invalid and even malicious input data, faulty hardware, broken
network connections, and (in the background and often exacerbating the other problems)
human frailty. So if an algorithm is an idealized recipe, a program is the instructions for
a cooking robot preparing a month of meals for an army while under enemy attack.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“During the late 1950s and early 1960s, another step was taken towards getting the
computer to do more for programmers, arguably the most important step in the history of
programming. This was the development of ‘high-level’ programming languages that were
independent of any particular CPU architecture. High-level languages make it possible to
express computations in terms that are closer to the way a person might express them.”
<span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“Programming in the real world tends to happen on a large scale. The strategy is similar
to what one might use to write a book or undertake any other big project: figure out what
to do, starting with a broad specification that is broken into smaller and smaller pieces,
then work on the pieces separately, while making sure that they hang together. In programming,
pieces tend to be of a size such that one person can write the precise computational steps
in some programming language. Ensuring that the pieces written by different programmers
work together is challenging, and failing to get this right is a major source of errors.
For instance, NASA’s Mars Climate Orbiter failed in 1999 because the flight system software
used metric units for thrust, but course correction data was entered in English units,
causing an erroneous trajectory that brought the Orbiter too close to the planet’s
surface.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“If you’re going to build a house today, you don’t start by cutting down trees to make
lumber and digging clay to make your own bricks. Instead, you buy prefabricated pieces like
doors, windows, plumbing fixtures, a furnace, and a water heater. House construction is still
a big job, but it’s manageable because you can build on the work of many others and rely
on an infrastructure, indeed an entire industry, that will help. The same is true of
programming. Hardly any significant program is created from nothing. Many components written
by others can be taken off the shelf and used. For instance, if you’re writing a program for
Windows or a Mac, there are libraries of prefabricated menus, buttons, text editors, graphics,
network connections, database access, and so on. Much of the job is understanding the components
and gluing them together in your own way. Of course, many of these components in turn rest on
other simpler and more basic ones, often for several layers. Below that, everything runs on
the operating system, a program that manages the hardware and controls everything that happens.”
<span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“At the simplest level, programming languages provide a mechanism called functions that make
it possible for one programmer to write code that performs a useful a useful task, then package
it in a form that other programmers can use in their programs without having to know how it
works.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“A function has a name and a set of input data values that it needs to do its job; it does
a computation and returns a result to the part of the program that called it. … Functions
make it possible to create a program by building on components that have been created separately
and can be used as necessary by all programmers. A collection of related functions is usually
called a <em>library</em>. … The services that a function library provides are described to programmers
in terms of an <em>Application Programming Interface</em>, or <em>API</em>, which lists the functions, what
they do, how to use them in a program, what input data they require, and what values they
produce. The API might also describe data structures—the organization of data that is passed
back and forth—and various other bits and pieces that all together define what a programmer
has to do to request services and what will be computed as a result. This specification must
be detailed and precise, since in the end the program will be interpreted by a dumb literal
computer, not by a friendly and accomodating human.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“The code that a programmer writes, whether in assembly language or (much more likely) in
a high-level language, is called <em>source code</em>. … Source code is readable by other programmers,
though perhaps with some effort, so it can be studied and adapted, and any innovations or ideas
it contains are visible.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“In early times, most software was developed by companies and most source code was
unavailable, a trade secret of whoever developed it.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“An <em>operating system</em> is the software underpinning that manages the hardware of a
computer and makes it possible to run other programs, which are called <em>applications</em>.
… It’s a clumsy but standard terminology for programs that are more or less self-contained
and focused on a single task.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“Software, like many other things in computing, is organized into layers, analogous to
geological strata, that separate one concern from another. Layering is one of the important
ideas that help programmers to manage complexity.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“I think that it’s important for a well-informed person to know something about
programming, perhaps only that it can be surprisingly difficult to get very simple
programs working properly. There is nothing like doing battle with a computer to teach
this lesson, but also to give people a taste of the wonderful feeling of accomplishment
when a program does work for the first time. It may also be valuable to have enough
programming experience that you are cautious when someone says that programming is easy,
or that there are no errors in a program. If you have trouble making 10 lines of code
work after a day of struggle, you might be legitimately skeptical of someone who claims
that a million-line program will be delivered on time and bug-free.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“Programming languages share certain basic ideas, since they are all notations for spelling
out a computation as a sequence of steps. Every programming language thus will provide ways
to get input data upon which to compute; do arithmetic; store and retrieve intermediate
values as computation proceeds; display results along the way; decide how to proceed on the basis
of previous computations; and save results when the computation is finished. Languages have
<em>syntax</em>, that is, rules that define what is grammatically legal and what is not.
Programming languages are picky on the grammatical side: you have to say it right or there
will be a complaint. Languages also have <em>semantics</em>, that is, a defined meaning for every
construction in the language.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“In programming, a <em>library</em> is a collection of related pieces of code. A library typically
includes the code in compiled form, along with needed source code declarations [for C++].
Libraries can include stand-alone functions, classes, type declarations, or anything else that
can appear in code.” <span class="citation">(Spraul 2012)</span></p>
</blockquote>
<blockquote>
<p>“One way to write R code is simply to enter it interactively at the command line… This
interactivity is beneficial for experimenting with R or for exploring a data set in a casual
manner. … However, interactively typing code at the R command line is a very bad approach from
the perspective of recording and documenting code because the code is lost when R is shut down.
A superior approach in general is to write R code in a file and get R to read the code from the file.” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“The features of R are organized into separate bundles called <em>packages</em>. The standard R
installation includes about 25 of those packages, but many more can be downloaded from CRAN and
installed to expand the things that R can do. … Once a package is installed, it must be
<em>loaded</em> within an R session to make the extra features available. … Of the 25 packages
that are installed by default, nine packages are <em>loaded</em> by default when we start a new
R session; these provide the basic functionality of R. All other packages must be loaded
before the relevant features can be used.” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“The R environment is the software used to run R code.” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“<em>Document your methods and workflows.</em> This should include full command lines (copied
and pasted) that are run through the shell that generate data or intermediate results.
Even if you use the default values in software, be sure to write these values down;
later versions of the program may use different default values. Scripts naturally
document all steps and parameters …, but be sure to document any command-line options
used to run this script. In general, any command that produces results in your work needs
to be documented somewhere.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“<em>Document the version of the software that you ran.</em> This may seem unimportant, but
remember the example from ‘Reproducible Research’ on page 6 where my colleagues and I
traced disagreeing results down to a single piece of software being updated. These
details matter. Good bioinformatices software usually has a command-line option to
return the current version. Software managed with a version control system such as
Git has explicit identifiers to every version, which can be used to document the
precise version you ran… If no version information is available, a release date,
link to the software, and download date will suffice.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“<em>Document when you downloaded data.</em> It’s important to include when the data was downloaded,
as the external data source (such as a website or server) might change in the future. For example,
a script that downloads data directly from a database might produce different results if
rerun after the external database is updated. Consequently, it’s important to document
when data came into your repository.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“All of this [documentation] information is best stored in plain-text README files.
Plain text can easily be read, searched, and edited directly from the command line,
making it the perfect choice for portable and accessible README files. It’s also available
on all computer systems, meaning you can document your steps when working directly on
a server or computer cluster. Plain text also lacks complex formatting, which can create
issues when copying and pasting commands from your documentation back into the command
line.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“The computer is a very flexible and powerful tool, and it is a tool that is ours
to control. Files and documents, especially those in open standard formats, can be
manipulated using a variety of software tools, not just one specific piece of software.
A programming lanuage is a tool that allows us to manipulate data stored in files and
to manipulate data held in RAM in unlimited ways. Even with a basic knowledge of
programming, we can perform a huge variety of data processing tasks.” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“Computer code is the preferred approach to communicating our instructions to the
computer. The approach allows us to be precise and expressive, it provides a complete
record of our actions, and it allows others to replicate our work.” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“Programming in R is carried out, primarily, by manipulating and modifying data structures.
These different transformations are carried out using functions and operators. In R,
virtually every operation is a function call, and though we separate our discussion into
operators and function calls, the distinction is not strong … The R evaluator and
many functions are written in C but most R functions are written in R itself.”
<span class="citation">(Gentleman 2008)</span></p>
</blockquote>
<blockquote>
<p>“Many biologists are first exposed to the R language by following a cookbook-type
approach to conduct a statistical analysis like a t-test or an analysis of
variance (ANOVA). ALthough R excels at these and more complicated statistical
tests, R’s real power is as a data programming lanugage you can use to explore and
understand data in an open-ended, highly interactive, iterative way. Learning R as a
data programming language will give you the freedom to experiment and problem solve
during data analysis—exactly what we need as bioinformaticians.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Popularized by statistician John W. Tukey, EDA is an approach that emphasizes
understanding data (and its limitations) through interactive investigation
rather than explicit statitical modeling. In his 1977 book <em>Exploratory Data
Analysis</em>, Tukey described EDA as ‘detective work’ involved in ‘finding and
revealing the clues’ in data. As Tukey’s quote emphasizes, EDA is much more an approach
to exploring data than using specific statistical methods. In the face of rapidly
changing sequencing technologies, bioinformatics software, and statistical methods,
EDA skills are not only widely applicable and comparatively stable—they’re also
essential to making sure that our analyses are robust to these new data and methods.”
<span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Developing code in R is a back-and-forth between writing code in a rerunnable script
and exploring data interactively in the R interpreter. To be reproducible, all steps
that lead to results you’ll use later must be recorded in the R script that accompanies
your analysis and interactive work. While R can save a history of the commands you’ve
entered in the interpreter during a session (with the command <code>savehistory()</code>),
storing your steps in a well-commented R script makes your life much easier when you
need to backtrack to understand what you did or change your analysis.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“It’s a good idea to avoid referring to specific dataframe rows in your analysis code.
This would produce code fragile to row permutations or new rows that may be generated
by rerunning a previous analysis step. In every case in which you might need to refer
to a specific row, it’s avoidable by using subsetting… Similarly, it’s a good idea
to refer to columns by their column name, <em>not</em> their position. While columns may be
less likely to change across dataset versions than rows, it still happens. Column names
are more specific than positions, and also lead to more readable code.”
<span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“In bioinformatics, we often need to extract data from strings. R has several
functions to manipulate strings that are handy when working with bioinformatics data in
R. Note, however, that for most bioinformatics text-processing tasks, R is <em>not</em>
the preferred language to use for a few reasons. First, R works with all data stored
in memory; many bioinformatics text-processing tasks are best tackled with the
stream-based approaches…, which explicityly avoid loading all data in memory at
once. Second, R’s string processing functions are admittedly a bit clunky compared
to Python’s.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Versions fo R and any R pakcages installed change over time. This can lead to
reproducibility headaches, as the results of your analyses may change with the
changing version of R and R packages. … you should always record the versions
of R and any packages you use for analysis. R actually makes this incrediably
easy to do—just call the <code>sessionInfo()</code> function.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Bioconductor is an open source R software project focused on developing tools
for high-throughput genomics and molecular biology data.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Bioconductor’s pakcage system is a bit different than those on the Comprehensive R
Archive Network (CRAN). Bioconductor packages are released on a set schedule, twice
a year. Each release is coordinated with a version of R, making Bioconductor’s versions
tied to specific R versions. The motivation behind this strict coordination is that it
allows for packages to be thoroughly tested before being released for public use.
Additionally, because there’s considerable code re-use within the Bioconductor project,
this ensures that all package versions within a Bioconductor release are compatible
with one another. For users, the end result is that packages work as expected and
have been rigorously tested before you use it (this is good when your scientific
results depend on software reliability!). If you need the cutting-edge version of a
package for some reason, it’s always possible to work with their development branch.”
<span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“When installing Bioconductor packages, we use the <code>biocLite()</code> function. <code>biocLite()</code>
installs the correct version of a package for your R version (and its corresponding
Bioconductor version).” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“In addition to a careful release cycle that fosters package stability, Bioconductor
also has extensive, excellent documentation. The best, most up-to-date documentation
for each package will always be a Bioconductor [web address]. Each package has a full
reference manual covering all functions and classes included in a package,
as well as one or more in-depth vignettes. Vignettes step through many examples and
common workflows using packages.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Quite often, users don’t appreciate the opportunities. Noncomputational
biologists don’t know when to complain about the status quo. With modest amounts
of computational consulting, long or impossible jobs can become much shorter or
richer.” — Barry Demchak in <span class="citation">(Altschul et al. 2013)</span></p>
</blockquote>
<blockquote>
<p>“People not doing the computational work tend to think that you can write a
program very fast. That, I think, is frankly not true. It takes a lot of time to
implement a prototype. Then it actually takes a lot of time to really make it
better.” — Heng Li in <span class="citation">(Altschul et al. 2013)</span></p>
</blockquote>
<blockquote>
<p>“There is also a problem with discovering software that exists; often people
reinvent the wheel just because they don’t know any better. Good repositories
for software and best practice workflows, especially if citable, would be a
start.” — James Taylor in <span class="citation">(Altschul et al. 2013)</span></p>
</blockquote>
<blockquote>
<p>” Now there are a lot of strong, young, faculty members who label themselves
as computational analysts, yet very often want wet-lab space. They’re not
content just working off data sets that come from other people. They want to be
involved in data generation and experimental design and mainstreaming
computation as a valid research tool. Just as the boundaries of biochemistry and
cell biology have kind of blurred, I think the same will be true of
computational biology. It’s going to be alongside biochemistry, or molecular
biology or microscopy as a core component.” — Richard Durbin in
<span class="citation">(Altschul et al. 2013)</span></p>
</blockquote>
<blockquote>
<p>“I would say that computation is now as important to biology as chemistry is.
Both are useful background knowledge. Data manipulation and use of information
are part of the technology of biology research now. Knowing how to program also
gives people some idea about what’s going on inside data analysis. It helps them
appreciate what they can and can’t expect from data analysis software.” —
Richard Durbin in <span class="citation">(Altschul et al. 2013)</span></p>
</blockquote>
<blockquote>
<p>“<strong>Does every new biology PhD student need to learn how to program?</strong> To some,
the answer might be “no” because that’s left to the experts, to the people
downstairs who sit in front of a computer. But a similar question would be: does
every graduate student in biology need to learn grammar? Clearly, yes. Do they
all need to learn to speak? Clearly, yes. We just don’t leave it to the
literature experts. That’s because we need to communicate. Do students need to
tie their shoes? Yes. It has now come to the point where using a computer is as
essential as brushing your teeth. If you want some kind of a competitive edge,
you’re going to want to make as much use of that computer as you can. The
complexity of the task at hand will mean that canned solutions don’t exist. It
means that if you’re using a canned solution, you’re not at the edge of
research.” — Martin Krzywinski in <span class="citation">(Altschul et al. 2013)</span></p>
</blockquote>
<blockquote>
<p>“Although we are tackling many different types of data, questions, and
statistical methods hands-on, we maintain a consistent computational approach
by keeping all the computation under one roof: the R programming language and
statistical environment, enhanced by the biological data infrastructure and
specialized method packages from the Bioconductor project.” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“The availablility of over 10,000 packages [in R] ensures that almost all
statistical methods are available, including the most recent developments.
Moreover, there are implementations of or interfaces to many methods from
computer science, mathematics, machine learning, data management, visualization
and internet technologies. This puts thousands of person-years of work by
experts at your fingertips.” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“Bioconductor packages support the reading of many of the data types and formats
produced by measurement instruments used in modern biology, as well as the
needed technology-specific ‘preprocessing’ routines. This community is
actively keeping these up-to-date with the rapid developments in the
instrument market.” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“An equivalent to the laboratory notebook that is standard good practice in
labwork, we advocate the use of a computational diary written in the R markdown
format. … Together with a version control system, R markdown helps with
tracking changes.” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“There are (at least) two types of data visualization. The first enables
a scientist to explore data and make discoveries about the complex processes
at work. The other type of visualization provides informative, clear and
visually attractive illustrations of her results that she can show to others
and eventually include in a publication.” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“A common task in biological data analysis is comparison between several
samples of univariate measurements. … As an example, we’ll use the intensities
of a set of four genes… A popular way to display [this] is through
barplots [and boxplots, violin plots, dot plots, and beeswarm plots].” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“At different stages of their development, immune cells express unique
combinations of proteins on their surfaces. These protein-markers are called
CDs (clusters of differentiation) and are collected by flow cytometry
(using fluorescence…) or mass cytometry (using single-cell atomic mass
spectrometry of heavy metal reporters). An example of a commonly used CD is
CD4; this protein is expressed by helper T cells that are referred to as
being ‘CD4+’. Note, however, that some cells express CD4 (thus are CD4+)
but are not actually helper T cells. We start by loading some useful Bioconductor
packages for flow cytometry, <code>flowCore</code> and <code>flowViz</code>. …
First we load the table data that reports the mapping between isotopes and
markers (antibodies), and then we replace the isotope names in the column
names … with the marker names. Changing the column names makes the subsequent
analysis and plotting easier to read. … Plotting the data in two dimensions…
already shows that the cells can be grouped into subpopulations. Sometimes just
one of the markers can be used to define populations on its own; in that case,
simple rectangular gating is used to separate the populations. For instance,
CD4+ populations can be gating by taking the subpopulation with high values
for the CD4 marker. Cell clustering can be improved by carefully choosing
transformations of the data. … [Such a transformation] reveals
bimodality and the existence of two cell populations… It is standard
to transform both flow and mass cytometry data using one of several special
functions. We take the example of the inverse hyperbolic arcsine (asinh) …
for large values of x, asinh(x) behaves like the log and is practically
equal to log(x) + log(2); for small x the function is close to linear in
x. … This is another example of a variance-stabilizing transformation.” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“Consider a set of measurements that reflect some underlying true values
(say, species represented by DNA sequences from their genomes) but
have been degraded by technical noise. Clustering can be used to remove
such noise.” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“In the bacterial 16SrRNA gene there are so-called variable regions that are
taxa-specific. These provide fingerprints that enable taxon identification. The
raw data are FASTQ-files with quality scored sequences of PCR-amplified DNA
regions. We use an iterative alternating approach to build a probabilistic noise
mode from the data. We call this a de novo method, because we use clustering,
and we use the cluster centers as our denoised sequence variants… After
finding all the denoised variants, we create contingency tables of their counts
across the different samples. … these tables can be used to infer properties
of the underlying bacterial communities using networks and graphs. <strong>In order to
improve data quality, we often have to start with the raw data and model all the
sources of variation carefully.</strong> We can think of this as an example of cooking
from scratch. … The DADA method … uses a parameterized model of substitution
errors that distinguishes sequencing errors from real biological variation. …
The dereplicated sequences are read in, and then divisive denoising and
estimation is run with the <code>dada</code> function… In order to verify that the error
transition rates have been reasonably well estimated, we inspect the fit between
the observed error rates … and the fitted error rates .. Once the errors have
been estimated, the algorithm is rerun on the data to find the sequence
variants. … Sequence inference removes nearly all substition and indel errors
from the data. [Footnote: ’The term indel stands for insertion-deletion; when
comparing two sequences that differ by a small stretch of characters, it is a
matter of viewpoint whether this is an insertion or a deletion, hence the name].
We merge the inferred forward and reverse sequences while removing paired
sequences that do not perfectly overlap, as a final control against residual
errors.” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“Chimera are sequences that are artificially created during the PCR
amplification by the melding of two (or, in rare cases, more) of the
original sequences. To complete our denoising workflow, we remove them
with a call to the function <code>removeBimeraDenovo</code>, leaving us with a
clean contingency table that we will use later.” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“We load up the RNA-Seq dataset <code>airway</code>, which contains gene expression
measurements (gene-level counts) of four primary human airway smooth muscle
cell lines with and without treatment with dexamethasone, a synthetic
glucocorticoid. We’ll use the <code>DESeq2</code> method … it performs a test
for differential expression for each gene.” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“In many cases, different variables are measured in different units, so they
have different baselines and different scales. [Footnote: ‘Common measures of
scale are the range and the standard deviation…’] For PCA and many other
methods, we therefore need to transform the numeric values to some common scale
in order to make comparisons meaningful. Centering means subtracting the mean,
so that the mean of the centered data is at the origin. Scaling or standardizing
then means dividing by the standard deviation, so that the new standard
deviation is 1. … To perform these operations, there is the R function
<code>scale</code>, whose default behavior when given a matrix or a dataframe is to make
each column have a mean of 0 and a standard deviation of 1. … We have already
encountered other data transformation choices in Chapters 4 and 5, where we used
the log and asinh functions. The aim of these transformations is (usually)
variance stabilization, i.e., to make the variances of the replicate
measurements of one and the same variable in different parts of the dynamic
range more similar. In contrast, the standardizing transformation described
above aims to make the scale (as measured by mean and standard deviation) of
<em>different</em> variables the same. Sometimes it is preferable to leave variables at
different scales because they are truely of different importance. If their
original scale is relevant, then we can (and should) leave the data alone. In
other cases, the variables have different precisions known a priori. We will see
in Chapter 9 that there are several ways of weighting such variables. After
preprocessing the data, we are ready to undertake data simplification through
dimension reduction.” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>With data that give the number of reads for each gene in a sample, “The
data have a large dynamic range, starting from zero up to millions. The
variance and, more generally, the distribution shape of the data in different
parts of the dynamic range are very different. We need to take this
phenomenon, called heteroscedascity, into account. The data are non-negative
integers, and their distribution is not symmetric—thus normal or log-normal
distribution models may be a poor fit. We need to understand the systematic
sampling biases and adjust for them. Confusingly, such adjustment is often
called normalization. Examples are the total sequencing depth of an experiment
(even if the true abundance of a gene in two libraries is the same, we expect
different numbers of reads for it depending on the total number of reads
sequenced) and differing sampling probabilities (even if the true abundance of
two genes within a biological sample is the same, we expect different numbers
of reads for them if they have differing biophysical properties, such as length,
GC content, secondary structure, binding partners).” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“Often, systematic biases affect the data generation and are worth taking
into account. Unfortunately, the term normalization is commonly used for that
aspect of the analysis, even though it is misleading; it has nothing to do
with the normal distribution, nor does it involve a data transformation.
Rather, what we aim to do is identify the nature and magnitude of
systematic biases and take them into account in our model-based analysis of the
data. The most important systematic bias [for count data from high-throughput
sequencing applications like RNA-Seq] stems from variations in the total number
of reads in each sample. If we have more reads for one library than for another,
then we might assume that, everything else being equal, the counts are
proportional to each other with some proportionality factor <em>s</em>. Naively,
we could propose that a decent estimate of <em>s</em> for each sample is simply
given by the sum of the counts of all genes. However, it turns out that we
can do better…” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“When testing for differential expression, we operate on raw counts and
use discrete distributions. For other downstream analyses—e.g., for
visualization or clustering—it can be useful to work with transformed versions
of the count data. Maybe the most obvious choice of transformation is the
logarithm. However, since count values for a gene can be zero, some analysts
advocate the use of pseudocounts, i.e., transformations of the form
y = log2(n + 1) or more generally y = log2(n + n0).” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“The data sometimes contain isolated instances of very large counts that
are apparently unrelated to the experimental or study design and may be
considered outliers. Outliers can arise for many reasons, including rare
technical or experimental artifacts, read mapping problems in the case of
genetically differing samples, and genuine but rare biological events. In
many cases, users appear primarily interested in genes that show consistent
behavior, and this is the reason why, by default, genes that are affected by such
outliers are set aside by <code>DESeq</code>. The function calculates, for every gene
and for every sample, a diagnostic test for outliers called Cook’s distance.
Cook’s distance is a measure of how much a single sample is influencing the
fitted coefficients for a gene, and a large value of Cook’s distance is
intended to indicate an outlier count. <code>DESeq2</code> automatically flags genes
with Cook’s distance above a cutoff and sets their p-values and adjusted
p-values to NA. … With many degrees of freedom—i.e., many more samples
than number of parameters to be estimated—it might be undesirable to remove
entire genes from the analysis just becuase their data include a single
count outlier. An alternative strategy is to replace the outlier counts with
the trimmed mean over all sample, adjusted by the size factor for that
sample. This approach is conservative: it will not lead to false positives,
as it replaces the outlier value with the value predicted by the null hypothesis.” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“Since the sampling depthy is typically different for different sequencing
runs (replicates), we need to estimate the effect of this variable parameter
and take it into account in our model. … Often this part of the analysis
is called normalization (the term is not particularly descriptive, but
unfortunately it is now well established in the literature).” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“Our experimental interventions and our measurement instruments have limited
precision and accuracy; often we don’t know these limitations at the outset and
have to collect preliminary data to estimate them.” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“Our treatment conditions may have undesired but hard-to-avoid side effects;
our measurements may be overlaid with interfering signals or ‘background noise’.”
<span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“Sometimes we explicitly know about factors that cause bias, for instance, when
different reagent batches were used in different phases of the experiment. We
call these batch effects (Leek et al., 2010). At other times, we may expect that
such factors are at work but have no explicit record of them. We call these
latent factors. We can treat them as adding to the noise, and in Chapter
4 we saw how to use mixture models to do so. But this may not be enough; with
high-dimensional data, noise caused by latent factors tends to be correlated,
and this can lead to faulty inference (Leek et al., 2010). The good news is that
these same correlations can be exploited to estimate latent factors from
the data, model them as bias, and thus reduce the noise (Leek and Storey 2007;
Stegle et al. 2010).” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“Regular noise can be modeled by simple probability models such as independent
normal distributions or Poissons, or by mixtures such as the gamma-Poisson or
Laplace. We can use relatively straightforward methods to take such noise into
account in our data analyses and to compute the probability of extraordinarily
large or small values. In the real world, this is only part of the story:
measurements can be completely off-scale (a sample swap, a contamination, or
a software bug), and they can all go awry at the same time (a whole microtiter
plate went bad, affecting all data measured from it). Such events are hard to model
or even correct for—our best chance of dealing with them is data quality
assessment, outlier detection, and documented removal.” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“In Chapters 4 and 8 we saw examples of data transformations that compress or
stretch the space of quantitative measurements in such a way that the
measurements’ variance is more similar throughout. Thus the variance between
replicated measurements is no longer highly dependent on the mean value. The
mean-variance relationship of our data before transformation can, in
principle, be any function, but in many cases, the following prototypic relationships
are found, at least approximately: 1. Constant: the variance is independent
of the mean…; 2. Poisson: the variance is proproational to the mean…;
3. Quadratic: the standard deviation is proportional to the mean; therefore
the variance grows quadratically… The mean-variance relationship in real data
can also be a combination of these basic types. For instance, with DNA microarrays,
the fluorescence intensities are subject to a combination of background noise that
is largely independent of the signal, and multiplicative noise whose standard
deviation is proportional to the signal (Rocke and Durbin 2001). …
What is the point of applying a variance-stabilizing transformation? Analyzing the
data on the transformed scale tends to: 1. Improve visualization, since the
physical space on the plot is used more ‘fairly’ throughout the range of the
data. A similar argument applies to the color space in the case of a heatmap.
2. Improve the outcome of ordination methods such as PCA or clustering based
on correlation, as the results are not so much dominated by the signal from a few
very highly expressed genes, but more uniformly from many genes throughout the
dynamic range. 3. Improve the estimates and inference from statistical models
that are based on the assumption of identially distributed (and, hence,
homoscedastic) noise.”
<span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“We distinguis between data quality assessment (QA)—steps taken to measure
and monitor data quality—and quality control—the removal of bad data.
These activities pervade all phases of an analysis, from assembling the raw
data over transformation, summarization, model fitting, hypothesis testing or
screening for ‘hits’ to interpretation. QA-related questions include:
1. How do the marginal distributions of the variables look (histograms,
ECDF plots)? 2. How do their joint distributions look (scatterplots, pair plots)?
3. How well do replicated agree (as compared to different biological conditions)?
Are the magnitudes of different between several conditions plausible?
4. Is there evidence of batch effects? These could be of a categorical (stepwise)
or continuous (gradual) nature, e.g., due to changes in experimental reagents,
protocols or environmental factors. Factors associated with such effects may
be explicitly known, or unknown and latent, and often they are somewhere in
between (e.g., when a measurement apparatus slowly degrades over time, and
we have recorded the times, but don’t really know exactly when the degradation
becomes bad). For the last two sets of questions, heatmaps, principal components
plots, and other ordination plots (as we have seen in Chapters 7 and 9) are
useful.” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“It’s not easy to define quality, and the word is used with many meanings. The
most pertinent for us is fitness for purpose, and this contrasts with other
definitions that are based on normative specifications. For instance, in
differential expression analysis with RNA-Seq data, our purpose may be the
detection of differentially expressed genes between two biological conditions.
We can check specificiations such as the number of reads, read length, base
calling quality and fraction of aligned reads, but ultimately these measures in
isolation have little bearing on our purpose. More to the point will be the
identification of samples that are not behaving as expected, e.g., because of
a sample swap or degradation, or genes that were not measured properly.
… Useful plots include ordination plots … and heatmaps …
A quality metric is any value that we use to measure quality, and having
explicit quality metrics helps in automating QA/QC.” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“<strong>Use literate programming tools.</strong> Examples are Rmarkdown and Jupyter. This
makes code more readable (for yourself and for others) than burying
explanations and usage instructions in comments in the source code or in
separate README files. In addition, you can directly embed figures and tables
in these documents. Such documents are good starting points for the supplementary
material of your paper. Moreover, they’re great for reporting analyses to your
collaborators.” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“<strong>Use functions.</strong> It’s better than copy-pasting (or repeatedly source-ing)
stretches of code.” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p><strong>Use the R package system.</strong> Soon you’ll note recurring function or variable
definitions that you want to share between your different scripts. It is fine to
use the R function <code>source</code> to manage them initially, but it is never too early
to move them into your own package—at the latest when you find yourself staring
to write emails or code comments explaining to others (or to yourself) how to use
some functionality. Assembling existing code into an R package is not hard, and it
offers you many goodies, including standardized ways of composing documentation,
showing code usage examples, code testing, versioning and provision to others.
And quite likely you’ll soon appreciate the benefits of using namespaces.”
<span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“<strong>Think in terms of cooking recipes and try to automate them.</strong> When developing
downstream analysis ideas that bring together several different data types, you
don’t want to do the conversion from data-type-specific formats into a
representation suitable for machine learning or a generic statistical method
each time anew, on an ad hoc basis. Have a recipe script that assembles the
different ingredients and cooks them up as an easily consumable [footnote:
In computer science, the term data warehouse is sometimes used for such a concept]
matrix, dataframe or Bioconductor <code>SummarizedExperiment</code>.” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“<strong>Centralize the location of the raw data files and automate the derivation of
intermediate data.</strong> Store the input data on a centralized file server that is
profesionally backed up. Mark the files as read-only. Have a clear and linear
workflow for computing the derived data (e.g., normalized, summarized, transformed,
etc.) from the raw files, and store these in a separate directory. Anticipate that
this workflow will need to be run several times, and version it. Use the
<code>BiocFileCache</code> package to mirror these files on your personal computer.
[footnote: A more basic alternative is the rsync utility. A popular solution offered
by some organizations is based on ownCloud. Commercial options are Dropbox,
Google Drive and the like].” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“<strong>Keep a hyperlinked webpage with an index of all analyses.</strong> This is helpful
for collaborators (especially if the page and the analysis can be accessed via
a web browser) and also a good starting point for the methods part of your paper.
Structure it in chronological or logical order, or a combination of both.”
<span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“Getting data ready for analysis or visualization often involves a lot of
shuffling until they are in the right shape and format for an analytical
algorithm or a graphics routine.” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<blockquote>
<p>“Data analysis pipelines in high-throughput biology often work as ‘funnels’
that successively summarize and compress the data. In high-throughput
sequencing, we may start with individual sequencing reads, then align them to
a reference, then only count the aligned reads for each position, summarize
positions to genes (or other kinds of regions), then ‘normalize’ these numbers
by library size to make them comparable across libraries, etc. At each step,
we lose information, yet it is important to make sure we still have enough
information for the task at hand. [footnote: For instance, for the RNA-Seq
differential experession analysis we saw in Chapter 8, we needed the actual
read counts, not ‘normalized’ versions; for some analyses, gene-level summaries
might suffice, for others, we’ll want to look at the exon or isoform level.]
The problem is particularly acute if we build our data pipeline with a series
of components from separate developers. Statisticians have a concept for
whether certain summaries enable the reconstruction of all the relevant
information in the data: sufficiency. … Iterative approaches akin to what
we saw when we used the EM algorithm can sometimes help to avoid information
loss. For instance, when analyzing mass spectroscopy data, a first run
guesses at peaks individually for each sample. After this preliminary
spectrum-spotting, another iteration allows us to borrow strength from the
other samples to spot spectra that may have been overlooked (or looked like
noise) before.” <span class="citation">(Holmes and Huber 2018)</span></p>
</blockquote>
<p>Try to avoid adding in supplementary / meta data “by hand”. For example, if you
need to add information about the ID of each sample, and this information is
included somewhere in the filename, it will be more robust to use regular
expressions to extract this information from the file names, rather than
entering it by hand. If you later add new files, the automated approach will
be robust to this update, while errors might be introduced for information
added by hand.</p>
<p>Example of quality control functionality in <code>xcms</code>:
“Below we create boxplots representing the distribution of total ion currents per file. Such plots can be very useful to spot problematic or failing MS runs.
… Also, we can cluster the samples based on similarity of their base peak chromatogram. This can also be helpful to spot potentially problematic samples in an experiment or generally get an initial overview of the sample grouping in the experiment. Since the retention times between samples are not exactly identical, we use the bin function to group intensities in fixed time ranges (bins) along the retention time axis. In the present example we use a bin size of 1 second, the default is 0.5 seconds. The clustering is performed using complete linkage hierarchical clustering on the pairwise correlations of the binned base peak chromatograms.”
<span class="citation">(Smith 2013)</span></p>
<p>After some quality checks on the data from LCMS, the next step is to
detect the chromatographic peaks in the samples. This requires some
specifications to the algorithm that will depend on the settings used
on the equipment when the samples were run.
“Next we perform the chromatographic peak detection using the centWave algorithm [2]. Before running the peak detection it is however strongly suggested to visually inspect e.g. the extracted ion chromatogram of internal standards or known compounds to evaluate and adapt the peak detection settings since the default settings will not be appropriate for most LCMS experiments. The two most critical parameters for centWave are the peakwidth (expected range of chromatographic peak widths) and ppm (maximum expected deviation of m/z values of centroids corresponding to one chromatographic peak; this is usually much larger than the ppm specified by the manufacturer) parameters. To evaluate the typical chromatographic peak width we plot the EIC for one peak.”
<span class="citation">(Smith 2013)</span></p>
<blockquote>
<p>“Peak detection will not always work perfectly leading to peak detection artifacts, such as overlapping peaks or artificially split peaks. The refineChromPeaks function allows to refine peak detection results by either removing identified peaks not passing a certain criteria or by merging artificially split chromatographic peaks.” <span class="citation">(Smith 2013)</span></p>
</blockquote>
<blockquote>
<p>“The time at which analytes elute in the chromatography can vary between samples (and even compounds). Such a difference was already observable in the extracted ion chromatogram plot shown as an example in the previous section. The alignment step, also referred to as retention time correction, aims at adjusting this by shifting signals along the retention time axis to align the signals between different samples within an experiment. A plethora of alignment algorithms exist (see [3]), with some of them being implemented also in xcms. The method to perform the alignment/retention time correction in xcms is adjustRtime which uses different alignment algorithms depending on the provided parameter class. … In some experiments it might be helpful to perform the alignment based on only a subset of the samples, e.g. if QC samples were injected at regular intervals or if the experiment contains blanks. Alignment method in xcms allow to estimate retention time drifts on a subset of samples (either all samples excluding blanks or QC samples injected at regular intervals during a measurement run) and use these to adjust the full data set.” <span class="citation">(Smith 2013)</span></p>
</blockquote>
<blockquote>
<p>“The final step in the metabolomics preprocessing is the correspondence that matches detected chromatographic peaks between samples (and depending on the settings, also within samples if they are adjacent). The method to perform the correspondence in xcms is groupChromPeaks. We will use the peak density method [5] to group chromatographic peaks. The algorithm combines chromatographic peaks depending on the density of peaks along the retention time axis within small slices along the mz dimension.” <span class="citation">(Smith 2013)</span></p>
</blockquote>
<blockquote>
<p>“The performance of peak detection, alignment and correspondence should always be evaluated by inspecting extracted ion chromatograms e.g. of known compounds, internal standards or identified features in general.” <span class="citation">(Smith 2013)</span></p>
</blockquote>
<p>Normalization can help adjust for technical bias across the samples:</p>
<blockquote>
<p>“At last we perform a principal component analysis to evaluate the grouping of the samples in this experiment. Note that we did not perform any data normalization hence the grouping might (and will) also be influenced by technical biases. … We can see the expected separation between the KO and WT samples on PC2. On PC1 samples separate based on their ID, samples with an ID &lt;= 18 from samples with an ID &gt; 18. This separation might be caused by a technical bias (e.g. measurements performed on different days/weeks) or due to biological properties of the mice analyzed (sex, age, litter mates etc).” <span class="citation">(Smith 2013)</span></p>
</blockquote>
<blockquote>
<p>“Normalizing features’ signal intensities is required, but at present not (yet) supported in xcms (some methods might be added in near future). It is advised to use the SummarizedExperiment returned by the quantify method for any further data processing, as this type of object stores feature definitions, sample annotations as well as feature abundances in the same object. For the identification of e.g. features with significant different intensities/abundances it is suggested to use functionality provided in other R packages, such as Bioconductor’s excellent limma package.” <span class="citation">(Smith 2013)</span></p>
</blockquote>

</div>
</div>
<p style="text-align: center;">
<a href="3.1-module12.html"><button class="btn btn-default">Previous</button></a>
<a href="3.3-module13.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
