<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Chapter 3 Benefits of a plain text format | Improving the Reproducibility of Experimental Data Recording and Pre-Processing" />
<meta property="og:type" content="book" />


<meta property="og:description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing." />
<meta name="github-repo" content="rstudio/bookdown-demo" />

<meta name="author" content="Brooke Anderson, Michael Lyons, Mercedes Gonzalez-Juarrero, Marcela Henao-Tamayo, and Gregory Robertson" />



<meta name="description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing.">

<title>Chapter 3 Benefits of a plain text format | Improving the Reproducibility of Experimental Data Recording and Pre-Processing</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />




<link rel="stylesheet" href="toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="has-sub"><a href="index.html#overview"><span class="toc-section-number">1</span> Overview</a><ul>
<li class="has-sub"><a href="1-1-license.html#license"><span class="toc-section-number">1.1</span> License</a><ul>
<li><a href="1-1-license.html#extra-quotes-for-future-revisions"><span class="toc-section-number">1.1.1</span> Extra quotes for future revisions</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="2-experimental-data-recording.html#experimental-data-recording"><span class="toc-section-number">2</span> Experimental Data Recording</a><ul>
<li class="has-sub"><a href="2-1-separating-data-recording-and-analysis.html#separating-data-recording-and-analysis"><span class="toc-section-number">2.1</span> Separating data recording and analysis</a><ul>
<li><a href="2-1-separating-data-recording-and-analysis.html#data-recording-versus-data-analysis"><span class="toc-section-number">2.1.1</span> Data recording versus data analysis</a></li>
<li><a href="2-1-separating-data-recording-and-analysis.html#hazards-of-combining-recording-and-analysis"><span class="toc-section-number">2.1.2</span> Hazards of combining recording and analysis</a></li>
<li><a href="2-1-separating-data-recording-and-analysis.html#approaches-to-separate-recording-and-analysis"><span class="toc-section-number">2.1.3</span> Approaches to separate recording and analysis</a></li>
<li><a href="2-1-separating-data-recording-and-analysis.html#discussion-questions"><span class="toc-section-number">2.1.4</span> Discussion questions</a></li>
<li><a href="2-1-separating-data-recording-and-analysis.html#additional-notes-quotes-for-revisions"><span class="toc-section-number">2.1.5</span> Additional notes / quotes for revisions</a></li>
</ul></li>
<li class="has-sub"><a href="2-2-principles-and-power-of-structured-data-formats.html#principles-and-power-of-structured-data-formats"><span class="toc-section-number">2.2</span> Principles and power of structured data formats</a><ul>
<li><a href="2-2-principles-and-power-of-structured-data-formats.html#focus-on-data-up-to-recording-less-on-for-post-recording"><span class="toc-section-number">2.2.1</span> Focus on data up to recording, less on for post-recording</a></li>
<li><a href="2-2-principles-and-power-of-structured-data-formats.html#characteristics-of-a-structured-data-format"><span class="toc-section-number">2.2.2</span> Characteristics of a structured data format</a></li>
<li><a href="2-2-principles-and-power-of-structured-data-formats.html#benefits-of-a-structured-data-format"><span class="toc-section-number">2.2.3</span> Benefits of a structured data format</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="3-benefits-of-a-plain-text-format.html#benefits-of-a-plain-text-format"><span class="toc-section-number">3</span> Benefits of a plain text format</a><ul>
<li><a href="3-benefits-of-a-plain-text-format.html#when-two-dimensional-structured-data-formats-might-not-work"><span class="toc-section-number">3.0.1</span> When two-dimensional structured data formats might not work</a></li>
<li><a href="3-benefits-of-a-plain-text-format.html#recording-expermental-data-in-a-structured-format"><span class="toc-section-number">3.0.2</span> Recording expermental data in a structured format</a></li>
<li><a href="3-benefits-of-a-plain-text-format.html#applied-exercise"><span class="toc-section-number">3.0.3</span> Applied exercise</a></li>
<li class="has-sub"><a href="3-1-the-tidy-data-format.html#the-tidy-data-format"><span class="toc-section-number">3.1</span> The ‘tidy’ data format</a><ul>
<li><a href="3-1-the-tidy-data-format.html#the-tidy-data-format-1"><span class="toc-section-number">3.1.1</span> The “tidy” data format</a></li>
<li><a href="3-1-the-tidy-data-format.html#the-tidy-data-format-as-a-structured-data-format"><span class="toc-section-number">3.1.2</span> The “tidy” data format as a structured data format</a></li>
<li><a href="3-1-the-tidy-data-format.html#practice-quiz"><span class="toc-section-number">3.1.3</span> Practice quiz</a></li>
</ul></li>
<li class="has-sub"><a href="3-2-designing-templates-for-tidy-data-collection.html#designing-templates-for-tidy-data-collection"><span class="toc-section-number">3.2</span> Designing templates for “tidy” data collection</a><ul>
<li><a href="3-2-designing-templates-for-tidy-data-collection.html#subsection-1"><span class="toc-section-number">3.2.1</span> Subsection 1</a></li>
<li><a href="3-2-designing-templates-for-tidy-data-collection.html#applied-exercise-1"><span class="toc-section-number">3.2.2</span> Applied exercise</a></li>
</ul></li>
<li class="has-sub"><a href="3-3-example-creating-a-template-for-tidy-data-collection.html#example-creating-a-template-for-tidy-data-collection"><span class="toc-section-number">3.3</span> Example: Creating a template for “tidy” data collection</a><ul>
<li><a href="3-3-example-creating-a-template-for-tidy-data-collection.html#subsection-1-1"><span class="toc-section-number">3.3.1</span> Subsection 1</a></li>
<li><a href="3-3-example-creating-a-template-for-tidy-data-collection.html#subsection-2"><span class="toc-section-number">3.3.2</span> Subsection 2</a></li>
<li><a href="3-3-example-creating-a-template-for-tidy-data-collection.html#discussion-questions-1"><span class="toc-section-number">3.3.3</span> Discussion questions</a></li>
</ul></li>
<li class="has-sub"><a href="3-4-power-of-using-a-single-structured-project-directory-for-storing-and-tracking-research-project-files.html#power-of-using-a-single-structured-project-directory-for-storing-and-tracking-research-project-files"><span class="toc-section-number">3.4</span> Power of using a single structured ‘Project’ directory for storing and tracking research project files</a><ul>
<li><a href="3-4-power-of-using-a-single-structured-project-directory-for-storing-and-tracking-research-project-files.html#subsection-1-2"><span class="toc-section-number">3.4.1</span> Subsection 1</a></li>
<li><a href="3-4-power-of-using-a-single-structured-project-directory-for-storing-and-tracking-research-project-files.html#subsection-2-1"><span class="toc-section-number">3.4.2</span> Subsection 2</a></li>
<li><a href="3-4-power-of-using-a-single-structured-project-directory-for-storing-and-tracking-research-project-files.html#practice-quiz-1"><span class="toc-section-number">3.4.3</span> Practice quiz</a></li>
</ul></li>
<li class="has-sub"><a href="3-5-creating-project-templates.html#creating-project-templates"><span class="toc-section-number">3.5</span> Creating ‘Project’ templates</a><ul>
<li><a href="3-5-creating-project-templates.html#subsection-1-3"><span class="toc-section-number">3.5.1</span> Subsection 1</a></li>
<li><a href="3-5-creating-project-templates.html#subsection-2-2"><span class="toc-section-number">3.5.2</span> Subsection 2</a></li>
<li><a href="3-5-creating-project-templates.html#discussion-questions-2"><span class="toc-section-number">3.5.3</span> Discussion questions</a></li>
</ul></li>
<li class="has-sub"><a href="3-6-example-creating-a-project-template.html#example-creating-a-project-template"><span class="toc-section-number">3.6</span> Example: Creating a ‘Project’ template</a><ul>
<li><a href="3-6-example-creating-a-project-template.html#subsection-1-4"><span class="toc-section-number">3.6.1</span> Subsection 1</a></li>
<li><a href="3-6-example-creating-a-project-template.html#subsection-2-3"><span class="toc-section-number">3.6.2</span> Subsection 2</a></li>
<li><a href="3-6-example-creating-a-project-template.html#applied-exercise-2"><span class="toc-section-number">3.6.3</span> Applied exercise</a></li>
</ul></li>
<li class="has-sub"><a href="3-7-harnessing-version-control-for-transparent-data-recording.html#harnessing-version-control-for-transparent-data-recording"><span class="toc-section-number">3.7</span> Harnessing version control for transparent data recording</a><ul>
<li><a href="3-7-harnessing-version-control-for-transparent-data-recording.html#subsection-1-5"><span class="toc-section-number">3.7.1</span> Subsection 1</a></li>
<li><a href="3-7-harnessing-version-control-for-transparent-data-recording.html#subsection-2-4"><span class="toc-section-number">3.7.2</span> Subsection 2</a></li>
<li><a href="3-7-harnessing-version-control-for-transparent-data-recording.html#discussion-questions-3"><span class="toc-section-number">3.7.3</span> Discussion questions</a></li>
</ul></li>
<li class="has-sub"><a href="3-8-enhance-the-reproducibility-of-collaborative-research-with-version-control-platforms.html#enhance-the-reproducibility-of-collaborative-research-with-version-control-platforms"><span class="toc-section-number">3.8</span> Enhance the reproducibility of collaborative research with version control platforms</a><ul>
<li><a href="3-8-enhance-the-reproducibility-of-collaborative-research-with-version-control-platforms.html#subsection-1-6"><span class="toc-section-number">3.8.1</span> Subsection 1</a></li>
<li><a href="3-8-enhance-the-reproducibility-of-collaborative-research-with-version-control-platforms.html#subsection-2-5"><span class="toc-section-number">3.8.2</span> Subsection 2</a></li>
<li><a href="3-8-enhance-the-reproducibility-of-collaborative-research-with-version-control-platforms.html#discussion-questions-4"><span class="toc-section-number">3.8.3</span> Discussion questions</a></li>
</ul></li>
<li class="has-sub"><a href="3-9-using-git-and-gitlab-to-implement-version-control.html#using-git-and-gitlab-to-implement-version-control"><span class="toc-section-number">3.9</span> Using git and GitLab to implement version control</a><ul>
<li><a href="3-9-using-git-and-gitlab-to-implement-version-control.html#subsection-1-7"><span class="toc-section-number">3.9.1</span> Subsection 1</a></li>
<li><a href="3-9-using-git-and-gitlab-to-implement-version-control.html#subsection-2-6"><span class="toc-section-number">3.9.2</span> Subsection 2</a></li>
<li><a href="3-9-using-git-and-gitlab-to-implement-version-control.html#applied-exercise-3"><span class="toc-section-number">3.9.3</span> Applied exercise</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="4-experimental-data-preprocessing.html#experimental-data-preprocessing"><span class="toc-section-number">4</span> Experimental Data Preprocessing</a><ul>
<li class="has-sub"><a href="4-1-principles-and-benefits-of-scripted-pre-processing-of-experimental-data.html#principles-and-benefits-of-scripted-pre-processing-of-experimental-data"><span class="toc-section-number">4.1</span> Principles and benefits of scripted pre-processing of experimental data</a><ul>
<li><a href="4-1-principles-and-benefits-of-scripted-pre-processing-of-experimental-data.html#subsection-1-8"><span class="toc-section-number">4.1.1</span> Subsection 1</a></li>
<li><a href="4-1-principles-and-benefits-of-scripted-pre-processing-of-experimental-data.html#subsection-2-7"><span class="toc-section-number">4.1.2</span> Subsection 2</a></li>
<li><a href="4-1-principles-and-benefits-of-scripted-pre-processing-of-experimental-data.html#discussion-questions-5"><span class="toc-section-number">4.1.3</span> Discussion questions</a></li>
</ul></li>
<li class="has-sub"><a href="4-2-introduction-to-scripted-data-pre-processing-in-r.html#introduction-to-scripted-data-pre-processing-in-r"><span class="toc-section-number">4.2</span> Introduction to scripted data pre-processing in R</a><ul>
<li><a href="4-2-introduction-to-scripted-data-pre-processing-in-r.html#subsection-1-9"><span class="toc-section-number">4.2.1</span> Subsection 1</a></li>
<li><a href="4-2-introduction-to-scripted-data-pre-processing-in-r.html#subsection-2-8"><span class="toc-section-number">4.2.2</span> Subsection 2</a></li>
<li><a href="4-2-introduction-to-scripted-data-pre-processing-in-r.html#applied-exercise-4"><span class="toc-section-number">4.2.3</span> Applied exercise</a></li>
</ul></li>
<li class="has-sub"><a href="4-3-simplify-scripted-pre-processing-through-rs-tidyverse-tools.html#simplify-scripted-pre-processing-through-rs-tidyverse-tools"><span class="toc-section-number">4.3</span> Simplify scripted pre-processing through R’s ‘tidyverse’ tools</a><ul>
<li><a href="4-3-simplify-scripted-pre-processing-through-rs-tidyverse-tools.html#subsection-1-10"><span class="toc-section-number">4.3.1</span> Subsection 1</a></li>
<li><a href="4-3-simplify-scripted-pre-processing-through-rs-tidyverse-tools.html#subsection-2-9"><span class="toc-section-number">4.3.2</span> Subsection 2</a></li>
<li><a href="4-3-simplify-scripted-pre-processing-through-rs-tidyverse-tools.html#practice-quiz-2"><span class="toc-section-number">4.3.3</span> Practice quiz</a></li>
</ul></li>
<li class="has-sub"><a href="4-4-complex-data-types-in-experimental-data-pre-processing.html#complex-data-types-in-experimental-data-pre-processing"><span class="toc-section-number">4.4</span> Complex data types in experimental data pre-processing</a><ul>
<li><a href="4-4-complex-data-types-in-experimental-data-pre-processing.html#subsection-1-11"><span class="toc-section-number">4.4.1</span> Subsection 1</a></li>
<li><a href="4-4-complex-data-types-in-experimental-data-pre-processing.html#subsection-2-10"><span class="toc-section-number">4.4.2</span> Subsection 2</a></li>
<li><a href="4-4-complex-data-types-in-experimental-data-pre-processing.html#practice-quiz-3"><span class="toc-section-number">4.4.3</span> Practice quiz</a></li>
</ul></li>
<li class="has-sub"><a href="4-5-complex-data-types-in-r-and-bioconductor.html#complex-data-types-in-r-and-bioconductor"><span class="toc-section-number">4.5</span> Complex data types in R and Bioconductor</a><ul>
<li><a href="4-5-complex-data-types-in-r-and-bioconductor.html#subsection-1-12"><span class="toc-section-number">4.5.1</span> Subsection 1</a></li>
<li><a href="4-5-complex-data-types-in-r-and-bioconductor.html#subsection-2-11"><span class="toc-section-number">4.5.2</span> Subsection 2</a></li>
<li><a href="4-5-complex-data-types-in-r-and-bioconductor.html#applied-exercise-5"><span class="toc-section-number">4.5.3</span> Applied exercise</a></li>
</ul></li>
<li class="has-sub"><a href="4-6-example-converting-from-complex-to-tidy-data-formats.html#example-converting-from-complex-to-tidy-data-formats"><span class="toc-section-number">4.6</span> Example: Converting from complex to ‘tidy’ data formats</a><ul>
<li><a href="4-6-example-converting-from-complex-to-tidy-data-formats.html#subsection-1-13"><span class="toc-section-number">4.6.1</span> Subsection 1</a></li>
<li><a href="4-6-example-converting-from-complex-to-tidy-data-formats.html#subsection-2-12"><span class="toc-section-number">4.6.2</span> Subsection 2</a></li>
<li><a href="4-6-example-converting-from-complex-to-tidy-data-formats.html#applied-exercise-6"><span class="toc-section-number">4.6.3</span> Applied exercise</a></li>
</ul></li>
<li class="has-sub"><a href="4-7-introduction-to-reproducible-data-pre-processing-protocols.html#introduction-to-reproducible-data-pre-processing-protocols"><span class="toc-section-number">4.7</span> Introduction to reproducible data pre-processing protocols</a><ul>
<li><a href="4-7-introduction-to-reproducible-data-pre-processing-protocols.html#subsection-1-14"><span class="toc-section-number">4.7.1</span> Subsection 1</a></li>
<li><a href="4-7-introduction-to-reproducible-data-pre-processing-protocols.html#subsection-2-13"><span class="toc-section-number">4.7.2</span> Subsection 2</a></li>
<li><a href="4-7-introduction-to-reproducible-data-pre-processing-protocols.html#discussion-questions-6"><span class="toc-section-number">4.7.3</span> Discussion questions</a></li>
</ul></li>
<li class="has-sub"><a href="4-8-rmarkdown-for-creating-reproducible-data-pre-processing-protocols.html#rmarkdown-for-creating-reproducible-data-pre-processing-protocols"><span class="toc-section-number">4.8</span> RMarkdown for creating reproducible data pre-processing protocols</a><ul>
<li><a href="4-8-rmarkdown-for-creating-reproducible-data-pre-processing-protocols.html#subsection-1-15"><span class="toc-section-number">4.8.1</span> Subsection 1</a></li>
<li><a href="4-8-rmarkdown-for-creating-reproducible-data-pre-processing-protocols.html#subsection-2-14"><span class="toc-section-number">4.8.2</span> Subsection 2</a></li>
<li><a href="4-8-rmarkdown-for-creating-reproducible-data-pre-processing-protocols.html#applied-exercise-7"><span class="toc-section-number">4.8.3</span> Applied exercise</a></li>
</ul></li>
<li class="has-sub"><a href="4-9-example-creating-a-reproducible-data-pre-processing-protocol.html#example-creating-a-reproducible-data-pre-processing-protocol"><span class="toc-section-number">4.9</span> Example: Creating a reproducible data pre-processing protocol</a><ul>
<li><a href="4-9-example-creating-a-reproducible-data-pre-processing-protocol.html#subsection-1-16"><span class="toc-section-number">4.9.1</span> Subsection 1</a></li>
<li><a href="4-9-example-creating-a-reproducible-data-pre-processing-protocol.html#subsection-2-15"><span class="toc-section-number">4.9.2</span> Subsection 2</a></li>
<li><a href="4-9-example-creating-a-reproducible-data-pre-processing-protocol.html#practice-quiz-4"><span class="toc-section-number">4.9.3</span> Practice quiz</a></li>
</ul></li>
</ul></li>
<li><a href="5-references.html#references"><span class="toc-section-number">5</span> References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="benefits-of-a-plain-text-format" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Benefits of a plain text format</h1>
<p>Structuring data in a gridded, two-dimensional format will be helpful even if it is
in a file format that is binary, like Excel. However, there are added benefits to
saving the structured data in a plain text format. A plain text format may take
more space (in terms of computer memory) and take longer to process within
other programs; however, its benefits typically outweigh these limitations
<span class="citation">(Hunt, Thomas, and Cunningham 2000)</span>. Advantages include: (1) humans can read the file, and
should always be able to, regardless of changes in and future obsolescence of
computer programs; (2) almost all software programs for analyzing and
processing files can input plain-text files; (3) the Unix system,
which has influenced many existing software programs, especially open-source
programs for data analysis, are based on inputting and outputtin line-based
plain-text files; and (4) plain-text files can be easily tracked with version
control <span class="citation">(Hunt, Thomas, and Cunningham 2000)</span>.
These advantages might become particularly important in cases where researchers
need to combine and integrate heterogeneous data, for example data coming from
different instruments.</p>
<p>Another advantage of storing data in a plain text format is that it makes
version control a much more powerful tool. With plain text files, you can
use version control to see the specific changes to a file. With binary files,
you can typically see if a file was changed, but it’s much harder to see exactly
what within the file was changed.</p>
<blockquote>
<p>“We believe that the best format for storing knowledge persistently is <em>plain text</em>.
With plain text, we give ourselves the ability to manipulate knowledge, both manually
and programmatically, using virtually every tool at our disposal.” <span class="citation">(Hunt, Thomas, and Cunningham 2000)</span></p>
</blockquote>
<blockquote>
<p>“<em>Plain text</em> is made up of printable characters in a form that can be read and
understood directly by people. … Plain text doesn’t mean that the text is
unstructured; XML, XGML, and HTML are great examples of plain text that has
a well-defined structure. You can do everything with plain text that you could
do with some binary format, including versioning.” <span class="citation">(Hunt, Thomas, and Cunningham 2000)</span></p>
</blockquote>
<blockquote>
<p>“The problem with most binary formats is that the context necessary to understand
the data is separate from the data itself. You are artifically divorcing the data
from its meaning. The data may as well be encrypted; it is absolutely meaningless
without the application logic to parse it. With plain text, however, you can
achieve a self-describing data stream that is independent of the application
that created it.” <span class="citation">(Hunt, Thomas, and Cunningham 2000)</span></p>
</blockquote>
<blockquote>
<p>“There are two major drawbacks to using plain text: (1) It may take
more space to store than a compressed binary format, and (2) it may be
computationally more expensive to interpret and process a plain text
file. Depending on your application, either or both of these situations
may be unacceptable—for example, when storing satellite telemetry data,
or as the internal format of a relational database. But even in these
situations, it may be acceptable to store <em>metadata</em> about the raw
data in plain text.” <span class="citation">(Hunt, Thomas, and Cunningham 2000)</span></p>
</blockquote>
<p>There are cases where it may not be best to store laboratory-generated data
in a plain text format. For example, the output from a flow cytometer is large
and would take up a lot (more) computer memory if stored in a plain text format,
and it would take much longer to read and work with the data in analysis software
if it were in that format. However, it will be easier to combine other data from the
experiment (e.g., CFUs counted by hand [?]) with the flow cytometry data if the
other data is in a plain text format.</p>
<blockquote>
<p>“Human-readable forms of data, and self-describing data, will outlive all
other forms of data and the applications that created them. Period. As long as
the data survives, you will have a chance to be able to use it—potentially
long after the original application that wrote it is defunct. You can parse
such a file with only partial knowledge of its format; with most binary files,
you must know all the details of the entire format in order to parse it
successfully.” <span class="citation">(Hunt, Thomas, and Cunningham 2000)</span></p>
</blockquote>
<blockquote>
<p>“Virtually every tool in the computing universe, from source code management
systems to compiler environments to editors and stand-alone filters, can operate on
plain text.” <span class="citation">(Hunt, Thomas, and Cunningham 2000)</span></p>
</blockquote>
<blockquote>
<p>“Unix is famous for being designed around the philosophy of small, sharp tools, each
intended to do one thing well. This philosophy is enabled by using a common underlying
format—the line-oriented, plain text file. Databases used for system administration
(users and passwords, network configuration, and so on) are all kept as plain
text files. … When a system crashes, you may be faced with only a minimal
environment to restore it (you may not be able to access graphics drivers,
for instance). Situations such as this can really make you appreciate the simplicity of
plain text.” <span class="citation">(Hunt, Thomas, and Cunningham 2000)</span></p>
</blockquote>
<blockquote>
<p>“Even in the future of XML-based intelligent agents that travel the wild and dangerous
Internet autonomously, negotiating data interchange among themselves, the ubiquitous
text file will still be there. In fact, in heterogeneous environments the advantages of
plain text can outweight all of the drawbacks. You need to ensure that all parties
can communicate using a common standard. Plain text is that standard.” <span class="citation">(Hunt, Thomas, and Cunningham 2000)</span></p>
</blockquote>
<blockquote>
<p>“Data should be formatted in a way that facilitates computer readability. All too often,
we as humans record data in a way that maximizes its readability to us, but takes a
considerable amount of cleaning and tidying before it can be processed by a
computer. The more data (and metadata) that is computer readable, the more we can
leverage our computers to work with this data.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Unix is the foundational computing environment in bioinformatics because its
design is the antithesis of [a] inflexible and fragile approach. The Unix shell
was designed to allow users to easily build complex programs by interfacing
smaller modular programs together. This approach is the Unix philosophy:
‘This is the Unix philosophy: Write programs that do one thing and do it well.
Write programs to work together. Write programs to handle text streams, because
that is a universal interface.’–Doug McIlory”. <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“While checksums are a great method to check if files are different, they don’t tell
us <em>how</em> the files differ. One approach to this is to compute the <em>diff</em> between
two files using the Unix tool <em>diff</em>. Unix’s <em>diff</em> works line by line, and outputs
blocks (called <em>hunks</em>) that differ between files (resembling Git’s <em>git diff</em> command).”
<span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Many formats in bioinformatics are simple tabular plain-text files delimited by a
character. The most common tabular plain-text file format used in bioinformatics is
tab-delimited. This is not an accident: most Unix tools such as <em>cut</em> and <em>awk</em> treat
tabs as delimiters by default. Bioinformatics evolved to favor tab-delimited formats
because of the convenience of woorking with these files using Unix tools. Tab-delimited
file formats are also simple to parse with scripting languages like Python and Perl,
and easy to load into R.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Tabular plain-text data formats are used extensively in computing. The basic format
is incrediably simple: each row (also known as a record) is kept on its own line, and
each column (also known as a field) is separate by some delimiter. There are three
flavors you will encounter: tab-delimited, comma-separated, and variable space-delimited.
Of these three formats, tab-delimited is the most commonly used in bioinformatics. File
formats such as BED, GTF/GFF, SAM, tabular BLAST output, and VCF are all examples of
tab-delimited files. Columns of a tab-delimited file are separated by a single tab
character (which has the escape code . A common convention (but not a standard)
is to include metadata on the first few lines of a tab-delimited file. These metadata
lines begin with # to differentiate them from the tabular dataa records. Because
tab-delimated files use a tab to delimit columns, tabs in data are not allowed.
Comma-separated values (CSV) is another common format. CSV is similar to tab-delimited,
except the delimiter is a comma character. While not a common in bioinformatics, it
is possible that the data stored in CSV format contain commas (which would interfere with
the ability to parse it). Some variants just don’t allow this, while others use quotes
around entries that could contain commas. Unfortunately, there’s no standard CSV format
that defines how to handle this and many other issues with CSV—though some guidelines
are given in RFC 4180. Lastly, there are space-delimited formats. A few stubborn
bioinformatics programs use a variable number of spaces to separate columns. In general,
tab-delimited formats and CSV are better choices than space-delimited formats because
it’s quite common to encounter data containing spaces.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“In bioinformatics, the plain-text data we work with is often encoded in <em>ASCII</em>. ASCII
is a character encoding scheme that uses 7 bits to represent 128 different values,
including letters (upper- and lowercase), numbers, and special nonvisible characters.
While ASCII only uses 7 bits, nowadays computers use an 8-bit <em>byte</em> (a unit representing
8 bits) to store ASCII characters. … Because plain-text data uses characters to
encode information, our encoding scheme matters. When working with a plain-text file,
98% of the time you won’t have to worry about the details of ASCII and how your file is
encoded. However, the 2% of the time when encoding data does matter—usually when an
invisible non-ASCII character has entered the data—it can lead to major headaches.”
<span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“‘One of the core issues of Bioinformatics is dealing with a profusion of (often poorly
defined or ambiguous) file formats. Some <em>ad hoc</em> simple human readable formats have
over time attained the status of de facto standards.’– Peter Cock et al. (2010)”</p>
</blockquote>
<blockquote>
<p>“Nucelotide (and protein) sequences are stored in two plain-text formats widespread
in bioinformatics: FASTA and FASTQ.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Beware of common pitfalls when working with <em>ad hoc</em> bioinformatics formats. Simple
mistakes over minor details like file formats can consume a disproportionate amount of
time and energy to discover and fix, so mind these details early on.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“It would be unwise to bet that these formats [SAM/BAM files] won’t change (or even be replaced
at some point)—the field of bioinformatics is notorious for inventing new data formats (the same goes with
computing in general) … So learning how to work with specific bioinformatics formats may
seem like a lost cause, skills such as following a format specification, manipulating binary
files, extracting information from bitflags, and working with application programming
interfaces (API) are essential skills when working with any format.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“One of the best features of the SAM/BAM format is that is supports including an extensive
amount of metadata about the samples, the alignment reference, processing steps, etc. to be
included with the file. (Note that in contrast, the FASTQ format doesn’t provide a standard
way to include this metadata; in practice, we use filenames to connect metadata kept in
a separate spreadsheet or tab-delimited file.) Many downstream applications make use of the
metadata contained in the SAM header (and many programs require it).” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<div id="when-two-dimensional-structured-data-formats-might-not-work" class="section level3">
<h3><span class="header-section-number">3.0.1</span> When two-dimensional structured data formats might not work</h3>
<blockquote>
<p>“There are several good reasons why researchers need to know about data storage
options. One is that we may not have control over the format in which the
data is given to us. For example, data from NASA’s Live Access Server is in a
format decided by NASA and we are unlikely to be able to convince NASA to provide it
in a different format. This says that we must know about different formats in order
to gain access to data. Another common situation is that we may have to transfer data
between different applications or between different operating systems. This
effectively involves temporary data storage, so it is useful to understand how
to select an appropriate storage format. It is also possible to be involved in
deciding the format for archiving aa data set. There is no overall best storage
format; the correct choice will depend on the size aand complexity of the data
set aand what the data set will be used for. It is necessary to gain an overview of
the relative merits of all of the data storage formats in order to be able to make an
informed decision for any particular situation.” <span class="citation">(Murrell 2009)</span></p>
</blockquote>
<blockquote>
<p>“Samtools now supports (after version 1) a new, highly compressed file format known as <em>CRAM</em>.
Compressing alignments with CRAM can lead to a 10%–30% filesize reduction compared to BAM (and
quite remarkably, with no significant increase in compression or decompression time compared to
BAM). CRAM is a <em>reference-based</em> compression scheme, meaning only the aligned sequence that’s
different from the reference sequence is recorded. This greatly reduces file size, as many
sequence may align with minimal difference from the reference. As a consequence of this
reference-based approach, it is imperative that the reference is available and does not
change, as this would lead to a loss of data kept in the CRAM format. Because the reference
is so important, CRAM files contain an MD5 checksum of the reference file to ensure it has not changed.
CRAM also has support for multiple different <em>lossy compression</em> methods. Lossy compression
entails some information about an alignment and the original read is lost. For example, it’s possible
to bin base quality scores using a lower resolution binning scheme to reduce the filesize.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Very often we need efficient random access to subsequences of a FASTA file (given regions).
At first glance, writing a script to do this doesn’t seem difficult. We could, for example,
write a script that iterates through FASTA entries, extracting sequences that
overlaps the range specified. However, this is not an efficient method when extracting a
few <em>random</em> subsequences. To see why, consider accessing the sequence from position
chromosome 8 (123,407,082 to 123,419,742) from the mouse genome. This approach would
needlessly parse and load chromosomes 1 through 7 into memory, even though we don’t need to
extract subsequences from these chromosomes. Reading entire chromosomes from disk and copying them
into memory can be quite inefficient—we would have to load all 125 megabytes of chromosome 8
to extract 3.6kb! Extracting numerous random subsequences from a FASTA file can be quite
computationaally costly. A common computational strategy that allows for easy and fast random
access is <em>indexing</em> the file. Indexed files are ubiquitous in bioinformatics.”
<span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“We can avoid needlessly reading the entire file off of the disk by using an index that
points to where certian blocks are in the file. In the case of our FASTA file, the index
essentially stores the location of where each sequence begins in the file (as well as
other necessary information). When we look up a range like chromosome 8 (123,407,082–123,410,744),
<em>samtools faidx</em> uses the information in the index to quickly calculate exactly where
in the file those bases are. Then, using an operation called a file <em>seek</em>, the program
jumps to this exact position (called the <em>offset</em>) in the file and starts reading the
sequence. Having precomputed file offsets combined with the ability to jump to those
exact positions is what makes accessing sections of an indexed file fast.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Many standard bioinformatics data formats (GTF/GFF, BED, VCF/BCF, SAM/BAM) … store tabular data in
single <em>flat file</em> formats. Flat file formats don’t have an internal hierarchy or structured
relationship with other tables. While we’re able to join tables using Unix tools like <em>join</em>,
and R’s <em>match</em> and <em>merge</em> functions, the files themselves do not encode any relationships
between tables. Flat file formats are widely used in bioinformatics because they’re simple,
flexible, and portable, but occasionally we do need to store and manipulate data thaat is
best represented in many related tables—this is where <em>relational databases</em> are useful.
Unlike flat files, relational databases can contain multiple tables. Relational databases
also support methods that use relationships between tables to join and extract specific
records using specialized queries.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>SQLite “doesn’t require any setup—you can create a database and start making queries with
minimal time spent on configuring and administrating your database. In contrast, other
database systems like MySQL and PostgreSQL require extensive configuration just to get
them up and running. While SQLite is not as powerful as these larger database systems,
it works surprisingly well for databases in the gigabytes range.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“A lane of sequencing data is too big to fit in the memory of most standard
desktops. If I needed to search for the exact string ‘GTGATTAACTGCGAA’ in this
data, I couldn’t open up a lane of data in Notepad and use the Find feature
to pinpoint where it occurs—there simply isn’t enough memory to hold all
those nucleotides in memory. Instead, most tools rely on streams of data,
being read from a source and actively processed. Both general Unix tools and
many bioinformatics programs are designed to take input through a stream and
pass output through a different stream. It’s these text streams that allow us
to both couple programs together into workflows and process data without
storing huge amounts of data in our computers’ memory.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Passing the output of one program directly into the input of another
program with pipes is a computationally efficient and simple way to interface
Unix programs. This is another reason why bioinformaticians (and software engineers
in general) like Unix. Pipes allow us to build larger, more complex tools from
modular parts. It doesn’t matter what language a program is written in, either; pipes
will work between anything as long as both programs understand the data passed
between them. As the lowest common denominator between most programs, plain-text
streams are often used—a point that McIlroy makes in his quote about the
Unix philosophy.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Data compression, the process of condensing data so that it takes up less space (on
disk drives, in memory, or across network transfers), is an indespensible technology
in modern bioinformatics. For example, sequences from a recent Illumina HiSeq run
when compressed with Gzip take up 21,408,674,240 bytes, which is a bit under 20
gigabytes. Uncompressed, this file is a whopping 63,203,414,514 bytes (around 58
gigabytes). This FASTQ file has 150 million 200bp reads, which is 10x coverage of
the hexaploid wheat genome. The compression ratio (uncompressed size/ compressed size)
of this data is approximately 2.95, which translates to a significant space saving of
about 66%. Your own bioinformatics projects will likely contain much more data, especially
as sequencing costs continue to drop and it’s possible to sequence genomes to higher depth,
include more biological replicates or time points in expression studies, or sequence
more individuals in genotyping studies. For the most part, data can remain compressed on
the disk throughout processing and analysis. Most well-writted bioinformatics tools can
work natively with compressed data as input, without requiring us to decompress it to disk
first. Using pipes and redirection, we can stream compressed data and write compressed files
directly to the disk. Additionally, common Unix tools like <em>cat</em>, <em>grep</em>, and <em>less</em> all
have variants that work with compressed data, and Python’s <em>gzip</em> module allows us to
read and write compressed data from within Python. So while working with large datasets
in bioinformatics can be challenging, using the compression tools in Unix and software
libraries make our lives much easier.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“The <em>pileup format</em> [is] a plain-text format that summarizes reads’ bases at each chromosome
position by stacking or ‘piling up’ aligned reads.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“<em>Out-of-memory approaches</em> [are] computational strategies built arouond storing and working
with data kept out of memory on the disk. Reading data from a disk is much, much slower than working
with data in memory… but in many cases this is the approach we have to take when in-memory
(e.g., loading the entire dataset into R) or streaming approaches (e.g., using Unix pipes …)
aren’t appropriate.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“We often need fast read-only access to data linked to a genomic location or range. For the scale
of data we encounter in genomics, retrieving this type of data is not trivial for a few reasons.
First the data might not fit entirely in memory, requiring an approach where data is kept out of
memory (in other words, on a slow disk). Second, even powerful relational database systems can
be sluggish when querying out millions of entries that overlap a specific region—an incrediably
common operation in genomics. [BGZF and Tabix] are specifically designed to get around these
limitations, allowing fast random-access of tab-delimited genome position data.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“While <em>gzip</em> compresses the <em>entire</em> file, BGZF files are compressed in <em>blocks</em>. These blocks
provide BGZF files with a useful feature that <em>gzip</em>-compressed files don’t have: we can jump to
and decompress a specific block without having to decompress the entire file. Block compression
combined with file indexing is what enables fast random access of alignments from large BAM
files with <em>samtools view</em>.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“The data revolution within the biological and physical science world is generating massive amounts
of data from … a wide range of … projects, such as those undertaken at the Large Hadron Collider
and genomics-proteomics-metabolomics research.” <span class="citation">(Keller et al. 2017)</span></p>
</blockquote>
<blockquote>
<p>“Information in a Unix system is stored in <em>files</em>, which are much like ordinary office files.
Each file has a name, contents, a place to keep it, and some administrative information, such
as who owns it and how bit it is. A file might contain a letter, or a list of names and addresses,
or the source statements of a program, or data to be used by a program, or even programs in their
executable form and other non-textual material.” <span class="citation">(Kernighan and Pike 1984)</span></p>
</blockquote>
<blockquote>
<p>“The Unix file system is organized so you can maintain your own personal files without interfering
with files belonging to other people, and keep people from interfering with you too.” <span class="citation">(Kernighan and Pike 1984)</span></p>
</blockquote>
<blockquote>
<p>“Everything in the Unix system is a file. That is less of an oversimplification than you
might think. When the first version of the system was designed, before it even had
a name, the discussions focused on the structure of a file system that would be clean
and easy to use. The file system is central to the success and convenience of the Unix
system. It is one of the best examples of the ‘keep it simple’ philosophy, showing
the power achieved by careful implementation of a few well-chosen ideas.” <span class="citation">(Kernighan and Pike 1984)</span></p>
</blockquote>
<blockquote>
<p>“A file is a sequence of bytes. (A byte is aa small chunk of information, typically 8 bits
long. For our purposes, a byte is equivalent to a character.) No structure is imposed on
a file by the system, and no meaning is attaached to its contents—the meaning of the bytes
depends solely on the programs that interpret the file. Furthermore, … this is true not
just of disc files but of peripheral devices as well. Magnetic tapes, mail messages, characters
typed on the keyboard, line printer output, data flowing in pipes—each of these is just
a sequence of bytes as far as the systems and the programs in it are concerned.” <span class="citation">(Kernighan and Pike 1984)</span></p>
</blockquote>
<blockquote>
<p>“Programs [in Unix] retreive the data in a file by a system call (a subroutine in the kernel) called
<code>read</code>. Each time <code>read</code> is called, it returns the next part of a file—the next line of text typed
on the terminal, for example. <code>read</code> also says how many bytes of the file were returned, so end of file
is assumed when a <code>read</code> says ‘zero bytes are being returned’. If there were any bytes left, <code>read</code>
would have returned some of them.” <span class="citation">(Kernighan and Pike 1984)</span></p>
</blockquote>
<blockquote>
<p>“The format of a file is determined by the programs that use it; there is a wide variety of file
types, perhaps because there is a wide variety of programs. But since file types are not determined
by the file system, the kernel can’t tell you the type of a file: it doesn’t know it. The <code>file</code>
command makes an educated guess … To determine the types, <code>file</code> doesn’t pay attention to the
names (although it could have), because naming conventions are just conventions, and thus not
perfectly reliable. For example, files suffixed ‘.c’ are almost always C source, but there is
nothing to prevent you from creating a ‘.c’ file with arbitrary contents. Instead, <code>file</code> reads the
first hundred bytes of a file and looks for clues to that file type. … In Unix systems there is
just one kind of file, and all that is required to access a file is its name. The lack of file formats
is an advantage overall—programmers don’t need to worry about file types, and all
the standard programs will work on any file.” <span class="citation">(Kernighan and Pike 1984)</span></p>
</blockquote>
<blockquote>
<p>“Non-text files definitely have their place. For example, very laarge databases usually need extra
address information for rapid access; this has to be binary for efficiency. But every file format
that is not text must have its own family of support programs to do things that the standard tools
could perform if the format were text. Text files may be a little less efficient in maachine cycles,
but this must be balanced against the cost of extra software to maintain more specialized formats.
If you design a file format, you should think carefully before choosing a non-textual representation.”
<span class="citation">(Kernighan and Pike 1984)</span></p>
</blockquote>
</div>
<div id="recording-expermental-data-in-a-structured-format" class="section level3">
<h3><span class="header-section-number">3.0.2</span> Recording expermental data in a structured format</h3>
<blockquote>
<p>“It is important that the goal of standardization is well defined and limited.
Instead of trying to standardize everything, it is better to rely on other
standards and be consistent with them… In fact it is detrimental to the
community not to use relevant standards developed by others.” <span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span></p>
</blockquote>
<blockquote>
<p>“Simplicity, but not oversimplification, is the key to success [in developing
standards].” <span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span></p>
</blockquote>
<p>First, you can still use spreadsheets, but reduce their use to recording data,
leaving all data cleaning and analysis to be handled with other software. To make
it easier to collaborate with statisticians and to interface with a program like
R for data cleaning and analysis, it will be easiest if you set up your data
recording to include with other statistical programs like R or Python. These
steps are described in a later section, “…”.</p>
<ul>
<li>Each sheet of the spreadsheet should contain data from a single
experiment.</li>
<li>Never use whitespace to represent a meaningful separation in data within
a spreadsheet. Never include multiple tables of data in the same sheet.</li>
<li>The first row of the spreadsheet should include a short column name for
each column with data. All column name information should be within a single row
(i.e., avoid subheadings). Avoid any special characters (e.g., “%”) in column
names. Instead, use only letters, numbers, and underscores ("_"), and start with a letter.
It is especially helpful if you can avoid spaces in column names.</li>
<li>Missing data should be represented consistently in cells. “NA” is one choice. If
you want to clarify why data is missing, it’s much better to add a column (e.g., “why_missing”)
where you can provide those details in text, rather than combining within a single column
numerical observation data with textual reasons for missingness in cells with missing values.</li>
</ul>
<p>Next, you could record data using a statistical language like R. There is an
excellent Integrated Development Environment for R called RStudio, and it creates a
much clearer interface with R compared to running R from a commond line, particularly for new users. RStudio allows you to open delimited plain text files, like csvs, using
a grid-style interface. This grid-style interface looks very similar to a
spreadsheet, but lacks the ability to include formulas or macros. Therefore, this
format enforces a separation of the recording of raw data from the cleaning and
analysis of the data.</p>
<p>[R Project templates]</p>
<p>Data cleaning and analysis can then be shifted away from the files used to
record the data and into reproducible scripts. These scripts can be clearly
documented, either through comments in the code or through open source
documentation tools like RMarkdown than interweave code and text in a way that
allows the creation of documents that are easier to read than commented code.</p>
<p>This documentation should explain why each step is being done. In cases where
it is not immediately evident from the code <em>how</em> the step is being done, this
should be documented as well. Any assumptions being used should be clarified in
the documentation.</p>
</div>
<div id="applied-exercise" class="section level3">
<h3><span class="header-section-number">3.0.3</span> Applied exercise</h3>

</div> 
</div>
<p style="text-align: center;">
<a href="2-2-principles-and-power-of-structured-data-formats.html"><button class="btn btn-default">Previous</button></a>
<a href="3-1-the-tidy-data-format.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
