<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="3.5 Complex data types in R and Bioconductor | Improving the Reproducibility of Experimental Data Recording and Pre-Processing" />
<meta property="og:type" content="book" />


<meta property="og:description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing." />
<meta name="github-repo" content="rstudio/bookdown-demo" />

<meta name="author" content="Brooke Anderson, Michael Lyons, Mercedes Gonzalez-Juarrero, Marcela Henao-Tamayo, and Gregory Robertson" />



<meta name="description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing.">

<title>3.5 Complex data types in R and Bioconductor | Improving the Reproducibility of Experimental Data Recording and Pre-Processing</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>




<link rel="stylesheet" href="toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="has-sub"><a href="index.html#overview"><span class="toc-section-number">1</span> Overview</a><ul>
<li><a href="1-1-license.html#license"><span class="toc-section-number">1.1</span> License</a></li>
</ul></li>
<li class="has-sub"><a href="2-experimental-data-recording.html#experimental-data-recording"><span class="toc-section-number">2</span> Experimental Data Recording</a><ul>
<li class="has-sub"><a href="2-1-separating-data-recording-and-analysis.html#separating-data-recording-and-analysis"><span class="toc-section-number">2.1</span> Separating data recording and analysis</a><ul>
<li><a href="2-1-separating-data-recording-and-analysis.html#data-recording-versus-data-analysis"><span class="toc-section-number">2.1.1</span> Data recording versus data analysis</a></li>
<li><a href="2-1-separating-data-recording-and-analysis.html#hazards-of-combining-recording-and-analysis"><span class="toc-section-number">2.1.2</span> Hazards of combining recording and analysis</a></li>
<li><a href="2-1-separating-data-recording-and-analysis.html#approaches-to-separate-recording-and-analysis"><span class="toc-section-number">2.1.3</span> Approaches to separate recording and analysis</a></li>
<li><a href="2-1-separating-data-recording-and-analysis.html#discussion-questions"><span class="toc-section-number">2.1.4</span> Discussion questions</a></li>
</ul></li>
<li class="has-sub"><a href="2-2-principles-and-power-of-structured-data-formats.html#principles-and-power-of-structured-data-formats"><span class="toc-section-number">2.2</span> Principles and power of structured data formats</a><ul>
<li><a href="2-2-principles-and-power-of-structured-data-formats.html#data-recording-standards"><span class="toc-section-number">2.2.1</span> Data recording standards</a></li>
<li><a href="2-2-principles-and-power-of-structured-data-formats.html#defining-data-standards-for-a-research-group"><span class="toc-section-number">2.2.2</span> Defining data standards for a research group</a></li>
<li><a href="2-2-principles-and-power-of-structured-data-formats.html#two-dimensional-structured-data-format"><span class="toc-section-number">2.2.3</span> Two-dimensional structured data format</a></li>
<li><a href="2-2-principles-and-power-of-structured-data-formats.html#saving-two-dimensional-structured-data-in-plain-text-file-formats"><span class="toc-section-number">2.2.4</span> Saving two-dimensional structured data in plain text file formats</a></li>
<li><a href="2-2-principles-and-power-of-structured-data-formats.html#occassions-for-more-complex-data-structures-and-file-formats"><span class="toc-section-number">2.2.5</span> Occassions for more complex data structures and file formats</a></li>
<li><a href="2-2-principles-and-power-of-structured-data-formats.html#levels-of-standardizationresearch-group-to-research-community"><span class="toc-section-number">2.2.6</span> Levels of standardization—research group to research community</a></li>
<li><a href="2-2-principles-and-power-of-structured-data-formats.html#applied-exercise"><span class="toc-section-number">2.2.7</span> Applied exercise</a></li>
</ul></li>
<li class="has-sub"><a href="2-3-the-tidy-data-format.html#the-tidy-data-format"><span class="toc-section-number">2.3</span> The ‘tidy’ data format</a><ul>
<li><a href="2-3-the-tidy-data-format.html#what-makes-data-tidy"><span class="toc-section-number">2.3.1</span> What makes data “tidy”?</a></li>
<li><a href="2-3-the-tidy-data-format.html#why-make-your-data-tidy"><span class="toc-section-number">2.3.2</span> Why make your data “tidy”?</a></li>
<li><a href="2-3-the-tidy-data-format.html#using-tidyverse-tools-with-data-in-the-tidy-data-format"><span class="toc-section-number">2.3.3</span> Using tidyverse tools with data in the “tidy data” format</a></li>
<li><a href="2-3-the-tidy-data-format.html#practice-quiz"><span class="toc-section-number">2.3.4</span> Practice quiz</a></li>
</ul></li>
<li class="has-sub"><a href="2-4-designing-templates-for-tidy-data-collection.html#designing-templates-for-tidy-data-collection"><span class="toc-section-number">2.4</span> Designing templates for “tidy” data collection</a><ul>
<li><a href="2-4-designing-templates-for-tidy-data-collection.html#creating-the-rules-for-collecting-data-in-the-same-time-each-time"><span class="toc-section-number">2.4.1</span> Creating the rules for collecting data in the same time each time</a></li>
<li><a href="2-4-designing-templates-for-tidy-data-collection.html#subsection-1"><span class="toc-section-number">2.4.2</span> Subsection 1</a></li>
<li><a href="2-4-designing-templates-for-tidy-data-collection.html#dont-repeat-yourself"><span class="toc-section-number">2.4.3</span> Don’t Repeat Yourself!</a></li>
<li><a href="2-4-designing-templates-for-tidy-data-collection.html#dont-repeat-your-report-writing"><span class="toc-section-number">2.4.4</span> Don’t repeat your report-writing!</a></li>
<li><a href="2-4-designing-templates-for-tidy-data-collection.html#automating-reports"><span class="toc-section-number">2.4.5</span> Automating reports</a></li>
<li><a href="2-4-designing-templates-for-tidy-data-collection.html#scripts-and-automated-reports-as-simple-pipelines"><span class="toc-section-number">2.4.6</span> Scripts and automated reports as simple pipelines</a></li>
<li><a href="2-2-principles-and-power-of-structured-data-formats.html#applied-exercise"><span class="toc-section-number">2.4.7</span> Applied exercise</a></li>
</ul></li>
<li class="has-sub"><a href="2-5-example-creating-a-template-for-tidy-data-collection.html#example-creating-a-template-for-tidy-data-collection"><span class="toc-section-number">2.5</span> Example: Creating a template for “tidy” data collection</a><ul>
<li><a href="2-4-designing-templates-for-tidy-data-collection.html#subsection-1"><span class="toc-section-number">2.5.1</span> Subsection 1</a></li>
<li><a href="2-5-example-creating-a-template-for-tidy-data-collection.html#subsection-2"><span class="toc-section-number">2.5.2</span> Subsection 2</a></li>
<li><a href="2-5-example-creating-a-template-for-tidy-data-collection.html#example-datasets"><span class="toc-section-number">2.5.3</span> Example datasets</a></li>
<li><a href="2-5-example-creating-a-template-for-tidy-data-collection.html#issues-with-these-data-sets"><span class="toc-section-number">2.5.4</span> Issues with these data sets</a></li>
<li><a href="2-5-example-creating-a-template-for-tidy-data-collection.html#final-tidy-examples"><span class="toc-section-number">2.5.5</span> Final “tidy” examples</a></li>
<li><a href="2-5-example-creating-a-template-for-tidy-data-collection.html#options-for-recording-tidy-data"><span class="toc-section-number">2.5.6</span> Options for recording tidy data</a></li>
<li><a href="2-5-example-creating-a-template-for-tidy-data-collection.html#examples-of-how-tidy-data-can-be-easily-analyzed-visualized"><span class="toc-section-number">2.5.7</span> Examples of how “tidy” data can be easily analyzed / visualized</a></li>
<li><a href="2-1-separating-data-recording-and-analysis.html#discussion-questions"><span class="toc-section-number">2.5.8</span> Discussion questions</a></li>
</ul></li>
<li class="has-sub"><a href="2-6-power-of-using-a-single-structured-project-directory-for-storing-and-tracking-research-project-files.html#power-of-using-a-single-structured-project-directory-for-storing-and-tracking-research-project-files"><span class="toc-section-number">2.6</span> Power of using a single structured ‘Project’ directory for storing and tracking research project files</a><ul>
<li><a href="2-6-power-of-using-a-single-structured-project-directory-for-storing-and-tracking-research-project-files.html#organizing-project-files-through-the-file-system"><span class="toc-section-number">2.6.1</span> Organizing project files through the file system</a></li>
<li><a href="2-6-power-of-using-a-single-structured-project-directory-for-storing-and-tracking-research-project-files.html#organizing-files-within-a-project-directory"><span class="toc-section-number">2.6.2</span> Organizing files within a project directory</a></li>
<li><a href="2-6-power-of-using-a-single-structured-project-directory-for-storing-and-tracking-research-project-files.html#using-rstudio-projects-with-project-file-directories"><span class="toc-section-number">2.6.3</span> Using RStudio Projects with project file directories</a></li>
<li><a href="2-4-designing-templates-for-tidy-data-collection.html#subsection-1"><span class="toc-section-number">2.6.4</span> Subsection 1</a></li>
<li><a href="2-5-example-creating-a-template-for-tidy-data-collection.html#subsection-2"><span class="toc-section-number">2.6.5</span> Subsection 2</a></li>
<li><a href="2-3-the-tidy-data-format.html#practice-quiz"><span class="toc-section-number">2.6.6</span> Practice quiz</a></li>
</ul></li>
<li class="has-sub"><a href="2-7-creating-project-templates.html#creating-project-templates"><span class="toc-section-number">2.7</span> Creating ‘Project’ templates</a><ul>
<li><a href="2-7-creating-project-templates.html#making-an-existing-file-directory-an-rstudio-project"><span class="toc-section-number">2.7.1</span> Making an existing file directory an RStudio Project</a></li>
<li><a href="2-7-creating-project-templates.html#making-an-rstudio-project-template"><span class="toc-section-number">2.7.2</span> Making an RStudio Project Template</a></li>
<li><a href="2-1-separating-data-recording-and-analysis.html#discussion-questions"><span class="toc-section-number">2.7.3</span> Discussion questions</a></li>
</ul></li>
<li class="has-sub"><a href="2-8-example-creating-a-project-template.html#example-creating-a-project-template"><span class="toc-section-number">2.8</span> Example: Creating a ‘Project’ template</a><ul>
<li><a href="2-4-designing-templates-for-tidy-data-collection.html#subsection-1"><span class="toc-section-number">2.8.1</span> Subsection 1</a></li>
<li><a href="2-5-example-creating-a-template-for-tidy-data-collection.html#subsection-2"><span class="toc-section-number">2.8.2</span> Subsection 2</a></li>
<li><a href="2-2-principles-and-power-of-structured-data-formats.html#applied-exercise"><span class="toc-section-number">2.8.3</span> Applied exercise</a></li>
</ul></li>
<li class="has-sub"><a href="2-9-harnessing-version-control-for-transparent-data-recording.html#harnessing-version-control-for-transparent-data-recording"><span class="toc-section-number">2.9</span> Harnessing version control for transparent data recording</a><ul>
<li><a href="2-9-harnessing-version-control-for-transparent-data-recording.html#what-is-version-control"><span class="toc-section-number">2.9.1</span> What is version control?</a></li>
<li><a href="2-9-harnessing-version-control-for-transparent-data-recording.html#recording-data-in-the-laboratoryfrom-paper-to-computers"><span class="toc-section-number">2.9.2</span> Recording data in the laboratory—from paper to computers</a></li>
<li><a href="2-1-separating-data-recording-and-analysis.html#discussion-questions"><span class="toc-section-number">2.9.3</span> Discussion questions</a></li>
</ul></li>
<li class="has-sub"><a href="2-10-enhance-the-reproducibility-of-collaborative-research-with-version-control-platforms.html#enhance-the-reproducibility-of-collaborative-research-with-version-control-platforms"><span class="toc-section-number">2.10</span> Enhance the reproducibility of collaborative research with version control platforms</a><ul>
<li><a href="2-10-enhance-the-reproducibility-of-collaborative-research-with-version-control-platforms.html#git-and-github-features"><span class="toc-section-number">2.10.1</span> git and GitHub features</a></li>
<li><a href="2-4-designing-templates-for-tidy-data-collection.html#subsection-1"><span class="toc-section-number">2.10.2</span> Subsection 1</a></li>
<li><a href="2-5-example-creating-a-template-for-tidy-data-collection.html#subsection-2"><span class="toc-section-number">2.10.3</span> Subsection 2</a></li>
<li><a href="2-1-separating-data-recording-and-analysis.html#discussion-questions"><span class="toc-section-number">2.10.4</span> Discussion questions</a></li>
</ul></li>
<li class="has-sub"><a href="2-11-using-git-and-gitlab-to-implement-version-control.html#using-git-and-gitlab-to-implement-version-control"><span class="toc-section-number">2.11</span> Using git and GitLab to implement version control</a><ul>
<li><a href="2-11-using-git-and-gitlab-to-implement-version-control.html#how-to-use-version-control"><span class="toc-section-number">2.11.1</span> How to use version control</a></li>
<li><a href="2-11-using-git-and-gitlab-to-implement-version-control.html#leveraging-git-and-github-as-a-project-director"><span class="toc-section-number">2.11.2</span> Leveraging git and GitHub as a project director</a></li>
<li><a href="2-11-using-git-and-gitlab-to-implement-version-control.html#leveraging-git-and-github-as-a-scientist-who-programs"><span class="toc-section-number">2.11.3</span> Leveraging git and GitHub as a scientist who programs</a></li>
<li><a href="2-11-using-git-and-gitlab-to-implement-version-control.html#notes"><span class="toc-section-number">2.11.4</span> Notes</a></li>
<li><a href="2-2-principles-and-power-of-structured-data-formats.html#applied-exercise"><span class="toc-section-number">2.11.5</span> Applied exercise</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="3-experimental-data-preprocessing.html#experimental-data-preprocessing"><span class="toc-section-number">3</span> Experimental Data Preprocessing</a><ul>
<li class="has-sub"><a href="3-1-principles-and-benefits-of-scripted-pre-processing-of-experimental-data.html#principles-and-benefits-of-scripted-pre-processing-of-experimental-data"><span class="toc-section-number">3.1</span> Principles and benefits of scripted pre-processing of experimental data</a><ul>
<li><a href="3-1-principles-and-benefits-of-scripted-pre-processing-of-experimental-data.html#what-is-pre-processing"><span class="toc-section-number">3.1.1</span> What is pre-processing?</a></li>
<li><a href="3-1-principles-and-benefits-of-scripted-pre-processing-of-experimental-data.html#approaches-to-simple-preprocessing-tasks"><span class="toc-section-number">3.1.2</span> Approaches to simple preprocessing tasks</a></li>
<li><a href="3-1-principles-and-benefits-of-scripted-pre-processing-of-experimental-data.html#approaches-to-more-complex-preprocessing-tasks"><span class="toc-section-number">3.1.3</span> Approaches to more complex preprocessing tasks</a></li>
<li><a href="3-1-principles-and-benefits-of-scripted-pre-processing-of-experimental-data.html#scripting-preprocessing-tasks"><span class="toc-section-number">3.1.4</span> Scripting preprocessing tasks</a></li>
<li><a href="3-1-principles-and-benefits-of-scripted-pre-processing-of-experimental-data.html#potential-quotes"><span class="toc-section-number">3.1.5</span> Potential quotes</a></li>
<li><a href="2-1-separating-data-recording-and-analysis.html#discussion-questions"><span class="toc-section-number">3.1.6</span> Discussion questions</a></li>
</ul></li>
<li class="has-sub"><a href="3-2-introduction-to-scripted-data-pre-processing-in-r.html#introduction-to-scripted-data-pre-processing-in-r"><span class="toc-section-number">3.2</span> Introduction to scripted data pre-processing in R</a><ul>
<li><a href="2-4-designing-templates-for-tidy-data-collection.html#subsection-1"><span class="toc-section-number">3.2.1</span> Subsection 1</a></li>
<li><a href="2-5-example-creating-a-template-for-tidy-data-collection.html#subsection-2"><span class="toc-section-number">3.2.2</span> Subsection 2</a></li>
<li><a href="2-2-principles-and-power-of-structured-data-formats.html#applied-exercise"><span class="toc-section-number">3.2.3</span> Applied exercise</a></li>
</ul></li>
<li class="has-sub"><a href="3-3-simplify-scripted-pre-processing-through-rs-tidyverse-tools.html#simplify-scripted-pre-processing-through-rs-tidyverse-tools"><span class="toc-section-number">3.3</span> Simplify scripted pre-processing through R’s ‘tidyverse’ tools</a><ul>
<li><a href="3-3-simplify-scripted-pre-processing-through-rs-tidyverse-tools.html#limitations-of-object-oriented-programming"><span class="toc-section-number">3.3.1</span> Limitations of object-oriented programming</a></li>
<li><a href="3-3-simplify-scripted-pre-processing-through-rs-tidyverse-tools.html#the-tidyverse-approach"><span class="toc-section-number">3.3.2</span> The “tidyverse” approach</a></li>
<li><a href="3-3-simplify-scripted-pre-processing-through-rs-tidyverse-tools.html#how-to-tidyverse"><span class="toc-section-number">3.3.3</span> How to “tidyverse”</a></li>
<li><a href="2-4-designing-templates-for-tidy-data-collection.html#subsection-1"><span class="toc-section-number">3.3.4</span> Subsection 1</a></li>
<li><a href="2-5-example-creating-a-template-for-tidy-data-collection.html#subsection-2"><span class="toc-section-number">3.3.5</span> Subsection 2</a></li>
<li><a href="2-3-the-tidy-data-format.html#practice-quiz"><span class="toc-section-number">3.3.6</span> Practice quiz</a></li>
</ul></li>
<li class="has-sub"><a href="3-4-complex-data-types-in-experimental-data-pre-processing.html#complex-data-types-in-experimental-data-pre-processing"><span class="toc-section-number">3.4</span> Complex data types in experimental data pre-processing</a><ul>
<li><a href="2-4-designing-templates-for-tidy-data-collection.html#subsection-1"><span class="toc-section-number">3.4.1</span> Subsection 1</a></li>
<li><a href="2-5-example-creating-a-template-for-tidy-data-collection.html#subsection-2"><span class="toc-section-number">3.4.2</span> Subsection 2</a></li>
<li><a href="3-4-complex-data-types-in-experimental-data-pre-processing.html#subsection-3"><span class="toc-section-number">3.4.3</span> Subsection 3</a></li>
<li><a href="2-3-the-tidy-data-format.html#practice-quiz"><span class="toc-section-number">3.4.4</span> Practice quiz</a></li>
</ul></li>
<li class="has-sub"><a href="3-5-complex-data-types-in-r-and-bioconductor.html#complex-data-types-in-r-and-bioconductor"><span class="toc-section-number">3.5</span> Complex data types in R and Bioconductor</a><ul>
<li><a href="2-4-designing-templates-for-tidy-data-collection.html#subsection-1"><span class="toc-section-number">3.5.1</span> Subsection 1</a></li>
<li><a href="3-5-complex-data-types-in-r-and-bioconductor.html#exploring-and-extracting-data-from-r-list-data-structures"><span class="toc-section-number">3.5.2</span> Exploring and extracting data from R list data structures</a></li>
<li><a href="3-5-complex-data-types-in-r-and-bioconductor.html#interfacing-between-object-based-and-tidyverse-workflows"><span class="toc-section-number">3.5.3</span> Interfacing between object-based and tidyverse workflows</a></li>
<li><a href="3-5-complex-data-types-in-r-and-bioconductor.html#extras"><span class="toc-section-number">3.5.4</span> Extras</a></li>
<li><a href="2-5-example-creating-a-template-for-tidy-data-collection.html#subsection-2"><span class="toc-section-number">3.5.5</span> Subsection 2</a></li>
<li><a href="2-2-principles-and-power-of-structured-data-formats.html#applied-exercise"><span class="toc-section-number">3.5.6</span> Applied exercise</a></li>
</ul></li>
<li class="has-sub"><a href="3-6-example-converting-from-complex-to-tidy-data-formats.html#example-converting-from-complex-to-tidy-data-formats"><span class="toc-section-number">3.6</span> Example: Converting from complex to ‘tidy’ data formats</a><ul>
<li><a href="2-4-designing-templates-for-tidy-data-collection.html#subsection-1"><span class="toc-section-number">3.6.1</span> Subsection 1</a></li>
<li><a href="2-5-example-creating-a-template-for-tidy-data-collection.html#subsection-2"><span class="toc-section-number">3.6.2</span> Subsection 2</a></li>
<li><a href="2-2-principles-and-power-of-structured-data-formats.html#applied-exercise"><span class="toc-section-number">3.6.3</span> Applied exercise</a></li>
</ul></li>
<li class="has-sub"><a href="3-7-introduction-to-reproducible-data-pre-processing-protocols.html#introduction-to-reproducible-data-pre-processing-protocols"><span class="toc-section-number">3.7</span> Introduction to reproducible data pre-processing protocols</a><ul>
<li><a href="2-4-designing-templates-for-tidy-data-collection.html#subsection-1"><span class="toc-section-number">3.7.1</span> Subsection 1</a></li>
<li><a href="2-5-example-creating-a-template-for-tidy-data-collection.html#subsection-2"><span class="toc-section-number">3.7.2</span> Subsection 2</a></li>
<li><a href="2-1-separating-data-recording-and-analysis.html#discussion-questions"><span class="toc-section-number">3.7.3</span> Discussion questions</a></li>
</ul></li>
<li class="has-sub"><a href="3-8-rmarkdown-for-creating-reproducible-data-pre-processing-protocols.html#rmarkdown-for-creating-reproducible-data-pre-processing-protocols"><span class="toc-section-number">3.8</span> RMarkdown for creating reproducible data pre-processing protocols</a><ul>
<li><a href="2-4-designing-templates-for-tidy-data-collection.html#subsection-1"><span class="toc-section-number">3.8.1</span> Subsection 1</a></li>
<li><a href="2-5-example-creating-a-template-for-tidy-data-collection.html#subsection-2"><span class="toc-section-number">3.8.2</span> Subsection 2</a></li>
<li><a href="2-2-principles-and-power-of-structured-data-formats.html#applied-exercise"><span class="toc-section-number">3.8.3</span> Applied exercise</a></li>
</ul></li>
<li class="has-sub"><a href="3-9-example-creating-a-reproducible-data-pre-processing-protocol.html#example-creating-a-reproducible-data-pre-processing-protocol"><span class="toc-section-number">3.9</span> Example: Creating a reproducible data pre-processing protocol</a><ul>
<li><a href="2-4-designing-templates-for-tidy-data-collection.html#subsection-1"><span class="toc-section-number">3.9.1</span> Subsection 1</a></li>
<li><a href="2-5-example-creating-a-template-for-tidy-data-collection.html#subsection-2"><span class="toc-section-number">3.9.2</span> Subsection 2</a></li>
<li><a href="2-3-the-tidy-data-format.html#practice-quiz"><span class="toc-section-number">3.9.3</span> Practice quiz</a></li>
</ul></li>
</ul></li>
<li><a href="4-references.html#references"><span class="toc-section-number">4</span> References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="complex-data-types-in-r-and-bioconductor" class="section level2">
<h2><span class="header-section-number">3.5</span> Complex data types in R and Bioconductor</h2>
<p>Many R extension packages for pre-processing experimental data use complex
(rather than ‘tidy’) data formats within their code, and many output data in
complex formats. Very recently, the <em>broom</em> and <em>biobroom</em> R
packages have been developed to extract a ‘tidy’ dataset from a complex data
format. These tools create a clean, simple connection between the complex data
formats often used in pre-processing experimental data and the ‘tidy’ format
required to use the ‘tidyverse’ tools now taught in many introductory R courses.
In this module, we will describe the ‘list’ data structure, the common backbone
for complex data structures in R and provide tips on how to explore and extract
data stored in R in this format, including through the <em>broom</em> and
<em>biobroom</em> packages.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Describe the structure of R’s ‘list’ data format</li>
<li>Take basic steps to explore and extract data stored in R’s complex, list-based
structures</li>
<li>Describe what the <em>broom</em> and <em>biobroom</em> R packages can do</li>
<li>Explain how converting data to a ‘tidy’ format can improve reproducibility</li>
</ul>
<div id="subsection-1" class="section level3">
<h3><span class="header-section-number">3.5.1</span> Subsection 1</h3>
<p>When you are writing scripts in R to work with your code, if you are at a
point in your pipeline when you can use a “tidyverse” approach, then you will
“keep” your data in a dataframe, as your data structure, throughout your
work. However, at earlier stages in your preprocessing, you may need to use
tools that use other data structures. It’s helpful to understand the
basic building blocks of R data structures, so you can find elements of your
data in these other, more customized data structures.</p>
<p>Many R data structures are built on a general structure called a “list”. This
data structure is a useful basic general data structure, because it is
extraordinarily flexible. The list data structure is flexible in two
important ways: it allows you to include data of different <em>types</em> in the
same data structure, and it allows you to include data with different
dimensions—and data stored hierarchically, including various other data
structures—within the list structure. We’ll cover each of these points a bit
more below and describe why they’re helpful in making the list a very good
general purpose data structure.</p>
<p>In R, your data can be stored as different <em>types</em> of data: whole numbers can be
stored as an <em>integer</em> data type, continuous [?] numbers through a few types of
<em>floating</em> data types, character strings as a <em>character</em> data type, and logical
data (which can only take the two values of “TRUE” and “FALSE”) as a <em>logical</em>
data type. More complex data types can be built using these—for example,
there’s a special data type for storing dates that’s based on a combination of
an [integer?] data type, with added information counting the number of days [?]
from a set starting date (called the [Unix epoch?]), January 1, 1970. (This
set-up for storing dates allows them to be printed to look like dates, rather
than numbers, but at the same time allows them to be manipulated through
operations like finding out which date comes earliest in a set, determining the
number of days between two dates, and so on.) R uses these different data types
for several reasons. First, by using different data types, R can improve its
efficiency [?] in storing data. Each piece of data must—as you go deep in the
heart of how the computer works—as a series of binary digits (0s and 1s).
Some types of data can be stored using fewer of these <em>bits</em> (<em>bi</em>nary dig<em>its</em>).
Each measurement of logical data, for example, can be stored in a single bit,
since it only can take one of two values (0 or 1, for FALSE and TRUE, respectively).
For character strings, these can be divided into each character in the string
for storage (for example, “cat” can be stored as “c”, “a”, “t”). There is a set
of characters called the ASCII character set that includes the lowercase and
uppercase of the letters and punctuation sets that you see on a standard
US keyboard [?], and if the character strings only use these characters, they
can be stored in [x] bits per character. For numeric data types, integers can
typically be stores in [x] bits per number, while continuous [?] numbers,
stored in single or double floating point notation [?], are stored in [x]
and [x] bits respectively. When R stores data in specific types, it can be
more memory efficient by packing the types of data that can be stored in less
space (like logical data) into very compact structures.</p>
<p>The second advantage of the list structure in R is that it has enormous
flexibility in terms of storing lots of data in lots of possible places. This
data can have different types and even different substructures. Some data
structures in R are very constrained in what type of data they can store and
what structure they use to store it. For example, one of the “building block”
data structures in R is the vector. This data structure is one dimensional and
can only contain data that have the same data type—you can think of this as a
bead string of values, each of the same type. For example, you could have a
vector that gives a series of names of study sites (each a character string), or
a vector that gives the dates of time points in a study (each a date data type),
or a vector that gives the weights of mice in a study (each a numeric data
type). You cannot, however, have a vector that includes some study site names
and then some dates and then some weights, since these should be in different
data types. Further, you can’t arrange the data in any structure except a
straight, one-dimensional series if you are using a vector. The dataframe
structure provides a bit more flexibility—you can expand into two dimensions,
rather than one, and you can have different data types in different columns of
the dataframe (although each column must itself have a single data type).</p>
<p>The list data structure is much more flexible. It essentially allows you to
create different “slots”, and you can store any type of data in each of these
slots. In each slot you can store any of the other types of data structures in
R—for example, vectors, dataframes, or other lists. You can even store unusual
things like R <em>environments</em> [?] or <em>pointers</em> that give the directions to where
data is stored on the computer without reading the data into R (and so saving
room in the RAM memory, which is used when data is “ready to go” in R, but which
has much more limited space than the mass [?] storage on your computer).</p>
<p>Since you can put a list into the slot of a list, it allows you to create deep,
layered structures of data. For example, you could have one slot in a list where
you store the metadata for your experiment, and this slot might itself be a list
where you store one dataframe with some information about the settings of the
laboratory equipement you used to collect the data, and another dataframe that
provides information about the experimental design variables (e.g., which animal
received which treatment). Another slot in the larger list then might have
experimental measurements, and these might either be in a dataframe or, if the
data are very large, might be represented through pointers to where the data is
stored in memory, rather than having the data included directly in the data
structure.</p>
<p>Given all these advantages of the list data structure, then, why not use it all
the time? While it is a very helpful building block, it turns out that its flexibility
can have some disadvantages in some cases. This flexibility means that you can’s
always assume that certain bits of data are in a certain spot in each instance
of a list in R. Conversely, if you have data stored in a less flexible structure,
you can often rely on certain parts of the data always being in a certain part of
the data structure. In a “tidy” dataframe, for example, you can always assume
that each row represents the measurements for one observation at the unit of
observation for that dataframe, and that each column gives values across all
observations for one particular value that was measured for all the observations.
For example, if you are conducting an experiment with mice, where a certain number
of mice were sacrificed at certain time points, with their weight and the bacteria
load in their lungs measured when the mouse was sacrificed, then you could store
the data in a dataframe, with a row for each mouse, and columns giving the
experimental characteristics for each mouse (e.g., treatment status, time point
when the mouse was sacrificed), the mouse’s weight, and the mouse’s bacteria
load when sacrificed. You could store all of this information in a list, as
well, but the defined, two-dimensional structure of the dataframe makes it much
more clearly defined where all the data goes in the dataframe structure, while
you could order the data in many ways within a list.</p>
<p>There is a big advantage to having stricter standards for what parts of data go
where when it comes to writing functions that can be used across a lot of data.
You can think of this in terms of how cars are set up versus how kitchens are
set up. Cars are very standardized in the “interface” that you get when you sit
down to drive them. The gas and brakes are typically floor pedals, with the gas
to the right of the brake. The steering is almost always provided through a wheel
centered in front of the driver’s torso. The mechanism for shifting gears (e.g.,
forward, reverse) is typically to the right of the steering wheel, while
mecahnisms for features like lights and windshield wipers, are typically to the
left of the steering wheel. Because this interface is so standardized, you can
get into a car you’ve never driven before and typically figure out how to
drive it very quickly. You don’t need a lot of time exploring where everything
is or a lot of directions from someone familiar with the car to figure out where
things are. Think of the last time that you drove a rental car—within five minutes,
at most, you were probably able to orient yourself to figure out where everything
you needed was. This is like a dataframe in R—you can pretty quickly figure out
where everything you might need is stored in the data structure, and people can
write functions to use with these dataframes that work well generally across lots
of people’s data because they can assume that certain pieces of data are in
certain places.</p>
<p>By contrast, think about walking into someone else’s kitchen and orienting yourself
to use that. Kitchen designs do tend to have some general features—most will have
a few common large elements, like a stove somewhere, a refrigerator somewhere,
a pantry somewhere, and storage for pots, pans, and utensils somewhere. However,
there is a lot of flexibility in where each of these are in the kitchen design,
and further flexibility in how things are organized within each of these structures.
If you cook in someone else’s kitchen, it is easy to find yourself disoriented in the
middle of cooking a recipe, where a utensil that you can grab almost without
thinking in your own kitchen requires you to stop and search many places in
someone else’s kitchen. This is like a list in R—there are so many places that
you can store data in a list, and so much flexibility, that you often find yourself
having to dig around to find a certain element in a list data structure that someone
else has created, and you often can’t assume that certain pieces are in certain
places if you are writing your own functions, so it becomes hard to write
functions that are “general purpose” for generic list structures in R.</p>
<p>There is a way that list structures can be used in R in a way that retains some
of their flexibility while also leveraging some of the benefits of
standardization. This is R’s system for creating <em>objects</em>. These object
structures are built on the list data structure, but each object is constrained
to have certain elements of data in certain structures of the data. These
structures cannot be used as easily as dataframes in a “tidyverse” approach,
since the tidyverse tools are built based on the assumption that data is stored
in a tidy dataframe. However, they are used in many of the Bioconductor
approaches that allow powerful tools for the earlier stages in preprocessing
biological data. The types of standards that are imposed in the more specialized
objects include which slots the list can have, the names they have, what order
they’re in (e.g., in a certain object, the metadata about the experiment might
always be stored in the first slot of the list), and what structures and/or data
types the data in each slot should have.</p>
<p>R programmers get a lot of advantages from using these classes because they can
write functions under the assumption that certain pieces of the data will always
be in the same spot for that type of object. There is still flexibility in the
object, in that it can store lots of different types of data, in a variety of
different structures. While this “object oriented” approach in R data structures
does provide great advantages for programmers, and allow them to create powerful
tools for you to use in R, it does make it a little trickier in some cases for
you to explore your data by hand as you work through preprocessing. This is
because there typically are a variety of these object classes that your data
will pass through as you go through different stages of preprocessing, because
different structures are suited to different stages of analysis. Functions often
can only be used for a single class of objects, and so you have to keep track of
which functions pair up with which classes of data. Further, it can be a bit
tricky—at least in comparison to when you have data in a dataframe—to
explore your data by hand, because you have to navigate through different slots
in the object. By contrast, a dataframe always has the same two-dimensional,
rectangular structure, and so it’s very easy to navigate and explore data in
this structure, and there are a large number of functions that are built to be
used with dataframes, providing enormous flexibility in what you can do with
data stored in this structure.</p>
<p>While it is a bit trickier to explore your data when it is stored in a
list—either a general list you created, or one that forms the base for a
specialized class structure through functions from a Bioconductor package—you
can certainly learn how to do this navigation. This is a powerful and critical
tool for you to learn as you learn to preprocess your data in R, as you should
<em>never</em> feel like you data is stored in a “black box” structure, where you can’t
peek in and explore it. You should <em>always</em> feel like you can take a look at any
part of your data at any step in the process of preprocessing, analyzing, and
visualizing it.</p>
</div>
<div id="exploring-and-extracting-data-from-r-list-data-structures" class="section level3">
<h3><span class="header-section-number">3.5.2</span> Exploring and extracting data from R list data structures</h3>
<p>To feel comfortable exploring your data at any stage during the preprocessing
steps, you should learn how to investigate and explore data that’s stored
in a list structure in R. Because the list structure is the building block
for complex data structures, including Bioconductor class structures, this will
serve you well throughout your work. You should get in the habit of checking
the structure and navigating where each piece of data is stored in the data
structure at each step in preprocessing your data. Also, by checking your data
throughout preprocessing, you might find that there are bits of information
tucked in your data at early stages that you aren’t yet using. For example,
many file formats for laboratory equipment include slots for information about
the equipment and its settings during when running the sample. This information
might be read in from the file into R, but you might not know it’s there for you
to use if you’d like, to help you in creating reproducible reports that include
this metadata about the experimental equipment and settings.</p>
<p>First, you will want to figure out whether your data is stored in a generic
list, or if it’s stored in a specific class-based data structure, which means it
will have a bit more of a standardized structure. To do this, you can run the
<code>class</code> function on your data object. The output of this might be a single value
(e.g., “list” [?]) or a short list. If it’s a short list, it will include both
the specific class of the object and, as you go down the list, the more
general data structure types that this class is built on. For example, if the
<code>class</code> function returns this list:</p>
<pre><code>[Example list of data types---maybe some specific class, then &quot;list&quot;?]</code></pre>
<p>it means that the data’s in a class-based structure called … which is built on
the more general structure of a list. You can apply to this data any of the functions
that are specifically built for … data structures, but you can also apply
functions built for the more general list data structure.</p>
<p>There are several tools you can use to explore data structured as lists in R.
R lists can sometimes be very large—in terms of the amount of data stored in
them—particularly for some types of biomedical data. With some of the tools
covered in this subsection, that will mean that your first look might seem
overwhelming. We’ll also cover some tools, therefore, that will let you peel
away levels of the data in a bit more manageable way, which you can use when
you encounter list-structured data that at first feels overwhelming.</p>
<p>First, if your data is stored in a specific class-based data structure, there
likely will also be help files specifically for the class structure that can
help you navigate it and figure out where things are. [Example]</p>
<p>[More about exploring data in list structures.]</p>
</div>
<div id="interfacing-between-object-based-and-tidyverse-workflows" class="section level3">
<h3><span class="header-section-number">3.5.3</span> Interfacing between object-based and tidyverse workflows</h3>
<p>The tidyverse approach in R is based on keeping data in a dataframe structure.
By keeping this common structure, the tidyverse allows for straightforward but
powerful work with your data by chaining together simple, single-purpose
functions. This approach is widely covered in introductory R programming courses
and books. A great starting point is the book <em>R Programming for Data Science</em>,
which is available both in print and freely online at [site]. Many excellent
resources exist for learning this approach, and so we won’t recover that
information here. Instead, we will focus on how to interface between this
approach and the object-based approach that’s more common with Bioconductor
packages. Bioconductor packages often take an object-based approach, and with
good reason because of the complexity and size of many early versions of
biomedical data in the preprocessing process. There are also resources for
learning to use specific Bioconductor packages, as well as some general
resources on Bioconductor, like <em>R Programming for Bioinformatics</em> [ref].
However, there are fewer resources available online that teach how to coordinate
between these two approaches in a pipeline of code, so that you can leverage the
needed power of Bioconductor approaches early in your pipeline, as you
preprocess large and complex data, and then shift to use a tidyverse approach
once your data is amenible to this more straightforward approach to analysis and
visualization.</p>
<p>The heart of making this shift is learning how to convert data, when possible,
from a more complex, class-type data structure (built on the flexible list
data structure) to the simpler, more standardized two-dimensional dataframe
structure that is required for the tidyverse approach. In this subsection, we’ll
cover approaches for converting your data from Bioconductor data structures to
dataframes.</p>
<p>If you are lucky, this might be very straightforward. A pair of packages called
<code>broom</code> and <code>biobroom</code> have been created specifically to facilitate the conversion
of data from more complex structures to dataframes. The <code>broom</code> package was
created first, by David Robinson, to convert the data stored in the objects that
are created by fitting statistical models into tidy dataframes. Many of the functions
in R that run statistical tests or fit statistical models output results in a
more complex, list-based data structure. These structures have nice “print” methods,
so if fitting the model or running the test is the very last step of your pipeline,
you can just read the printed output from R. However, often you want to include
these results in further code—for example, creating plots or tables that show
results from several statistical tests or models. The <code>broom</code> package includes
several functions for pulling out different bits of data that are stored in the
complex data structure created by fitting the model or running the test and convert
those pieces of data into a tidy dataframe. This tidy dataframe can then be
easily used in further code using a tidyverse approach.</p>
<p>The <code>biobroom</code> package was created to meet a similar need with data stored in some
of the complex structures commonly used in Bioconductor packages. [More about
<code>biobroom</code>.]</p>
<p>[How to convert data if there isn’t a <code>biobroom</code> method.] If you are unlucky,
there may not be a <code>broom</code> or <code>biobroom</code> method that you can use for the particular
class-based data structure that your data’s in, or it might be in a more general
list, rather than a specific class with a <code>biobroom</code> method. In this case, you’ll
need to extract the data “by hand” to move it into a dataframe once your data is
simple enough to work with using a tidyverse approach. If you’ve mastered how to
explore data stored in a list (covered in the last subsection), you’ll have a headstart
on how to do this. Once you know where to find each element of the data in the
structure of the list, you can assign these specific pieces to their own R objects
using typical R assignment (e.g., with the <em>gets arrow</em>, <code>&lt;-</code>, or with <code>=</code>, depending
on your preferred R programming style). …</p>
</div>
<div id="extras" class="section level3">
<h3><span class="header-section-number">3.5.4</span> Extras</h3>
<p>[Comparison of complexity of biological systems versus complexity of code and
algorithms for data pre-processing—for the later, nothing is unknowable or even
unknown. Someone somewhere is guaranteed to know exactly how it works, what it’s
doing, and why. By contrast, with biological systems, there are still things
that noone anywhere completely understands. It’s helpful to remember that all
code and algorithms for data pre-processing is knowable, and that the details
are all there if and when you want to dig to figure out what’s going on.]</p>
<p>[There are ways to fully package up and save the computer environment used to
run a pipeline of pre-processing and analysis, including any system settings,
all different software used in analysis steps, and so on. Some of the approaches
that are being explored for this include the use of “containers”, including
Docker containers. This does allow, typically, for full reproducibility of
the workflow. However, this approach isn’t very proactive in emphasizing the
robustness of a workflow or its comprehensibility to others—instead, it
makes the workflow reproducible by putting everything in a black box that must
be carefully unpackaged and explored if someone wants to understand or adapt
the pipeline.]</p>
<blockquote>
<p>“Object-oriented design doesn’t have to be over-complicated design, but we’ve
observed that too often it is. Too many OO designs are spaghetti-like tangles of
is-a and has-a relationships, or feature thick layers of glue in which many of the
objects seem to exist simply to hold places in a steep-sided pyramid of abstractions.
Such designs are the opposite of transparent; they are (notoriously) opaque and
difficult to debug.” <span class="citation">(Raymond 2003)</span></p>
</blockquote>
<blockquote>
<p>“Unix programmers are the original zealots about modularity, but tend to go about it
in a quiter way [that with OOP]. Keeping glue layers thin is part of it; more generally,
our tradition teaches us to build lower, hugging the ground with algorithms and structures
that are designed to be simple and transparent.” <span class="citation">(Raymond 2003)</span></p>
</blockquote>
<blockquote>
<p>“A <em>standard</em> is a precise and detailed description of how some artifact is built
or is supposed to work. Examples of software standards include programming
languages (the definition of syntax and semantics), data formats (how information is
represented), algorithmic processing (the steps necessary to do a computation), and
the like. Some standards, like the Word <code>.doc</code> file format, are <em>de facto</em> standards—they
have no official standing but everyoen uses them. The word ‘standard’ is best reserved for
formal descriptions, often developed and maintained by a quasi-neutral party like a
government or a consortium, that define how something is built or operated. The definition
is sufficiently complete and precise that separate entities can interact or provide independent
implementations. We benefit from hardware standards all the time, though we may not notice
how many there are. If I buy a new television set, I can plug it inot the electrical outlets
in my home thanks to standards for the size and shape of plugs and the voltage they provide.
The set itself will receive signals and display pictures because of standards for broadcast
and cable television. I can plug other devices into it through standard cables and connectors
like HDMI, USB, S-video and so on. But every TV needs its own remote control and every cell
phone needs a different charger because those have not been standardized. Computing has plenty
of standards as well, including character sets like ASCII and Unicode, programming languages
like C and C++, algorithms for encryption and compression, and protocols for exchanging
information over networks.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“Standards are important. They make it possible for independently created things to cooperate,
and they open an area to competition from multiple suppliers, while proprietary systems tend
to lock everyone in. … Standards have disadvantages, too—a standard can impede progress if
it is inferior or outdated yet everyone is forced to use it. But these are modest drawbacks
compared to the advantages.” <span class="citation">(Kernighan 2011)</span></p>
</blockquote>
<blockquote>
<p>“A <em>class</em> is a blueprint for constructing a particular package of code and data; each
variable created according to a class’s blueprint is known as an <em>object</em> of that class.
Code outside of a class that creates and uses an object of that class is known as a <em>client</em>
of the class. A <em>class declaration</em> names the class and lists all of the <em>members</em>, or
items inside that class. Each item is either a <em>data member</em>—a variable declared within the
class—or a <em>method</em> (also known as a <em>member function</em>), which is a function declared within
the class. Member functions can include a special type called a <em>constructor</em>, which has the
same name as the class and is invoked implicitly when an object of the class is declared.
In addition to the normal attributes of a variable or function declaration (such as type, and
for functions, the parameter list), each member has an <em>access specifier</em>, which indicates
what functions can access the member. A <em>public member</em> can be accessed by any code using the
object: code inside the class, a client of the class, or code in a <em>subclass</em>, which is a class
that ‘inherits’ all the code and data of an existing class. A <em>private member</em> can be
accessed only by the code inside the class. <em>Protected members</em> … are similar to private
members, except that methods in subclasses can also reference them. Both private and
protected members, though, are inaccessible from client code.” <span class="citation">(Spraul 2012)</span></p>
</blockquote>
<blockquote>
<p>“An object should be a meaningful, closely knit collection of data and code that operates
on the data.” <span class="citation">(Spraul 2012)</span></p>
</blockquote>
<blockquote>
<p>“Recognizing a situation in which a class would be useful is essential to reaching the
higher levels of programming style, but it’s equally important to recognize situations in
which a class is going to make things worse.” <span class="citation">(Spraul 2012)</span></p>
</blockquote>
<blockquote>
<p>“The word <em>encapsulation</em> is a fancy way of saying that classes put multiple pieces of
data and code together in a single package. If you’ve ever seen a gelatin medicine capsule
filled with little spheres, that’s a good analogy: The patient takes one capsule and swallows
all the individual ingredient spheres inside. … From a problem-solving standpoint,
encapsulation allows us to more easily reuse the code from previous problems to solve current
problems. Often, even though we have worked on a problem similar to our current project,
reusing what we learned before still takes a lot of work. A fully encapsulated class can
work like an external USB drive; you just plug it in and it works. FOr this to happen,
though, we must design the class correctly to make sure that the code and data is truly
encapsulated and as independent as possible from anything outside of the class. For example,
a class that references a global variable can’t be copied into a new project without
copying the global variable, as well.” <span class="citation">(Spraul 2012)</span></p>
</blockquote>
<blockquote>
<p>“Beyond reusing classes from one program to the next, classes offer the potential for
a more immediate form of code reuse: inheritance. … Using inheritance, we create parent
classes with methods common to two or more child classes, thereby ‘factoring out’ not
just a few lines of code [as with helper functions in procedural code] but whole methods.”
<span class="citation">(Spraul 2012)</span></p>
</blockquote>
<blockquote>
<p>“One technique we’re returned to again and again is dividing a complex problem into
smaller, more manageable pieces. Classes are great at dividing programs up into functional
units. Encapsulation not only holds data and code together in a reusable package; it also
cordons off that data and code from the rest of the program, allowing us to work on that
class, and everything else separately. The more classes we make in a program, the greater
the problem-dividing effect.” <span class="citation">(Spraul 2012)</span></p>
</blockquote>
<blockquote>
<p>“Some people use the terms <em>information hiding</em> and <em>encapsulation</em> interchangeable, but
we’ll separate the ideas here. As described previously …, encapsulation is
packaging data and code together. Information hiding means separating the interface of a
data structure—the definition of the operations and their parameters—from the implementation
of a data structure, or the code inside the functions. If a class has been written with
information hiding as a goal, then it’s possible to change the implementation of the methods
without requiring any changes in the client code (the code that uses the class). Again, we
have to be clear on the term <em>interface</em>; this means not only the name of the methods and
their parameter list but also the explanation (perhaps expressed in code documentation) of
what the different methods do. When we talk about changing the implementation without
changing the interface, we mean that we change <em>how</em> the class methods work but not
<em>what</em> they do. Some programming authors have referred to this as a kind of implicit contract
between the class and the client: The class agrees never to change the effects of
existing operations, and the client agrees to use the class strictly on the basis of its
interface and to ignore any implementation details.” <span class="citation">(Spraul 2012)</span></p>
</blockquote>
<blockquote>
<p>“So how does information hiding affect problem solving? The principle of information hiding
tells the programmer to put aside the class implementation details when working on the
client code, or more broadly, to be concerned about a particular class’s implementation
only when working inside that class. When you can put implementation details out of your
mind, you can eliminate distracting thoughts and concentrate on solving the problem at hand.”
<span class="citation">(Spraul 2012)</span></p>
</blockquote>
<blockquote>
<p>“A final goal of a well-designed class is expressiveness, or what might be broadly called
writability—the ease with which code can be written. A good class, once written, makes
the rest of the code simpler to write in the way that a good function makes code simpler to
write. Classes effectively extend a language, becoming high-level counterparts to basic
low-level features such as loops, if statements, and so forth. … With classes, programming
actions that previously took many steps can be done in just a few steps or just one.”
<span class="citation">(Spraul 2012)</span></p>
</blockquote>
<blockquote>
<p>“Right now, in labs across the world, machines are sequencing the genomes of the life
on earth. Even with rapidly decreasing costs and huge technological advancements in
genome sequencing, we’re only seeing a glimpse of the biological information contained
in every cell, tissue, organism, and ecosystem. However, the smidgen of total biological
information we’re gathering amounts to mountains of data biologists need to work with. At
no other point in human history has our ability to understand life’s complexities been so
dependent on our skills to work with and analyze data.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Bioinformaticians are concerned with deriving biological understanding from large
amounts of data with specialized skills and tools. Early in biology’s history, the
datasets were small and manageable. Most biologists could analyze their own data after
taking a statistics course, using Microsoft Excel on a personal desktop computer.
However, this is all rapidly changing. Large sequencing datasets are widespread, and will
only become more common in the future. Analyzing this data takes different tools, new skills,
and many computers with large amounts of memory, processing power, and disk space.”
<span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“In a relatively short period of time, sequencing costs dropped drastically, allowing
researchers to utilize sequencing data to help answer important biological questions.
Early sequencing was low-throughput and costly. Whole genome sequencing efforts were
expensive (the human genome cost around $2.7 billion) and only possible through large
collaborative efforts. Since the release of the human genome, sequencing costs have
decreased explonentially until about 2008 … With the introduction of next-generation
sequencing technologies, the cost of sequencing a megabase of DNA dropped even more
rapidly. At this crucial point, a technology that was only affordable to large collaborative
sequencing efforts (or individual researchers with very deep pockets) became affordable
to researchers across all of biology. … What was the consequence of this drop in
sequencing costs due to these new technologies? As you may have guessed, lots and lots
of data. Biological databases have swelled with data after exponential growth. Whereas once
small databases shared between collaborators were sufficient, now petabytes of useful
data are sitting on servers all over the world. Key insights into biological questions are
stored not just in the unanalyzed experimental data sitting on your hard drive, but also
spinning around a disk in a data center thousands of miles away.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“To make matters even more complicated, new tools for analyzing biological data are
continually being created, and their underlying algorithms are advancing. A 2012 review
listed over 70 short-read mappers … Likewise, our approach to genome assembly has
changed considerably in the past five years, as methods to assemble long sequences
(such as overlap-layout-consensus algorithms) were abandoned with the emergence of
short high-throughput sequencing reads. Now, advances in sequencing chemistry are
leading to longer sequencing read lengths and new algorithms are replacing others that
were just a few years old. Unfortunately, this abundance and rapid development of
bioinformatics tools has serious downsides. Often, bioinformatics tools are not adequately
benchmarked, or if they are, they are only benchmarked in one organism. This makes it
difficult for new biologists to find and choose the best tool to analyze their data.
To make matters more difficult, some bioinformatics programs are not actively developed
so that they lose relevance or carry bugs that could negatively affect results. All of
this makes choosing an appropriate bioinformatics program in your own research difficult.
More importantly, it’s imperative to critically assess the output of bioinformatics
programs run on your own data.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“With the nature of biological data changing so rapidly, how are you supposed to
learn bioinformatics? With all of the tools out there and more continually being
created, how is a biologist supposed to know whether a program will work appropriately
on her organism’s data? The solution is to approach bioinformatics as a bioinformatician
does: try stuff, and assess the results. In this way, bioinformatics is just about having
the skills to experiment with data using a computer and understanding your results.
The experimental part is easy: this comes naturally to most scientists. The limiting
factor for most biologists is having the data skills to freely experiment and work with
large data on a computer.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Unfortunately, many of the biologist’s common computational tools can’t scale to the
size and complexity of modern biological data. Complex data formats, interfacing
numerous programs, and assessing software and data make large bioinformatics datasets
difficult to work with.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“In 10 years, bioinformaticians may only be using a few of the bioinformatics
software programs around today. But we most certainly will be using data skills and
experimentation to assess data and methods of the future.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“Biology’s increasing use of large sequencing datasets is changing more that the tools
and skills we need: it’s also changing how reproducible and robust our scientific
findings are. As we utilize new tools and skills to analyze genomics data, it’s
necessary to ensure that our approaches are still as reproducible and robust as
any other experimental approaches. Unfortunately, the size of our data and the complexity
of our analysis workflows make these goals especially difficult in genomics.”
<span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>“The requisite of reproducibility is that we share our data and methods. In the pre-genomics
era, this was much easier. Papers coule include detailed method summaries and entire
datasets—exactly as Kreitman’s 1986 paper did with a 4,713bp Adh gene flanking sequence
(it was embedded in the middle of the paper). Now papers have long supplementary methods,
code, and data. Sharing data is no longer trivial either, as sequencing projects can include
terabytes of accompanying data. Reference genomes and annotation datasets used in analyses are
constantly updated, which can make reproducibility tricky. Links to supplemental materials,
methods, and data on journal websites break, materials on faculty websites disappear when
faculty members move or update their sites, and software projects become stale when
developers leave and don’t update code. … Additionally, the complexity of bioinformatics
analyses can lead to findings being susceptible to errors and technical confounding.
Even fairly routine genomics projects can use dozens of different programs, complicated
input paramter combinations, and many sample and annotation datasets; in addition, work
may be spread across servers and workstations. All of these computational data-processing
steps create results used in higher-level analyses where we draw our biological conclusions.
The end result is that research findings may rest on a rickety scaffold of numerous
processing steps. To make matters worse, bioinformatics workflows and analyses are usually
only run once to produce results for a publication, and then never run or tested again.
These analyses may rely on very specific versions of all software used, which can make it
difficult to reproduce on a different system. In learning bioinformatics data skills, it’s
necessary to concurrently learn reproducibility and robust best practices.”
<span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<blockquote>
<p>"When we are writing code in a programming language, we work most of the time with RAM,
combining and restructuring data values to produce new values in RAM. … The computer memory
in RAM is a series of 0’s and 1’s, just like the computer memory used to store files in mass
storage. In order to work with data values, we need to get those values into RAM in some
format. At the basic level of representing a single number or a single piece of text, the
solution is the same as it was in Chapter 5 [on file formats for mass storage]. Everything
is represented as a pattern of bits, using various numbers of bytes for different sorts of
values. In R, in an English locale, and on a 32-bit operating system, a single character
usually takes up one byte, an</p>
</blockquote>
</div>
<div id="subsection-2" class="section level3">
<h3><span class="header-section-number">3.5.5</span> Subsection 2</h3>
</div>
<div id="applied-exercise" class="section level3">
<h3><span class="header-section-number">3.5.6</span> Applied exercise</h3>
<ul>
<li>[Example data in a basic list]</li>
<li>[Example data in a Bioconductor list-based class]</li>
<li>[Explore each example dataset. What slots do each have? What are the names of each slot?
What data structures / data types are in each slot?]</li>
<li>[Extract certain elements from each dataset by hand. Assign to its own object name so
you can use it by itself.]</li>
<li>[Use <code>biobroom</code> to extract pieces of data in the Bioconductor dataset as tidy dataframes.
Try using this with further tidyverse code to create a nice table/visualization.]</li>
</ul>

</div>
</div>
<p style="text-align: center;">
<a href="3-4-complex-data-types-in-experimental-data-pre-processing.html"><button class="btn btn-default">Previous</button></a>
<a href="3-6-example-converting-from-complex-to-tidy-data-formats.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
