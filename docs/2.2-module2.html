<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="2.2 Principles and power of structured data formats | Improving the Reproducibility of Experimental Data Recording and Pre-Processing" />
<meta property="og:type" content="book" />


<meta property="og:description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing." />
<meta name="github-repo" content="rstudio/bookdown-demo" />

<meta name="author" content="Brooke Anderson, Michael Lyons, Mercedes Gonzalez-Juarrero, Marcela Henao-Tamayo, and Gregory Robertson" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing.">

<title>2.2 Principles and power of structured data formats | Improving the Reproducibility of Experimental Data Recording and Pre-Processing</title>

<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#overview"><span class="toc-section-number">1</span> Overview</a></li>
<li><a href="2-experimental-data-recording.html#experimental-data-recording"><span class="toc-section-number">2</span> Experimental Data Recording</a></li>
<li><a href="3-experimental-data-preprocessing.html#experimental-data-preprocessing"><span class="toc-section-number">3</span> Experimental Data Preprocessing</a></li>
<li><a href="4-references.html#references"><span class="toc-section-number">4</span> References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="module2" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Principles and power of structured data formats</h2>
<p>The format in which experimental data is recorded can have a large influence on
how easy and likely it is to implement reproducibility tools in later stages of
the research workflow. Recording data in a “structured” format brings many
benefits. In this module, we will explain what makes a dataset “structured” and
why this format is a powerful tool for reproducible research.</p>
<p>Every extra step of data cleaning is another chance to introduce errors in
experimental biomedical data, and yet laboratory-based researchers often share
experimental data with collaborators in a format that requires extensive
additional cleaning before it can be input into data analysis
<span class="citation">(Broman and Woo 2018)</span>. Recording data in a “structured” format brings many
benefits for later stages of the research process, especially in terms of
improving reproducibility and reducing the probability of errors in analysis
<span class="citation">(Ellis and Leek 2018)</span>. Data that is in a structured, tabular, two-dimensional
format is substantially easier for collaborators to understand and work with,
without additional data formatting <span class="citation">(Broman and Woo 2018)</span>. Further, by using a
consistent structured format across many or all data in a research project, it
becomes much easier to create solid, well-tested code scripts for data
pre-processing and analysis and to apply those scripts consistently and
reproducibly across datasets from multiple experiments <span class="citation">(Broman and Woo 2018)</span>.
However, many biomedical researchers are unaware of this simple yet powerful
strategy in data recording and how it can improve the efficiency and
effectiveness of collaborations <span class="citation">(Ellis and Leek 2018)</span>.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>List the characteristics of a structured data format</li>
<li>Describe benefits for research transparency and reproducibility</li>
<li>Outline other benefits of using a structured format when recording data</li>
</ul>
<div id="data-recording-standards" class="section level3" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Data recording standards</h3>
<p>For many areas of biological data, there has been a push to create standards for
how data is recorded and communicated. Standards can clarify both the <em>content</em>
that should be included in a dataset, the <em>format</em> in which that content is
stored, and the <em>vocabulary</em> used within this data. One article names these three
facets of a data standard as the <strong>minimum information</strong>, <strong>file formats</strong>, and
<strong>ontologies</strong> <span class="citation">(Ghosh et al. 2011)</span>.</p>
<p><label for="tufte-mn-6" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-6" class="margin-toggle"><span class="marginnote"><span style="display: block;">“It is important to distinguish between standards that specify how to actually do experiments and standards that specify how to describe experiments. Recommendations such as what standard reporters (probes) should be printed on microarrays or what quality control steps should be used in an experiment belong to the first category. Here we focus on the standards that specify how to describe and communicate data and information.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span></span></span></span></p>
<p>Many people and organizations (including funders) are excited about the idea
of developing and using data standards, especially at the community level.
Good standards, that are widely adapted by researchers, can help in making
sure that data submitted to data repositories are used widely and that software
can be developed that is <em>interoperable</em> with data from many research group’s
experiments. There are also many advantages, if there are not community-level
standards for recording a certain type of data, to develop and use local
data standards for recording data from your own experiments. This section
describes the elements that go into a data standard, discusses some choices
to be made when definining a data standard (especially choices on data
structure and file formats), and some of the advantages and disadvantages
of developing and using data recording standards at both the research group
and community levels.</p>
<p><label for="tufte-mn-7" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-7" class="margin-toggle"><span class="marginnote"><span style="display: block;">“The first of four root causes for irreproducibility in biomedical research: ‘First, a lack of standards for data generation leads to problems with the comparability and integration of data sets.’”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Waltemath and Wolkenhauer 2016)</span></span></span></span></p>
<p><strong>Ontology standards.</strong> Although it has the most complex name, an <em>ontology</em>
(sometimes called a <em>terminology</em> <span class="citation">(Sansone et al. 2012)</span>) might be the easiest and
quickest to adapt in recording data. An ontology helps define a vocabulary that
is controlled and consistent to use that researchers can use to refer to
concepts and concrete things within an area of research. It helps researchers,
when they want to talk about an idea or thing, to use one word, and just one
word, and to ensure that it will be the same word used by other researchers when
they refer to that idea or thing. Ontologies also help to define the relationships
between ideas or concrete things in a research area <span class="citation">(Ghosh et al. 2011)</span>, but
here we’ll focus on their use in provided a consistent vocabulary to use when
recording data.</p>
<p>For example, when recording a dataset, what do you call a small mammal that is
often kept as a pet and that has four legs and whiskers and purrs? Do you record
this as “cat” or “feline” or maybe, depending on the animal, even “tabby” or
“tom” or “kitten?” Similarly, do you record tuberculosis as “tuberculosis” or
“TB” or or maybe even “consumption?” If you do not use the same word
consistently in a dataset to record an idea, then while a human might be able to
understand that two words should be considered equivalent, a computer will not
be able to immediately tell that “TB” should be treated equivalently to
“tuberculosis.”</p>
<p>At a larger scale, if a research community can adapt an ontology they agree to
use throughout their studies, it will make it easier to understand and integrate
datasets produced by different research laboratories. If every research group
uses the term “cat,” then code can easily be written to extract and combine
all data recorded for cats across a large repository of experimental data.
On the other hand, if different terms are used, then it might be necessary to
first create a list of all terms used in datasets in the respository, then pick
through that list to find any terms that are exchangeable with “cat,” then write
script to pull data with any of those terms.</p>
<p>Several onotologies already exist or are being created for biological and other
biomedical research <span class="citation">(Ghosh et al. 2011)</span>. Existing community-level ontologies in
the biological sciences include [Gene Ontology and Systems Biology Ontology are
listed in the Ghosh et al. paper as two examples]. For biomedical science,
practice, and research, the BioPortal website
(<a href="http://bioportal.bioontology.org/" class="uri">http://bioportal.bioontology.org/</a>) provides access to almost 800 ontologies,
including several versions of the International Classification of Diseases, the
Medical Subject Headings (MESH), the National Cancer Institute Thesaurus, the
Orphanet Rare Disease Ontology and the National Center for Biotechnology
Information (NCBI) Organismal Classification. For each ontology in the BioPortal
website, the website provides a link for downloading the ontology in several
formats. If you download the ontology as a “CSV,” you can open it in your
favorite spreadsheet program and explore how it defines specific terms to use
for each idea or thing you might need to discuss within that topic area, as well
as synonyms for some of the terms. To use an ontology when recording your own
data, just make sure you use the ontology’s suggested terms in your data. For
example, if you’d like to use the Ontology for Biomedical Investigations
(<a href="http://bioportal.bioontology.org/ontologies/OBI" class="uri">http://bioportal.bioontology.org/ontologies/OBI</a>) and you are recording how many
children a woman has had who were born alive, you should name that column of the
data “number of live births,” not “# live births” or “live births (N)” or
anything else. Other collections of ontologies exist for fields of scientific
research, including the Open Biological and Biomedical Ontology (OBO) Foundry
(<a href="http://www.obofoundry.org/" class="uri">http://www.obofoundry.org/</a>).</p>
<p>If there are community-wide ontologies in your field, it is worthwhile to use
them in recording experimental data in your research group. Even better is to
not only consistently use the defined terms, but also to follow any conventions
with capitalization. While most statistical programs provide tools to change
capitalization (for example, to change all letters in a character string to
lower case), this process does require an extra step of data cleaning and an
extra chance for confusion or for errors to be introduced into data.</p>
<p><strong>Minimum information standards.</strong> The next easiest facet of a data standard to
bring into data recording in a research group is <em>minimum information</em>. Within a
data recording standard, <em>minimum information</em> (sometimes also called <em>minimum
reporting guidelines</em> <span class="citation">(Sansone et al. 2012)</span> or <em>reporting requirements</em>
<span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span>) specify <em>what</em> should be included in a dataset
<span class="citation">(Ghosh et al. 2011)</span>. Using minimum information standards help ensure that data
within a laboratory, or data posted to a repository, contain a number of
required elements. This makes it easier to re-use the data, either to compare it
to data that a lab has newly generated, or to combine several posted datasets to
aggregate them for a new, integrated analysis, considerations that are growing
in importance with the increasing prevalence of research repositories and
research consortia in many fields of biomedical science <span class="citation">(Keller et al. 2017)</span>.</p>
<p><label for="tufte-mn-8" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-8" class="margin-toggle"><span class="marginnote"><span style="display: block;">“Minimum information is a checklist of required supporting information for datasets from different experiments. Examples include: Minimum Information About a Microarray Experiment (MIAME), Minimum Information About a Proteomic Experiment (MIAPE), and the Minimum Information for Biological and Biomedical Investigations (MIBBI) project.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Ghosh et al. 2011)</span></span></span></span></p>
<p><em>Standardized file formats.</em> While using a standard ontology and a standard for
minimum information is a helpful start, it just means that each dataset has the
required elements <em>somewhere</em>, and using a consistent vocabulary—it doesn’t
specify where those elements are in the data or that they’ll be in the same
place in every dataset that meets those standards. As a result, datasets that all
meet a common standard can still be very hard to combine, or to create common
data analysis scripts and tools for, since each dataset will require a different
process to pull out a given element.</p>
<p>Computer files serve as a way to organize data, whether that’s recorded
datapoints or written documents or computer programs <span class="citation">(Kernighan and Pike 1984)</span>. As
the programmer Paul Ford writes,</p>
<p><label for="tufte-mn-9" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-9" class="margin-toggle"><span class="marginnote"><span style="display: block;">“Data is just stuff, or rather, structured stuff: The cells of a spreadsheet, the structure of a Word document, computer programs themselves—all data.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Ford 2015)</span></span></span></span></p>
<p>A <em>file format</em> defines the rules for how the bytes in the chunk of
memory that makes up a certain file should be parsed and interpreted anytime you
want to meaningfully access and use the data within that file
<span class="citation">(Murrell 2009)</span>. There are many file formats you may be familiar
with—a file that ends in “.pdf” must be opened with a Portable Document Format
(PDF) Reader like Adobe Acrobat, or it won’t make much sense (you can try this
out by trying to open a “.pdf” file with a text editor, like TextEdit or
Notepad). The Reader has been programmed to interpret the data in a “.pdf” file
based on rules defining what data is stored where in the section of computer
memory for that file. Because most “.pdf” files conform to the same <em>file
format</em> rules, powerful software can be built that works with any file in that
format.</p>
<p>For certain types of biomedical data, the challenge of standardizing a format
has similarly been addressed through the use of well-defined rules for not only
the content of data, but also the way that content is <em>structured</em>. This can be
standardized through <em>standardized file formats</em> (sometimes also called <em>data
exchange formats</em> <span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span>) and often defines not only the
upper-level file format (e.g., use of a “.csv” file type), but also how data
within that file type should be organized. If data from different research
groups and experiments is recorded using the same file format, researchers can
develop software tools that can be repeatedly used to interpret and visualize
that data; on the other hand, if different experiments record data using
different formats, bespoke analysis scripts must be written for each separate
dataset. This is a blow not only to the efficiency of data analysis, but also a
threat to the accuracy of that analysis. If a set of tools can be developed that
will work over and over, more time can be devoted to refining those tools and
testing them for potential errors and bugs, while one-shot scripts often can’t
be curated with similar care. One paper highlights the dangers that come with
working with files that don’t follow a defined format:</p>
<p><label for="tufte-mn-10" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-10" class="margin-toggle"><span class="marginnote"><span style="display: block;">“Beware of common pitfalls when working with <em>ad hoc</em> bioinformatics formats. Simple mistakes over minor details like file formats can consume a disproportionate amount of time and energy to discover and fix, so mind these details early on.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Buffalo 2015)</span></span></span></span></p>
<p>Some biomedical data file formats have been created to help smooth over the
transfer of data that’s captured by complex equipment into software that can
analyze that data. For example, many immunological studies need to measure
immune cell populations in experiments, and to do so they use piece of
equipment called a flow cytometer that probes cells in a sample with lasers
and measures resulting intensities to determine characteristics of that cell.
The data created by this equipment is large (often measurements from [x] or more
lasers are taken for [x] cells in a single run) and somewhat complex, with a need
to record not only the intensity measurements from each laser, but also some metadata
about the equipment and characteristics of the run.
If every company that manufactured flow cytometers used a different file format for
saving the resulting data, then a different set of analysis software would need to
be developed to accompany each piece of equipment. For example, a laboratory at
a university with flow cytometers from two different companies would need licenses
for two different software programs to work with data recorded by flow cytometers,
and they would need to learn how to use each software package separately. There is a
chance that software could be developed that used shared code for data analysis, but only
if it also included separate sets of code to read in data from all types of equipment
and to reformat them to a common format.</p>
<p>This isn’t the case, however. Instead, there is a commonly agreed on file
format that flow cytometers should use to record the data they collect, called
the the <em>FCS file format</em>. This format has been defined through a series of
papers [refs], with several separate versions as the file format has evolved
over the years. It provides clear specifications on where to save each relevant
piece of information in the block of memory devoted to the data recorded by the
flow cytometer (in some cases, leaving a slot in the file blank if no relevant
information was collected on that element). As a result, people have been able
to create software, both proprietary and open-source, that can be used with any
data recorded by a flow cytometer, regardless of which company manufacturer the
piece of equipment that was used to generate the data. Other types of biomedical
data also have standardized file formats, including [example popular file
formats for biomedical data]. In some cases these were defined by an
organization, society, or initiative (e.g., the Metabolomics Standards
Initiative) <span class="citation">(Ghosh et al. 2011)</span>, while in some cases the file format developed
by a specific equipment manufacturer has become popular enough that it’s
established itself as the standard for recording a type of data
<span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span>.</p>
<p>For an even simpler example, thing about recording dates. The <em>minimum
information standard</em> for a date might always be the same—a recorded value
must include the day of the month, month, and year. However, this information
can be <em>structured</em> in a variety of ways. In many scientific data, it’s common
to record this information going from the largest to smallest units, so March
12, 2006, would be recorded “2006-03-12.” Another convention (especially in the
US) is to record the month first (e.g., “3/12/06”), while another (more common
in Europe) is to record the day of the month first (e.g., “12/3/06”).</p>
<p>If you are trying to combine data from different datasets with dates, and all
use a different structure, it’s easy to see how mistakes could be introduced
unless the data is very carefully reformatted. For example, March 12 (“3-12”
with month-first, “12-3” with day-first) could be easily mistaken to be December
3, and vice versa. Even if errors are avoided, combining data in different
structures will take more time than combining data in the same structure,
because of the extra needs for reformatting to get all data in a common
structure.</p>
<p><label for="tufte-mn-11" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-11" class="margin-toggle"><span class="marginnote"><span style="display: block;">“Vast swathes of bioscience data remain locked in esoteric formats, are described using nonstandard terminology, lack sufficient contextual information, or simply are never shared due to the perceived cost or futility of the exercise.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Sansone et al. 2012)</span></span></span></span></p>
</div>
<div id="defining-data-standards-for-a-research-group" class="section level3" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Defining data standards for a research group</h3>
<p>If some of the data you record from your experiments comes from complex
equipment, like flow cytometers [or?], you may be recording much of that data in
a standardized format without any extra effort, because that format is the
default output format for the equipment. However, you may have more control over
other data recorded from your experiments, including smaller, less complex data
recorded directly into a laboratory notebook or spreadsheet. You can derive a
number of benefits from defining and using a standard for collecting this data,
as well.</p>
<p>As already mentioned, for many of the complex types of biological data,
standardized file formats exist. For example, flow cytometry data is typically
collected and recorded in <em>.fcs</em> files. Every piece of flow cytometry equipment
can then be built to output data in this format, and every piece of software to
analyze flow cytometry data can be built to read in this input. The <em>.fcs</em> file
format species how both raw data and metadata (e.g., compensation information,
equipment details) can be saved within the file—everyone who uses that file
format knows where to store data and where to find data of a certain type.</p>
<p>Much of the data collected in a laboratory is smaller, less complex, or less
structured than these types of data, data that is recorded “by hand,” often into
a laboratory notebook or a spreadsheet. One paper describes this type of data as
the output of “traditional, low-throughput bench science” <span class="citation">(Wilkinson et al. 2016)</span>.
For this data recording, the data may be written down in an <em>ad hoc</em>
way—however the particular researcher doing the experiment thinks makes
sense—and that format might change with each experiment, even if many
experiments have similar data outputs. As a result, it becomes harder to create
standardized data processing and analysis scripts that work with this data or
that integrate it with more complex data types. Further, if everyone in a
laboratory sets up their spreadsheets for data recording in their own way, it is
much harder for one person in the group to look at data another person recorded
and immediately find what they need within the spreadsheet.</p>
<p>As a step in a better direction, the head of a research group may
designate some common formats (e.g., a spreadsheet template) that all
researchers in the group should use when recording the data from a specific type
of experiments. This provides consistency across the recorded data for the
laboratory, making easier for one lab member to quickly understand and navigate
data saved by another lab member. It also opens the possibility to create tools
or scripts that read in and analyze the data that can be re-used across multiple
experiments with minor changes. This helps improve the efficiency and
reproducibility of data analysis, visualization, and reporting steps of the
research project.</p>
<p>This does require some extra time commitment <span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span>. First, time
is needed to design the format, and it does take a while to develop a format
that is inclusive enough that it includes a place to put all data you might want
to record for a certain type of experiment. Second, it will take some time to
teach each laboratory member what the format is and how to make sure they comply
with it when they record data.</p>
<p>On the flip side, the longer-term advantages of using a defined, structured
format will outweigh the short-term time investments for many laboratory groups
for frequently used data types. By creating and using a consistent structure to
record data of a certain type, members of a laboratory group can increase their
efficiency (since they do not need to re-design a data recording structure
repeatedly). They can also make it easier for downstream collaborators, like
biostatisticians and bioinformaticians, to work with their output, as those
collaborators can create tools and scripts that can be recycled across
experiments and research projects if they know the data will always come to them
in the same format. These benefits increase even more if data format standards
are created and used by a whole research field (e.g., if a standard data
recording format is always used for researchers conducting a certain type of
drug development experiment), because then the tools built at one institution
can be used at other insitutions. However, this level of field-wide coordination
can be hard to achieve, and so a more realistic immediate goal might be
formalizing data recording structures within your research group or department,
while keeping an eye out for formats that are gaining popularity as standards in
your field to adopt within your group.</p>
<p>One key advantage to using standardized data formats even for recording simple,
“low-throughput” data is that everyone in the research group will be able to
understand and work with data recorded by anyone else in the group—data will
not become impenetrable once the person who recorded it leaves the group. Also,
once a group member is used to the format, the process of setting up to record
data from a new experiment will be quicker, as it won’t require the effort of
deciding and setting up a <em>de novo</em> format for a spreadsheet or other recording
file. Instead, a template file can be created that can be copied as a starting
point for any new data recording.</p>
<p>Finally, there are huge benefits further down the data analysis pipeline that
come with always recording data in the same format. If your group is working
with a statistician or data analyst, it becomes much easier for that person to
quickly understand a new file if it follows the same format as previous files.
Further, if you work with a statistician or data analyst, he or she probably
creates code scripts to read in, re-format, analyze, and visualize the data
you’ve shared. If you always record data using the same format, these scripts
can be reused with very little modification. This saves valuable time, and it
helps make more time for more interesting statistical analysis if your
collaborator can trim time off reading in and reformatting the data in their
statistical programming language.</p>
<p>One paper suggests that the balance can be found, in terms of deciding whether
the benefits of developing a standard outweigh the costs, by considering how
often data of a certain type is generated and used:</p>
<p><label for="tufte-mn-12" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-12" class="margin-toggle"><span class="marginnote"><span style="display: block;">“To develop and deploy a standard creates an overhead, which can be expensive. Standards will help only if a particular type of information has to be exchanged often enough to pay off the development, implementation, and usage of the standard during its lifespan.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span></span></span></span></p>
</div>
<div id="two-dimensional-structured-data-format" class="section level3" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Two-dimensional structured data format</h3>
<p>So far, this module has explored <em>why</em> you might want to use standardized
data formats for recording experimental data. The rest of the module
aims to give you tips for how to design and define your own standardized
data formats, if you decide that is worthwhile for certain data types
recorded within your research group.</p>
<p>Once you commit to creating a defined, structured format, you’ll need to decide
what that structure should be. There are many options here, and it’s very
tempting to use a format that is easy on human eyes
<span class="citation">(Buffalo 2015)</span>. For example, it may seem appealing to create a
format that could easily be copied and pasted into presentations and Word
documents and that will look nice in those presentation formats. To facilitate
this use, a laboratory might set up a recording format base on a spreadsheet
template that includes multiple tables of different data types on the same
sheet, or multi-level column headings.</p>
<p>Unfortunately, many of the characteristics that make a format attractive
to human eyes will make it harder for a computer to make sense of. For example,
if you include two tables in the same spreadsheet, it might make it easier for a
person to get a one-screen look at two small data tables. However, if you want
to read that data into a statistical program (or work with a collaborator who
would), it will likely take some complex code to try to tell the computer how to
find the second table in the spreadsheet. The same applies if you include some
blank lines at the top of the spreadsheet, or use multi-level headers, or use
“summary” rows at the bottom of a table. Further, any information you’ve
included with colors or with text boxes in the spreadsheet will be lost when the
data’s read into a statistical program. These design elements in a data
format make it much harder to read the data embedded in a spreadsheet into other
computer programs, including programs for more complex data analysis and
visualization, like R and Python.</p>
<p><label for="tufte-mn-13" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-13" class="margin-toggle"><span class="marginnote"><span style="display: block;">“Data should be formatted in a way that facilitates computer readability. All too often, we as humans record data in a way that maximizes its readability to us, but takes a considerable amount of cleaning and tidying before it can be processed by a computer. The more data (and metadata) that is computer readable, the more we can leverage our computers to work with this data.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Buffalo 2015)</span></span></span></span></p>
<p>For most statistical programs, data can be easily read in from a spreadsheet if
the computer can parse it in the following way: first, read in the first row,
and assign each cell in that row as the <em>name</em> of a column. Then, read in the
second row, and put each cell in the column the corresponds with the name of the
cell in the same position in the first row. Also, set the data type for that
column (e.g., number, character) based on the data type in this cell. Then, keep
reading in rows until getting to a row that’s completely blank, and that will be
the end of the data. If any of the rows has more cell than the first row, then
that means that something went wrong, and should result in stopping or giving a
warning. If any of the rows have fewer cells than the first row, then that means
that there are missing data in that row, and should probably be recorded as
missing values for any cells the row is “short” compared to the first row.</p>
<p>One of the easiest format for a computer to read is therefore a two-dimensional
“box” of data, where the first row of the spreadsheet gives the column names,
and where each row contains an equal number of entries. This type of
two-dimensional tabular structure forms the basis for several popular
“delimited” file formats that serve as a <em>lingua franca</em> across many simple
computer programs, like the comma-separated values (CSV) format, the
tab-delimited values (TSV) format, and the more general delimiter-separated
values (DSV) format, which are a common format for data exchange across
databases, spreadsheet programs, and statistical programs <span class="citation">(Janssens 2014; E. S. Raymond 2003; Buffalo 2015)</span>.</p>
<p><label for="tufte-mn-14" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-14" class="margin-toggle"><span class="marginnote"><span style="display: block;">“Tabular plain-text data formats are used extensively in computing. The basic format is incrediably simple: each row (also known as a record) is kept on its own line, and each column (also known as a field) is separate by some delimiter.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Buffalo 2015)</span></span></span></span></p>
<p>If you think of the computer parsing a spreadsheet as described above, hopefully
it clarifies why some spreadsheet formats would cause problems. For example, if
you have two tables in the same spreadsheet, with blank lines between them, the
computer will likely either think it’s read all the data after the first table,
and so not read in any data from the second table, or it will think the data
from both tables belong in a single table, with some rows of missing data in the
center. To write the code to read in data from two tables into two separate
datasets in a statistical program, it will be necessary to write some complex
code to tell the computer how to search out the start of the second table in the
spreadsheet.</p>
<p>Similar problems come up if a spreadsheet diverges from a regular,
two-dimensional format, with a single row of column names to start the data.
For example, if the data uses multiple rows to create multi-level
column headers, anyone reading it into another program will need to
either skip some of the rows of the column headers, and so lose information
in the original spreadsheet, or write complex code to parse the column
headers separately, then read in the later rows with data, and then stick
the two elements back together. “Summary” rows at the end of a dataset
(for example, the sums or means of all values in a column) will need to
be trimmed off when the data is read into other programs, since most
of the analysis and visualization someone would want to do in another
program will calculate any summaries fresh, and will want each row of
a dataset to represent the same “type” and level of data (e.g., one measurement
from one animal).</p>
<p>For anything in a data format that requires extra coding when reading data into
another program, you are introducing a new opportunity for errors at the
interface between data recording and data analysis. If there are strong reasons
to use a format that requires these extra steps, it will still be possible to
create code to read in and parse the data in statistical programs, and if the
same format is consistently used, then scripts can be developed and thoroughly
tested to allow this. However, do keep in mind that this will be an extra
burden on any data analysis collaborators who are using a program besides a
spreadsheet program. The extra time this will require could be large, since
this code should be vetted and tested thoroughly to ensure that the data
cleaning process is not introducing errors. By contrast, if the data is
recorded in a two-dimensional format with a single row of column names as
the first row, data analysts can likely read it quickly and cleanly into
other programs, with low risks of errors in the transfer of data from the
spreadsheet.</p>
<p><label for="tufte-mn-15" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-15" class="margin-toggle"><span class="marginnote"><span style="display: block;">“Cleaning data is a short-term solution, and preventing errors is promoted as a permanent solution. The drawback to cleaning data is that the process never ends, is costly, and may allow many errors to avoid detection.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Keller et al. 2017)</span></span></span></span></p>
</div>
<div id="saving-two-dimensional-structured-data-in-plain-text-file-formats" class="section level3" number="2.2.4">
<h3><span class="header-section-number">2.2.4</span> Saving two-dimensional structured data in plain text file formats</h3>
<p>If you have recorded data in a two-dimensional structured format, you can choose
to save it in either a <em>plain text</em> format or a <em>binary</em> format. With a plain
text format, a file is “human readable” when it’s opened in a text editor
<span class="citation">(Hunt, Thomas, and Cunningham 2000; Janssens 2014)</span>, because each byte that encodes the file
translates to a single character <span class="citation">(Murrell 2009)</span>, usually using an
ASCII or Unicode encoding. Common plain text file formats used for biomedical
research include CSV and TSV files (these are distinguished only by the
character used as a delimiter—commas for CSV files versus taabs for TSV files)
<span class="citation">(Buffalo 2015)</span>, other more complex file formats like SAM and XML
are also typically saved in plain text.</p>
<p>A binary file format, on the other hand, encodes data within the file using an
encoding system that differs from ANSCII or Unicode. To extract the data in a
meaningful way, a computer program must know and use rules for the encoding and
structure of that file format, and those rules will be different for each
different binary file format <span class="citation">(Murrell 2009)</span>. Some binary file
formats are “open,” with all the information on these rules and encodings
available for anyone to read. On the otherhand, other binary file formats are
proprietary, without available guidance on how to interpret or use the data
stored in them when creating new software tools. Binary files, because they
don’t follow the restrictions of plain text encoding and format, can encode and
organize data in a way that’s often much more compressed, because it’s optimized
to suit a specific type of data. This means that binary file formats can often
store more data within a certain amount of computer memory compared to plain
text file formats. Binary files can also be designed so that the computer can
find and read a specific piece of data, rather than needing to read data in
linearly from the start to the end of a file as with plain text formats. This
means that programs can often access specific bits of data much more quickly
from a binary file format that from a plain text format, making computation
processing run much faster.</p>
<p>However, even with the speed and size advantages of many binary file formats, it
is often worthwhile to record and save experimental data in a plain text, rather
than binary, file format. There are a number of advantages to using a plain text
format. A plain text format may take more space (in terms of computer memory)
and take longer to process within other programs; however, its benefits
typically outweigh these limitations <span class="citation">(Hunt, Thomas, and Cunningham 2000)</span>. Advantages include:
(1) humans can read the file directly <span class="citation">(Hunt, Thomas, and Cunningham 2000; Janssens 2014)</span>,
and should always be able to, regardless of changes in and future obsolescence
of computer programs; (2) almost all software programs for analyzing and
processing files can input plain-text files, while binary file formats often
require specialized software <span class="citation">(Murrell 2009)</span>; (3) the
Unix system, which has influenced many existing software programs, especially
open-source programs for data analysis and command-line tools, are based on
inputting and outputtin line-based plain-text files <span class="citation">(Janssens 2014)</span>; and (4)
plain-text files can be easily tracked with version control
<span class="citation">(Hunt, Thomas, and Cunningham 2000)</span>. These advantages might become particularly important in
cases where researchers need to combine and integrate heterogeneous data, for
example data coming from different instruments.</p>
<p>Another advantage of storing data in a plain text format is that it makes
version control, which we’ll discuss in a later module, a much more powerful
tool. With plain text files, you can use version control to see the specific
changes to a file. With binary files, you can typically see if a file was
changed, but it’s much harder to see exactly what within the file was changed.</p>
<p>The book <em>The Pragmatic Programmer</em> highlights some of the advantages of plain
text:</p>
<p><label for="tufte-mn-16" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-16" class="margin-toggle"><span class="marginnote"><span style="display: block;">“Human-readable forms of data, and self-describing data, will outlive all other forms of data and the applications that created them. Period. As long as the data survives, you will have a chance to be able to use it—potentially long after the original application that wrote it is defunct. … Even in the future of XML-based intelligent agents that travel the wild and dangerous Internet autonomously, negotiating data interchange among themselves, the ubiquitous text file will still be there. In fact, in heterogeneous environments the advantages of plain text can outweight all of the drawbacks. You need to ensure that all parties can communicate using a common standard. Plain text is that standard.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Hunt, Thomas, and Cunningham 2000)</span></span></span></span></p>
<p>Paul Ford, by contrast, describes some of the disadvantages of
a binary file format, using the Photoshop file format as an example:</p>
<p><label for="tufte-mn-17" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-17" class="margin-toggle"><span class="marginnote"><span style="display: block;">“A Photoshop file is a lump of binary data. It just sits there on your hard drive. Open it in Photoshop, and there are your guides, your color swatches, and of course, the manifold pixels of your intent. But outside of Photoshop that file is an enigma. There is not ‘view source.’ You can, if you’re passionate, read the standard on the web, and it’s all piled in there, the history of pictures on computers. That’s when it becomes clear: only Photoshop’s creator Adobe can understand this thing.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Ford 2014)</span></span></span></span></p>
<p>Structuring data in a gridded, two-dimensional format, as described in the last
section, will be helpful even if it is in a file format that is binary, like
Excel. However, there are added benefits to saving the structured data in a
plain text format. Older Excel spreadsheets are typically saved in a proprietary
file format (“.xls”), while more recently Excel has saved files to an open
binary format based on packaging XML files with the data (“.xlsx” file format)
<span class="citation">(Janssens 2014)</span>. While the open proprietary format is preferable, since
tools can be developed to work with them by people other than the Microsoft
team, both file formats still face some of the limitations of binary file
formats as a way of recording experimental data. However, even if you have used
a spreadsheet program like Excel to record data, it’s very easy to still save
that data in a plain text file format <span class="citation">(Murrell 2009)</span>. In most
spreadsheet programs, you can choose to save a file “As CSV.”</p>
</div>
<div id="occassions-for-more-complex-data-structures-and-file-formats" class="section level3" number="2.2.5">
<h3><span class="header-section-number">2.2.5</span> Occassions for more complex data structures and file formats</h3>
<p>There are some cases where a two-dimensional data format may not be adequate
for recording experimental data, despite this format’s advantages in
improving reproducibility through later data analysis steps. Similarly,
there may be cases where a binary file format, or use of a database, will
outweigh the benefits of saving data to a plain text format. Being familiar
with different file formats can also be helpful when you need to integrate
data stored in different formats <span class="citation">(Murrell 2009)</span>.</p>
<p><strong>Non-tabular plain-text formats.</strong>
First, some data has a linked or hierarchical nature, in terms of how data
points are connected through the dataset. For example, data on a family try
might have a hierarchical structure, where different numbers of children are
recorded for each parent. As another example, if you were building a dataset
describing how scientists have collaborated together as coauthors, that data
might form a network. In many cases, it is possible to structure datasets with
these types of “non-tabular” structure using the “tidy data” tabular format
described in the next section. However, in very complex cases, it may work
better to use a non-tabular data format <span class="citation">(E. S. Raymond 2003)</span>. Popular data formats
that are non-tabular include the eXtensible Markup Language (XML) and JavaScript
Object Notation (JSON) formats, both of which are well-suited for
hierarchically-structured data. You may also have data you would like to use in
XML or JSON formats if you are using web services to pull datasets from online
repositories, as open data application programming interfaces (APIs) often
return data in these formats <span class="citation">(Janssens 2014)</span>.</p>
<p>Another use of file formats that are plain text but meant to be streamed, rather
than read in as a whole. When reading in data stored in a delimited plain text
file, like a CSV file, a statistical program like R will typically read in all
the data and then operate on the dataset as a whole. If a data file is very
large, then reading in all the data at once might require so much memory that it
slows down processing, or even exceed the program’s memory cap [?]. One strategy
is to design a data format so that the program can read in a small amount of the
file, process that piece of the data, write the result out, and remove that bit
of data from the program’s memory before moving into the next portion of data
<span class="citation">(Buffalo 2015)</span>. This <em>streaming</em> approach is sometimes used with
some file formats used for biomedical research, including FASTA and FASTQ files.</p>
<p><strong>Databases.</strong>
When research datasets include not only data that can be expressed in plain
text, but also data like images, photographs, or videos, it may be worth
considering using a database to store the data <span class="citation">(Murrell 2009)</span>.
Relational database managment system software, like [examples. MySQL?
PostgreSQL?] can be used to organize data in a way that records connections
(<em>relations</em>) between different pieces of data and allows you to access
different combinations of that data quickly using Structured Query Language, or
<em>SQL</em> <span class="citation">(Ford 2015)</span>. Further, some statistical programming languages, including
R, now have tools that allow you to directly access and work with data from a
database from within the statistical program, and in some cases using scripts
that are very similar or identical to the code that would be used if you’d read
the data into the program from a plain text file.</p>
<p><label for="tufte-mn-18" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-18" class="margin-toggle"><span class="marginnote"><span style="display: block;">“The database is the unsung infrastructure of the world, the shared memory of every corporation, and the foundation of every major web site. And they are everywhere. Nearly every host-your-own-web-site package comes with access to a database called MySQL; just about every cell phone has SQLite3, aa tiny, pocket-sized database, built in.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Ford 2015)</span></span></span></span></p>
<p>It will be more complicated to set up a database for recording experimental
data, and so it’s often preferable to instead save data in plain text files
within a file directory, if the data is simple enough to allow that. However,
there are some fairly simple database solutions that are now available,
including SQLite <span class="citation">(Buffalo 2015)</span>.</p>
<p><strong>Binary file formats.</strong></p>
<p>There are cases where it may not be best to store laboratory-generated data in a
plain text format. For example, the output from a flow cytometer is large and
would take up a lot (more) computer memory if stored in a plain text format, and
it would take much longer to read and work with the data in analysis software if
it were in that format. For very large datasets like this, it may be necessary
to use a binary data format, either for size or speed or both
<span class="citation">(Kernighan and Pike 1984; Hunt, Thomas, and Cunningham 2000)</span>. For very large biomedical datasets,
binary file formats are sometimes designed for <em>out-of-memory approaches</em>
<span class="citation">(Buffalo 2015)</span>, where a file format is designed in a way that
allows computer programs to find and read only specific pieces of data in a file
through a process called <em>random access</em>, rather than needing to read the full
file into memory before a specific piece of data in the file can be accessed
(a.k.a., <em>sequential access</em>) <span class="citation">(Murrell 2009)</span>.</p>
</div>
<div id="levels-of-standardizationresearch-group-to-research-community" class="section level3" number="2.2.6">
<h3><span class="header-section-number">2.2.6</span> Levels of standardization—research group to research community</h3>
<p>Standards can operate both at the level of individual research groups and at the
level of the scientific community as a whole. The potential advantages of
community-level standards are big: they offer the chance to develop
common-purpose tools and code scripts for data analysis, as well as make it
easier to re-use and combine experimental data from previous research that is
posted in open data repositories. If a software tool can be reused, then more
time can be spent in developing and testing it, and as more people use it, bugs
and shortcomings can be identified and corrected. Community-wide standards can
lead to databases with data from different experiments, and from different
laboratory groups, structured in a way that makes it easy for other researchers
to understand each dataset, find pieces of data of interest within datasets, and
integrate different datasets <span class="citation">(Lynch 2008)</span>. Similarly, with community-wide
standards, it can become much easier for different research groups to
collaborate with each other or for a research group to use data generated by
equipment from different manufacturers <span class="citation">(Schadt et al. 2010)</span>.</p>
<p><label for="tufte-mn-19" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-19" class="margin-toggle"><span class="marginnote"><span style="display: block;">“Without community-level harmonization and interoperability, many community projects risk becoming data silos.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Sansone et al. 2012)</span></span></span></span></p>
<p><label for="tufte-mn-20" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-20" class="margin-toggle"><span class="marginnote"><span style="display: block;">“Solutions to integrating the new generation of large-scale data sets require approaches akin to those used in physics, climatology and other quantitative disciplines that have mastered the collection of large data sets.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Schadt et al. 2010)</span></span></span></span></p>
<p>However, there are important limitations to community-wide standards, as well.
It can be very difficult to impose such standards top-down and community-wide,
particularly for low-throughput data collection (e.g., laboratory bench
measurements), where research groups have long been in the habit of recording
data in spreadsheets in a format defined by individual researchers or research
groups. One paper highlights this point:</p>
<p><label for="tufte-mn-21" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-21" class="margin-toggle"><span class="marginnote"><span style="display: block;">“The data exchange formats PSI-MI and MAGE-ML have helped to get many of the high-throughput data sets into the public domain. Nevertheless, from a bench biologist’s point of view benefits from adopting standards are not yet overwhelming. Most standardization efforts are still mainly an investment for biologists.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span></span></span></span></p>
<p>Further, in some fields, community-wide standards have struggled to remain
stable, which can frustrate community members, as scripts and software must be
revamped to handle shifting formats <span class="citation">(Buffalo 2015; Barga et al. 2011)</span>. In some cases, a useful compromise is to follow a
general data recording format, rather than one that is very prescriptive. For
example, committing to recording data in a format that is “tidy” (which we
discuss extensively in the next module) may be much more flexible—and able to
meet the needs of a large range of experimental designs—than the use of a
common spreadsheet template or a more proscriptive standardized data format.</p>
</div>
<div id="applied-exercise" class="section level3" number="2.2.7">
<h3><span class="header-section-number">2.2.7</span> Applied exercise</h3>
<!---

### Additional materials to consider working in

> **Designed data** is "data that have traditionally been used in scientific
discovery. Designed data include statistically designed data collections, such
as surveys or experiments, and intentional observational collections. Examples
of intentional observational collections include data obtained from specially
designed instruments such as telescopes, DNA sequencers, or sensors on an ocean
buoy, and also data from systematically designed case studies such as health
registries Researchers have frequently devoted decades of systematic research to
understanding and characterizing the properties of designed data collections."
This contrasts with administrative data and opportunity data.
[@keller2017evolution]

> "The need to address data quality is a persistent one in the physical and
biological sciences, where scientists often seek to understand subtle effects
that leave minute traces in large volumes of data. ... For most scientists,
three factors motivate their work on data quality: first, the need to create a
strong foundation of data from which to draw their own conclusions; second, the
need to protect their data and conclusions from the criticisms of others; and
third, the need to understand the potential flaws in data collected by others.
The work of these scientists in data quality primarily concentrates on the
design and execution of experiments, including in laboratory, field, and
clinical settings. The key ingredients are measurement implementation,
laboratory and experimental controls, documentation, analysis, and curation of
data." [@keller2017evolution]

> "The concept of data quality management developed in the 1980s in step with
the technological ability to access stored data in a random fashion.
Specifically, as data management encoding process moved from the highly
controlled and defined linear process of transcribing information to tape, to a
system allowing for the random updating and transformation of data fields in a
record, the need for a higher level of control over what exactly can go into a
data field (including type, size, and values) became evident. Two key data
quality concepts came from these data management advances---ensuring data
integrity and cleansing the legacy data. Data integrity refers to the rules and
processes put in place to maintain and assure the accuracy and consistency of a
system that stores, processes, and retrieves data. Data cleaning refers to the
identificatio of incomplete, incorrect, inaccurate, or irrelevant parts of the
data and then replacing, modifying, or deleting this so-called dirty or coarse
data." [@keller2017evolution]

> "As the capability to store increasing amounts of data grew, so did the
business motivation to improve the quality of administrative data and thereby
improve decision making, reduce costs, and gain trust of customers."
[@keller2017evolution]

> "Determine whether there is a community-based metadata schema or standard (i.e., 
preferred sets of metadata elements) that can be adopted." [@michener2015ten]

> Might make more sense in tidy data section: "Although many standards have been
defined for data and model representations, they only ensure that data and
models that comply with these standards can be used by software that support
these standards; they do not ensure that multiple software tools can be used
seamlessly. When software tools are developed by independent research groups or
companies without an explicit agreement as to how they can be integrated, this
can cause problems when forming a workflow of multiple tools. This is because
the tools are likely to be inconsistent in their operating procedures and their
use of various non-standardized data formats. Thus, users often have to convert
data formats, to learn operating procedures for each tool, and sometimes even to
adjust operating environments. This impedes productivity, undermines the
flexibility of the workflow, and is prone to errors." [@ghosh2011software]

> "Ontologies define the relationships and hierarchies between different terms
and allow the unique, semantic annotation of data." [@ghosh2011software]

> "There are several international and national bodies, such as OMG, W3C, IEEE,
ANSI, and IETF... that formally approve standards or provide a framework for
standards development. Although some of the systems biology standards have been
certified (for example, SBML is officially adapted by IETF), in general in life
sciences this procedure is not particularly important---many of the most
successful standards such as GO have not undergone any official approval
procedure, but instead have become *de facto* standards. In fact, many of the
most successful standards in other domains are *de facto* standards."
[@brazma2006standards]

> "There are four steps involved in developing a complete and self-contained
standard: conceptual model design, model formalization, development of a data
exchange format, and implementation of the supporting tools."
[@brazma2006standards]

> "Two competing goals should be balanced when developing the conceptual model
of a domain: domain specificity and the need to find the common ground for all
related applications. Arguably, the most useful standards are those that consist
of the minimum number of most informative parameters. Keeping the list short
makes it simple and practicable, while selecting the most informative features
ensures accuracy and efficiency. The need for minimalism is reflected by the
titles for many such standards---Minimum Information About XYZ."

> "For a standard to be successful, laboratory information management systems
(LIMS), databases and data analysis, and modeling tools shoudl comply with it.
One way of fostering this is to develop easy-to-use 'middleware'---software
components that hide the technical complexities of the standard and facilitate
manipulation of the standard format in an easy way." [@brazma2006standards]

> "**Semantics**: The meaning of something; in computer science, it is usually
used in opposition to syntax (that is, format)." [@brazma2006standards]

> "Excel is by far the most common spreadsheet software, but many other
spreaadsheet programs exist, notably the Open Office Calc software, which is an
open source alternative and stores spreadsheets in an XML-based open standard
format called Open Document Format (ODF). This allows the data to be accessed by
a wider variety of software. However, even ODF is not ideal is a storage format
for aa research data set because spreadsheet formats contain not only the data
that is stored in the spreadsheet, but also information about how to display the
data, such as fonts and colors, borders and shading." [@murrell2009introduction]

> "Spreadsheet software is useful because it displays a data set in a nice
rectangular grid of cells. The arrangement of cells into distinct columns shares
the same benefits as fixed-width format text files: it makes it very easy for a
human to view and navigate within the data. ... because most spreadsheet
software provides facilities to import a data set from a wide variety of
formats, these benefits of the spreadsheet *software* can be enjoyed without
having to suffer the negatives of using a spreadsheet *format* for data storage.
For example, it is possible to store the data set in a CSV format and use
spreadsheet software to view or explore the data." [@murrell2009introduction]

> "Thinking about what sorts of questions will be asked of the data is a good
way to guide the design of data storage." [@murrell2009introduction]

First, you can still use spreadsheets, but reduce their use to recording data, 
leaving all data cleaning and analysis to be handled with other software. To make 
it easier to collaborate with statisticians and to interface with a program like 
R for data cleaning and analysis, it will be easiest if you set up your data
recording to include with other statistical programs like R or Python. These
steps are described in a later section, "...".

- Each sheet of the spreadsheet should contain data from a single
experiment.
- Never use whitespace to represent a meaningful separation in data within 
a spreadsheet. Never include multiple tables of data in the same sheet. 
- The first row of the spreadsheet should include a short column name for each
column with data. All column name information should be within a single row
(i.e., avoid subheadings). Avoid any special characters (e.g., "%") in column
names. Instead, use only letters, numbers, and underscores ("_"), and start with
a letter. It is especially helpful if you can avoid spaces in column names.
- Missing data should be represented consistently in cells. "NA" is one choice.
If you want to clarify why data is missing, it's much better to add a column
(e.g., "why_missing") where you can provide those details in text, rather than
combining within a single column numerical observation data with textual reasons
for missingness in cells with missing values.

Next, you could record data using a statistical language like R. There is an
excellent Integrated Development Environment for R called RStudio, and it
creates a much clearer interface with R compared to running R from a commond
line, particularly for new users. RStudio allows you to open delimited plain
text files, like csvs, using a grid-style interface. This grid-style interface
looks very similar to a spreadsheet, but lacks the ability to include formulas
or macros. Therefore, this format enforces a separation of the recording of raw
data from the cleaning and analysis of the data.

[R Project templates]

Data cleaning and analysis can then be shifted away from the files used to 
record the data and into reproducible scripts. These scripts can be clearly 
documented, either through comments in the code or through open source 
documentation tools like RMarkdown than interweave code and text in a way that
allows the creation of documents that are easier to read than commented code. 

This documentation should explain why each step is being done. In cases where 
it is not immediately evident from the code *how* the step is being done, this
should be documented as well. Any assumptions being used should be clarified in 
the documentation.

> "When we have only the letters, digits, special symbols, and punctuation marks
that appear on a standard (US) English keyboard, then we can use a single byte
to store each character. This is called an ASCII encoding (American Standard
Code for Information Exchange). ... UNICODE is an attempt to allow computers to
work with all of the characters in all of the languages of the world. Every
character has its own number, called a 'code point', often written in the form
U+xxxxxx, where every x is a hexidecimal digit. ... There are two main
'encodings' that are used to store a UNICODE code point in memory. UTF-16 always
uses two bytes per character of text and UTF-8 uses one or more bytes, depending
on which characters are stored. If the text is only ASCII, UTF-8 will only use
one byte per character. ... This encoding is another example of additional
information that may have to be provided by a human before the computer can read
data correctly from a plain text file, although many software packages will cope
with different encodings automatically." [@murrell2009introduction]

> "A PDF document is primarily a description of how to *display* information.
Any data values within a PDF document will be hopelessly entwined with
information about how the data values should be displayed."
[@murrell2009introduction]

> "Another major weakness of free-form text files is the lack of information
*within the file itself* about the structure of the file. For example, plain
text files do not contain information about which special character is being
used to separate fileds in a delimited file, or any information about the widths
of fields within a fixed-width format. This means that the computer cannot
automatically determine where different fields are within each row of a plain
text file, or even how many fields there are. A fixed-width format avoids this
problem, but enforcing a fixed length for fields can create other difficulties
if we do not know the maximum possible length for all variables. Also, if the
values for a variable can have very different lengths, a fixed-width format can
be inefficient because we store lots of empty space for short values. The
simplicity of plain text files make it easy for a computer to read a file as a
series of characters, but the computer cannot easily distinguish individual data
values from the series of characters. Even worse, the computer has no way of
tellins what sort of data is stored in each field. ... In practice, humans must
suppy additional information about a plain text file before a computer can
successfully determine where the different fields are within a plain text file
*and* what sort of data is stored in each field." [@murrell2009introduction]

> "In bioinformatics, the plain-text data we work with is often encoded in
*ASCII*. ASCII is a character encoding scheme that uses 7 bits to represent 128
different values, including letters (upper- and lowercase), numbers, and special
nonvisible characters. While ASCII only uses 7 bits, nowadays computers use an
8-bit *byte* (a unit representing 8 bits) to store ASCII characters. ... Because
plain-text data uses characters to encode information, our encoding scheme
matters. When working with a plain-text file, 98% of the time you won't have to
worry about the details of ASCII and how your file is encoded. However, the 2%
of the time when encoding data does matter---usually when an invisible non-ASCII
character has entered the data---it can lead to major headaches."
[@buffalo2015bioinformatics]

> "Programs [in Unix] retreive the data in a file by a system call (a subroutine
in the kernel) called `read`. Each time `read` is called, it returns the next
part of a file---the next line of text typed on the terminal, for example.
`read` also says how many bytes of the file were returned, so end of file is
assumed when a `read` says 'zero bytes are being returned'. If there were any
bytes left, `read` would have returned some of them." [@kernighan1984unix]

> "The format of a file is determined by the programs that use it; there is a
wide variety of file types, perhaps because there is a wide variety of programs.
But since file types are not determined by the file system, the kernel can't
tell you the type of a file: it doesn't know it. The `file` command makes an
educated guess ... To determine the types, `file` doesn't pay attention to the
names (although it could have), because naming conventions are just conventions,
and thus not perfectly reliable. For example, files suffixed '.c' are almost
always C source, but there is nothing to prevent you from creating a '.c' file
with arbitrary contents. Instead, `file` reads the first hundred bytes of a file
and looks for clues to that file type. ... In Unix systems there is just one
kind of file, and all that is required to access a file is its name. The lack of
file formats is an advantage overall---programmers don't need to worry about
file types, and all the standard programs will work on any file."
[@kernighan1984unix]

> "The Unix file system is organized so you can maintain your own personal files
without interfering with files belonging to other people, and keep people from
interfering with you too." [@kernighan1984unix]

> "The clinical patient health record is a longitudinal administrative record of
an individual's health information: all the data related to an individual's or a
population's health. The health record is a set of nonstandardized data that
spans multiple levels of aggregation, from a single measurement element (blood
pressure) to collections of diagnoses and related clinical observations. This
complexity is compounded by the high degree of human interaction involved in the
productio of clinical records, including self-reported data, medical diagnosis,
and other patient information." [@keller2017evolution]

> "Sharing data through repositories enhances both the quality and the value of
the data through standardized processes for curation, analysis, and quality
control. By allowing broad access to data, these repositories encourage and
support the use of previously collected data to test and extend previous
results. Data repositories are quite common in science fields such as astronomy,
genomics, and earth sciences. ... These repositories have accelerated discovery
by expanding the reach of these data to scientists who are not involved in the
initial data collection and experiments. Repositories address challenges that
affect data quality through governance, interoperability across systems, and
costs." [@keller2017evolution]

> One example of a repository is "the sharing of cDNA microarray data through
research consortia, which has led to a common set of standards and relatively
homogeneous data classes. There are many issues with the sharing of these data,
which requires the transformation of biologic to numeric data. These issues may
include loss of context, such as laboratory processes followed, and therefore
lack of information about the quality of the data when they are transformed. To
avoid this loss of information, the consortium ensures that documentation is
comprehensive so that other researchers can assess the quality of the data and
make comparisons with other studies using the same data. This documentation also
include information on when incorrect assignments of sequence identity are made
so that errors are not perpetuated in other studies." [@keller2017evolution]


> "**Data exchange format:** A file or message format that is formally defined
so that software can be built that 'knows' where to find various pieces of
information." [@brazma2006standards]

When a well-defined file format is used for saving data, the advantages can be huge. 
In the case of the FCS file format, the use of this common format means that people have
been able to create both open-source and proprietory software that inputs raw 
flow cytometry data regardless of the laboratory that collected it and of the 
manufacturor of the flow cytometer used for data collection (as long as that manufacturor
output the data in the FCS file format).


> "Everything in the Unix system is a file. That is less of an
oversimplification than you might think. When the first version of the system
was designed, before it even had a name, the discussions focused on the
structure of a file system that would be clean and easy to use. The file system
is central to the success and convenience of the Unix system. It is one of the
best examples of the 'keep it simple' philosophy, showing the power achieved by
careful implementation of a few well-chosen ideas." [@kernighan1984unix]

> "A file is a sequence of bytes. (A byte is aa small chunk of information,
typically 8 bits long. For our purposes, a byte is equivalent to a character.)
No structure is imposed on a file by the system, and no meaning is attaached to
its contents---the meaning of the bytes depends solely on the programs that
interpret the file. Furthermore, ... this is true not just of disc files but of
peripheral devices as well. Magnetic tapes, mail messages, characters typed on
the keyboard, line printer output, data flowing in pipes---each of these is just
a sequence of bytes as far as the systems and the programs in it are concerned."
[@kernighan1984unix]

> "The Comma-Separated Value (CSV) format is a special case of a plain text
format. Although not a formal standard, CSV files are very common and are a
quite reliable plain text delimited format that at least solves the problem of
where the fields are in each row of the file. The main rules for the CSV format
are: (1) **Comma-delimited**: Each field is separated by a comma (i.e., the
character , is the delimiter).; (2) **Double-quotes are special**: Fields
containing commas must be surrounded by double-quotes ... . (3) **Double-quote
escape sequence:** Fields containing double quotes must be surrounded by
double-quotes *and* each embedded double-quote must be represented using two
double quotes ... .; (4) **Header information**: There can be a single header
containing the names of the fields." [@murrell2009introduction]

> "A data file metaformat is a set of syntactic and lexical conventions that is
either formally standardized or sufficiently well established by practice that
there are standard service libraries to handle marshaling and unmarshaling it.
Unix has evolved or adopted metaformats suitable for a wide range of
applications [including delimiter-separated values and XML]. It is good practice
to use one of these (rather than an idiosyncratic custom format) whenever
possible. The benefits begin with the amount of custom paarsing and generation
code that you may be able to avoid writing by using a service library. But the
most important benefit is that developers and even many users will instantly
recognize these formats and feel comfortable with them, which reduces the
friction costs of learning new programs." [@raymond2003art]

> "There are three flavors you will encounter: tab-delimited, comma-separated, and
variable space-delimited. Of these three formats, tab-delimited is the most
commonly used in bioinformatics. File formats such as BED, GTF/GFF, SAM, tabular
BLAST output, and VCF are all examples of tab-delimited files. Columns of a
tab-delimited file are separated by a single tab character (which has the escape
code \t). A common convention (but not a standard) is to include metadata on the
first few lines of a tab-delimited file. These metadata lines begin with # to
differentiate them from the tabular dataa records. Because tab-delimated files
use a tab to delimit columns, tabs in data are not allowed. Comma-separated
values (CSV) is another common format. CSV is similar to tab-delimited, except
the delimiter is a comma character. While not a common in bioinformatics, it is
possible that the data stored in CSV format contain commas (which would
interfere with the ability to parse it). Some variants just don't allow this,
while others use quotes around entries that could contain commas. Unfortunately,
there's no standard CSV format that defines how to handle this and many other
issues with CSV---though some guidelines are given in RFC 4180. Lastly, there
are space-delimited formats. A few stubborn bioinformatics programs use a
variable number of spaces to separate columns. In general, tab-delimited formats
and CSV are better choices than space-delimited formats because it's quite
common to encounter data containing spaces." [@buffalo2015bioinformatics]

> "There are long-standing Unix traditions aabout how textual data formats ought
to look. Most of these derive from one or more of the standard Unix metaformats
... just described [e.g., DSV, XML]. It is wise to follow these conventions
unless you have strong and specific reasons to do otherwise. ... (1) *One record
per newline-terminated line, if possible.* This makes it easy to extract records
with text-stream tools. For data interchange with other operating systems, it's
wise to make your file-format parser indifferent to whether the line ending is
LF or CR-LF. It's also conventional to ignore trailing whitespace in such
formats; this protects against common editor bobbles. (2) *Less than 80
characters per line if possible.* This makes the format browseable in an
ordinary-sized terminal window. If many records must be longer than 80
characters, consider a stanza format... (3) *Use # as an introducer for
comments.* It's good to have a way to embed annotations and comments in data
files. It's best if they're actually part of the file structure, and so will be
preserved by tools that know its format. For comments that are not preserved
during parsing, # is the conventional start character. (4) *Support the
backslash convention.* The least surprising way to support nonprintable control
characters is by parsing C-like backslash escapes ... " [@raymond2003art]




> "You can take apart these formats and find out which decisions were made to
create them ... even old Microsoft Word, which in a long and painful political
bottle, finally settled down and 'opened' its format, countless hundres of pages
of documentation defining how words apper, how tables of contents are
registered, how all of the things that make up a Word document are to be
represented. The Microsoft Office File Formats specifications are of a most
disturbing, fascinating quality: one can read through them and think: *Yes, I
see this, I think I understand. But why?* ... Even Word is opened now, just
regular XML. Strange XML to be sure. All the codes once hidden are revealed."
[@ford2015on]


> "We wish to draw a distinction between data that is machine-actionable as a
result of specific investment in software supporting that data-type, for
example, bespoke parsers that understand life science wwPDB files or space
science Space Physics Archive Search and Extract (SPASE) files, and data that is
machine-actionable exclusively through the utilization of general-purpose, open
technologies. To reiterate the earlier point---ultimate machine actionability
occurs when a machine can make a useful decision regarding data that it has not
encountered before. This distinction is important when considering both (a) the
rapidly growing and evolving data environment, with new technologies and new,
more complex data-types continuously being developed, and (b) the growth of
general-purpose repositories, where the data-types encounted by an agent are
unpredictable. Creating bespoke parsers, in all computer languages, is not a
sustainable activity." [@wilkinson2016fair]

> "[One] way data can come from the Internet is through a web API, which stands
for *application programming interface*. The number of APIs that are being
offered by organizations is growing at an ever increasing rate... Web APIs are
not meant to be presented in a nice layout, such as websites. Instead, most web
APIs return data in a structured format, such as JSON or XML. Having data in a
structured format has the advantage that the data can be easily processed by
other tools." [@janssens2014data]


> "The *pileup format* [is] a plain-text format that summarizes reads' bases at
each chromosome position by stacking or 'piling up' aligned reads."
[@buffalo2015bioinformatics]


> "Data compression, the process of condensing data so that it takes up less
space (on disk drives, in memory, or across network transfers), is an
indespensible technology in modern bioinformatics. For example, sequences from a
recent Illumina HiSeq run when compressed with Gzip take up 21,408,674,240
bytes, which is a bit under 20 gigabytes. Uncompressed, this file is a whopping
63,203,414,514 bytes (around 58 gigabytes). This FASTQ file has 150 million
200bp reads, which is 10x coverage of the hexaploid wheat genome. The
compression ratio (uncompressed size/ compressed size) of this data is
approximately 2.95, which translates to a significant space saving of about 66%.
Your own bioinformatics projects will likely contain much more data, especially
as sequencing costs continue to drop and it's possible to sequence genomes to
higher depth, include more biological replicates or time points in expression
studies, or sequence more individuals in genotyping studies. For the most part,
data can remain compressed on the disk throughout processing and analysis. Most
well-writted bioinformatics tools can work natively with compressed data as
input, without requiring us to decompress it to disk first. Using pipes and
redirection, we can stream compressed data and write compressed files directly
to the disk. Additionally, common Unix tools like *cat*, *grep*, and *less* all
have variants that work with compressed data, and Python's *gzip* module allows
us to read and write compressed data from within Python. So while working with
large datasets in bioinformatics can be challenging, using the compression tools
in Unix and software libraries make our lives much easier."
[@buffalo2015bioinformatics]

> "Non-text files definitely have their place. For example, very laarge
databases usually need extra address information for rapid access; this has to
be binary for efficiency. But every file format that is not text must have its
own family of support programs to do things that the standard tools could
perform if the format were text. Text files may be a little less efficient in
maachine cycles, but this must be balanced against the cost of extra software to
maintain more specialized formats. If you design a file format, you should think
carefully before choosing a non-textual representation." [@kernighan1984unix]


> "*Out-of-memory approaches* [are] computational strategies built arouond
storing and working with data kept out of memory on the disk. Reading data from
a disk is much, much slower than working with data in memory... but in many
cases this is the approach we have to take when in-memory (e.g., loading the
entire dataset into R) or streaming approaches (e.g., using Unix pipes ...)
aren't appropriate." [@buffalo2015bioinformatics]

> "In general, it is possible to jump directly to a specific location within a
binary format file, whereas it is necessary to read a text-based format from the
beginning and one character at a time. This feature of accessing binary formats
is called **random access** and it is generlaly faster than the typically
**sequential access** of text files." [@murrell2009introduction]

> "We often need fast read-only access to data linked to a genomic location or
range. For the scale of data we encounter in genomics, retrieving this type of
data is not trivial for a few reasons. First the data might not fit entirely in
memory, requiring an approach where data is kept out of memory (in other words,
on a slow disk). Second, even powerful relational database systems can be
sluggish when querying out millions of entries that overlap a specific
region---an incrediably common operation in genomics. [BGZF and Tabix] are
specifically designed to get around these limitations, allowing fast
random-access of tab-delimited genome position data."
[@buffalo2015bioinformatics]

> "Samtools now supports (after version 1) a new, highly compressed file format
known as *CRAM*. Compressing alignments with CRAM can lead to a 10%--30%
filesize reduction compared to BAM (and quite remarkably, with no significant
increase in compression or decompression time compared to BAM). CRAM is a
*reference-based* compression scheme, meaning only the aligned sequence that's
different from the reference sequence is recorded. This greatly reduces file
size, as many sequence may align with minimal difference from the reference. As
a consequence of this reference-based approach, it is imperative that the
reference is available and does not change, as this would lead to a loss of data
kept in the CRAM format. Because the reference is so important, CRAM files
contain an MD5 checksum of the reference file to ensure it has not changed. CRAM
also has support for multiple different *lossy compression* methods. Lossy
compression entails some information about an alignment and the original read is
lost. For example, it's possible to bin base quality scores using a lower
resolution binning scheme to reduce the filesize." [@buffalo2015bioinformatics]




> "Very often we need efficient random access to subsequences of a FASTA file
(given regions). At first glance, writing a script to do this doesn't seem
difficult. We could, for example, write a script that iterates through FASTA
entries, extracting sequences that overlaps the range specified. However, this
is not an efficient method when extracting a few *random* subsequences. To see
why, consider accessing the sequence from position chromosome 8 (123,407,082 to
123,419,742) from the mouse genome. This approach would needlessly parse and
load chromosomes 1 through 7 into memory, even though we don't need to extract
subsequences from these chromosomes. Reading entire chromosomes from disk and
copying them into memory can be quite inefficient---we would have to load all
125 megabytes of chromosome 8 to extract 3.6kb! Extracting numerous random
subsequences from a FASTA file can be quite computationaally costly. A common
computational strategy that allows for easy and fast random access is *indexing*
the file. Indexed files are ubiquitous in bioinformatics."
[@buffalo2015bioinformatics]

> "We can avoid needlessly reading the entire file off of the disk by using an
index that points to where certian blocks are in the file. In the case of our
FASTA file, the index essentially stores the location of where each sequence
begins in the file (as well as other necessary information). When we look up a
range like chromosome 8 (123,407,082--123,410,744), *samtools faidx* uses the
information in the index to quickly calculate exactly where in the file those
bases are. Then, using an operation called a file *seek*, the program jumps to
this exact position (called the *offset*) in the file and starts reading the
sequence. Having precomputed file offsets combined with the ability to jump to
those exact positions is what makes accessing sections of an indexed file fast."
[@buffalo2015bioinformatics]


> "The data revolution within the biological and physical science world is
generating massive amounts of data from ... a wide range of ... projects, such
as those undertaken at the Large Hadron Collider and
genomics-proteomics-metabolomics research." [@keller2017evolution]


> "Community standards for data description and exchange are crucial. These
facilitate data reuse by making it easier to import, export, compare, combine,
and understand data. Standards also eliminate the need for the data creator to
develop unique descriptive practices. They open the door to development of
disciplinary repositories for specific classes of data and specialized software
management tools. GenBank, the US NIH genetic sequence database, and the US
National Virtual Observatory are good examples of what is possible here. In
2007, the US National Science Foundation, recognizing the importance of such
standards, established the Community Based Data Interoperability Networks
(INTEROP) funding programme for the development of tools, standards, and data
management best practices within specific disciplinary communities. ... Although
many classes of scientific data aren't ready, or aren't appropriate, for
standardization, well chosen investments in standardization show a consistently
high pay-off." [@lynch2008big]


> "For certain types of important digital objects, there are well-curated,
deeply-integrated, special-purpose repositories such as Genbank, Worldwide
Protein Data Bank, and UniProt... However, not all datasets or even data types
can be captured by, or submitted to, these repositories. Many important datasets
emerging from traditional, low-throughput bench science don't fit in the data
models of these special-purpose repositories, yet these datasets are no less
important with respect to integrative research, reproducibility, and reuse in
general. Apparently in response to this, we see the emergence of numerous
general-purpose data repositories [e.g., FigShare, Mendeley]. ... Such
repositories accept a wide range of data types in a wide range of formats,
generally do not attempt to integrate or harmonize the distributed data, and
place few restrictions (or requirements) on the descriptors of the data
deposition. The resulting data ecosystem, therefore, appears to be moving away
from centralization, is becoming more diverse, and less integrated, thereby
exacerbating the discovery and re-usability problem for both human and
computational stakeholders." [@wilkinson2016fair]

> "It would be unwise to bet that these formats [SAM/BAM files] won't change (or
even be replaced at some point)---the field of bioinformatics is notorious for
inventing new data formats (the same goes with computing in general) ... So
learning how to work with specific bioinformatics formats may seem like a lost
cause, skills such as following a format specification, manipulating binary
files, extracting information from bitflags, and working with application
programming interfaces (API) are essential skills when working with any format."
[@buffalo2015bioinformatics]

> From a working group on bioinformatics and data-intensive science: "Many simple
analyses are not automated because data formats are a moving target. ... The
community has been slow to share tools, partially because tools are not robust
against different input formats." [@barga2011bioinformatics]

> "Different centres generate data in different formats, and some analysis tools
require data to be in particular formats or require different types of data to
be linked together. Thus, time is wasted reformatting and reintegrating data
multiple times during a single analysis. For example, next-generation sequencing
companies do not deliver raw sequencing data in a format common to all
platforms, as there is no industry-wide standard beyond simple text files that
include the nucleotide sequence and the corresponding quality values. As a
result, carrying out sequencing analyses across different platforms requires
tools to be adapted to specific platforms. It is therefore crucial to develop
interoperable sets of analysis tools that can be run on different computational
platforms depending on which is best suited for a given application, and then
stitch those tools together to form analysis pipelines."
[@schadt2010computational]

> "Many important datasets emerging from traditional, low-throughput bench
science don't fit in the data models of ... special-purpose repositories [like
Genbank, Worldwide Protein Data Bank, and UniProt], yet these datasets are no
less important with respect to integrative research, reproducibility, and reuse
in general. Apparently in response to this, we see the emergence of numerous
general-purpose data repositories [e.g., FigShare, Mendeley]. ... Such
repositories accept a wide range of data types in a wide range of formats,
generally do not attempt to integrate or harmonize the distributed data, and
place few restrictions (or requirements) on the descriptors of the data
deposition. The resulting data ecosystem, therefore, appears to be moving away
from centralization, is becoming more diverse, and less integrated, thereby
exacerbating the discovery and re-usability problem for both human and
computational stakeholders." [@wilkinson2016fair]

> "Simplicity, but not oversimplification, is the key to success [in developing
standards]." [@brazma2006standards]


> "Minimum reporting guidelines, terminologies, and formats (hereafter referred to 
as reporting standards) are increasingly used in the structuring and curation of
datasets, enabling data sharing to varying degrees. However, the mountain of 
frameworks needed to support data sharing between communities inhibits the 
development of tools for data management, reuse and integration. ... The same
framework [on the other hand] enables researchers, bioinformaticians, and data
managers to operate within an open data commons." [@sansone2012toward]


> "'One of the core issues of Bioinformatics is dealing with a profusion of (often poorly
defined or ambiguous) file formats. Some *ad hoc* simple human readable formats have
over time attained the status of de facto standards.'-- Peter Cock et al. (2010)"
[@buffalo2015bioinformatics]

> "Developing and using a standard is often an investment that will not pay off
immediately, therefore there is a much better chance of success if the user community
decides that the respective standard is needed." [@brazma2006standards]

For high-throughput data recording, including output from equipment like ..., the
use of data standards is in some cases enforced by equiment manufacturers, and 
research scientists are able to develop tools that address the specific file
formats imposed by the equipment.

> "Although standardization is not a goal in itself, its importance is growing
in a high-throughput era. This is similar to what happened to manufacturing
during industrialization. The data from high-throughput technologies are being
generated at a rate that makes managing and using these data sets impossible on
a case-by-case basis. Although some of the data generated by the newest
technologies might have a low signal-to-noise ratio to make data re-usable, the
data quality is improving as the technology matures, and it is a waste of
resources not to share and re-use these expensive datasets. However, this is
only possible if the instrumentation that generates these data, laboratory-based
storage information management systems and databases, data analysis tools, and
systems modeling software can talk to each other easily. This is the purpose of
standardization." [@brazma2006standards]

> "A standard is successful only if it is used, and it is important to ensure
that supporting software tools are designed and implemented."
[@brazma2006standards]



> "In the late 2000s, there arose the 'NoSQL movement', coalescing around a
collective desire of many programmers to move beyond the strictures of the
relational model and unshackle themeselves from SQL. *Our data varied and
diverse,* they said, even if programmers weren't that varied and diverse, *and
we are tired of pretending that one technology will address the need for speed.*
Dozens of new databases appeared, each with different merits. There were
key-value databases, like Kyoto Cabinet, which optimized for speed of retrieval.
There were search-engine libraries, like Apache Lucene, which made it relatively
eaasy to search through enormous corpora of text---your own Google. There was
Mongo DB, which allowed for 'documents', big arbitrary blobs of dataa, to be
stored without nice rows and consistent structure. People debated, and continue
to debate, the value of each. ... There is as yet no absolute challenger to the
relationship model. When people think *database*, they still think *SQL*."
[@ford2015i]


> "By information or data communication standard we mean a convention on how to
encode data or information about a particular domain (such as gene function)
that enables unambiguous transfer and interpretation of this information or
data." [@brazma2006standards]

> "The proper acquisition and handling of data is crucially important for both
the generation and verification of hypotheses. The rapid development of
high-throughput experimental techniques is transforming life-science research
into 'big data' science, and although numerous data-management systems exist,
the heterogeneity of formats, identifiers, and data schema pose serious
challenges. In this context, data-management systems need standardized formats
for data exchange, globally unique identifiers for data mapping, and common
interfaces that allow the integration of disparate software tools in a
computational workflow." [@ghosh2011software]

> "Data quality [for health registries data] is driven by multiple dimensions
such as clinical data standardization, the existence of common definitions of
data fields, and the validity of self-reported patient conditions and outcomes.
Recognized issues include the definitions of data fields and their relational
structure, the training of personnel related to data collection data processing
issues (data cleaning), and curation." [@keller2017evolution]

--->

</div>
</div>
<p style="text-align: center;">
<a href="2.1-module1.html"><button class="btn btn-default">Previous</button></a>
<a href="2.3-module3.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
