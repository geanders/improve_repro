<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="2.2 Principles and power of structured data formats | Improving the Reproducibility of Experimental Data Recording and Pre-Processing" />
<meta property="og:type" content="book" />


<meta property="og:description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing." />
<meta name="github-repo" content="rstudio/bookdown-demo" />

<meta name="author" content="Brooke Anderson, Michael Lyons, Mercedes Gonzalez-Juarrero, Marcela Henao-Tamayo, and Gregory Robertson" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing.">

<title>2.2 Principles and power of structured data formats | Improving the Reproducibility of Experimental Data Recording and Pre-Processing</title>

<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#rigor-and-reproducibility-in-computation"><span class="toc-section-number">1</span> Rigor and reproducibility in computation</a></li>
<li><a href="2-experimental-data-recording.html#experimental-data-recording"><span class="toc-section-number">2</span> Experimental Data Recording</a></li>
<li><a href="3-experimental-data-preprocessing.html#experimental-data-preprocessing"><span class="toc-section-number">3</span> Experimental Data Preprocessing</a></li>
<li><a href="4-references.html#references"><span class="toc-section-number">4</span> References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="module2" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Principles and power of structured data formats</h2>
<p>The format in which experimental data is recorded can have a large influence on
how easy and likely it is to implement reproducibility tools in later stages of
the research workflow. Recording data in a “structured” format brings many
benefits. In this module, we will explain what makes a dataset “structured” and
why this format is a powerful tool for reproducible research.</p>
<p>Every extra step of data cleaning is another chance to introduce errors in
experimental biomedical data, and yet laboratory-based researchers often share
experimental data with collaborators in a format that requires extensive
additional cleaning before it can be input into data analysis
<span class="citation">(Broman and Woo 2018)</span>. Recording data in a “structured” format brings many
benefits for later stages of the research process, especially in terms of
improving reproducibility and reducing the probability of errors in analysis
<span class="citation">(Ellis and Leek 2018)</span>. Data that is in a structured, tabular, two-dimensional
format is substantially easier for collaborators to understand and work with,
without additional data formatting <span class="citation">(Broman and Woo 2018)</span>. Further, by using a
consistent structured format across many or all data in a research project, it
becomes much easier to create solid, well-tested code scripts for data
pre-processing and analysis and to apply those scripts consistently and
reproducibly across datasets from multiple experiments <span class="citation">(Broman and Woo 2018)</span>.
However, many biomedical researchers are unaware of this simple yet powerful
strategy in data recording and how it can improve the efficiency and
effectiveness of collaborations <span class="citation">(Ellis and Leek 2018)</span>.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>List the characteristics of a structured data format</li>
<li>Describe benefits for research transparency and reproducibility</li>
<li>Outline other benefits of using a structured format when recording data</li>
</ul>
<div id="data-recording-standards" class="section level3" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Data recording standards</h3>
<p>For many areas of biological data, there has been a push to create standards for
how data is recorded and communicated. Standards can clarify both the <em>content</em>
that should be included in a dataset, the <em>format</em> in which that content is
stored, and the <em>vocabulary</em> used within this data. One article names these three
facets of a data standard as the <strong>minimum information</strong>, <strong>file formats</strong>, and
<strong>ontologies</strong> <span class="citation">(Ghosh et al. 2011)</span>.</p>
<p><label for="tufte-mn-1" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-1" class="margin-toggle"><span class="marginnote"><span style="display: block;">“It is important to distinguish between standards that specify how to actually do experiments and standards that specify how to describe experiments. Recommendations such as what standard reporters (probes) should be printed on microarrays or what quality control steps should be used in an experiment belong to the first category. Here we focus on the standards that specify how to describe and communicate data and information.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span></span></span></span></p>
<p>Many people and organizations (including funders) are excited about the idea
of developing and using data standards, especially at the community level.
Good standards, that are widely adapted by researchers, can help in making
sure that data submitted to data repositories are used widely and that software
can be developed that is <em>interoperable</em> with data from many research group’s
experiments. There are also many advantages, if there are not community-level
standards for recording a certain type of data, to develop and use local
data standards for recording data from your own experiments. This section
describes the elements that go into a data standard, discusses some choices
to be made when definining a data standard (especially choices on data
structure and file formats), and some of the advantages and disadvantages
of developing and using data recording standards at both the research group
and community levels.</p>
<p><label for="tufte-mn-2" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-2" class="margin-toggle"><span class="marginnote"><span style="display: block;"><strong>On the root causes for irreproducibility in biomedical research:</strong> “First, a lack of standards for data generation leads to problems with the comparability and integration of data sets.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Waltemath and Wolkenhauer 2016)</span></span></span></span></p>
<p><strong>Ontology standards.</strong></p>
<p>Although it has the most complex name, an <em>ontology</em>
(sometimes called a <em>terminology</em> <span class="citation">(Sansone et al. 2012)</span>) might be the easiest and
quickest to adapt in recording data. An ontology helps define a vocabulary that
is controlled and consistent to use that researchers can use to refer to
concepts and concrete things within an area of research. It helps researchers,
when they want to talk about an idea or thing, to use one word, and just one
word, and to ensure that it will be the same word used by other researchers when
they refer to that idea or thing. Ontologies also help to define the relationships
between ideas or concrete things in a research area <span class="citation">(Ghosh et al. 2011)</span>, but
here we’ll focus on their use in provided a consistent vocabulary to use when
recording data.</p>
<p>For example, when recording a dataset, what do you call a small mammal that is
often kept as a pet and that has four legs and whiskers and purrs? Do you record
this as “cat” or “feline” or maybe, depending on the animal, even “tabby” or
“tom” or “kitten?” Similarly, do you record tuberculosis as “tuberculosis” or
“TB” or or maybe even “consumption?” If you do not use the same word
consistently in a dataset to record an idea, then while a human might be able to
understand that two words should be considered equivalent, a computer will not
be able to immediately tell that “TB” should be treated equivalently to
“tuberculosis.”</p>
<p>At a larger scale, if a research community can adapt an ontology they agree to
use throughout their studies, it will make it easier to understand and integrate
datasets produced by different research laboratories. If every research group
uses the term “cat,” then code can easily be written to extract and combine
all data recorded for cats across a large repository of experimental data.
On the other hand, if different terms are used, then it might be necessary to
first create a list of all terms used in datasets in the respository, then pick
through that list to find any terms that are exchangeable with “cat,” then write
script to pull data with any of those terms.</p>
<p>Several onotologies already exist or are being created for biological and other
biomedical research <span class="citation">(Ghosh et al. 2011)</span>. For biomedical science,
practice, and research, the BioPortal website
(<a href="http://bioportal.bioontology.org/" class="uri">http://bioportal.bioontology.org/</a>) provides access to almost 800 ontologies,
including several versions of the International Classification of Diseases, the
Medical Subject Headings (MESH), the National Cancer Institute Thesaurus, the
Orphanet Rare Disease Ontology and the National Center for Biotechnology
Information (NCBI) Organismal Classification. For each ontology in the BioPortal
website, the website provides a link for downloading the ontology in several
formats. If you download the ontology using the “CSV” format, you can open it in your
favorite spreadsheet program and explore how it defines specific terms to use
for each idea or thing you might need to discuss within that topic area, as well
as synonyms for some of the terms. To use an ontology when recording your own
data, just make sure you use the ontology’s suggested terms in your data. For
example, if you’d like to use the Ontology for Biomedical Investigations
(<a href="http://bioportal.bioontology.org/ontologies/OBI" class="uri">http://bioportal.bioontology.org/ontologies/OBI</a>) and you are recording how many
children a woman has had who were born alive, you should name that column of the
data “number of live births,” not “# live births” or “live births (N)” or
anything else. Other collections of ontologies exist for fields of scientific
research, including the Open Biological and Biomedical Ontology (OBO) Foundry
(<a href="http://www.obofoundry.org/" class="uri">http://www.obofoundry.org/</a>).</p>
<p>If there are community-wide ontologies in your field, it is worthwhile to use
them in recording experimental data in your research group. Even better is to
not only consistently use the defined terms, but also to follow any conventions
with capitalization. While most statistical programs provide tools to change
capitalization (for example, to change all letters in a character string to
lower case), this process does require an extra step of data cleaning and an
extra chance for confusion or for errors to be introduced into data.</p>
<p><strong>Minimum information standards.</strong> The next easiest facet of a data standard to
bring into data recording in a research group is <em>minimum information</em>. Within a
data recording standard, <em>minimum information</em> (sometimes also called <em>minimum
reporting guidelines</em> <span class="citation">(Sansone et al. 2012)</span> or <em>reporting requirements</em>
<span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span>) specify <em>what</em> should be included in a dataset
<span class="citation">(Ghosh et al. 2011)</span>. Using minimum information standards help ensure that data
within a laboratory, or data posted to a repository, contain a number of
required elements. This makes it easier to re-use the data, either to compare it
to data that a lab has newly generated, or to combine several posted datasets to
aggregate them for a new, integrated analysis, considerations that are growing
in importance with the increasing prevalence of research repositories and
research consortia in many fields of biomedical science <span class="citation">(Keller et al. 2017)</span>.</p>
<p><label for="tufte-mn-3" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-3" class="margin-toggle"><span class="marginnote"><span style="display: block;">“Minimum information is a checklist of required supporting information for datasets from different experiments. Examples include: Minimum Information About a Microarray Experiment (MIAME), Minimum Information About a Proteomic Experiment (MIAPE), and the Minimum Information for Biological and Biomedical Investigations (MIBBI) project.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Ghosh et al. 2011)</span></span></span></span></p>
<p><em>Standardized file formats.</em> While using a standard ontology and a standard for
minimum information is a helpful start, it just means that each dataset has the
required elements <em>somewhere</em>, and using a consistent vocabulary—it doesn’t
specify where those elements are in the data or that they’ll be in the same
place in every dataset that meets those standards. As a result, datasets that all
meet a common standard can still be very hard to combine, or to create common
data analysis scripts and tools for, since each dataset will require a different
process to pull out a given element.</p>
<p>Computer files serve as a way to organize data, whether that’s recorded
datapoints or written documents or computer programs <span class="citation">(Kernighan and Pike 1984)</span>. As
the programmer Paul Ford writes,</p>
<blockquote>
<p>“Data is just stuff, or rather, structured stuff: The cells of a spreadsheet,
the structure of a Word document, computer programs themselves—all data.” <span class="citation">(Ford 2015)</span></p>
</blockquote>
<p>A <em>file format</em> defines the rules for how the bytes in the chunk of
memory that makes up a certain file should be parsed and interpreted anytime you
want to meaningfully access and use the data within that file
<span class="citation">(Murrell 2009)</span>. There are many file formats you may be familiar
with—a file that ends in “.pdf” must be opened with a Portable Document Format
(PDF) Reader like Adobe Acrobat, or it won’t make much sense (you can try this
out by trying to open a “.pdf” file with a text editor, like TextEdit or
Notepad). The PDF Reader software has been programmed to interpret the data in a “.pdf” file
based on rules defining what data is stored where in the section of computer
memory for that file. Because most “.pdf” files conform to the same <em>file
format</em> rules, powerful software can be built that works with any file in that
format.</p>
<p>For certain types of biomedical data, the challenge of standardizing a format
has similarly been addressed through the use of well-defined rules for not only
the content of data, but also the way that content is <em>structured</em>. This can be
standardized through <em>standardized file formats</em> (sometimes also called <em>data
exchange formats</em> <span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span>) and often defines not only the
upper-level file format (e.g., use of a “.csv” file type), but also how data
within that file type should be organized. If data from different research
groups and experiments is recorded using the same file format, researchers can
develop software tools that can be repeatedly used to interpret and visualize
that data; on the other hand, if different experiments record data using
different formats, bespoke analysis scripts must be written for each separate
dataset. This is a blow not only to the efficiency of data analysis, but also a
threat to the accuracy of that analysis. If a set of tools can be developed that
will work over and over, more time can be devoted to refining those tools and
testing them for potential errors and bugs, while one-shot scripts often can’t
be curated with similar care. One paper highlights the dangers that come with
working with files that don’t follow a defined format:</p>
<blockquote>
<p>“Beware of common pitfalls when working with <em>ad hoc</em> bioinformatics formats.
Simple mistakes over minor details like file formats can consume a
disproportionate amount of time and energy to discover and fix, so mind these
details early on.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<p>Some biomedical data file formats have been created to help smooth over the
transfer of data that’s captured by complex equipment into software that can
analyze that data. For example, many immunological studies need to measure
immune cell populations in experiments, and to do so they use piece of
equipment called a flow cytometer that probes cells in a sample with lasers
and measures resulting intensities to determine characteristics of that cell.
The data created by this equipment is large (often measurements from [x] or more
lasers are taken for [x] cells in a single run) and somewhat complex, with a need
to record not only the intensity measurements from each laser, but also some metadata
about the equipment and characteristics of the run.
If every company that makes flow cytometers used a different file format for
saving the resulting data, then a different set of analysis software would need to
be developed to accompany each piece of equipment. For example, a laboratory at
a university with flow cytometers from two different companies would need licenses
for two different software programs to work with data recorded by flow cytometers,
and they would need to learn how to use each software package separately. There is a
chance that software could be developed that used shared code for data analysis, but only
if it also included separate sets of code to read in data from all types of equipment
and to reformat them to a common format.</p>
<p>This isn’t the case, however. Instead, there is a commonly agreed on file
format that flow cytometers should use to record the data they collect, called
the the <em>FCS file format</em>. This format has been defined through a series of
papers [refs], with several separate versions as the file format has evolved
over the years. It provides clear specifications on where to save each relevant
piece of information in the block of memory devoted to the data recorded by the
flow cytometer (in some cases, leaving a slot in the file blank if no relevant
information was collected on that element). As a result, people have been able
to create software, both proprietary and open-source, that can be used with any
data recorded by a flow cytometer, regardless of which company manufacturer the
piece of equipment that was used to generate the data. Other types of biomedical
data also have standardized file formats, including [example popular file
formats for biomedical data]. In some cases these were defined by an
organization, society, or initiative (e.g., the Metabolomics Standards
Initiative) <span class="citation">(Ghosh et al. 2011)</span>, while in some cases the file format developed
by a specific equipment manufacturer has become popular enough that it’s
established itself as the standard for recording a type of data
<span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span>.</p>
<p>For an even simpler example, thing about recording dates. The <em>minimum
information standard</em> for a date might always be the same—a recorded value
must include the day of the month, month, and year. However, this information
can be <em>structured</em> in a variety of ways. In many scientific data, it’s common
to record this information going from the largest to smallest units, so March
12, 2006, would be recorded “2006-03-12.” Another convention (especially in the
US) is to record the month first (e.g., “3/12/06”), while another (more common
in Europe) is to record the day of the month first (e.g., “12/3/06”).</p>
<p>If you are trying to combine data from different datasets with dates, and all
use a different structure, it’s easy to see how mistakes could be introduced
unless the data is very carefully reformatted. For example, March 12 (“3-12”
with month-first, “12-3” with day-first) could be easily mistaken to be December
3, and vice versa. Even if errors are avoided, combining data in different
structures will take more time than combining data in the same structure,
because of the extra needs for reformatting to get all data in a common
structure.</p>
<p><label for="tufte-mn-4" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-4" class="margin-toggle"><span class="marginnote"><span style="display: block;">“Vast swathes of bioscience data remain locked in esoteric formats, are described using nonstandard terminology, lack sufficient contextual information, or simply are never shared due to the perceived cost or futility of the exercise.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Sansone et al. 2012)</span></span></span></span></p>
</div>
<div id="defining-data-standards-for-a-research-group" class="section level3" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Defining data standards for a research group</h3>
<p>If some of the data you record from your experiments comes from complex
equipment, like flow cytometers or mass spectrometers, you may be recording much of that data in
a standardized format without any extra effort, because that format is the
default output format for the equipment. However, you may have more control over
other data recorded from your experiments, including smaller, less complex data
recorded directly into a laboratory notebook or spreadsheet. You can derive a
number of benefits from defining and using a standard for collecting this data,
as well.</p>
<p>As already mentioned, for many of the complex types of biological data,
standardized file formats exist. For example, flow cytometry data is typically
collected and recorded in <em>.fcs</em> files. Every piece of flow cytometry equipment
can then be built to output data in this format, and every piece of software to
analyze flow cytometry data can be built to read in this input. The <em>.fcs</em> file
format species how both raw data and metadata (e.g., compensation information,
equipment details) can be saved within the file—everyone who uses that file
format knows where to store data and where to find data of a certain type.</p>
<p>Much of the data collected in a laboratory is smaller, less complex, or less
structured than these types of data, data that is recorded “by hand,” often into
a laboratory notebook or a spreadsheet. One paper describes this type of data as
the output of “traditional, low-throughput bench science” <span class="citation">(Wilkinson et al. 2016)</span>.
For this data recording, the data may be written down in an <em>ad hoc</em>
way—however the particular researcher doing the experiment thinks makes
sense—and that format might change with each experiment, even if many
experiments have similar data outputs. As a result, it becomes harder to create
standardized data processing and analysis scripts that work with this data or
that integrate it with more complex data types. Further, if everyone in a
laboratory sets up their spreadsheets for data recording in their own way, it is
much harder for one person in the group to look at data another person recorded
and immediately find what they need within the spreadsheet.</p>
<p>As a step in a better direction, the head of a research group may
designate some common formats (e.g., a spreadsheet template) that all
researchers in the group should use when recording the data from a specific type
of experiments. This provides consistency across the recorded data for the
laboratory, making easier for one lab member to quickly understand and navigate
data saved by another lab member. It also opens the possibility to create tools
or scripts that read in and analyze the data that can be re-used across multiple
experiments with minor changes. This helps improve the efficiency and
reproducibility of data analysis, visualization, and reporting steps of the
research project.</p>
<p>This does require some extra time commitment <span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span>. First, time
is needed to design the format, and it does take a while to develop a format
that is inclusive enough that it includes a place to put all data you might want
to record for a certain type of experiment. Second, it will take some time to
teach each laboratory member what the format is and how to make sure they comply
with it when they record data.</p>
<p>On the flip side, the longer-term advantages of using a defined, structured
format will outweigh the short-term time investments for many laboratory groups
for frequently used data types. By creating and using a consistent structure to
record data of a certain type, members of a laboratory group can increase their
efficiency (since they do not need to re-design a data recording structure
repeatedly). They can also make it easier for downstream collaborators, like
biostatisticians and bioinformaticians, to work with their output, as those
collaborators can create tools and scripts that can be recycled across
experiments and research projects if they know the data will always come to them
in the same format. These benefits increase even more if data format standards
are created and used by a whole research field (e.g., if a standard data
recording format is always used for researchers conducting a certain type of
drug development experiment), because then the tools built at one institution
can be used at other insitutions. However, this level of field-wide coordination
can be hard to achieve, and so a more realistic immediate goal might be
formalizing data recording structures within your research group or department,
while keeping an eye out for formats that are gaining popularity as standards in
your field to adopt within your group.</p>
<p>One key advantage to using standardized data formats even for recording simple,
“low-throughput” data is that everyone in the research group will be able to
understand and work with data recorded by anyone else in the group—data will
not become impenetrable once the person who recorded it leaves the group. Also,
once a group member is used to the format, the process of setting up to record
data from a new experiment will be quicker, as it won’t require the effort of
deciding and setting up a <em>de novo</em> format for a spreadsheet or other recording
file. Instead, a template file can be created that can be copied as a starting
point for any new data recording.</p>
<p>Finally, there are huge benefits further down the data analysis pipeline that
come with always recording data in the same format. If your group is working
with a statistician or data analyst, it becomes much easier for that person to
quickly understand a new file if it follows the same format as previous files.
Further, if you work with a statistician or data analyst, he or she probably
creates code scripts to read in, re-format, analyze, and visualize the data
you’ve shared. If you always record data using the same format, these scripts
can be reused with very little modification. This saves valuable time, and it
helps make more time for more interesting statistical analysis if your
collaborator can trim time off reading in and reformatting the data in their
statistical programming language.</p>
<p>One paper suggests that the balance can be found, in terms of deciding whether
the benefits of developing a standard outweigh the costs, by considering how
often data of a certain type is generated and used:</p>
<blockquote>
<p>“To develop and deploy a standard creates an overhead, which can be expensive.
Standards will help only if a particular type of information has to be
exchanged often enough to pay off the development, implementation, and usage
of the standard during its lifespan.” <span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span></p>
</blockquote>
</div>
<div id="two-dimensional-structured-data-format" class="section level3" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Two-dimensional structured data format</h3>
<p>So far, this module has explored <em>why</em> you might want to use standardized
data formats for recording experimental data. The rest of the module
aims to give you tips for how to design and define your own standardized
data formats, if you decide that is worthwhile for certain data types
recorded within your research group.</p>
<p>Once you commit to creating a defined, structured format, you’ll need to decide
what that structure should be. There are many options here, and it’s very
tempting to use a format that is easy on human eyes
<span class="citation">(Buffalo 2015)</span>. For example, it may seem appealing to create a
format that could easily be copied and pasted into presentations and Word
documents and that will look nice in those presentation formats. To facilitate
this use, a laboratory might set up a recording format base on a spreadsheet
template that includes multiple tables of different data types on the same
sheet, or multi-level column headings.</p>
<p>Unfortunately, many of the characteristics that make a format attractive
to human eyes will make it harder for a computer to make sense of. For example,
if you include two tables in the same spreadsheet, it might make it easier for a
person to get a one-screen look at two small data tables. However, if you want
to read that data into a statistical program (or work with a collaborator who
would), it will likely take some complex code to try to tell the computer how to
find the second table in the spreadsheet. The same applies if you include some
blank lines at the top of the spreadsheet, or use multi-level headers, or use
“summary” rows at the bottom of a table. Further, any information you’ve
included with colors or with text boxes in the spreadsheet will be lost when the
data’s read into a statistical program. These design elements in a data
format make it much harder to read the data embedded in a spreadsheet into other
computer programs, including programs for more complex data analysis and
visualization, like R and Python.</p>
<p><label for="tufte-mn-5" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-5" class="margin-toggle"><span class="marginnote"><span style="display: block;">“Data should be formatted in a way that facilitates computer readability. All too often, we as humans record data in a way that maximizes its readability to us, but takes a considerable amount of cleaning and tidying before it can be processed by a computer. The more data (and metadata) that is computer readable, the more we can leverage our computers to work with this data.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Buffalo 2015)</span></span></span></span></p>
<p>For most statistical programs, data can be easily read in from a spreadsheet if
the computer can parse it in the following way: first, read in the first row,
and assign each cell in that row as the <em>name</em> of a column. Then, read in the
second row, and put each cell in the column the corresponds with the name of the
cell in the same position in the first row. Also, set the data type for that
column (e.g., number, character) based on the data type in this cell. Then, keep
reading in rows until getting to a row that’s completely blank, and that will be
the end of the data. If any of the rows has more cell than the first row, then
that means that something went wrong, and should result in stopping or giving a
warning. If any of the rows have fewer cells than the first row, then that means
that there are missing data in that row, and should probably be recorded as
missing values for any cells the row is “short” compared to the first row.</p>
<p>One of the easiest format for a computer to read is therefore a two-dimensional
“box” of data, where the first row of the spreadsheet gives the column names,
and where each row contains an equal number of entries. This type of
two-dimensional tabular structure forms the basis for several popular
“delimited” file formats that serve as a <em>lingua franca</em> across many simple
computer programs, like the comma-separated values (CSV) format, the
tab-delimited values (TSV) format, and the more general delimiter-separated
values (DSV) format, which are a common format for data exchange across
databases, spreadsheet programs, and statistical programs <span class="citation">(Janssens 2014; E. S. Raymond 2003; Buffalo 2015)</span>.</p>
<p><label for="tufte-mn-6" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-6" class="margin-toggle"><span class="marginnote"><span style="display: block;">“Tabular plain-text data formats are used extensively in computing. The basic format is incrediably simple: each row (also known as a record) is kept on its own line, and each column (also known as a field) is separate by some delimiter.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Buffalo 2015)</span></span></span></span></p>
<p>If you think of the computer parsing a spreadsheet as described above, hopefully
it clarifies why some spreadsheet formats would cause problems. For example, if
you have two tables in the same spreadsheet, with blank lines between them, the
computer will likely either think it’s read all the data after the first table,
and so not read in any data from the second table, or it will think the data
from both tables belong in a single table, with some rows of missing data in the
center. To write the code to read in data from two tables into two separate
datasets in a statistical program, it will be necessary to write some complex
code to tell the computer how to search out the start of the second table in the
spreadsheet.</p>
<p>Similar problems come up if a spreadsheet diverges from a regular,
two-dimensional format, with a single row of column names to start the data.
For example, if the data uses multiple rows to create multi-level
column headers, anyone reading it into another program will need to
either skip some of the rows of the column headers, and so lose information
in the original spreadsheet, or write complex code to parse the column
headers separately, then read in the later rows with data, and then stick
the two elements back together. “Summary” rows at the end of a dataset
(for example, the sums or means of all values in a column) will need to
be trimmed off when the data is read into other programs, since most
of the analysis and visualization someone would want to do in another
program will calculate any summaries fresh, and will want each row of
a dataset to represent the same “type” and level of data (e.g., one measurement
from one animal).</p>
<p>For anything in a data format that requires extra coding when reading data into
another program, you are introducing a new opportunity for errors at the
interface between data recording and data analysis. If there are strong reasons
to use a format that requires these extra steps, it will still be possible to
create code to read in and parse the data in statistical programs, and if the
same format is consistently used, then scripts can be developed and thoroughly
tested to allow this. However, do keep in mind that this will be an extra
burden on any data analysis collaborators who are using a program besides a
spreadsheet program. The extra time this will require could be large, since
this code should be vetted and tested thoroughly to ensure that the data
cleaning process is not introducing errors. By contrast, if the data is
recorded in a two-dimensional format with a single row of column names as
the first row, data analysts can likely read it quickly and cleanly into
other programs, with low risks of errors in the transfer of data from the
spreadsheet.</p>
<p><label for="tufte-mn-7" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-7" class="margin-toggle"><span class="marginnote"><span style="display: block;">“Cleaning data is a short-term solution, and preventing errors is promoted as a permanent solution. The drawback to cleaning data is that the process never ends, is costly, and may allow many errors to avoid detection.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Keller et al. 2017)</span></span></span></span></p>
</div>
<div id="saving-two-dimensional-structured-data-in-plain-text-file-formats" class="section level3" number="2.2.4">
<h3><span class="header-section-number">2.2.4</span> Saving two-dimensional structured data in plain text file formats</h3>
<p>If you have recorded data in a two-dimensional structured format, you can choose
to save it in either a <em>plain text</em> format or a <em>binary</em> format. With a plain
text format, a file is “human readable” when it’s opened in a text editor
<span class="citation">(Hunt, Thomas, and Cunningham 2000; Janssens 2014)</span>, because each byte that encodes the file
translates to a single character <span class="citation">(Murrell 2009)</span>, usually using an
ASCII or Unicode encoding. Common plain text file formats used for biomedical
research include CSV and TSV files (these are distinguished only by the
character used as a delimiter—commas for CSV files versus taabs for TSV files)
<span class="citation">(Buffalo 2015)</span>, other more complex file formats like SAM and XML
are also typically saved in plain text.</p>
<p>A binary file format, on the other hand, encodes data within the file using an
encoding system that differs from ANSCII or Unicode. To extract the data in a
meaningful way, a computer program must know and use rules for the encoding and
structure of that file format, and those rules will be different for each
different binary file format <span class="citation">(Murrell 2009)</span>. Some binary file
formats are “open,” with all the information on these rules and encodings
available for anyone to read. On the other hand, other binary file formats are
proprietary, without available guidance on how to interpret or use the data
stored in them when creating new software tools. Binary files, because they
don’t follow the restrictions of plain text encoding and format, can encode and
organize data in a way that’s often much more compressed, because it’s optimized
to suit a specific type of data. This means that binary file formats can often
store more data within a certain amount of computer memory compared to plain
text file formats. Binary files can also be designed so that the computer can
find and read a specific piece of data, rather than needing to read data in
linearly from the start to the end of a file as with plain text formats. This
means that programs can often access specific bits of data much more quickly
from a binary file format that from a plain text format, making computation
processing run much faster.</p>
<p>However, even with the speed and size advantages of many binary file formats, it
is often worthwhile to record and save experimental data in a plain text, rather
than binary, file format. There are a number of advantages to using a plain text
format. A plain text format may take more space (in terms of computer memory)
and take longer to process within other programs; however, its benefits
typically outweigh these limitations <span class="citation">(Hunt, Thomas, and Cunningham 2000)</span>. Advantages include:
(1) humans can read the file directly <span class="citation">(Hunt, Thomas, and Cunningham 2000; Janssens 2014)</span>,
and should always be able to, regardless of changes in and future obsolescence
of computer programs; (2) almost all software programs for analyzing and
processing files can input plain-text files, while binary file formats often
require specialized software <span class="citation">(Murrell 2009)</span>; (3) the
Unix system, which has influenced many existing software programs, especially
open-source programs for data analysis and command-line tools, are based on
inputting and outputtin line-based plain-text files <span class="citation">(Janssens 2014)</span>; and (4)
plain-text files can be easily tracked with version control
<span class="citation">(Hunt, Thomas, and Cunningham 2000)</span>. These advantages might become particularly important in
cases where researchers need to combine and integrate heterogeneous data, for
example data coming from different instruments.</p>
<p>Another advantage of storing data in a plain text format is that it makes
version control, which we’ll discuss in a later module, a much more powerful
tool. With plain text files, you can use version control to see the specific
changes to a file. With binary files, you can typically see if a file was
changed, but it’s much harder to see exactly what within the file was changed.</p>
<p>The book <em>The Pragmatic Programmer</em> highlights some of the advantages of plain
text:</p>
<blockquote>
<p>“Human-readable forms of data, and self-describing data, will outlive all
other forms of data and the applications that created them. Period. As long as
the data survives, you will have a chance to be able to use it—potentially
long after the original application that wrote it is defunct. … Even in the
future of XML-based intelligent agents that travel the wild and dangerous
Internet autonomously, negotiating data interchange among themselves, the
ubiquitous text file will still be there. In fact, in heterogeneous environments
the advantages of plain text can outweight all of the drawbacks. You need to
ensure that all parties can communicate using a common standard. Plain text is
that standard.” <span class="citation">(Hunt, Thomas, and Cunningham 2000)</span></p>
</blockquote>
<p>Paul Ford, by contrast, describes some of the disadvantages of
a binary file format, using the Photoshop file format as an example:</p>
<blockquote>
<p>“A Photoshop file is a lump of binary data. It just sits there on your hard
drive. Open it in Photoshop, and there are your guides, your color swatches, and
of course, the manifold pixels of your intent. But outside of Photoshop that
file is an enigma. There is not ‘view source.’ You can, if you’re passionate,
read the standard on the web, and it’s all piled in there, the history of
pictures on computers. That’s when it becomes clear: only Photoshop’s creator
Adobe can understand this thing.”
<span class="citation">(Ford 2014)</span></p>
</blockquote>
<p>Structuring data in a gridded, two-dimensional format, as described in the last
section, will be helpful even if it is in a file format that is binary, like
Excel. However, there are added benefits to saving the structured data in a
plain text format. Older Excel spreadsheets are typically saved in a proprietary
file format (“.xls”), while more recently Excel has saved files to an open
binary format based on packaging XML files with the data (“.xlsx” file format)
<span class="citation">(Janssens 2014)</span>. While the open proprietary format is preferable, since
tools can be developed to work with them by people other than the Microsoft
team, both file formats still face some of the limitations of binary file
formats as a way of recording experimental data. However, even if you have used
a spreadsheet program like Excel to record data, it’s very easy to still save
that data in a plain text file format <span class="citation">(Murrell 2009)</span>. In most
spreadsheet programs, you can choose to save a file “As CSV.”</p>
</div>
<div id="occassions-for-more-complex-data-structures-and-file-formats" class="section level3" number="2.2.5">
<h3><span class="header-section-number">2.2.5</span> Occassions for more complex data structures and file formats</h3>
<p>There are some cases where a two-dimensional data format may not be adequate
for recording experimental data, despite this format’s advantages in
improving reproducibility through later data analysis steps. Similarly,
there may be cases where a binary file format, or use of a database, will
outweigh the benefits of saving data to a plain text format. Being familiar
with different file formats can also be helpful when you need to integrate
data stored in different formats <span class="citation">(Murrell 2009)</span>.</p>
<p><strong>Non-tabular plain-text formats.</strong>
First, some data has a linked or hierarchical nature, in terms of how data
points are connected through the dataset. For example, data on a family tree have a hierarchical structure, where different numbers of children are
recorded for each parent. As another example, if you were building a dataset
describing how scientists have collaborated together as coauthors, that data
might form a network. In many cases, it is possible to structure datasets with
these types of “non-tabular” structure using the “tidy data” tabular format
described in the next section. However, in very complex cases, it may work
better to use a non-tabular data format <span class="citation">(E. S. Raymond 2003)</span>. Popular data formats
that are non-tabular include the eXtensible Markup Language (XML) and JavaScript
Object Notation (JSON) formats, both of which are well-suited for
hierarchically-structured data. You may also have data you would like to use in
XML or JSON formats if you are using web services to pull datasets from online
repositories, as open data application programming interfaces (APIs) often
return data in these formats <span class="citation">(Janssens 2014)</span>.</p>
<p>Another use of file formats that are plain text but meant to be streamed, rather
than read in as a whole. When reading in data stored in a delimited plain text
file, like a CSV file, a statistical program like R will typically read in all
the data and then operate on the dataset as a whole. If a data file is very
large, then reading in all the data at once might require so much memory that it
slows down processing, or even exceed the program’s memory cap [?]. One strategy
is to design a data format so that the program can read in a small amount of the
file, process that piece of the data, write the result out, and remove that bit
of data from the program’s memory before moving into the next portion of data
<span class="citation">(Buffalo 2015)</span>. This <em>streaming</em> approach is sometimes used with
some file formats used for biomedical research, including FASTA and FASTQ files.</p>
<p><strong>Databases.</strong>
When research datasets include not only data that can be expressed in plain
text, but also data like images, photographs, or videos, it may be worth
considering using a database to store the data <span class="citation">(Murrell 2009)</span>.
Relational database managment system software, like [examples. MySQL?
PostgreSQL?] can be used to organize data in a way that records connections
(<em>relations</em>) between different pieces of data and allows you to access
different combinations of that data quickly using Structured Query Language, or
<em>SQL</em> <span class="citation">(Ford 2015)</span>. Further, some statistical programming languages, including
R, now have tools that allow you to directly access and work with data from a
database from within the statistical program, and in some cases using scripts
that are very similar or identical to the code that would be used if you’d read
the data into the program from a plain text file.</p>
<p><label for="tufte-mn-8" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-8" class="margin-toggle"><span class="marginnote"><span style="display: block;">“The database is the unsung infrastructure of the world, the shared memory of every corporation, and the foundation of every major web site. And they are everywhere. Nearly every host-your-own-web-site package comes with access to a database called MySQL; just about every cell phone has SQLite3, aa tiny, pocket-sized database, built in.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Ford 2015)</span></span></span></span></p>
<p>It will be more complicated to set up a database for recording experimental
data, and so it’s often preferable to instead save data in plain text files
within a file directory, if the data is simple enough to allow that. However,
there are some fairly simple database solutions that are now available,
including SQLite <span class="citation">(Buffalo 2015)</span>.</p>
<p><strong>Binary file formats.</strong></p>
<p>There are cases where it may not be best to store laboratory-generated data in a
plain text format. For example, the output from a flow cytometer is large and
would take up a lot (more) computer memory if stored in a plain text format, and
it would take much longer to read and work with the data in analysis software if
it were in that format. For very large datasets like this, it may be necessary
to use a binary data format, either for size or speed or both
<span class="citation">(Kernighan and Pike 1984; Hunt, Thomas, and Cunningham 2000)</span>. For very large biomedical datasets,
binary file formats are sometimes designed for <em>out-of-memory approaches</em>
<span class="citation">(Buffalo 2015)</span>, where a file format is designed in a way that
allows computer programs to find and read only specific pieces of data in a file
through a process called <em>random access</em>, rather than needing to read the full
file into memory before a specific piece of data in the file can be accessed
(a.k.a., <em>sequential access</em>) <span class="citation">(Murrell 2009)</span>.</p>
</div>
<div id="levels-of-standardizationresearch-group-to-research-community" class="section level3" number="2.2.6">
<h3><span class="header-section-number">2.2.6</span> Levels of standardization—research group to research community</h3>
<p>Standards can operate both at the level of individual research groups and at the
level of the scientific community as a whole. The potential advantages of
community-level standards are big: they offer the chance to develop
common-purpose tools and code scripts for data analysis, as well as make it
easier to re-use and combine experimental data from previous research that is
posted in open data repositories. If a software tool can be reused, then more
time can be spent in developing and testing it, and as more people use it, bugs
and shortcomings can be identified and corrected. Community-wide standards can
lead to databases with data from different experiments, and from different
laboratory groups, structured in a way that makes it easy for other researchers
to understand each dataset, find pieces of data of interest within datasets, and
integrate different datasets <span class="citation">(Lynch 2008)</span>. Similarly, with community-wide
standards, it can become much easier for different research groups to
collaborate with each other or for a research group to use data generated by
equipment from different manufacturers <span class="citation">(Schadt et al. 2010)</span>.</p>
<p><label for="tufte-mn-9" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-9" class="margin-toggle"><span class="marginnote"><span style="display: block;">“Without community-level harmonization and interoperability, many community projects risk becoming data silos.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Sansone et al. 2012)</span></span></span></span></p>
<p><label for="tufte-mn-10" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-10" class="margin-toggle"><span class="marginnote"><span style="display: block;">“Solutions to integrating the new generation of large-scale data sets require approaches akin to those used in physics, climatology and other quantitative disciplines that have mastered the collection of large data sets.”</span>
<span style="display: block;"><span class="citation"><span class="citation">(Schadt et al. 2010)</span></span></span></span></p>
<p>However, there are important limitations to community-wide standards, as well.
It can be very difficult to impose such standards top-down and community-wide,
particularly for low-throughput data collection (e.g., laboratory bench
measurements), where research groups have long been in the habit of recording
data in spreadsheets in a format defined by individual researchers or research
groups. One paper highlights this point:</p>
<blockquote>
<p>“The data exchange formats PSI-MI and MAGE-ML have helped to get many of the
high-throughput data sets into the public domain. Nevertheless, from a bench
biologist’s point of view benefits from adopting standards are not yet
overwhelming. Most standardization efforts are still mainly an investment for
biologists.” <span class="citation">(Brazma, Krestyaninova, and Sarkans 2006)</span></p>
</blockquote>
<p>Further, in some fields, community-wide standards have struggled to remain
stable, which can frustrate community members, as scripts and software must be
revamped to handle shifting formats <span class="citation">(Buffalo 2015; Barga et al. 2011)</span>. In some cases, a useful compromise is to follow a
general data recording format, rather than one that is very prescriptive. For
example, committing to recording data in a format that is “tidy” (which we
discuss extensively in the next module) may be much more flexible—and able to
meet the needs of a large range of experimental designs—than the use of a
common spreadsheet template or a more proscriptive standardized data format.</p>
<!-- ### Applied exercise -->

</div>
</div>
<p style="text-align: center;">
<a href="2.1-module1.html"><button class="btn btn-default">Previous</button></a>
<a href="2.3-module3.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
