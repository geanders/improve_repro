<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="1.1 Overview of these modules | Improving the Reproducibility of Experimental Data Recording and Pre-Processing" />
<meta property="og:type" content="book" />

<meta property="og:description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing." />
<meta name="github-repo" content="rstudio/bookdown-demo" />

<meta name="author" content="Brooke Anderson, Michael Lyons, Mercedes Gonzalez-Juarrero, Marcela Henao-Tamayo, and Gregory Robertson" />



<meta name="description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing.">

<title>1.1 Overview of these modules | Improving the Reproducibility of Experimental Data Recording and Pre-Processing</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#rigor-and-reproducibility-in-computation" id="toc-rigor-and-reproducibility-in-computation"><span class="toc-section-number">1</span> Rigor and reproducibility in computation</a></li>
<li><a href="2-experimental-data-recording.html#experimental-data-recording" id="toc-experimental-data-recording"><span class="toc-section-number">2</span> Experimental Data Recording</a></li>
<li><a href="3-experimental-data-preprocessing.html#experimental-data-preprocessing" id="toc-experimental-data-preprocessing"><span class="toc-section-number">3</span> Experimental Data Preprocessing</a></li>
<li><a href="4-references.html#references" id="toc-references"><span class="toc-section-number">4</span> References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="overview-of-these-modules" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Overview of these modules</h2>
<p>The recent NIH-Wide Strategic Plan <span class="citation">(U.S. Department of Health and Human Services, National Institutes of Health 2016)</span>
describes an integrative view of biology and human health that includes
translational medicine, team science, and the importance of capitalizing on an
exponentially growing and increasingly complex data ecosystem <span class="citation">(U.S. Department of Health and Human Services, National Institutes of Health 2018)</span>.
Underlying this view is the need to use, share, and re-use biomedical data
generated from widely varying experimental systems and researchers. Basic
sources of biomedical data range from relatively small sets of measurements,
such as animal body weights and bacterial cell counts that may be recorded by
hand, to thousands or millions of instrument-generated data points from various
imaging, -omic, and flow cytometry experiments. In either case, there is a
generally common workflow that proceeds from measurement to data recording,
pre-processing, analysis, and interpretation. However, in practice the distinct
actions of data recording, data pre-processing, and data analysis are often
merged or combined as a single entity by the researcher using commercial or open
source spreadsheets, or as part of an often proprietary experimental measurement
system / software combination (Figure <a href="1.1-overview-of-these-modules.html#fig:workflow">1.1</a>), resulting in key
failure points for reproducibility at the stages of data recording and
pre-processing.</p>
<div class="figure"><span style="display:block;" id="fig:workflow"></span>
<p class="caption marginnote shownote">
Figure 1.1: Two scenarios where ‘black boxes’ of non-transparent, non-reproducible data handling exist in research data workflows at the stages of data recording and pre-processing. These create potential points of failure for reproducible research. Red arrows indicate where data is passed to other research team members, including statisticians / data analysts, often within complex or unstructured spreadsheet files.
</p>
<img src="figures/existing_blackboxes.jpg" alt="Two scenarios where 'black boxes' of non-transparent, non-reproducible data handling exist in research data workflows at the stages of data recording and pre-processing. These create potential points of failure for reproducible research. Red arrows indicate where data is passed to other research team members, including statisticians / data analysts, often within complex or unstructured spreadsheet files." width="\textwidth"  />
</div>
<p>It is widely known and discussed among data scientists, mathematical modelers,
and statisticians <span class="citation">(Broman and Woo 2018; Krishnan et al. 2016)</span> that there is
frequently a need to discard, transform, and reformat various elements of the
data shared with them by laboratory-based researchers, and that data is often
shared in an unstructured format, increasing the risks of introducing errors
through reformatting before applying more advanced computational methods.
Instead, a critical need for reproducibility is for the transparent and clear
sharing across research teams of: (1) raw data, directly from hand-recording or
directly output from experimental equipment; (2) data that has been
pre-processed as necessary (e.g., gating for flow cytometry data, feature
identification for metabolomics data), saved in a consistent, structured format,
and (3) a clear and repeatable description of how the pre-processed data was
generated from the raw data <span class="citation">(Broman and Woo 2018; Ellis and Leek 2018)</span>.</p>
<p>To enhance data reproducibility, it is critical to create a clear separation
among data recording, data pre-processing, and data analysis—breaking up
commonly existing ``black boxes” in data handling across the research process.
Such a rigorous demarcation requires some change in the conventional
understanding and use of spreadsheets and a recognition by biomedical
researchers that recent advances in computer programming languages, especially
the R programming language, provide user-friendly and accessible tools and
concepts that can be used to extend a transparent and reproducible data workflow
to the steps of data recording and pre-processing. Among our team, we have found
that there are many common existing practices—including use of spreadsheets
with embedded formulas that concurrently record and analyze experimental data,
problematic management of project files, and reliance on proprietary,
vendor-supplied point-and-click software for data pre-processing—that can
interfere with the transparency, reproducibility, and efficiency of
laboratory-based biomedical research projects, problems that have also been
identified by others as key barriers to research reproducibility
<span class="citation">(Broman and Woo 2018; Bryan 2018; Ellis and Leek 2018; Marwick, Boettiger, and Mullen 2018)</span>. In
these training modules, we have choosen topics that tackle barriers to
reproducibility that have straightforward, easy-to-teach solutions, but which
are still very common in biomedical laboratory-based research programs.</p>
<hr />
<blockquote>
<p>“Today, one often hears that life sciences are faced with the ‘big data problem.’
However, data are just a small facet of a much bigger challenge. The true
difficulty is that most biomedical researchers have no capacity to carry out
analyses of modern data sets using appropriate tools and computational infrastructure
in a way that can be fully understood and reused by others. This struggle began with
the introduction of microarray technology, which, for the first time, introduced
life sciences to truly large amounts of data and the need for quantitative training.
What is new, however, is that next-generation sequencing (NGS) has made this problem
vastly more challenging. Today’s sequencing-based experiments generate substantially
more data and are more broadly applicable than microarray technology, allowing for
various novel functional assays, including quantification of protein–DNA binding or
histone modifications (using chromatin immunoprecipitation followed by high-throughput
sequencing (ChIP-seq)), transcript levels (using RNA sequencing (RNA-seq)), spatial
interactions (using Hi-C) and others. These individual applications can be combined
into larger studies, such as the recently published genomic profiling of a human
individual whose genome was sequenced and gene expression tracked over an extended
period in a series of RNA-seq experiments. As a result, meaningful interpretation
of sequencing data has become particularly important. Yet such interpretation
relies heavily on complex computation—a new and unfamiliar domain to many of our
biomedical colleagues—which, unlike data generation, is not univerally
accessible to everyone.” <span class="citation">(Nekrutenko and Taylor 2012)</span></p>
</blockquote>
<blockquote>
<p>“We note that this discussion thus far has dealt only with technical reproducibility
challenges or with the ability to repeat published analyses using the original data
to verify the results. Most biomedical researchers are much more acquainted with
biological reproducibility, in which conceptual results are verified by an
alternative analysis of different samples. However, we argue that the computational
nature of modern biology blurs the distinction between technical and biological
reproducibility.” <span class="citation">(Nekrutenko and Taylor 2012)</span></p>
</blockquote>
<blockquote>
<p>“The overwhelming majority of currently published papers using NGS technologies include
analyses that are not detailed … Moreover, the computational approaches used in these
publications cannot be readily reused by others.” <span class="citation">(Nekrutenko and Taylor 2012)</span></p>
</blockquote>
<blockquote>
<p>“Many classical publications in life sciences have become influential because they
provide complete information on how to repeat reported analyses so others can adopt
these approaches in their own research, such as for chain termination sequencing
technology that was developed by Sanger and colleagues and for PCR. Today’s publications
that include computational analyses are very different. Next-generation sequencing (NGS)
technologies are undoubtedly as transformative as DNA sequencing and PCR were more than
30 years abgo. As more and more researchers use high-throughput sequencing in their
research, they consult other publications for examples of how to carry out computational
analyses. Unfortunately, they often find that the extensive informatics component that
is required to analyse NGS data makes it much more difficult to repeat studies published
today. Note that the lax standards of computational reproducibility are not unique to
life sciences; the importance of being able to repeat computational experiments was
first brought up in geosciences and became relevant in life sciences following the
establishment of microarray technology and high-throughput sequencing. Replication
of computational experiments requires access to input data sets, source code or binaries
of exact versions of software used to carry out the initial analysis (this includes
all helper scripts that are used to convert formats, groom data, and so on) and
knowing all parameter settings exactly as they were used. In our experience, …
publications rarely provide such a level of detail, making biomedical computational
analyses almost irreproducible.” <span class="citation">(Nekrutenko and Taylor 2012)</span></p>
</blockquote>
<blockquote>
<p>“The biological sciences have depended on other, less-reliable techniques
for reproducbility. The most long-standing is the assumption that reproducibility
studies will occur organically as different researchers work on related problems.
In the past five years or so, funding agencies and journals have implemented
more-stringent experimental-reporting and data-availablility requirements for
grant proposals and submitted manuscripts. A handful of initiatives have attempted
to replicate published studies. The peer-reviewed ‘Journal of Visualized Experiments’
creates videos to disseminate details that are hard to convey in conventional
methods sections. Yet pitfalls persist. Scientists might waste resources trying
to build on unproven techniques. And real discoveries can be labelled irreproducible
because too few resources are available to conduct a validation.” <span class="citation">(Raphael, Sheehan, and Vora 2020)</span></p>
</blockquote>
<blockquote>
<p>“The scientific community has lost the connection with the original culture of
skepticism which existed in the 17th century with the scientists of the
Royal Society who pioneered the scientific method as captured in their motton
nullius in verba (‘take nobody’s word’). They regarded the ability to replicate
results in independent studies as a fundamental criterion for the establishment
of a scientific fact. Modern scientific practice presents single experiments
as proofs. When work is published, it is typically presented without self-criticism.”
<span class="citation">(Neff 2021)</span></p>
</blockquote>
<blockquote>
<p>“Transparency and quality management are key to improving scientific rigor and
reproducibility. It is, therefore, good to see that the Open Science movement
is gaining traction, and that research institutions increasingly view poor
science as a reputation risk.” <span class="citation">(Neff 2021)</span></p>
</blockquote>
<blockquote>
<p>“We scientists search tenaciously for information about how nature works through
reason and experimentation. Who can deny the magnitude of knowledge we have
gleaned, its acceleration over time, and its expanding positive impact on society?
Of course, some data and models are fragile, and our understanding remains
punctuated by false premises. Holding fast to the three Rs [rigor, reproducibility,
and robustness] ensures that the path—although tortuous and treacherous at times—remains
well lit.” <span class="citation">(Garraway 2017)</span></p>
</blockquote>
<blockquote>
<p>“We have learnt that to understand how life works, describing how the research was
done is as important as describing what was observed.” <span class="citation">(Lithgow, Driscoll, and Phillips 2017)</span></p>
</blockquote>
<blockquote>
<p>“In computational science, ‘reproducible’ often means that enough information
is provided to allow a dedicated reader to repeat the calculations in the paper for
herself. In biomedical disciplines, ‘reproducible’ often means that a different lab,
starting the experiment from scratch, would get roughly the same experimental
result.” <span class="citation">(Stark 2018)</span></p>
</blockquote>
<blockquote>
<p>“Results that generalize to all universes (or perhaps do not even require a
universe) are part of mathematics. Results that generalize to our Universe belong
to physics. Results that generalize to all life on Earth underpin molecular
biology. Results that generalize to all mice are murine biology. And results that
hold only for a particular mouse in a particular lab in a particular experiment
are arguably not science.” <span class="citation">(Stark 2018)</span></p>
</blockquote>
<blockquote>
<p>“It should not need to be stated, but here goes. Reproducibility is the key
underlying principle of science.” <span class="citation">(Gibb 2014)</span></p>
</blockquote>
<blockquote>
<p>“Popularized by statistician John W. Tukey, EDA is an approach that emphasizes
understanding data (and its limitations) through interactive investigation
rather than explicit statitical modeling. In his 1977 book <em>Exploratory Data
Analysis</em>, Tukey described EDA as ‘detective work’ involved in ‘finding and
revealing the clues’ in data. As Tukey’s quote emphasizes, EDA is much more an approach
to exploring data than using specific statistical methods. In the face of rapidly
changing sequencing technologies, bioinformatics software, and statistical methods,
EDA skills are not only widely applicable and comparatively stable—they’re also
essential to making sure that our analyses are robust to these new data and methods.”
<span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<hr />
</div>
<p style="text-align: center;">
<a href="index.html"><button class="btn btn-default">Previous</button></a>
<a href="1.2-license.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
