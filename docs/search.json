[{"path":"index.html","id":"overview-of-these-modules","chapter":"Overview of these modules","heading":"Overview of these modules","text":"NIH-Wide Strategic Plan1\ndescribes integrative view biology human health includes\ntranslational medicine, team science, importance capitalizing \nexponentially growing increasingly complex data ecosystem.2\nUnderlying view need use, share, re-use biomedical data\ngenerated widely varying experimental systems researchers. Basic\nsources biomedical data range relatively small sets measurements,\nanimal body weights bacterial cell counts may recorded \nhand, thousands millions instrument-generated data points various\nimaging, -omic, flow cytometry experiments. either case, \ngenerally common workflow proceeds measurement data recording,\npre-processing, analysis, interpretation. However, practice distinct\nactions data recording, data pre-processing, data analysis often\nmerged combined single entity researcher using commercial open\nsource spreadsheets, part often proprietary experimental measurement\nsystem / software combination (Figure 0.1), resulting key\nfailure points reproducibility stages data recording \npre-processing.\nFigure 0.1: Two scenarios ‘black boxes’ non-transparent, non-reproducible data handling exist research data workflows stages data recording pre-processing. create potential points failure reproducible research. Red arrows indicate data passed research team members, including statisticians / data analysts, often within complex unstructured spreadsheet files.\nwidely known discussed among data scientists, mathematical modelers,\nstatisticians3 \nfrequently need discard, transform, reformat various elements \ndata shared laboratory-based researchers, data often\nshared unstructured format, increasing risks introducing errors\nreformatting applying advanced computational methods.\nInstead, critical need reproducibility transparent clear\nsharing across research teams : (1) raw data, directly hand-recording \ndirectly output experimental equipment; (2) data \npre-processed necessary (e.g., gating flow cytometry data, feature\nidentification metabolomics data), saved consistent, structured format,\n(3) clear repeatable description pre-processed data \ngenerated raw data.4To enhance data reproducibility, critical create clear separation\namong data recording, data pre-processing, data analysis—breaking \ncommonly existing ``black boxes” data handling across research process.\nrigorous demarcation requires change conventional\nunderstanding use spreadsheets recognition biomedical\nresearchers recent advances computer programming languages, especially\nR programming language, provide user-friendly accessible tools \nconcepts can used extend transparent reproducible data workflow\nsteps data recording pre-processing. Among team, found\nmany common existing practices—including use spreadsheets\nembedded formulas concurrently record analyze experimental data,\nproblematic management project files, reliance proprietary,\nvendor-supplied point--click software data pre-processing—can\ninterfere transparency, reproducibility, efficiency \nlaboratory-based biomedical research projects, problems also \nidentified others key barriers research reproducibility.5 \ntraining modules, choosen topics tackle barriers \nreproducibility straightforward, easy--teach solutions, \nstill common biomedical laboratory-based research programs.","code":""},{"path":"index.html","id":"license","chapter":"Overview of these modules","heading":"License","text":"book licensed Creative Commons\nAttribution-NonCommercial-ShareAlike 4.0 International\nLicense, code \nbook MIT license.","code":""},{"path":"module1.html","id":"module1","chapter":"Module 1 Separating data recording and analysis","heading":"Module 1 Separating data recording and analysis","text":"use spreadsheets within scientific work? , ’re alone. \nfact, studies surveyed scientists work practices, ’ve\nfound spreadsheets common tool. Examples include surveys 250\nbiomedical researchers University Washington,6\nneuroscience researchers University Newcastle. \nstudies, respondents reported used spreadsheets \ngeneral-purpose software research.7 working\ngroup bioinformatics data-intensive science similarly found spreadsheets\ncommon tool used across attendees.8These software tools, Microsoft Excel Google Sheets, provide \nmanual automated entry data rows columns cells. Standard \ncustom formulas operations can applied cells, \ncommonly used reformat clean data, calculate various statistics, \ngenerate simple plots; embedded additional data entries\nprogramming elements within spreadsheet. tools greatly\nimproved paper worksheets originally based,9 --one practice impedes transparency \nreproducibility recording analysis large complex data\nsets routinely generated life science experiments.improve computational reproducibility research project, \ncritical biomedical researchers learn importance maintaining\nrecorded experimental data “read-” files, separating data recording \ndata pre-processing data analysis steps.10In module, ’ll talk spreadsheets popular, well\nfeatures beneficial researchers. However, \nalso many problems can introduce, particularly spreadsheets \nused way combines data collection data pre-processing analysis.\n’ll walk problems, later modules ’ll walk \nalternatives, spreadsheets limited recording data (’re\nused ), steps pre-processing analysis done \ntools.Objectives. module, trainee able :Explain spreadsheets popular tool among scientistsExplain difference data recording data analysisUnderstand collecting data spreadsheets embedded formulas impedes\nreproducibilityCompare ways spreadsheets can used solid research tool\nversus ways use can problematicDiscuss downsides using spreadsheets data analysis scientific\nresearch","code":""},{"path":"module1.html","id":"popularity-of-spreadsheets","chapter":"Module 1 Separating data recording and analysis","heading":"1.1 Popularity of spreadsheets","text":"us authors old enough remember home computers novelty.\nfirst got computer home, opened kinds new powers.one us, one particularly exciting piece software something called\nPrint Shop. software let amateur graphic designer. \ndesign things like signs invitations. printer paper time\nconnected one sheet next, perforations , \neven make long banners. “Happy Birthday” banners, “Congratulations” banners,\n“Welcome Home” banners: . someone ’d never \ntools , thrilling.evidently early spreadsheet software made business executives feel.\nprograms, executive wanted crunch numbers, ’d\nsend request accounting department. initial spreadsheet\nprogram (VisiCalc) disrupted process. allowed one person quickly\napply test different models calculations recorded data.11 spreadsheet programs allowed non-programmers \nengage data, including data processing analysis tasks, way \npreviously required programming expertise.12 \nspreadsheet programs executive just play numbers .early target spreadsheet programs business\nexecutives, programs designed simple easy use—just\none step complexity crunching numbers back envelope.13Spreadsheet programs fact became popular within\nbusinesses many attribute programs driving uptake \npersonal computers.14Spreadsheets become popular part many people know \nuse , least basic ways, many people software \ncomputers files can shared almost guarantee everyone \nable open file computer.15\nSpreadsheets use visual metaphore traditional gridded ledger sheet,16 providing interface easy users \nimmediately understand can easily create mental map.17 visually clear interface also means \nspreadsheets can printed incorporated documents “-”, \nworkable understandable table data values. fact, \npopular plug-software packages early spreadsheet program Lotus 1-2-3\nprograms printing publishing spreadsheets.18\n“See Get” interface huge advance previous\nmethods data analysis first spreadsheet program, VisiCalc, providing\n“window data” accessible business executives others\nwithout programming expertise.19 Several surveys \nresearchers found spreadsheets popular \nsimplicity ease--use.20 contrast, databases scripted programming\nlanguages can perceived requiring cognitive load lengthy training\nworth investment easier tool available.21Software tools like Print Shop spreadsheet programs perfectly\ndesigned amateurs begin things otherwise required\noutsourcing professional. fantastic tools amateur exploration.\nmake fun test ideas.However, types software tools easy convenient use \ncan tempting let replace solid, production-level tools. ’s\neasy, words, make tool used tackle problem,\nrather just first tool use explore solution. tools \nalso often cheapest option, either monetary cost time\ninvestment learn . However, often fail ’re used \nreplacement solid options. can case spreadsheet\nprograms biomedical research, spreadsheets often used \nstraightforward way record data (can solid tool),\nalso develop complex pipelines process analyze data \ncollected.","code":""},{"path":"module1.html","id":"hazards-of-combining-recording-and-analysis","chapter":"Module 1 Separating data recording and analysis","heading":"1.2 Hazards of combining recording and analysis","text":"cases, researchers use spreadsheets solely record data, simple\ntype database.22 However, biomedical researchers often use\nspreadsheets record analyze experimental data.23 case, data processing analysis implemented\nuse formulas macros embedded within spreadsheet.spreadsheet formulas macros within , spreadsheet program\ncreates internal record cells connected formulas.\nexample, say value specific cell converted Fahrenheit \nCelsius fill second cell, value combined values\ncolumn calculate mean temperature across several observations. \ncase, spreadsheet program internally saved later cells\ndepend earlier ones. change value recorded cell \nspreadsheet, spreadsheet program queries record recalculates\ncells depend cell. process allows program quickly\n“react” change cell inputs, immediately providing update \ndownstream calculations analyses.24 Since early \ndevelopment, spreadsheet programs also included macros, “single\ncomputer instruction stands sequence operations.”25There important downsides using tools within scientific\nresearch create spreadsheets combine data recording data analysis.\ninclude:Raw data often lostAnalysis steps often opaqueThere higher potential errors analysisThere better software tools available data analysisIt make difficult collaborate statisticiansLet’s take look .","code":""},{"path":"module1.html","id":"raw-data-often-lost","chapter":"Module 1 Separating data recording and analysis","heading":"1.2.1 Raw data often lost","text":"One key tenets ensuring research computationally reproducible\nalways keep copy raw data, well steps taken get \nraw data cleaned version data results data\nanalysis. However, maintaining easily accessible copy original raw data\nproject common problem among biomedical researchers,26 especially team members move laboratory group.27 fact, one study \noperational spreadsheets found:“data used spreadsheets undocumented practical\nway check . Even original developer difficulty checking \ndata.”28One thing can contribute problem use spreadsheets \njointly record analyze data. First, data spreadsheet typically \nsaved “read-”, possible accidentally overwritten:\nsituations spreadsheets shared among multiple users, original cell\nvalues can easily accidentally written , may clear last\nchanged value, changed, .29 ,\nraw processed data combined spreadsheet, makes hard \nidentify data points within spreadsheet make raw data \nresult processing raw data.Another issue many spreadsheets use proprietary format. \ndevelopment spreadsheet programs, use proprietary binary file formats\nhelped software program keep users, increasing barriers user switch\nnew program (since new program wouldn’t able read old\nfiles).30 However, file format may hard open \nfuture, software changes evolves;31 comparison,\nplain text files widely accessible general purpose tools—\ntext editor type software available computers, \nexample—regardless changes proprietary software like Microsoft Excel.","code":""},{"path":"module1.html","id":"analysis-steps-are-often-opaque","chapter":"Module 1 Separating data recording and analysis","heading":"1.2.2 Analysis steps are often opaque","text":"keep analysis steps clear—whether calculation done scripted\ncode spreadsheets pen--paper calculations—important \ndocument done step .32 Scripted\nlanguages allow code comments, written directly script\nevaluated computer, can used document steps within\ncode without changing operation code. , program file\noften presents linear, step--step view pipeline, stored\nseparated data.33 Calculations done\npen--paper (e.g., laboratory notebook) can annotated text\ndocument steps. Spreadsheets, hand, often poorly\ndocumented, documented ways hard keep track .Within spreadsheets, logic methods behind pipeline data\nprocessing analysis often documented, documented cell\ncomments (hard see whole) emails, spreadsheet file.\nOne study investigated large collection spreadsheets found \ninclude documentation explaining logic implementation data\nprocessing analysis implemented within spreadsheet.34 survey neuroscience researchers UK\ninstitute found third respondents included documentation\nspreadsheets used research laboratories.35When spreadsheet pipelines documented, often methods \nhard find interpret later. One study scientific researchers found\n, research spreadsheets documented, often “cell\ncomments” added specific cells spreadsheet, can hard \ninterpret inclusively understand flow logic spreadsheet \nwhole.36In cases, teams use email chains, rather document , \ndiscuss document functionality changes spreadsheets. pass\nversions spreadsheet file attachments emails discuss \nspreadsheet email body.One research team investigated 700,000 emails employees Enron \nreleased legal proceedings.37 specifically\ninvestigated spreadsheets attached emails (15,000\nspreadsheets) teams discussed spreadsheets within emails\n. found logic methods calculations within \nspreadsheets often documented within bodies emails. means ,\nsomeone needs figure step taken identify error\nintroduced spreadsheet, must dig chain old emails\ndocumenting spreadsheet, rather relevant documentation\nwithin spreadsheet’s file.Adding issue data processing analysis pipelines \nspreadsheets carefully designed; instead, ’s typically \nspreadsheet user start directly entering data formulas without clear\noverall plan.38 result, research spreadsheets \noften designed follow common structure research field \nlaboratory group.39Another problem comes may one person team \nfully understands spreadsheet: person created spreadsheet.40 particularly common spreadsheet\nincludes complex macros complicated structure analysis pipeline.41 practice creates heavy dependence \nperson created spreadsheet anytime data results \nspreadsheet need interpreted. particularly problematic projects\nspreadsheet shared collaboration adapted used \nfuture project, often done scientific research groups. case,\ncan hard “onboard” new people use file, much work \nknowledge spreadsheet can lost person moves \nbusiness laboratory group.42If share spreadsheet numerous complex macros formulas\nincluded clean analyze data, can take extensive amount time,\ncases may impossible, researcher share \ndecipher done get original data input cells \nfinal results shown others graphs. , others can’t figure\nsteps done macros formulas spreadsheet, \nable check problems logic overall analysis\npipeline errors specific formulas used within pipeline. \nalso struggle extend adapt spreadsheet used \nprojects. problems come sharing collaborator, \nalso reviewing spreadsheets previously created used (\nmany noted, frequent collaborator likely “future ”).\nfact, one survey biomedical researchers University Washington\nnoted ,“profusion individually created spreadsheets containing overlapping \ninconsistently updated data created great deal confusion within labs.\nlittle consideration future data exchange submission\nrequirements time publication.”43","code":""},{"path":"module1.html","id":"potential-for-errors","chapter":"Module 1 Separating data recording and analysis","heading":"1.2.3 Potential for errors","text":"spreadsheets often poor job making analysis steps\ntransparent, can prone bugs analysis. one early article \nhistory spreadsheet programs notes:“People tend forget even elegantly crafted spreadsheet \nhouse cards, ready collapse first erroneous assumption. \nspreadsheet looks good turns tragically wrong becoming\nfamiliar phenomenon.”44Indeed, previous studies found errors common within\nspreadsheets.45 example, one study 50\noperational spreadsheets found 90% contained least one error.46In part, easier make errors spreadsheets harder catch errors\nlater work spreadsheet formulas connections \ncells aren’t visible look spreadsheet—’re behind \nscenes.47 makes hard get clear complete\nview pipeline analytic steps data processing analysis within \nspreadsheet, discern cells connected within across sheets \nspreadsheet.characteristics spreadsheets may heighten chances errors. \ninclude high conditional complexity, can result lots branching \ndata flow / else structures, well formulas depend \nlarge number cells incorporate many functions.48 Following logical chain spreadsheet formulas\ncan particularly difficult several calculations chained row.49 cases, trying figure long\nchains dependent formulas across spreadsheet cells, may even \nsketch hand flow information spreadsheet understand\n’s going .50 spreadsheet uses macros, can\nalso make particularly hard figure steps analysis \ndiagnose fix bugs steps.51 One study investigated spreadsheets used \npractice noted , “Many spreadsheets chaotically designed \nauditing (especially formulas) extremely difficult impossible.”52In cases, formula dependencies might span across different sheets \nspreadsheet file. cross-sheet dependencies can make analysis steps\neven opaque,53 change cell value \none sheet might immediately visible change another cell \nsheet (true spreadsheets large cells sheet\nconcurrently visible screen). common sources errors\nincluded incorrect references cells inside formulas incorrect use \nformulas54 errors introduced common practice \ncopying pasting developing spreadsheets.55There methods brought traditional programming work\nspreadsheet programming try help limit errors, including tool\ncalled assertions allows users validate data test logic within \nspreadsheets.56 However, often \nimplemented, part perhaps many spreadsheet users see \n“end-users”, creating spreadsheets personal use rather \nsomething robust future use others, don’t seek strategies\nadopted programmers creating stable tools others use.57 practice, though, spreadsheet often used\nmuch longer, people, originally intended. early \nhistory spreadsheet programs, users shared spreadsheet files \ninteresting functionality users,58 \nlifespan spreadsheet can extend extend—spreadsheet created one\nuser personal use can end used modified \nperson others years.59","code":""},{"path":"module1.html","id":"better-software-tools-are-available","chapter":"Module 1 Separating data recording and analysis","heading":"1.2.4 Better software tools are available","text":"spreadsheets serve widely-used tool data recording analysis,\nmany cases spreadsheets programs poorly suited pre-process analyze\nscientific data compared programs. tools interfaces continue \ndevelop make software user-friendly new programming,\nscientists may want reevaluate costs benefits, terms time\nrequired training aptness tools, spreadsheet programs compared \nscripted programming languages like R Python.Several problems identified spreadsheet programs context\nrecording , especially, analyzing scientific data. First, statistical\nmethods may inferior available statistical programming\nlanguage. Many statistical operations require computations \nperfectly achieved computer, since computer must ultimately solve\nmany mathematical problems using numerical approximations (e.g., calculus). \nchoice algorithms used approximations heavily influence \nclosely result approximates true answer. Since popular\nspreadsheet program (Excel) closed-source, hard identify \ndiagnose problems, likely less incentive problems \nstatistical methodology fixed (rather using development time \nfunds increase easier--see functionality program).series papers examined quality statistical methods several\nstatistical software programs, including Excel, starting 1990s.60 \nearliest studies, found concerns across programs considered.61 One biggest\nconcerns, however, little evidence years \nidentified problems Excel resolved, least improved, time.62 authors note may\nlittle incentive checking fixing problems algorithms \nstatistical approximation closed-source software like Excel, sales\nmight depend immediately evident functionality software,\nproblems statistical algorithms might less evident potential\nusers.63Open-source software, hand, offers pathways identifying fixing\nproblems software, including statistical algorithms methods\nimplemented software’s code. Since full source code available, researchers\ncan closely inspect algorithms used compare latest\nknowledge statistical computing methodology. , inferior algorithm \nuse, open-source software licenses allow user adapt extend software,\nincluding implement better statistical algorithms.Another problem spreadsheet programs can include automated functionality\n’s meant make something easier users, might invisibly\ncreate problems. critical problem, example, identified \nusing Excel genomics data. Excel encounters cell value format\nseems like date (e.g., “Mar-3-06”), try convert\ncell “date” class. Many software programs save date special\n“date” format, printed visually appears format like\n“3-Mar-06” saved internally program number (Microsoft\nExcel, number days since January 1, 1900).64\n, software can easily undertake calculations dates,\nlike calculating number days two dates two dates \nearlier. Bioinformatics researchers National Institutes Health found\nExcel type automatic irreversible date conversion \n30 gene names, including “MAR3” “APR-4”, resulting gene names \nlost analysis.65Avoiding automatic date conversion required specifying columns\nsusceptible problems, including columns gene names, \nretained “text” class Excel’s file import process. \nproblem originally identified published 2004,66\nalong tips identify avoid problem, study 2016 found \napproximately fifth genomics papers investigated large-scale review\ngene name errors resulting Excel automatic conversion, rate \nerrors actually increasing time.67Other automatic conversion problems caused lost clone identifiers \ncomposed digits letter “E.”68 assumed expressing number using scientific\nnotation automatically irreversibly converted numeric class.\nautomatic conversion problems can caused cells start \noperator (e.g., “+ control”) leading zeros numeric identifier\n(e.g., “007”).69Finally, spreadsheet programs can limited analysis needs become \ncomplex large.70 example, spreadsheets can \nproblematic integrating merging large, separate datasets.71 , spreadsheet programs continue expand \ncapacity data, large datasets continue face limits\nmay reached practical applications—72until\nrecently, example, Excel handle one million rows data\nper spreadsheet. Even spreadsheets can handle larger data, efficiency\nrunning data processing analysis pipelines across large datasets can \nslow compared code implemented programming languages.","code":""},{"path":"module1.html","id":"difficulty-collaborating-with-statisticians","chapter":"Module 1 Separating data recording and analysis","heading":"1.2.5 Difficulty collaborating with statisticians","text":"Modern biomedical researchers requires large teams, statisticians \nbioinformaticians often included, enable sophisticated processing \nanalysis experimental data. However, process combining data recording\nanalysis, especially use spreadsheet programs, can create\nbarriers working across disciplines. One group defined issues “data\nfriction” “science friction”—extra steps work required \ninterface data passes, example, machine analysis \ncollaborator one discipline one separate discipline.73When collaborating statisticians bioinformaticians, one key\nsources “data friction” can result use spreadsheets \njointly record analyze experiemental data. First, spreadsheets easy \nprint copy another format (e.g., PowerPoint presentation, Word\ndocument), researchers often design spreadsheets immediately\nvisually appealing viewers. example, spreadsheet might designed \ninclude hierarchically organized headers (e.g., heading subheading, \nwithin cell merged across several columns), show result \ncalculation bottom column observations (e.g., “Total” last\ncell column).74 Multiple separate small tables\nmight included sheet, empty cells used visual\nseparation, use “horizontal single entry” design , headers \nleftmost column rather top row.75These spreadsheet design choices make much difficult contents \nspreadsheet read statistical programs. types data\nrequire several extra steps coding, cases fairly complex coding, \nregular expressions logical rules needed parse data convert \nneeded shape, statistical work can done dataset.\npoor use time collaborating statistician, especially \ncan avoided design data recording template. , \nintroduces many chances errors cleaning data., information embedded formulas, macros, extra formatting like\ncolor text boxes lost spreadsheet file input \nprograms. Spreadsheets allow users use highlighting represent information\n(e.g., measurements control animals shown red, experiment\nanimals blue) include information documentation text boxes. \nexample, one survey study biomedical researchers University \nWashington included quote respondent: “one spreadsheet \nchromosomes … ’ve gone color coded \nhomozygosity linkage.”76 information encoded \nsheet color lost data spreadsheet read\nanother statistical program.","code":""},{"path":"module1.html","id":"approaches-to-separate-recording-and-analysis","chapter":"Module 1 Separating data recording and analysis","heading":"1.3 Approaches to separate recording and analysis","text":"remaining modules section, present describe\ntechniques can used limit remove problems. First, \nnext modules, walk techniques design data recording\nformats data saved consistent format across experiments within \nlaboratory group, way removes “data friction” collaboration\nstatisticians later use scripted code. techniques can \nimmediately used design better spreadsheet used solely data\ncollection.later modules, discuss use project directories coordinate\ndata recording analysis steps within directory, using separate files\ndata recording versus data processing analysis. advanced\nformats enable use quality assurance / control measures like testing\ndata entry analysis functionality, better documentation data analysis\npipelines, easy use version control track projects collaborate\ntransparently recorded history.","code":""},{"path":"module1.html","id":"discussion-questions","chapter":"Module 1 Separating data recording and analysis","heading":"1.4 Discussion questions","text":"types data record research? use spreadsheets \nrecord data scientific research? data aren’t recorded \nspreadsheets, alternatives use (paper laboratory notebooks,\nelectronic laboratory notebooks, data recorded directly equipment, )?types data record research? use spreadsheets \nrecord data scientific research? data aren’t recorded \nspreadsheets, alternatives use (paper laboratory notebooks,\nelectronic laboratory notebooks, data recorded directly equipment, )?use spreadsheets scientific research, like \ndislike ? reading module mentioned \nupsides include easy learn many people access\nsoftware use . advantages resonate ?use spreadsheets scientific research, like \ndislike ? reading module mentioned \nupsides include easy learn many people access\nsoftware use . advantages resonate ?variety ways scientists use spreadsheets research.\nDescribe examples ’ve used spreadsheets :\nRecord data\nClean data\nPreprocess data\nAnalyze data\nCreate graphs\nCreate reports\nBased reading experience, potential advantages \npitfalls using spreadsheets purposes?\nvariety ways scientists use spreadsheets research.\nDescribe examples ’ve used spreadsheets :Record dataClean dataPreprocess dataAnalyze dataCreate graphsCreate reports\nBased reading experience, potential advantages \npitfalls using spreadsheets purposes?Describe document steps take clean preprocess data\nrecord . Based reading, list advantages disadvantages\nmethod use versus alternative methods documentation.Describe document steps take clean preprocess data\nrecord . Based reading, list advantages disadvantages\nmethod use versus alternative methods documentation.mean consider raw data “read-”? ways\ninsure raw data treated read-? examples \nresearch raw data treated read-? problems \ncause research reproducibility?mean consider raw data “read-”? ways\ninsure raw data treated read-? examples \nresearch raw data treated read-? problems \ncause research reproducibility?someone new joins research group, teach \nrecord experimental data? experiences go\nsmoothly?someone new joins research group, teach \nrecord experimental data? experiences go\nsmoothly?research, ever used spreadsheet included formulas \nmacros? document steps check potential errors?\nsee advantages disadvantages including formulas \nmacros spreadsheets?research, ever used spreadsheet included formulas \nmacros? document steps check potential errors?\nsee advantages disadvantages including formulas \nmacros spreadsheets?","code":""},{"path":"module2.html","id":"module2","chapter":"Module 2 Principles and power of structured data formats","heading":"Module 2 Principles and power of structured data formats","text":"Guru Madhavan, Senior Director Programs National Academy \nEngineering, wrote book 2015 called Applied Minds: Engineers Think.\nbook, described powerful tool engineers—standards:“Standards products grammar language. People sometimes\ncriticize standards making life matter routine rather inspiration.\nargue standards hinder creativity keep us slaves past.\ntry imagining world without standards. tenderloin beef cuts \ngeometric design highways, standards may diminish variety \nauthenticity, improve efficiency. street signs nutrition\nlabels, standards provide common language reason. Internet\nprotocols MP3 audio formats, standards enable systems work together.\npaper sizes … George Laurer’s Universal Product Code, standards\noffer convenience comparability.”77Standards can powerful tool biomedical researchers, well, including\ncomes recording data.\nformat experimental data recorded can large influence \neasy likely implement reproducibility tools later stages \nresearch workflow. Recording data “structured” format brings many\nbenefits. module, explain makes dataset “structured” \nformat powerful tool reproducible research.Every extra step data cleaning another chance introduce errors \nexperimental biomedical data, yet laboratory-based researchers often share\nexperimental data collaborators format requires extensive\nadditional cleaning can input data analysis.78\nRecording data “structured” format brings many benefits later stages \nresearch process, especially terms improving reproducibility \nreducing probability errors analysis.79 Data \nstructured, tabular, two-dimensional format substantially easier \ncollaborators understand work , without additional data formatting.80 , using consistent structured format across many\ndata research project, becomes much easier create solid,\nwell-tested code scripts data pre-processing analysis apply \nscripts consistently reproducibly across datasets multiple experiments.81 However, many biomedical researchers unaware \nsimple yet powerful strategy data recording can improve \nefficiency effectiveness collaborations.82 \nmodule, ’ll walk several types standards can used \nrecording biomedical data.Objectives. module, trainee able :Define ontology, minimum information, file formatList elements structured data formatExplain standards can improve scientific data recordingFind existing ontologies biological biomedical research","code":""},{"path":"module2.html","id":"data-recording-standards","chapter":"Module 2 Principles and power of structured data formats","heading":"2.1 Data recording standards","text":"Many people organizations (including funders) excited idea \ndeveloping using data standards. Good standards—ones widely\nadapted researchers—can help making sure data submitted data\nrepositories used widely software can developed \ninteroperable data many research groups.simple example, think recording dates. minimum\ninformation standard date might always —recorded value\nmust include day month, month, year. However, information\ncan structured variety ways. Often scientific data, ’s common\nrecord information going largest smallest units, March\n12, 2006, recorded “2006-03-12”. Another convention (especially \nUS) record month first (e.g., “3/12/06”), another (common\nEurope) record day month first (e.g., “12/3/06”).trying combine data different datasets dates, \nuse different structure, ’s easy see mistakes introduced\nunless data carefully reformatted. example, March 12 (“3-12”\nmonth-first, “12-3” day-first) easily mistaken December\n3, vice versa. Even errors avoided, combining data different\nstructures take time combining data structure,\nextra needs reformatting get data common\nstructure.Standards can operate level individual research groups \nlevel scientific community whole. potential advantages \ncommunity-level standards big: offer chance develop\ncommon-purpose tools code scripts data analysis, well make \neasier re-use combine experimental data previous research \nposted open data repositories. software tool can reused, \ntime can spent developing testing , people use , bugs\nshortcomings can identified corrected. Community-wide standards can\nlead databases data different experiments, different\nlaboratory groups, structured way makes easy researchers\nunderstand dataset, find pieces data interest within datasets, \nintegrate different datasets.83 Similarly, community-wide\nstandards, can become much easier different research groups \ncollaborate research group use data generated \nequipment different manufacturers.84 \narticle interoperable bioscience data notes,“Without community-level harmonization interoperability, many community\nprojects risk becoming data silos.”85However, important limitations community-wide standards, well.\ncan difficult impose standards top-community-wide,\nparticularly low-throughput data collection (e.g., laboratory bench\nmeasurements), research groups long habit recording\ndata spreadsheets format defined individual researchers research\ngroups. One paper highlights point:“data exchange formats PSI-MI MAGE-ML helped get many \nhigh-throughput data sets public domain. Nevertheless, bench\nbiologist’s point view benefits adopting standards yet\noverwhelming. standardization efforts still mainly investment \nbiologists.”86Further, fields, community-wide standards struggled remain\nstable, can frustrate community members, scripts software must \nrevamped handle shifting formats.87 cases, useful compromise follow \ngeneral data recording format, rather one prescriptive. \nexample, committing recording data format “tidy” (\ndiscuss extensively module 3) may much flexible—able \nmeet needs large range experimental designs—use \ncommon spreadsheet template prescriptive standardized data format.","code":""},{"path":"module2.html","id":"elements-of-a-data-recording-standard","chapter":"Module 2 Principles and power of structured data formats","heading":"2.2 Elements of a data recording standard","text":"Standards can clarify several elements: vocabulary used within data, \ncontent included dataset, format \ncontent stored. One article names three facets data standard \nontologies, minimum information, file formats.88","code":""},{"path":"module2.html","id":"ontology-standards","chapter":"Module 2 Principles and power of structured data formats","heading":"2.2.1 Ontology standards","text":"first facet data standard called ontology (sometimes called \nterminology).89 ontology helps define vocabulary \ncontrolled consistent. helps researchers, want talk \nidea thing, use one word, just one word, ensure \nword used researchers refer idea \nthing. Ontologies also help define relationships ideas \nconcrete things research area,90 ’ll focus \nuse provided consistent vocabulary use recording data.Let’s start simple example give idea ontology\n. call small mammal often kept pet \nfour legs whiskers purrs? recording data includes \nanimal, record “cat” “feline” maybe, depending \nanimal, even “tabby” “tom” “kitten”? Similarly, record tuberculosis\n“tuberculosis” “TB” maybe even “consumption”? use \nword consistently dataset record idea, human might\nable understand two words considered equivalent, computer\nable immediately tell.larger scale, research community can adapt ontology—one \nagree use throughout studies—make easier understand \nintegrate datasets produced different research laboratories. every\nresearch group uses term “cat” example , code can easily \nwritten extract combine data recorded cats across large\nrepository experimental data. hand, different terms used,\nmight necessary first create list terms used datasets\nrespository, pick list find terms \nexchangeable “cat”, write script pull data terms.Several onotologies already exist created biological \nbiomedical research.91 biomedical science, practice, \nresearch, BioPortal website (http://bioportal.bioontology.org/) provides\naccess 1,000 ontologies, including several versions International\nClassification Diseases, Medical Subject Headings (MESH), National\nCancer Institute Thesaurus, Orphanet Rare Disease Ontology National\nCenter Biotechnology Information (NCBI) Organismal Classification. \nontology BioPortal website, website provides link downloading\nontology several formats.Try downloading one ontologies using plaintext file format (“CSV”\nchoice download options BioPortal link). , can open\nfavorite spreadsheet program explore defines specific\nterms use idea thing might need discuss within topic\narea, well synonyms terms.use ontology recording data, just make sure use \nontology’s suggested terms data. example, ’d like use \nOntology Biomedical Investigations\n(http://bioportal.bioontology.org/ontologies/OBI) recording many\nchildren woman born alive, name column \ndata “number live births”, “# live births” “live births (N)” \nanything else. collections ontologies exist fields scientific\nresearch, including Open Biological Biomedical Ontology (OBO) Foundry\n(http://www.obofoundry.org/).community-wide ontologies field, worthwhile use\nrecording experimental data research group. Even better \nconsistently use defined terms, also follow conventions\ncapitalization. statistical programs provide tools change\ncapitalization (example, change letters character string \nlower case), process require extra step data cleaning \nextra chance confusion errors introduced data.","code":""},{"path":"module2.html","id":"minimum-information-standards","chapter":"Module 2 Principles and power of structured data formats","heading":"2.2.2 Minimum information standards","text":"Another part data standard minimum information. Within data\nrecording standard, minimum information (sometimes also called minimum\nreporting guidelines92 reporting requirements)93 specify included dataset.94 Using minimum information standards help ensure data\nwithin laboratory, data posted repository, contain number \nrequired elements. makes easier re-use data, either compare \ndata lab newly generated, combine several posted datasets \naggregate new, integrated analysis, considerations growing\nimportance increasing prevalence research repositories \nresearch consortia many fields biomedical science.95One article discusses software systems biology provides definition\nwell examples minimum information within field:“Minimum information checklist required supporting information \ndatasets different experiments. Examples include: Minimum Information \nMicroarray Experiment (MIAME), Minimum Information Proteomic\nExperiment (MIAPE), Minimum Information Biological Biomedical\nInvestigations (MIBBI) project.”96","code":""},{"path":"module2.html","id":"standardized-file-formats","chapter":"Module 2 Principles and power of structured data formats","heading":"2.2.3 Standardized file formats","text":"using standard ontology standard minimum information \nhelpful start, just means dataset required elements\nsomewhere, using consistent vocabulary—doesn’t specify \nelements data ’ll place every dataset\nmeets standards. result, datasets meet common\nstandard can still hard combine, create common data analysis\nscripts tools , since dataset require different process \npull given element.Computer files serve way organize data, whether ’s recorded\ndatapoints written documents computer programs.97 \nfile format defines rules bytes chunk memory \nmakes certain file parsed interpreted anytime want \nmeaningfully access use data within file.98 many file formats may familiar\n—file ends “.pdf” must opened Portable Document Format\n(PDF) Reader like Adobe Acrobat, won’t make much sense (can try \ntrying open “.pdf” file text editor, like TextEdit \nNotepad). PDF Reader software programmed interpret data \n“.pdf” file based rules defining data stored section \ncomputer memory file. “.pdf” files conform \nfile format rules, powerful software can built works file \nformat.certain types biomedical data, challenge standardizing format\nsimilarly addressed use well-defined rules \ncontent data, also way content structured. can \nstandardized standardized file formats (sometimes also called data\nexchange formats)99 often defines \nupper-level file format (e.g., use comma-separated plain text, “.csv”,\nfile format), also data within file type organized. data\ndifferent research groups experiments recorded using file\nformat, researchers can develop software tools can repeatedly used \ninterpret visualize data. hand, different experiments\nrecord data using different formats, bespoke analysis scripts must written\nseparate dataset.blow efficiency data analysis, also \nthreat accuracy analysis. set tools can developed \nwork , time can devoted refining tools \ntesting potential errors bugs, one-shot scripts often can’t\ncurated similar care. One paper highlights problems come \nworking files don’t follow defined format:“Vast swathes bioscience data remain locked esoteric formats, \ndescribed using nonstandard terminology, lack sufficient contextual information,\nsimply never shared due perceived cost futility \nexercise.”100Some biomedical data file formats created help smooth \ntransfer data ’s captured complex equipment software can\nanalyze data. example, many immunological studies need measure\nimmune cell populations experiments, use piece equipment\ncalled flow cytometer probes cells sample lasers measures\nresulting intensities determine characteristics cell. data\ncreated equipment large (often measurements several lasers \ntaken million cells single run). data also complex, \nneed record intensity measurements laser, \nalso metadata equipment characteristics run.every model flow cytometer used different file format \nsaving resulting data, different set analysis software need\ndeveloped accompany piece equipment. example, laboratory\nuniversity flow cytometers two different companies need\nlicenses two different software programs work data recorded flow\ncytometers, need learn use software package\nseparately. chance software developed used shared\ncode data analysis, also included separate sets code \nread data types equipment reformat common\nformat.isn’t case, however. Instead, commonly agreed file format\nflow cytometers use record data collect, called \nFCS file format. format defined series papers\n(e.g., Josef Spidlen et al.101), several separate versions file format\nevolved. provides clear specifications regarding save relevant\npiece information block memory devoted data recorded \nflow cytometer. result, people able\ncreate software, proprietary open-source, can used \ndata recorded flow cytometer, regardless company manufacturer \npiece equipment used generate data.types biomedical data also standardized file formats,\nincluding FASTQ file format sequencing data mzML file format \nmetabolomics data. cases defined organization, society,\ninitiative (e.g., Metabolomics Standards Initiative),102 cases file format developed \nspecific equipment manufacturer become popular enough ’s established\nstandard recording type data.103","code":""},{"path":"module2.html","id":"defining-data-recording-standards-for-data-recorded-by-hand","chapter":"Module 2 Principles and power of structured data formats","heading":"2.3 Defining data recording standards for data recorded “by hand”","text":"data record experiments comes complex\nequipment, like flow cytometers mass spectrometers, may recording much\ndata standardized format without extra effort, \nformat default output format equipment. However, may \ncontrol data recorded experiments, including smaller,\nless complex data record directly laboratory notebook \nspreadsheet. can derive number benefits defining using \nstandard collecting data, one paper describes \noutput “traditional, low-throughput bench science.”104When recording type data, data may written ad hoc\nway—however particular researcher experiment thinks makes\nsense—format might change experiment, even many\nexperiments collect similar data. result, becomes harder create\nstandardized data processing analysis scripts work data \nintegrate data data collected experiment. , \neveryone laboratory sets spreadsheets data recording \nway, much harder one person group look data another\nperson recorded immediately find need within spreadsheet.step better direction, head research group may designate \ncommon formats (e.g., spreadsheet template) researchers group\nuse recording data specific type experiments. One key\nadvantage using standardized data formats even recording simple,\n“low-throughput” data everyone research group able \nunderstand work data recorded anyone else group—data \nbecome impenetrable person recorded leaves group. Also,\ngroup member used format, process setting record\ndata new experiment quicker, won’t require effort \ndeciding setting de novo format spreadsheet recording\nfile. Instead, template file can created can copied starting\npoint new data recording.also allows team create tools scripts read \nanalyze data can re-used across multiple experiments minor\nchanges. helps improve efficiency reproducibility data\nanalysis, visualization, reporting steps research project.Developing kinds standards require extra time commitment.105 First, time needed design format, \ntake develop format inclusive enough \nplace put data might want record certain type experiment.\nSecond, take time teach laboratory member format\noversight make sure comply record data.flip side, longer-term advantages using defined, structured\nformat outweigh short-term time investments many laboratory groups\nfrequently used data types. creating using consistent structure \nrecord data certain type, members laboratory group can increase \nefficiency (since need re-design data recording structure\nrepeatedly). can also make easier downstream collaborators, like\nbiostatisticians bioinformaticians, work output, \ncollaborators can create tools scripts can recycled across\nexperiments research projects know data always come \nformat. One paper suggests balance can found, terms deciding whether\nbenefits developing standard outweigh costs, considering \noften data certain type generated used:“develop deploy standard creates overhead, can expensive.\nStandards help particular type information \nexchanged often enough pay development, implementation, usage\nstandard lifespan.”106These benefits even dramatic data format standards\ncreated used whole research field (e.g., standard data\nrecording format always used researchers conducting certain type \ndrug development experiment). case, tools built one institution\ncan used insitutions. However, level field-wide coordination\ncan hard achieve, realistic immediate goal might \nformalizing data recording structures within research group department,\nkeeping eye formats gaining popularity standards \nfield adopt within group.commit creating defined, structured format, ’ll need decide\nstructure . many options , ’s \ntempting use format easy human eyes.107 example, may seem appealing create \nformat easily copied pasted presentations Word\ndocuments look nice presentation formats. facilitate\nuse, laboratory might set recording format based spreadsheet\ntemplate includes multiple tables different data types \nsheet, multi-level column headings.Unfortunately, many characteristics—make format attractive \nhuman eyes—make harder computer make sense . example,\ninclude two tables spreadsheet, might make easier \nperson get look two small data tables without toggle \ndifferent parts spreadsheet. However, want read data \nstatistical program (work collaborator ), likely\ntake complex code try tell computer find second table\nspreadsheet. applies include blank lines top\nspreadsheet, use multi-level headers, use “summary” rows \nbottom table. , information ’ve included colors \ntext boxes spreadsheet lost data’s read \nstatistical program. design elements make much harder read data\nembedded spreadsheet computer programs, including programs \ncomplex data analysis visualization, like R Python.one article notes:“Data formatted way facilitates computer readability. \noften, humans record data way maximizes readability \nus, takes considerable amount cleaning tidying can \nprocessed computer. data (metadata) computer readable,\ncan leverage computers work data.”108One easiest format computer read two-dimensional\n“box” data, first row spreadsheet gives column names,\nrow contains equal number entries. type \ntwo-dimensional tabular structure forms basis several popular\n“delimited” file formats serve lingua franca across many simple\ncomputer programs, like comma-separated values (CSV) format, \ntab-delimited values (TSV) format, general delimiter-separated\nvalues (DSV) format, common format data exchange across\ndatabases, spreadsheet programs, statistical programs.109Any deviations two-dimensional “box” shape can crate problems \ncomputer program tries parse data. anything data format \nrequires extra coding reading data another program, \nintroducing new opportunity errors interface data recording\ndata analysis. strong reasons use format requires\nextra steps, still possible create code read parse\ndata statistical programs, format consistently used,\nscripts can developed thoroughly tested allow . However, keep\nmind extra burden data analysis collaborators \nusing program besides spreadsheet program. extra time \nrequire large, since code vetted tested thoroughly\nensure data cleaning process introducing errors. contrast,\ndata recorded two-dimensional format single row column\nnames first row, data analysts can likely read quickly cleanly\nprograms, low risks errors transfer data \nspreadsheet. module 3, ’ll go detail refined\nformat two-dimensional data called tidy data format.","code":""},{"path":"module2.html","id":"discussion-questions-1","chapter":"Module 2 Principles and power of structured data formats","heading":"2.4 Discussion questions","text":"module discusses standards can facilitate scientific\nresearch. Give examples standards come across \nresearch. follow ? ? see advantages\ndisadvantages standards?module discusses standards can facilitate scientific\nresearch. Give examples standards come across \nresearch. follow ? ? see advantages\ndisadvantages standards?module discusses ontologies context “controlled vocabularies”,\ncan insure researchers always use term describing\nthing. examples research people\nusing different terms describe thing? see \nadvantages disadvantages controlled vocabulary?module discusses ontologies context “controlled vocabularies”,\ncan insure researchers always use term describing\nthing. examples research people\nusing different terms describe thing? see \nadvantages disadvantages controlled vocabulary?Find example research field minimum information \nminimum reporting guidelines. see advantages disadvantages\nscientific field whole establishing guidelines? \nadvantages disadvantages researcher?Find example research field minimum information \nminimum reporting guidelines. see advantages disadvantages\nscientific field whole establishing guidelines? \nadvantages disadvantages researcher?module discusses research data sometimes recorded directly \nequipment, times “low-throughput” data recorded\n“hand”. Can give examples type data ’ve come across\nscientific work? Discuss data follow\nstandards recorded. standards include ontology,\nminimum information, file format.module discusses research data sometimes recorded directly \nequipment, times “low-throughput” data recorded\n“hand”. Can give examples type data ’ve come across\nscientific work? Discuss data follow\nstandards recorded. standards include ontology,\nminimum information, file format.","code":""},{"path":"module3.html","id":"module3","chapter":"Module 3 The “tidy” data format","heading":"Module 3 The “tidy” data format","text":"module 2, explained benefits saving data structured\nformat, particular one follows standards discipline. \nsection, ’ll talk “tidy” data format. tidy data format \none implementation tabular, two-dimensional structured data format \nquickly gained popularity among statisticians data scientists since \ndefined 2014 paper.110These principles cover basic rules ordering data, even \nhaven’t heard term tidy data, may already implementing many \nstandards datasets. Datasets format tend easily\nwork , including clean, model, visualize data, well\nintegrate data datasets. particular, data format \ncompatible collection open-source tools R platform called \ntidyverse. characteristics mean , planning use \nstandardized data format recording experimental data research group,\nmay want consider creating one adheres tidy data format.Objectives. module, trainee able :List characteristics defining “tidy” structured data formatUnderstand reformat dataset make follow “tidy” formatExplain difference structured data format (general concept)\n“tidy’ data format (one popular implementation)Understand benefits recording data “tidy” format","code":""},{"path":"module3.html","id":"keeping-things-tidy","chapter":"Module 3 The “tidy” data format","heading":"3.1 Keeping things tidy","text":"Adam Savage built career making things. became famous \nhost TV show Mythbusters, crew builds contraptions test\nurban myths. many years , created models special effects\nmovies. thought lot effectively work teams make\nthings, 2019 published book life maker called Every\nTool Hammer.111Among many insights, Savage focuses importance\ntidying part creation process, saying “’s time, taken,\nmight feel slowing moment, fact saving \ntime long run.”112 introduces new word \nprocess straightening tools materials—“knolling”. borrowed \nterm artist, Tom Sachs, whose rules workshop include,\n“Always Knolling”.idea “knolling” includes key principles. First, \nneed . Put everything else somewhere else. Removing extras makes \nfaster find need need . Second, things need, make\nsure ’re available. “Drawers things go die,” Savage\nsays, highlighting inefficiency look\nthings hidden site work. Finally, organize things\n. Put like things together, arrange everything neatly,\naligning things parallel perpendicular patterns, rather piling \nhaphazardly.Just organizing tools materials improves efficiency workshop,\norganizing data can dramatically improve efficiency data\npre-processing, analysis, visualization. Indeed, “tidying ” data\ncan give dramatic improvements number researchers \ndeveloped systems written papers describe good organization schemes\nuse tidy data (e.g.,).113The principles tidying data follow principles knolling.\nexample, want make sure ’re saving data file \nspreadsheet includes data, removing extras. Lab groups\nsometimes design spreadsheets data collection include space \nrecording data, also space notes, embedded calculations, plots.\nextra elements can make hard extract use data . One\nway tidy dataset remove extra elements. can\n’ve collected data, ’s efficient design way \nrecord data first place without extra elements file \nspreadsheet.can tidy data format reformatting \nfollow rules data format called “tidy data” format. Just \nAdam Savage’s “knolling” helps find things need , using\ntidy data format puts elements data “right” place \nfound powerful collection tools called tidyverse.’ll start module describing rules dataset format must follow \n“tidy” clarifying can set data recording follow\nrules. later parts module, ’ll talk ’s\nhelpful use tidy data format, well bit tidyverse tools\ncan use data format.","code":""},{"path":"module3.html","id":"what-makes-data-tidy","chapter":"Module 3 The “tidy” data format","heading":"3.2 What makes data “tidy”?","text":"“tidy” data format describes one way structure tabular data. name\nfollows focus data format associated set tools—\n“tidyverse”—preparing cleaning (“tidying”) data, contrast sets \ntools focused steps, like data analysis.114 \nword “tidy” meant apply formats “dirty”, \ninclude data incorrect subpar. fact, set datapoints\nsaved file way either “tidy” (sense )115 untidy, depending data organized\nacross columns rows.Wickham notes article, first describes tidy data format,\nideas format evolved seeing many examples \ndifferent ways data organized within two-dimensional structure.\nnotes:“development tidy data driven experience working\nreal-world datasets. , , constraints organization,\ndatasets often constructed bizarre ways. spent countless\nhours struggling get datasets organized way makes data\nanalysis possible, let alone easy.”116To help understand tidy data format Wickham developed, let’s start\nchecklist rules make dataset tidy. drawn\ndirectly journal article originally defined data format.117 rules based common untidy patterns show\ndata recording templates laboratory research. checklist :Data recorded tabular, two-dimensional formatThe data collection file spreadsheet avoids extra elements\nlike plots embedded equations fileEach observation forms rowColumn headers variable names, valuesEach type observational unit forms tableEach variable forms columnA single variable single column, spread across multiple columnsA column contains one variable; multiple variables stored one columnData types consistent within columnIn module 1, discussed first two principles, highlighting \nimportant separate data collection steps data\nprocessing analysis. start module, ’ll go \nitems checklist, help understand makes dataset follow \ntidy data format. aim help able set data recording template\nfollow format, well able tell work data \nothers collect format, restructure .Tidy data, first, must tabular format—, two-dimensional, \ncolumns rows, rows columns length. ’s \nspreadsheet, stored without “extras”, like embedded plots \ncalculations. record data spreadsheet using basic strategy\nsaving single table per spreadsheet, first row giving column\nnames, data tabular format. general, recorded\ndata looks “boxy”, ’s probably two-dimensional tabular format.additional criteria tidy data format, though, \nevery structured, tabular dataset tidy format. Wickham notes\npaper defining format,“statistical datasets rectangular tables made rows columns\n… [] many ways structure underlying data. …\nReal datasets can, often , violate three precepts tidy data \nalmost every way imaginable.”118First, row tidy dataset records values single observation.119 figure data format follows rule, ’s\nimportant determine unit observation data, unit\ntake measurements.120 idea different \nunit analysis, unit ’re focusing study\nhypotheses conclusions (sometimes also called “sampling unit” \n“unit investigation”).121 cases, two might \nequivalent (unit unit observation unit \nmeasurement), often .122 Sedgwick notes:“unit observation unit analysis often confused.\nunit observation, sometimes referred unit \nmeasurement, defined statistically ‘’ ‘’\ndata measured collected. unit analysis\ndefined statistically ‘’ ‘’ \ninformation analysed conclusions made.”123As example, say testing immune system mice responds \ncertain drug time. case, unit analysis might drug,\ncombination drug dose—ultimately, may want test something\nlike one drug effective another. answer research\nquestion, likely several replicates mice treatment group. \nseparate mouse (replicate) used collect observation, mouse \nnever measured twice (.e., different time points, different\ninfection status), unit measurement—level data\npoint collected—mouse. mouse providing \nsingle observation help answer larger research question.another example, say conducted trial human subjects, see \ncertain treatment affects speed recovery, study\nsubject measured different time points. case, unit \nobservation combination study subject time point (unit\nanalysis treatment). means Subject 1’s measurement Time 1 one\nobservation, person’s measurement Time 2 separate\nobservation. dataset comply tidy data format, two\nobservations need recorded separate lines data. \ndata instead different columns record study subject’s measurements\ndifferent time points, data still tabular, \ntidy.dataset tidy, variable values \ncolumn names. ’s helpful talk example understand \nmight end variable value column name. measuring\nstudy subjects different times, one variable ting measure\ntimepoint (subject’s weight, example). Another variable,\nthough, timepoint . One observation might recorded 14 days\nstudy, timepoint “day 14”. Another might\nmeasured 28 days study, measure timepoint\n“day 28”.can tempting put types variables—set part\nstudy design—column names. tempting, example,\ncolumn timepoint, put weight measures within\ncells timepoint. type format look fine visually \neasy readers interpret. ’s problem, ? aren’t\ndata tidy?simply format doesn’t work well software tools\ncreated work tidy data. Remember “tidy” data format isn’t\nmeant contrast, everything else objectively “messy”. Instead,\n’s standard format—insisting certain things consistently \ncertain places, allows tools work format. \nrules tidy format, , exist get things right place \nwork tools.make dataset talked (repeated measures weights \nsubject) tidy, ’d just need move elements around. ’d need\nput description timepoint (e.g., “day 14”, “day 28”) \ncolumn name, instead cells column table. \nmean ’ll need add rows table, fewer columns.\noften refer change pivoting dataframe wider\nformat longer format.example human subjects measured repeated time points, may\ninitially find tidy format unappealing, seems like \nlead lot repeated data. example, wanted record study\nsubject’s sex, seems like tidy format require repeat \ninformation separate line data ’s used record measurements\nsubject different time points. isn’t case—instead, \ntidy data format, different “levels” data observations recorded\nseparate tables.124 words, design \nseparate table unit observation data several \nunits experiment. example, data study\nsubject change across time points study—like \nsubject’s ID, sex, age enrollment—form separate dataset, one\nunit observation study subject, just one\nrow data per study subject data table, measurements \ntime point recorded separate data table. unique\nidentifier, like subject ID, recorded data table can\nused link data two tables. using spreadsheet \nrecord data, mean data separate levels \nobservation recorded separate sheets, sheet \nspreadsheet file. read data scripting language like R \nPython, easy link larger smaller tidy datasets needed\nanalysis, visualizations, reports.Next, dataset tidy, column used\nmeasure separate characteristic measurement (variable) \nmeasurement.125 column either give characteristics \ndata pre-defined study design—example, treatment\nassigned mouse (type variable called fixed variable, since \nvalue fixed start experiment) observed measurements,\nlike level infection measured animal (type variable called \nmeasured variable, since value determined experiment).126Next, make sure column one one variable. Let’s look\nexample see type thing avoid make sure ’re\nfollowing rule. Say ’re recording weights study subjects,\nsometimes collect weight ounces sometimes grams. \nwant include data numeric measure weight \nsubject also unit measure. keep data tidy, need\none column record numeric value measured weight\nanother column units weight measured. \ndon’t use separate columns, instead record values like “22 g” “0.8 oz”\nsingle column, ’ll extra work read data \nprogram like R make data tidy. can done using tool\ncalled regular expressions, ’s even better set initial\ndata recording record numeric value (e.g., 22, 0.8) one column\nunits (e.g., “g”, “oz”) separate column.column dataframe, make sure one type data.\nexample, make sure values numbers, values \ncharacter strings. column dataframe treated R vector, \nvector must limited one data type. try mix different data\ntypes, entries may coerced different data type \ntreated missing value.One culprit look putting comments table cells record\ndata. Say ’re recording animal weights, forgot weigh one animal.\nput comment cell recorded \nweight, ’ll mix numeric values (since cells column\nrecord weight number) one cell character string (\ncomment). need record comments, handle making \nseparate column just .","code":""},{"path":"module3.html","id":"why-make-your-data-tidy","chapter":"Module 3 The “tidy” data format","heading":"3.3 Why make your data tidy?","text":"may seem like lot extra work make dataset tidy, \nbother already structured, tabular format? turns \n, get hang gives data tidy format, ’s pretty\nsimple design recording formats comply rules. ’s ,\ndata tidy format, can directly input collection\ntools R belong something called tidyverse.R’s tidyverse framework enables powerful user-friendly data management,\nprocessing, analysis combining simple tools solve complex, multi-step\nproblems.127 Since tidyverse tools simple share common\ninterface, easier learn, use, combine tools created \ntraditional base R framework.128 tidyverse framework quickly becoming standard\ntaught introductory R courses books,129 ensuring ample training resources researchers new \nprogramming, including books (e.g.,),130 massive open online courses (MOOCs), -site university courses,131 Software\nCarpentry workshops.132 , tools\nextend tidyverse created enable high-quality data\nanalysis visualization several domains, including text mining,133 microbiome studies,134 natural language\nprocessing,135 network analysis,136 ecology,137 genomics.138The tidyverse collection tools united common philosophy: complex\nthings can done simply efficiently small, sharp tools share \ncommon interface. Zev Ross, article tidy tools can\ndeclutter workflow, notes:“philosophy tidyverse similar \ninspired “unix philosophy”, set loose principles ensure\ncommand line tools play well together. … function solve one\nsmall well-defined class problems. solve complex problems, \ncombine simple pieces standard way.”139The tidyverse isn’t popular system follows \nphilosophy—one favorite Legos. Legos small, plastic bricks, \nsmall studs top tubes studs fit bottom. studs\n, standardized size spaced distance apart.\nTherefore, bricks can joined together combination, since \nbrick uses input format (studs standard size spaced \nstandard distance fit tubes bottom brick) \noutput format (, studs standard size spaced standard\ndistance top brick). design, bricks can joined\nregardless whether bricks different colors different heights \ndifferent widths depths. Legos, even though “tool” (brick) \nsimple, tools can combined infinite variations create complex\nstructures.tools tidyverse operate similar principle. input \ntidy dataset (column tidy dataset) (almost) output data\nformat input . tools, required format\ninput output tidy data format,140 called tidy\ndataframe R—dataframe follows rules detailed earlier\nsection.common input / output interface, use small tools follow\ninterface can combined various ways, makes tidyverse\ntools powerful. However, good things tidyverse \nmake popular. One ’s fairly easy learn use tools, \ncomparison learning write code R tools.141 developers created \ntidyverse tools taken lot effort try make sure \nclear consistent user interface.142To help understand user interface, consistent user interface\nacross tools useful, let’s think different example—cars. \ndrive car, get car want steering wheel, \ngas pedal, break pedal, different knobs buttons dashboard.\ncar needs give feedback, uses different gauges \ndashboard, like speedometer, well warning lights sounds.\nCollectively, ways interacting car make car’s user\ninterface. way, function programming language \ncollection parameters can set, let customize way \nfunction runs, well way providing output function \nfinished running way provide messages warnings \nfunction’s run. functions, software developer can usually choose design\nelements function’s user interface, including parameters \ninclude function, name parameters, provide\nfeedback user messages, warnings, final output.tools similar user interfaces, make \neasier users learn use tools \n’ve learned use one. cars, explains rental car\nbusiness able succeed. Even though different car models different\nmany characteristics—engines, colors, software—\nconsistent user interfaces. ’ve learned drive one\ncar, get new car, gas pedal, brake, steering wheel \nalmost guaranteed place operate \nway car learned drive . exceptions rare enough \nmemorable—think many movies laughline character trying \ndrive car driver side opposite side ’re used .tidyverse tools similarly designed similar\nuser interface. example, many tidyverse functions use parameter\nnamed “.data” refer input data. Similarly, parameters\nnamed “.vars” “.funs” repeatedly used tidyverse functions, \nmeaning case. ’s , tidyverse functions typically given names\nclearly describe action function , like filter,\nsummarize, mutate, group. result, final code clear\ncan almost “read” natural language, rather code. Jenny\nBryan notes, article data science:“Tidyverse\nphilosophy rigorously (ruthlessly) identify obey common\nconventions. applies objects passed one function another\nuser interface function presents. Taken isolation, \ninstance seems small unimportant. collectively, creates\ncohesive system: learned one component likely \nable guess another different component works.”143Many people teach\nR programming now focus first teaching tidyverse, given \ncharacteristics,144 ’s often \nfirst focus online courses workshops R programming. Since main\ndata structure tidy data structure, ’s often well worth recording\ndata format tools can easily used explore \nmodel data.","code":""},{"path":"module3.html","id":"using-tidyverse-tools-with-data-in-the-tidy-data-format","chapter":"Module 3 The “tidy” data format","heading":"3.4 Using tidyverse tools with data in the tidy data format","text":"download R, get ’s called base R. includes main code\ndrives anything R, well functions many core\ntasks. However, power R , addition base R, can also add\nonto R called packages (sometimes also referred \nextensions libraries). kind like “booster packs” add \nnew functions R. can created contributed anyone, many \ncollected key repositories like CRAN Bioconductor.tidyverse tools included R extension packages, rather base\nR, download R, ’ll need download packages well use\ntidyverse tools. core tidyverse functions include functions read \ndata (readr package reading plain text, delimited files, readxl\nread data Excel spreadsheets), clean summarize data (\ndplyr package, includes functions merge different datasets, make\nnew columns functions old ones, summarize columns data, either\nwhole group), reformat data needed get tidy\nformat (tidyr package). tidyverse also includes precise tools,\nincluding tools parse dates times (lubridate) tools work \ncharacter strings, including using regular expressions powerful way find\nuse certain patterns strings (stringr). Finally, tidyverse\nincludes powerful functions visualizing data, based around ggplot2\npackage, implements “grammar graphics” within R. cover \ntidyverse tools may find helpful pre-processing biomedical data \nmodule 16.can install load tidyverse packages one--one using \ninstall.packages library functions package name within R.\nplanning using many tidyverse packages, can also\ninstall load many tidyverse functions installing package called\ntidyverse, serves umbrella many tidyverse packages.addition original tools tidyverse, many people developed\ntidyverse extensions—R packages build tools principles \ntidyverse. often bring tidyverse conventions tools \nspecific areas science. example, tidytext package provides tools \nanalyze large datasets text, including books collections tweets, using\ntidy data format tidyverse-style tools. Similar tidyverse extensions\nexist working network data (tidygraph) geospatial data (sf).\nExtensions also exist visualization branch tidyverse\nspecifically. include ggplot extensions allow users create\nthings like calendar plots (sugrrants), gene arrow maps (gggene), network\nplots (igraph), phytogenetic trees (ggtree) anatogram images\n(gganatogram). extensions allow users work data ’s \ntidy data format, provide similar user interfaces, making \neasier learn large set tools range data analysis \nvisualization, compared set tools lacked coherence.","code":""},{"path":"module3.html","id":"discussion-questions-2","chapter":"Module 3 The “tidy” data format","heading":"3.5 Discussion questions","text":"main considerations decide record data?main considerations decide record data?Based reading, can define tidy data format? familiar format preparing discussion? use principles recording data?Based reading, can define tidy data format? familiar format preparing discussion? use principles recording data?Describe advantages, well potential limitations, storing data tidy data formatDescribe advantages, well potential limitations, storing data tidy data formatIn data collected, can think examples data collection format included extra elements, beyond simply space recording data? Examples might include plots, calculations, notes, highlighting. advantages extra elements template? Based reading experience, disadvantages including extra elements data collection template?data collected, can think examples data collection format included extra elements, beyond simply space recording data? Examples might include plots, calculations, notes, highlighting. advantages extra elements template? Based reading experience, disadvantages including extra elements data collection template?research collaborations, experienced case data format one researcher created difficulties ?research collaborations, experienced case data format one researcher created difficulties ?","code":""},{"path":"module4.html","id":"module4","chapter":"Module 4 Designing templates for “tidy” data collection","heading":"Module 4 Designing templates for “tidy” data collection","text":"module, use real example data collected biomedical\nlaboratory. ’ll use example show data often collected way\n“tidy” (module 3), focusing features data collection\nmake “untidy”. ’ll describe general principles \ninstead create use tidy (least tidier) templates collect\ndata laboratory. ’ll also show can first step \npipeline creating useful, attractive, reproducible reports describe\ndata collected. module focus principles templates\ntidy data collection, module 5 ’ll dig deeper \ndetails making conversion example dataset use \ndemonstration module.Objectives. module, trainee able :Detect features data collection template keep dataset \n“tidy”Discuss “untidy” data collection can affect rigor reproducibility\ndata recordingDistinguish “untidy” features data collection templates \ncan affect rigor reproducibility versus can easily addressed\nlater code pipelineDiscuss three principles designing data collection template “tidy”\ndata collectionList “extra” elements sometimes included data collection\nspreadsheet can impede reproducibilityCompare “wide” versus “long” data formatsExplain certain characters formatting data collection template\nmay cause problems later analysis","code":""},{"path":"module4.html","id":"exampledata-on-rate-of-bacterial-growth","chapter":"Module 4 Designing templates for “tidy” data collection","heading":"4.1 Example—Data on rate of bacterial growth","text":"Throughout module, ’ll use real dataset illustrate principles \ndata collection biomedical laboratory. First, let’s start looking \noriginal data collection template, use walk details \ndataset. Figure 4.1 provides annotated view \ndata set, showing format used data originally collected.\nFigure 4.1: Example Excel spreadsheet used record analyze data laboratory experiment. Annotations highlight data entered hand, calculations done hand, embedded Excel formulas used. figures created automatically using values specified column.\ndata collected measure compare growth yield doubling time\nMycobacterium tuberculosis (bacteria causes tuberculosis \nhumans) two conditions—high oxygen low oxygen. humans, M.\ntuberculosis can persist years decades granulomas, centers \ngranulomas often hypoxic (low oxygen). Therefore, ’s important \nunderstand bacteria grow hypoxic conditions.conduct experiment, researchers used test tubes capped\nsealed caps prevent air exchange contents tube\nenvironment. Inside tubes, amount oxygen controlled\nshifting ratio volume culture (liquid nutrients\nM. tuberculosis grow) versus volume air.\nhigh oxygen condition, lower volume culture used, leaves\nroom lot air top tube. low oxygen condition,\ntube filled almost top culture, left little air\ntop tube.tubes filled capped, left grow week.\ntime, researchers took several measurements determine \ngrowth bacteria tube. , used spectrophotometer\ntrack optical density time. method gives measurement \ndirectly proportional cell mass tube, provides measure\nmuch bacteria grown since start experiment.record data experiment, researchers used spreadsheet shown \nFigure 4.1. spreadsheet example data\ncollection template—created experiment, also \nexperiments research group conducts measure bacterial growth\ndifferent conditions. designed allow researcher working \nlaboratory record measurements course experiment.Let’s take closer look features spreadsheet. First, \nsection top right focuses data collection \nexperiment, one row time tubes measured cell\nmass. section spreadsheet starts several\ncolumns related time measurement, including clock time \nmeasurement (column ), difference time (hours) time point\ndata collected (column B), date data gathered\n(column C), time hours data point start \nstudy graphing purposes (column D). columns clock time () date\n(C) recorded hand, columns time since start \nexperiment (B D) calculated converted hand values \nentered column. remaining columns (E–) provide data \noptical density (absorbance 600 nm), directly proportional cell\nmass tube. one column per test tub, column\nlabels includes test tube ID (A1, A3, L1, L2, L3). tube ID starts \n“”, grown high oxygen conditions, starts “L”, \ngrown low oxygen conditions.Next, spreadsheet areas provide summaries data, calculated\nusing embedded formulas spreadsheet’s plotting functions. \nexample, rows 17–18 provide calculations doubling time bacteria\ntube two periods (early late experiment), two\ngrowth curves plotted bottom spreadsheet.Finally, spreadsheet includes couple features, including \nwritten notes one hand calculations macro top right\ncan used researcher calculate amount initial\ninoculum add tube start experiment.researchers found appealing format spreadsheet \nease researcher collecting data laboratory \naccomplish study goals. data graphed real time, \ninclusion simple macro calculate doubling time, allowed research \nlaboratory see tangible differences two assay conditions \ndata collected one-week experiment. also cited ease\nadditional sampling data points added.However, many features can undesired consequences. can increase\nchance errors recording data calculating summaries based \ndata. also make hard move data reproducible pipeline, \nlimit opportunities sophisticated analysis visualization. \nnext section module, ’ll highlight features data collection templates\nlike one can make data collection untidy. module 5,\n’ll discuss create new data collection template example\ndata tidier, use open general discussion \nprinciples tidy data collection templates.","code":""},{"path":"module4.html","id":"features-that-make-data-collection-templates-untidy","chapter":"Module 4 Designing templates for “tidy” data collection","heading":"4.2 Features that make data collection templates untidy","text":"several features data collection template shown Figure\n4.1 make untidy. make difficult read\ndata statistical program like R Python conduct data analysis\nvisualization. also features make prone errors \ndata collection analysis.First, data hard read statistical program \nraw data form part spreadsheet (Figure 4.2, area\nhighlighted blue box). “extra” elements spreadsheet, \ninclude output calculations, plots, macros, notes, make harder\nisolate raw data file using statistical program.\nFigure 4.2: Isolating raw data collected template extra elements. box figure highlights area spreadsheet data collected. elements spreadsheet focus aims (e.g., summarizing data, adding notes, macros experimental design). elements make difficult extract raw data advanced analysis visualization statistical program like R, Python, Perl.\nextra elements make hard extract raw data, isn’t\nimpossible. Programming languages like R include functions read data \nspreadsheet, functions often provide options specify sheet \nfile read , well rows columns read specific\nsheet. example spreadsheet Figure 4.2, example,\nspecify read rows 1–15 columns –, focus \nraw data.However, one goal reproducible research create tools pipelines \nrobust—, ones still work desired raw data \nchanged small ways, even across different raw data files. Therefore, \ncustomize code read data specific part complex\nspreadsheet, like shown Figure 4.2, customization\nmake code less robust. asked statistical program read \nrows 1–15 columns –, example, code perform incorrectly \nlater added one time point experiment, tried use \ntemplate experiment used test tubes. instead use \ntemplate records raw data, without additional elements, \ncan create robust tools, since can write code read whatever \nspreadsheet, rather restricting certain rows columns.Next, example template helps demonstrate specific ways recording data\ncan make template less tidy. First, let’s look template records\ntime measurement. using four separate columns\n(Figure 4.2). column C, researcher records date \nmeasurement taken, Column records clock time \nmeasurement. experiment started, example, 12:00 PM (“12:00” column )\nJuly 9 (“9-Jul” column C). values entered hand researcher.\nNext, values used calculate, measurement, long \nsince start experiment. value recorded two separate ways—\nhours minutes column B converted hours percents hours (using\ndecimals) column D. example, second measurement taken 4:05 PM\nJuly 9 (“16:05” column “9-Jul” column C), 4 hours 5 minutes\nstart experiment (“4hr 5min” column B) , since 5 minutes \n8% hour, 4.08 hours start experiment (“4.08” column D).\nFigure 4.3: Measurements time example data collection template. four highlighted columns (columns , B, C, D) used spreadsheet record time. methods recording time template, however, may make likely create errors data recording collection make harder use data reproducible pipeline.\nthings changed time data \nrecorded make data collection template tidier. First, \nbetter focus recording raw data, rather adding\ncalculations based data. Columns B D Figure 4.2\noutput calculations. Anytime spreadsheet includes \ncalculation, creates room mistakes data collection analysis.\nOften, calculations spreadsheet done using embedded formulas. \ncan cause problems new columns rows added data, \ncan shift cells meant used calculation. , formulas\nembedded spreadsheet, can’t seen checked \neasily, makes easy miss typo error formula.example Figure 4.2, columns B D aren’t\ncalculated embedded formulas, rather calculated researcher hand\nentered. creates room user error calculation\ndata entry. Later, ’ll see can tidy data collection\ntemplate removing columns calculate time (columns B D) instead\ncalculation raw data read statistical program.second thing changed template records date \ntime measurement. Currently, uses two columns (C) record \ninformation. However, piece information useless without \n—instead, must known jointly things like calculate time\nsince start experiment. therefore tidier record \ninformation single column. example, instead recording starting\ntime experiment “12:00” column “9-Jul” column C, \nrecord “July 9, 2019 12:00” single date-time column. example,\nadding year (“2019”) date also make data point easier \nwork programming language, languages like R Python often \nspecial functions work data date-time classes, elements \ndate /time must included convert data points useful\nformat.Next, let’s look template collects data related cell growth \ntube (columns E–, Figure 4.4). data \nrecorded format work pretty well. Strictly speaking, aren’t\nfully tidy (module 3), since column headers include information \nmight want use variables analysis visualization. Specifically, \ntest tube’s ID incorporated column name measurements \ntube recorded, since test tube recorded using separate column.\nFigure 4.4: Measurements bacterial growth example data collection template. five highlighted columns (columns E–) used spreadsheet record optical density test tube measurement time.\nwant run analysis estimate values test tube, \ncreate plots test tube’s measurements shown separate line,\nread data another program, ’ll need convert \nformat data bit. However, ’s quite easy statistical\nprogramming languages now, ’s reasonable compromise element\n“tidiness” data collection format. ’ll show module 5,\nchanging layout original data collection require \nresearcher re-type measurement date time several times \nresult spreadsheet longer, harder see \nrecording data.final element ’d like highlight example template \nmake data hard integrate reproducible pipeline. \ncases example template either column names cell values \nformatted way hard work data read \nprogram like R Python (Figure 4.5). \nexample, column names include spaces parentheses (e.g., “Time (clock)”).\nleft -, data read another program, column names\nneed cleaned take characters , column\nnames composed alphabetical characters, numbers, underscores.\ncan done code like R Python, add data\ncleaning process avoided using simpler column names \noriginal data collection template.\nFigure 4.5: Examples special characters formatting example template cause problems later data analysis pipeline.\n","code":""},{"path":"module4.html","id":"converting-to-a-tidier-format-for-data-collection-templates","chapter":"Module 4 Designing templates for “tidy” data collection","heading":"4.3 Converting to a “tidier” format for data collection templates","text":"Now ’ve looked characteristics can make data collection\ntemplate untidy, let’s go principles creating tidy templates \nrecord data. module, ’ll focus higher-level strategies,\nusing example data collection template highlight points. \nmodule 5, ’ll provide detailed walk-\nexample template can modified use tidier format.three basic principles designing tidy templates go \nlong way creating ways collect data research group can easily\nused within reproducible analysis pipeline. first principle designing \ntidier template collecting laboratory data limit template \ncollection data. key word “collection”. tidy template\navoid calculations done original data instead focus \ninitial data researcher records experiment. means \nexclude template element provides calculation,\nsummary, plot based initial recorded element. also exclude\nspecial formatting using encode information. example,\nsay collecting data, cases get warning \nreading may instrument’s detection limit. may tempting \nhighlight cells measurements warning displayed \nrecord data. However, avoid , color \nformatting information lost read data file \nstatistical program. Instead, add second column indicate \nmeasurement included warning.second principle make sensible choices dividing data collection\nrows columns. many different ways spread \ndata collection rows columns. One decision (whether) \ndivide recorded information across columns. Figure 4.6,\nexample, shows several ways divide data date time\none columns. example, typically makes sense \nuse single column record date time elements (top example \nFigure 4.6). mentioned earlier, statistical\nprograms powerful functions parsing dates times, \nstore data special classes allow time-related operations (\nexample, calculating time difference two date-time measurements). \nefficient record date time elements single column.\nFigure 4.6: Examples special characters formatting example template cause problems later data analysis pipeline.\nConversely complex data different elements (example, height\ncomponents inches feet), may make sense use separate columns \ncomponents. example, rather using one column record\n5'7\", divide information one column component \nfeet (5) one component inches (7). first case\n(recording 5'7\"), read data program like R need\nuse complex code split value parts able use . \nsecond case, easy work values two separate columns\ncalculate value use work (e.g., use formula like height_ft * 12 + height_in calculate full height inches).Another decision stage “long” versus “wide” make \ntemplate. “wide” design include columns, “long” design \ninclude rows. Often, can create different designs allow \ncollect values different designs along wide-versus-long\nspectrum. Figure 4.7 gives two examples templates \ncollect data, one using wider design using \nlonger design.\nFigure 4.7: Examples two ways arranging data data recording template. format left records optical density measurements test tube separate column, column header identifies test tube. example ‘wider’ format. format right records optical density test tupbes single column, using separate column record test tube measurement represents. example ‘longer’ format.\nmodule 3, described rules tidy format dataframes.\nrecord data directly tidy format, easy \nread programming language analyze visualize. However,\ntidy format can sometimes result datasets long. \nmay convenient record data wider format, especially \nrecording data laboratory setting inconvenient \nscroll within longer-format spreadsheet record.Fortunately, convenient tools programs like R Python \ncan used take data collected wider format reformat \ntidy format soon read software program. \nrequire extra code, usually code fairly simple \nstraightforward. Therefore, design data collection template, \ncan balance practical advantages using wider data collection format\nadvantages fully tidy format apply input \ndata statistical program analysis visualization. Often, wider\nformat might win balance, ’s fine.third principle avoid characters formatting make \nhard computer program process data. principle \nparticularly important column names column. read data\nstatistical program like R, names automatically used \ncolumn names R data frame object, code regularly use \ncolumn names refer parts data analyzing visualizing .\nfind easiest use data reproducible pipeline \nfollow couple rules column names. reason rules help\nreplicate rules naming objects programming languages,\nhelp seamlessly transitioning stages data\ncollection data analysis. First, always start column name letter.\nSecond, use letters, numbers, underscore character (“_“) \nrest characters column name.Based rules, , avoid putting spaces column names\ndesign data collection template. tempting include spaces \nmake names clearer humans read, understandable. Often,\nunderscore serves function, allowing easy human comprehension\nstill avoiding characters difficult statistical programs.\nexample, column named “Optical density”, can change \n“Optical_density” without making much difficult person \nunderstand. choices designing data collection template,\nchoices column names can balance making template\neasy researchers use laboratory easy statistical\nprogram parse later pipeline. example, statistical programs like\nR functions working character strings can used \nreplace spaces column names another character. However, \nisn’t unreasonable follow recommended rules writing column names\ndata collection template, can keep code later pipeline\nmuch simpler, ’s worth considering.Beyond spaces, number special characters might\ntempted include column names. include parentheses, dollar signs,\npercent signs, hash marks (“#”), . require extra code\nlater steps analysis pipeline, can cause severe problems\nspecial meanings programming language. example,\nhash marks used R programming language add comments within code, \ndollar signs used subsetting elements list data frame object.\nworth effort avoid characters column names data\ncollection template.also considerations can make terms record data within\ncells data collection template, can make big difference terms\nhard easy work data within statistical program. \nstatistical programs like R powerful terms able handle even\n“messy” input data, require lot code leverage power. \nthoughtful design template record data, can avoid \nuse lot code input clean data later stages pipeline.Figure 4.8 gives example choice make\nformat use record data. figure shows two columns \noriginal data collection template example experiment module.\ntemplate includes two columns record time since start \nexperiment, use different formats . column B, time \nrecorded hours minutes, characters “hr” “min” used \nseparate two time components. column D, information recorded,\ndecimals hours (e.g., 4.08 hours 4 hours 5 minutes). \nformat column B similar humans think time, take\ncode parse statistical program. reading data \nprogram like R, need use regular expressions split apart \ndifferent elements recombine format program\nunderstands. contrast, values recorded column D easily read\nstatistical program, minimal code needed used\nanalysis visualizations.\nFigure 4.8: Examples two ways recording time original template example experiment. Column B uses hours minutes, characters embedded separate hours minutes, column D uses hours decimal degrees. format column D much easier integrate larger data analysis pipeline.\nthree principles excellent starting point designing tidy\ntemplate collecting data. using , well way \ncollecting data way easy integrate longer reproducible data\nanalysis pipeline.convert data collection templates “tidier” formats, \ntypically look much simpler templates research group may \nusing. example experiment described earlier module,\nprocess tidying template results template like shown \nFigure 4.9 (module 5, ’ll walk \nsteps create tidier template, using principles ’ve covered \nmodule). comparison, starting template data collection \nexperiment shown Figure 4.1.\nFigure 4.9: Example simpler format can used record analyze data laboratory experiment previous figure. Annotations highlight data entered hand. calculations conducted figures created—done later, using code script.\ncomparing two templates, can see simpler template \n, , provide immediate, real-time summaries collected data. \nsimpler template removed elements like plots values calculated embedded\nformulas. first glance, might seem like disadvantage using tidier\ntemplate collect data. However, combining tools pipeline, \neasy connect tidier raw data file reporting tools. way, can\nquickly create real-time summaries data similar shown \nFigure 4.1, created reported outside file\nused originally record data.Figure 4.10 shows example simple report \ncreated example experiment. report generated using \nstatistical program, R, inputs data simple template shown \nFigure 4.9. report uses R code generate PDF\nWord file output shown . file report created \nway output can quickly regenerated single button click, \ncan applied data saved using template. fact, \ncan create templates reports coordinate data collection\ntemplate create. module 5, ’ll walk \ncreate generating file report, modules (modules\n18–20), provide thorough overview creating\ntypes “knitted” documents.\nFigure 4.10: Examples automated report can created quickly generate summaries estimates data collected simplified data collection template example experiment.\nreport shown Figure 4.10 repeats summaries\nshown complex original data collection template\n(Figure 4.1). number advantages, however, using\nseparate steps files processes collecting versus analyzing data.\nseparate report (Figure 4.10) provides starting point can\neasily adapted make complex figures analysis, well integrate\ncollected data data measured ways experiment.","code":""},{"path":"module4.html","id":"learning-more-about-tidy-data-collection-in-the-laboratory","chapter":"Module 4 Designing templates for “tidy” data collection","heading":"4.4 Learning more about tidy data collection in the laboratory","text":"may take iteration develop data collection templates \nconvenient appropriate input complex programs pre-processing,\nanalysis, visualization. module module 5 provide guidance \nexamples, can helpful see examples. Two excellent resources \ntopic articles Ellis Leek145 Broman Woo146.","code":""},{"path":"module5.html","id":"module5","chapter":"Module 5 Example: Creating a template for “tidy” data collection","heading":"Module 5 Example: Creating a template for “tidy” data collection","text":"walk example creating template collect data \n“tidy” format laboratory-based research project, based research\nproject drug efficacy murine tuberculosis models. important note\n’s reason can’t continue use spreadsheet program like\nExcel Google Sheets collect data. spreadsheet program can\neasily used create simple template use collect data. fact,\n’ll continue using spreadsheet format module show \nredesign data collection example experiment introduced \nlast module. important, however, think arrange\ntemplate spreadsheet make useful larger context \nreproducible research.show redesign data collection template, ’ll focus \nthree principles designing tidy templates data collection \nbiomedical laboratory introduced module 2.4. reminder, \nthree principles :Limit template collection data.Make sensible choices dividing data collection rows columns.Avoid characters formatting make hard computer program\nprocess data.module, ’ll show apply principles create tidier\ntemplate example dataset last module. Finally, show \ndata can easily analyzed visualized using reproducible tools.Objectives. module, trainee able :Understand principles “tidy” data can applied real, complex research projectList advantages “tidy” data format example projectApply steps follow three principles creating template “tidy”\ndata collectionConstruct template “tidy” data collectionExplain report template can help replace visualization tools \nspreadsheet collecting data","code":""},{"path":"module5.html","id":"example-datadata-on-rate-of-bacterial-growth","chapter":"Module 5 Example: Creating a template for “tidy” data collection","heading":"5.1 Example data—Data on rate of bacterial growth","text":", ’ll walk example using real data collected laboratory\nexperiment. described data detail previous module. \nreminder, collected measure growth rate Mycobacteria\ntuberculosis two conditions—high oxygen low oxygen. \ncollected five test tubes measured regularly one week \nbacteria growth using measure optical density. Figure\n5.1 shows original template research group used\nrecord data.\nFigure 5.1: Example Excel spreadsheet used record analyze data laboratory experiment. Annotations highlight data entered hand, calculations done hand, embedded Excel formulas used. figures created automatically using values specified column.\nprevious module, described features make template “untidy”\npotentially problematic include larger pipeline reproducible\nresearch. next sections module, ’ll walk step--step\nchanges make make template tidier. ’ll finish\nmodule showing design step \nanalysis pipeline visualize analyze collected data, \nadvantages real-time plotting complex spreadsheet \nmissed moving tidier template.","code":""},{"path":"module5.html","id":"limiting-the-template-to-the-collection-of-data","chapter":"Module 5 Example: Creating a template for “tidy” data collection","heading":"5.2 Limiting the template to the collection of data","text":"first principle designing template tidy data collection \nlimit template collection data. example template (Figure\n5.1), however, includes number “extra” elements beyond\nsimple data collection—elements outside rows 1–15 columns –.\nOutside area, number extra elements, including plots \nvisualize data, summaries generated based data (rows 16–18, \nexample), notes data, even macro (top right) wasn’t\ninvolved data collection instead used researcher calculate\ninitial volume inoculum include test tube. None \n“extras” can easily read statistical program like R Python—\nbest, ignored program. can even complicate reading \ncells measurements (rows 1–15 columns –), statistical\nprograms try read non-empty cells spreadsheet unless\ndirected otherwise.good starting point, , start designing tidy data collection\ntemplate experiment extracting content box \nFigure 4.2. result template looks like\nFigure 5.2. Notice , ’ve done , ’ve also removed \ncolor formatting spreadsheet. fine keep color \nspreadsheet help research find right spot record data\nworking laboratory, make sure ’re using\nencode information data—color formatting ignored\ndata read statistical program like R.\nFigure 5.2: First step designing tidy data collection template example project. template created focuses raw data, removing extra elements like plots, notes, macros, summaries.\n“extras” spreadsheets embedded elements like formulas macros.\nmake data collection tidy, remove computation steps \nfile use record data. template shown Figure\n5.2 removed lot calculated values original\ntemplate, removed . Two columns still values\ndetermined calculation original data collected.\nColumn B column D provide measures length time since \nstart experiment, calculated comparing measurement time\ntime start experiment.time since start experiment can easily calculated later \nanalysis pipeline, read data statistical program like R. \ndelaying step, can simplify data collection template\n(requiring fewer columns research laboratory fill ) \nalso avoid chance mistakes, occur hand\ncalculations values data entry, researcher enters \nresults calculations spreadsheet cell. Figure 5.3\nshows new version template, calculated columns \nremoved. template now restricted data points originally\ncollected course experiment. removed elements \nbased calculations derivatives original, raw data points.\nFigure 5.3: Second step designing tidy data collection template example project. template started previous one, removed columns hand-calculated entered researcher previous template. version removed calculated values template, limiting original recorded values required experiment.\n","code":""},{"path":"module5.html","id":"making-sensible-choices-about-rows-and-columns","chapter":"Module 5 Example: Creating a template for “tidy” data collection","heading":"5.3 Making sensible choices about rows and columns","text":"second principle designing template tidy data collection \nmake sensible choices dividing data collection rows columns.\nmany different ways spread data collection \nrows columns, step, can consider method meet \nreasonable balance making template easy researcher \nlaboratory use record data also making resulting data file easy \nincorporate reproducible data analysis pipeline.example experiment, Figure 4.2 shows three possibilities\ncan consider arrange data collection across rows columns.\nthree build changes made earlier step “tidying” template,\nresulted template shown Figure 5.3.\nFigure 5.4: Examples ways data collection divided rows columns example template. Panel shows example date time recorded different columns. Panel B similar Panel , case, date time recorded single column. Panel C shows classically ‘tidy’ data format, measurement date-time repeated five test tubes, columns give test tube ID absorbance measurement time tube (part data shown format, remaining rows page). Panel C provides ‘tidiest’ format, may practical constraints used laboratory setting. example, require data entry data collection (since date-time entered five times measurement time), long format prevent seen without scrolling computer screen.\nPanel (exact repeat template shown Figure 5.3) shows\nexample date time recorded different columns. Panel B \nsimilar Panel , case, date time recorded single\ncolumn. Panel C shows classically “tidy” data format, measurement’s\ndate-time repeated five test tubes, columns give test\ntube ID absorbance measurement time tube (part \ndata shown format, remaining rows page).example, template may reasonable one shown\nPanel B. Panel C provides “tidiest” format (module 2.3), \npractical constraints used laboratory setting. example, \nrequire data entry data collection (since date-time entered\nfive times measurement time), long format prevent \nseen without scrolling computer screen.comparing Panels B, template Panel B advantage. \ninformation date time useful together, individually. \nexample, calculate time since start experiment, \njust calculate difference dates just difference times, \ninstead must consider date time measurement comparison \ndate time start experiment. result, point \ndata analysis pipeline, ’ll need combine information date\ntime make use two elements.combination two columns can easily done within statistical\nprogram like R, can also directly designed original template \ncollecting data. Therefore, unless practical reason \neasier researcher enter date time separately, template\nshown Panel B preferable shown Panel terms allowing \n“tidy” collection research data file easy include \nreproducible pipeline.Figure 5.5 shows template design stage process\ntidying , highlighting column combines date time elements \nsingle column. version template, ’ve also careful \ndate time recorded, consideration ’ll discuss \nnext section.\nFigure 5.5: Third step designing tidy data collection template example project. template started previous one, combined collection date time measurement single column revised format include date elements prevent automatic conversion spreadsheet program.\n","code":""},{"path":"module5.html","id":"avoiding-problematic-characters-or-formatting","chapter":"Module 5 Example: Creating a template for “tidy” data collection","heading":"5.4 Avoiding problematic characters or formatting","text":"third principle designing templates tidy data collection \navoid characters formatting make hard computer program\nprocess data. number special characters formatting\nconventions can hard statistical program handle. example\ntemplate shown Figure 5.5, example, column names include\nspaces (example, “Date time”), well parentheses (example,\n“VA 001 (A1)”). statistical programs tools allow \nhandle convert characters data read , ’s even simpler\nchoose column names avoid problems original data collection\ntemplate. save extra coding along analysis\npipeline. Two general rules creating easy--use column names data\ncollection template : (1) start column name letter (2) \nrest column name, use letters, numbers, underscore\ncharacter (“_“). example,”aerated1” work well, “1–aerated” \n“aerated–1” .Within cell values column names, flexibility. \nexample, column gives IDs different samples, \nfine include spaces characters IDs. \nexceptions, however. big one values record dates date-time\ncombinations.First, important include elements date (date time, \nrecorded). year included recorded\ndate, even experiment took days. statistical\nprograms excellent functions working data dates \ndate-times, take advantage , data must converted \nspecial class program, conversion class requires specific\nelements (date, must include year, month, day month).Second, useful avoid recording dates date-times way \nresults spreadsheet program automatically converting . Surrounding \ninformation date quotation marks entering (shown Figure\n5.5) can avoid .Finally, consider using format record date unambiguous \nless likely recording errors. Dates sometimes recorded using \nnumbers—example, first date “July 9, 2019” example data\nrecorded “7/9/2019” “7/9/19”, even concise. However,\nformat ambiguity. can unclear refers July 9 \nSeptember 7, written “7/9”. version uses\ntwo digits year, can unclear date 2019 1919 (\ncentury). Using format “July 9, 2019”, done latest\nversion sample template, avoids potential ambiguity.Figure 5.6 shows template example experiment \ncolumn names revised avoid problematic characters. template now \nuseful format reproducible research pipeline—data collected using \ntemplate can easily read processed using statistical programs like\nR Python.\nFigure 5.6: Example simpler format can used record analyze data laboratory experiment previous figure. Annotations highlight data entered hand. calculations conducted figures created—done later, using code script.\n","code":""},{"path":"module5.html","id":"separating-data-analysis-from-data-collection","chapter":"Module 5 Example: Creating a template for “tidy” data collection","heading":"5.5 Separating data analysis from data collection","text":"created “tidy” template collecting data \nlaboratory, can create report template input data \nprovide summaries visualizations. allows separate steps (\nfiles) collecting data analyzing data. Figure\n5.7 shows example output report template\ncreated pair data collection template shown Figure\n5.6.\nFigure 5.7: Examples automated report can created quickly generate summaries estimates data collected simplified data collection template example experiment.\ncreate report template like , can use tools reproducible\nreports statistical programs like R Python. section, \ngive overview create report template shown Figure\n5.7.report written using framework called RMarkdown, allows \ninclude executable code inside nicely-formatted document, resulting \ndocument Word, PDF, HTML easy humans read also\ngenerating results based R code. cover format details \nmodules 3.7–3.9. code used generate results Figure\n5.7 programming language R. Module 3.3\nprovides guidance getting started R, used .programming language can seem, first glance, much difficult learn\nuse using spreadsheet program like Excel set formulae \nmacros. However, languages like R evolved substantially recent years \nallow much straightforward coding may seen past,\nbarrier learning use straightforward data management \nanalysis much higher effort required become proficient \nusing spreadsheet program. demonstrate , let’s look \ntasks required generate results shown Figure\n5.7. won’t cover code, just highlight \nkey steps. ’d like look details code output\ndocument, can download files explore : can access file\nRmarkdown\nfile,\ncan download output\nPDF.\n’d like try code Rmarkdown file, ’ll also need \nexample data, can download clicking\n.One key step read collected data R. use spreadsheet\ndata collection analysis, don’t need read data start\nworking , since everything saved file. separate\nsteps data collection data analysis, however, need take \nextra step read data file another program analysis. Fortunately,\nsimple R. data example recorded using Excel\nspreadsheet, simple function R lets read data \ntype spreadsheet (Figure 5.8). step \ncode, object R called growth_data, contains data\ntwo-dimensional form similar recorded spreadsheet\n(type object R called dataframe).\nFigure 5.8: Code read data data collection template R cleaning, analysis, visualization. data recorded tidy data collection template described earlier module. , data read R (code shown top). resulting data R stored format similar design spreadsheet, rows observations columns values recorded observation (bottom).\nAnother key step calculate, observation, time since start\nexperiment. original data collection template shown Figure\n5.1, calculation done hand researcher \nentered spreadsheet. converted spreadsheet tidier\nversion, took steps involved calculations data, \ninstead limited data collection raw, observed values. helps us\navoid errors typos—instead researcher calculate \ndifference time running experiment, can just record \ntime, can write code analysis document handles \ncalculations, using well-designed well-tested tools calculation.Figure 5.9 shows code can used \ncalculation. start code, data stored object named\ngrowth_data. mutate function adds column data, named\nsampling_delta_time, give difference time \nobservation start experiment. Within mutate call, special\nfunction named difftime calculates difference two time points. \nfunction lets us specify time units ’d like use, can pick\n\"hours\" units. first function lets us pull first value\ndata recorded time—words, time experiment\nstarted. lets us compare observation time time start \nexperiment. result code new version growth_data\ndataframe, new column giving time since start experiment:\nFigure 5.9: Code add column data gives time since start experiment. code (top) uses time recorded experiment compares first recorded time, start experiment. determines time since start experiment observation, given new column data (bottom).\nAnother key step plot results data. R, package\ncalled ggplot2 provides tools visualization. tools \npackage work building plot using “layers”, adding small elements\nline line simple functions one simple thing.\nresulting code can long, step simple, \nbecomes simple learn different “layers” learn combine\ncreate complex plots.Figure 5.10 walks code one \nvisualizations report. point report code, data \nreformatted object called growth_data_tidy, columns \nobservation time since start experiment\n(sampling_delta_time), measured optical density (optical_density),\nwhether tube aerated low oxygen (growth_conditions), short ID\ntest tube (short_tube_id). code starts creating plot object,\nspecifying plot color show growth conditions, \nposition x-axis show time since start experiment, \ny-axis show optical density. Layers added plot\nobject add points lines plot based mappings, \nlines, ’s specified type line show test\ntube ID (example, one tube shown dotted line, another \ndashed line). layers added customize scale labels \nlabs, including labels x-axis y-axis legends \ncolor linetype scales. Another layer used customize appearance \nplot—things like background color font used—another\nlayer added use log-10 scale x-axis.\nFigure 5.10: Code plot growth curves data. plotting code run, data transformed ‘tidy’ format (top), columns include time since start experiment, test tube ID, growth condition test tube, optical density measured test tube. code (middle) add layers implement element plot based input data. final plot shown bottom.\nlooks like lot code, process isn’t longer \ncustomize elements plot spreadsheet program. advantages \ncoded approach maintain full record steps took\ncustomize plot. something can use reproduce plot\nlater, even use starting point creating similar plot new\ndata.next key step ’d like point can write use small\nfunctions customized tasks experimental data. one example, \ndata example, want estimate doubling times based \nobserved data. principal investigator decided \nbased comparing bacteria levels two times points—measured time \nclosest 65 hours start experiment, time \nclosest 24 hours start experiment.original data collection template—data recorded\nanalyzed spreadsheet—step done hand researcher,\nlooking data selecting cell closest \ntimes, connecting cell spreadsheet formula calculation\ncalculate doubling time. can make process rigorous \nless prone error writing small function thing,\nusing function automate process identifying \nrelevant observations use calculating doubling rate.Figure 5.11 shows can write use small\nfunction R. function input growth_data dataset, well \ntime aiming , output sampling time data \nclosest —larger —time. steps\nwithin body function. First, code function filters \nobservations earlier target time. measures difference\ntimes observations target time, uses\nidentify observation closest time target. pulls\ntime observation returns .\nFigure 5.11: Code create apply small function. code top can used create function can input dataframe determine observation time data closest (without larger ) target time. function series small steps. function can applied find observation time data closest specific target times, like 24 hours 64 hours (bottom).\nSmall functions like can easily reused code research\ngroup. writing logic step function—rather redoing\nsteps hand step--step time need —can save\ntime later, return, extra time can spend writing\noriginal function carefully checking make sure works\ncorrectly.Finally, many steps require extensions base R. download R,\ngetting base set tools. Many people developed helpful\nextensions build base. stored shared \ncalled R packages. can install extra packages free, \nuse library function R load package ’ve installed, giving \naccess extra functions provides. Figure\n5.12 shows spot Rmarkdown code \nloaded packages needed report. include packages functions\nread data R Excel (readxl) package, well suite \npackages tools cleaning visualizing data (tidyverse package).\nlater modules, ’ll talk R coding tools might find\nuseful working biomedical data, including tools powerful \npopular tidyverse suite packages.\nFigure 5.12: Code load packages additional functionality. provide functions offered base R, useful working example data. include packages functions reading data Excel file, well packages functions cleaning visualizing data.\nOverall, can see code document provides step--step\nrecipe documents calculations cleaning \ndata, well create plots. code runs every time create\nreport shown Figure 5.7, gives us good\nstarting point run additional experiments generate similar data.","code":""},{"path":"module5.html","id":"applied-exercise","chapter":"Module 5 Example: Creating a template for “tidy” data collection","heading":"5.6 Applied exercise","text":"Rmarkdown document includes number steps, might find\ninteresting download document example data walk \nget feel process. steps documented \nRmarkdown document extensive code comments, explain ’s happening\nalong way.","code":""},{"path":"module6.html","id":"module6","chapter":"Module 6 Organizing project files","heading":"Module 6 Organizing project files","text":"earlier modules, discussed separate data collection data\nanalysis. separating data collection analysis separate files, can\nmake file step simpler. , separating steps different\nfiles, can save files plain text, makes easier track \nusing version control software (discussed later modules). helps create \nrecord changes made data analysis code research process.process helps reproducibility, results files \ncollected experiment. Instead data analysis collected within \nsingle spreadsheet file, may end multiple files data collected\nexperiment, well separate files scripts processing,\nanalyzing, visualizing data. complex experiments, may \ndifferent data files containing data collected different assays. example,\nmay run experiment collect data research animal \nbacterial load, well flow cytometry data, well measure antibody\nlevels ELISA. result, may one raw data file \nassay , assays, even one file per study subject (e.g., flow\ncytometry). files research project also include files \nwriting presentations (posters slides) associated project, \nwell code scripts pre-processing data, conducting data analysis, \ncreating sharing final figures tables.next modules, ’ll discuss can organize files\nexperiment using single directory designed follow similar\nformat across projects. modules discuss advantages \nwell-designed project directories, tips arranging files within project\ndirectory, create directory template allows use\nconsistent file organization across many experiments.Objectives. module, trainee able :Explain poor file organization can impede reproducibilityList benefits good file organizationList several principles organizing research project filesDefine design concept “discoverability”Apply idea discoverability organizing project filesExplain project directory template works","code":""},{"path":"module6.html","id":"advantages-of-organizing-project-files","chapter":"Module 6 Organizing project files","heading":"6.1 Advantages of organizing project files","text":"files project accumulate, clear plan keeping \norganized? Based one analysis, many biomedical researchers . One study,\nexample, surveyed 250 biomedical researchers University \nWashington. noted , “researchers admitted \norganizational methodology , others used whatever method best suited\nindividual needs.”147 One respondent answered, “’re\norganized way—’re just thrown files different\nprojects,” another said “grab need , ’re \norganized decent way,” another, “’s even organized—file \ncentral computer protocols use, common lab protocols \njust individual Word files within folder ’s searchable per se.”148This lack organization can make scientists reluctant share research\nfiles, impeding reproducibility. article organizing project files \nresearch, Marwick notes:“Virtually researchers use computers central tool \nworkflow. However, formal education rarely includes training \norganise computer files make easy reproduce results\nshare analysis pipeline others. Without clear instructions,\nmany researchers struggle avoid chaos file structures, \nunderstandable reluctant expose workflow others see.\nmay one reasons many requests details \nmethod, including requests data code, turned go\nunanswered.”149Sharing data code crucial research reproducibility, especially \nprojects include extensive proprocessing complex analysis data, \nmany biomedical research projects now . bonus, research articles\ninclude data, tend impactful, measured \ncitations paper receives.150In earlier module, introduced Adam Savage’s idea “knolling” keep \nworkspace tidy (module 3). talking physical workspace. \nworking data, computer files directories workspace.\ntype work, design workspace plays critical role \nworkers approach tasks solve problems. Rod Judkins, \nlecturer St Martin’s College Art, highlights book \ncreative thinking:“working environment, whether ’s supermarket, office, studio, \nbuilding site, persuades work think certain ways. aware\n, understand medium, can use\nadvantage.”151Adam Savage describes important another type work: gourmet\ncooking. describes idea organized workspace captured \ntechnique mise en place—laying elements needed \nwork ahead time organized way—introduced famous French\nchef August Escoffier:“Kitchens pressure cookers wasted movement hasty technique\ncan ruin dish, slice artery, burn hand, land weeds, \nultimately kill restaurant. Mise en place way reliably create\nperfect dish, exact specifications, , night night,\npaying customers demand nothing less.”152Good organization files can similarly encourage clear thinking, \ncan help reasoning analyze data. One article notes \n“mundane issues organizing files directories documenting\nprogress … important poor organizational choices can lead \nsignificantly slower research progress.”153 fact, files \norganized consistent way across multiple projects, can even allow \nstart automating necessary tasks code built work \nconsistent structure.154Organization also helps finding things, finding quickly. can\neven find things quickly come back project away \n(example, paper review). can teach others \nfind things quickly consistently across multiple projects, well\nput things ’re contributing.Good file organization also help find information need ’s\ntime write results. one article notes, good organization,\n“methods data sections papers practically write , time\nwasted frenzied hunting missing information.”155Finally, good file organization can improve efficiency. article \norganizing computational biology projects highlights :“Everything , probably . Inevitably, \ndiscover flaw initial preparation data analyzed,\nget access new data, decide \nparameterization particular model broad enough. means \nexperiment last week, even set experiments ’ve working\npast month, probably need redone. organized\ndocumented work clearly, repeating experiment new\ndata new parameterization much, much easier.”156","code":""},{"path":"module6.html","id":"how-to-organize-project-files","chapter":"Module 6 Organizing project files","heading":"6.2 How to organize project files","text":"Now ’ve explained organize project files, let’s talk \ncan . ’ll cover higher-level principles \nmodule. next modules, ’ll move details examples.First, minimum, get habit storing files\nexperiment place. Specifically, project files \nsingle directory within file system computer.157 can individual’s computer, may\nalso dedicated server online, cloud-based program.number advantages keeping project’s files inside \ndedicated file directory. First, provides clear obvious place search\nproject files work project, including lulls (like\nwaiting reviews paper submission).One article reproducibility scientific papers talks helpful\norganization can , describing experience project involved\nlarge research group:“Instead squirrelling away data individual folders lab books,\nresearchers now archive published data designated central drive, \ninformation accessible long haul. Initially, people thought\nprocess just extra bureaucratic work, invented \npolice data. Now, become norm, researchers tell \nsave time worry data organized archived.”158By keeping project files within single directory, also make \neasier share files unit. several reasons might\nwant share files. obvious one share\nproject files across members research team, can collaborate\nproject. However, also reasons ’d need \nshare files, one growing importance may asked \nshare files (data, code scripts, etc.) publish paper describing \nresults.files stored one directory, directory can compressed \nshared email attachment (file size small enough) \nfile sharing platform like Google Drive. materials project\nstored single directory, also makes easier share set \nfiles version control online version control platforms.159 later modules book (modules 9–11), \nintroduce Git version control software GitHub platform sharing files\ntype version control—one example dynamic\nway sharing files, requires stored single directory.gain advantages directory-based project file organization, \nfiles need within single directory, don’t within\n“level” directory. Instead, can use subdirectories \nstructure organize files, still retaining advantages \ndirectory-based file organization. Computer file systems well-structured \nuse hierarchical design, subdirectories nested inside directories. \ncan leverage structure manage complexity breadth files \nproject.help limit number files “level” directory, \nnone becomes overwhelming collection files different types. can help\nnavigate files directory, also help someone else quickly\nfigure ’s directory everything . However,\nleverage gains, need thoughtful exactly \norganize files subdirectories.decide organize files, keep mind concept called\ndiscoverability. classic design book Design Everyday\nThings, Don Norman presents discoverability key principle good design,\nexplaining ability user able figure , design\nsomething, use thing quickly, easily, correctly.illustrates example discoverability design doors.\ndoor, location pull handle push bar immediately shows\nsomeone use door: pull side door see pull\nhandle push see push bar. door lacking , makes\nharder user “discover” use first glance, might\ntry push need pull vice-versa.idea applies design organizational system project\nfiles. want make sure new user (future) able\neasily navigate directory find need. One article \norganizing research project files notes , comes deciding \norganize files, “core guiding principle simple: Someone unfamiliar\nproject able look computer files understand\ndetail .”160 Another notes, “key\nprinciple organize [project directory] another person can know\nexpect plain meaning file directory names.”161Another way improve discoverability name files subdirectories\nmeaningful ways. computer give wide flexibility setting\nnames files subdirectories, human find much easier \nnavigate directory names clear labels describe contents.\nexample, data different assays, might organize \ndirectory named “raw_data” divided subdirectories\nnamed type assay.develop names discoverable, keep mind users may\ninclude people outside field, shorthand common \nfield might unclear. example, studies infectious bacterial\ndisease, bacterial load measured assay counts colony forming\nunits. Among bench scientists field, assay often called “CFUs”.\ncollaborating statistician, however, may find \nfiles discoverable named subdirectory files something\nlike “bacterial_load” rather “cfus”, may familiar \nshorthand.One way improve discoverability follow standards exist \norganizing project files.162 use standards \nconventions tend make easier users navigate (“discover”) new\ninstances certain type thing. module 2, discussed role \nstandards comes format use record data. comes\nproject file organization, standards come form \nsubdirectories included, ’re organized hierarchically, \nsubdirectories files named.standards exist several levels: top level \ndiscipline, also just lab group, even individual.\nhelpful standards exist discipline-wide level, following\ntype high-level standard immediately make work discoverable\n(design sense) wide group people. one article notes,\n“Using widely held conventions… help people understand \nfiles relate without ask .”163As example , people develop R packages, package consists \nset files, clear highly enforced standard \nfiles arranged directory subdirectories named. \nenforcing standard, many different people can create packages \nwork similar way.opposite end spectrum, clear\nstandards level discipline, create clear standard\nplan follow either lab group even individual\nwork. ’re consistent organizing files using standard, \nmake easier navigate files move one project another.added bonus, subdirectory organization can also used clever ways\nwithin code scripts applied files directory. example, \nfunctions scripting languages list files specified\nsubdirectory. keep raw data files certain type (\nexample, output flow cytometry project) within single\nsubdirectory, can use type function code scripts list \nfiles directory apply code ’ve developed \npreprocess visualize data across files. code \ncontinue work added files directory, since starts \nlooking subdirectory time runs working files \nmoment.type automation can huge efficiency boost project.\nOne article describes type automation can increase efficiency\ncomparison simpler task working computer files:“Organizing data files single directory consistent filenames\nprepares us iterate data, whether ’s four example\nfiles used example, 40,000 files real project. Think \nway: remember discovered select many files mouse\ncursor? trick, move 60 files easily six files. \nalso select certain file types (e.g., photos) attach \nemail one movement. using consistent file naming directory\norganization, can programatically using Unix shell \nprogramming languages.”164A final way improve directory organization make sure directory\ncluttered unnecessary files. Unnecessary files can include old versions\nproject files, superseded newer versions. later modules\n(modules 9–11), ’ll describe version control can help avoid clutter\nold versions files retaining information older versions \nfiles evolve.","code":""},{"path":"module6.html","id":"what-is-a-project-directory-template","chapter":"Module 6 Organizing project files","heading":"6.3 What is a project directory template?","text":"Louis Pastuer famously said “Luck favors prepared mind.” file\norganization, much else, time spent preparing can pay \nexponentially later. case, next step use structured\ndirectory project experiment, start using ,\nstandardized structure every one projects experiments—\nwords, create standard file organziation use consistently.modules, talk templates can used improve rigor\nreproducibility collecting reporting data. Just ’s possible\ncreate templates data collection reports, ’s also possible \ncreate template organize file directories scientific\nprojects, creating applying standards things like subdirectories\nincluded files named. takes work—design \nstructure can used across many projects, rather set something\nad hoc start new experiment. However, gains terms \norganization efficiency can extraordinary.involves first designing common template directory structure \nprojects. decided structure template, can\ncreate version computer—file directory \nsubdirectories included, without files (template files ’d\nwant use starting point project, like templates data\ncollection reports presented modules 4 5). start \nnew project, can just copy template directory, rename , \nstart using new research project. using R begin \nuse R Projects (described next section), can also create R Studio\nProject template serve kind starting point time start \nnew project.areas science engineering, idea standardized directory\nstructures allowed development powerful techniques open-source\nsoftware developers work together. example, anyone may create \nextensions R programming language share others \nGitHub several large repositories. mentioned earlier \nmodule, coordinated enforcing common directory structure \nextension “packages”—create new package, must put certain types \nfiles certain subdirectories within project directory. \nstandardized rules directory structure content, packages\ncan interact base version R, since functions can tap\nnew packages assuming type file \nwithin package’s directory files.similar way, impose common directory structure across \nproject directories research lab, collaborators quickly \nable learn find element, even projects new , \nable write code can easily applied across project\ndirectories, allowing improve reproducibility comparability across\nprojects assuring conducting pre-processing \nanalysis across projects (, conducting things differently \ndifferent projects, deliberate aware ).\nCreating project template copy rename start new\nproject one way facilitate .use template project, can customize need. \nexample, included subdirectory flow cytometry data, \nrunning assay experiment, can remove subdirectory.\nSimilarly, can customize report go help work well \nspecific experiment. However, aim keep standard format \nmuch possible, since ’s standardization across projects provides\nmany advantages.module 7, walk steps designing project\ntemplate can use across experiments laboratory group. \nmodule 8, ’ll walk example creating using kind \nproject template example set studies.","code":""},{"path":"module7.html","id":"module7","chapter":"Module 7 Creating project directory templates","heading":"Module 7 Creating project directory templates","text":"Module 6 described advantages organizing files \nresearch project within single directory, added advantages using \nconsistent directory structure experiments projects \nresearch group. module, ’ll walk steps required design\ncreate template project directories. Creating using common\ntemplate directory structure projects help create consistency\nacross projects directory structure, can facilitate use \nre-use automated tools like code scripts across different experiments.Objectives. module, trainee able :able designed structured project directory template \nresearch projectsUnderstand project directories can turned RStudio “Projects”","code":""},{"path":"module7.html","id":"goals-in-designing-a-project-template","chapter":"Module 7 Creating project directory templates","heading":"7.1 Goals in designing a project template","text":"Designing project template include two parts—first, designing \nconceptual template file organization , second, creating \nphysical implementation concept. conceptual template \ndevelop structure rules ’ll organize name files within \nproject directory. physical template use ideas develop file\ndirectory follows organization, can copy, paste, \nadapt time start new project.words, open computer make “physical” template, \ndesign . involves deciding types data go \nproject directory, files organized within directory \nnaming conventions files. words, create blueprint\ntemplate create physical template.hardest part conceptual part—deciding structure \nrules consistently use. process designing, can\nmake process bit easier following principles facilitate design.\nexample, design, ’s useful start defining problem.165 aiming achieve file organization\nsystem?Based experiences advice others,166 key goals consider research project\ndirectory template system:Keeps files research project within single directory, using\nsubdirectories organize files hierarchical structureKeeps data collection analysis separate (see module 1)Avoids removes unnecessary filesUses meaningful names files subdirectories, allowing easy navigation\ndiscoverability (module 6) new userFacilitates creation reports analysis incorporate data \ndifferent assays experimentMakes easy share project files across team, well \npublicly, paper publishedMakes easy implement version control project (modules 9–11)Incorporates enough flexibility used minimal changes across many\nresearch projects","code":""},{"path":"module7.html","id":"steps-in-designing-the-conceptual-blueprint-for-a-project-directory-template","chapter":"Module 7 Creating project directory templates","heading":"7.2 Steps in designing the conceptual blueprint for a project directory template","text":"design conceptual framework project directory template, \ncan break process key steps:Observe current research project practicesDetermine subdirectories ’ll include ’ll name themDecide file name conventionsIn section, ’ll go detail tasks.","code":""},{"path":"module7.html","id":"observe-your-current-research-project-practices","chapter":"Module 7 Creating project directory templates","heading":"7.2.1 Observe your current research project practices","text":"work blueprint, want prioritize fit \nneeds user—research group. One way can follow \nkey early step design process: observe.167 One \nbest ways get idea research group needs within project\ndirectory take survey past research projects group. Make \nlist types data collected types pre-processing \nanalysis done using data. type data, ’s helpful make\nnote typical file type typical size. data specific assay\ndivided across files? data animals timepoints included\nsingle spreadsheet file? , saved sheet, \ndivided across sheets? Conversely, different files used data \ndifferent animals different time points?kind survey help create standard structure \nsubdirectories can use consistently across directories \nprojects research program. course, projects may include\ncertain files, might new unusual type file. can\ncustomize directory structure degree types cases, \nstill big advantage include many common elements possible\nacross projects. best way determine common elements\nmight future projects look past projects.can also helpful example file type, help capture\ntypical size, structure, contents type file. \ndata record lab, can templates \ndeveloped collect data tidy format (modules 3–5).\ndata equipment, can one example files \nequipment collected past project. example files\nhelp develop template project report can input type \ndata typically collect type project.also good stage diagnose data collection files \nsuccessful separating data collection data pre-processing \nanalysis (module 1). progress, may also want add\ntemplates serve starting point data collection files within \nproject. idea creating data collection templates described detail\nmodules 4 5.","code":""},{"path":"module7.html","id":"determine-which-subdirectories-youll-include-and-how-youll-name-them","chapter":"Module 7 Creating project directory templates","heading":"7.2.2 Determine which subdirectories you’ll include and how you’ll name them","text":"examined past projects determine types files ’ll\nnormally include project, can decide organize \nsubdirectories. subdirectory structure create core framework \nproject directory template.general, design structure subdirectories, keep mind \nkey aim create structure general enough can use \nconsistently many projects, also clear enough can quickly find\nthings within directory. one paper notes, want directory setup \n“flexible configurable.”168A number researchers put lot thought organize project\ndirectories scientific research.169 common theme\nacross papers include subdirectories store files four main\nareas:datacodereportsmeta-documentationWe’ll go discuss might included , \nwell might make sense name subdirectories areas.Data subdirectoriesData saved area separate code analysis. See\nmodule 1 deeper discussion benefits separating data \nanalysis improve reproducibility. raw data also treated \n“read-”—words, raw data never edited changed. \nwork data, including necessary quality control, pre-processing, \nanalysis, raw data read separate program analysis.\nway, can work data (even create save intermediary,\n“processed” versions data), maintaining original raw files\nwithout alteration.different recommendations name organize subdirectories\ndata. Several papers recommend separate subdirectories raw\ndata versus intermediate processed data. researchers suggested naming\nsubdirectory raw data “data-raw” one intermediate data \n“data.”170 Others suggested naming raw\ndata subdirectory “data” one intermediate data “outputs.”171 Either choices—reasonable\nalternative—fine, long use naming scheme consistently every\ntime set project directory. cases, may also decide use\nraw data directory keep code scripts used create\nintermediate processed data raw data.172One thing can challenging working raw data files \nextremely large, case may room personal computer\nstore full set raw data. One article suggested solution: store \nsmaller example dataset project directory can used test \ndemonstrate analysis code, storing full set raw data files \ncomputer adequate storage capacity.173 article\nnotes:“data large, streaming, alternative include \nsmall-sample dataset people can try techniques without \nrun expensive computations.”174Code subdirectoriesNext, ’ll want include one subdirectories code. , \nstructure helps separating data collection data analysis (module 1).\ncode may include data cleaning pre-processing data, although\nresearchers choose put code steps “raw-data”\nsubdirectory, separate files raw data files within \nsection project directory. code also include code analyze\nvisualize data. cases, might include code functions \nplan reuse within different code scripts project even across\nprojects.One article recommended single code subdirectory, named\n“code.”175 subdirectory can\nstore code scripts (outside code running part report\nRMarkdown file; see modules 18–20). Another recommends , \ncompiled code (like C code) code scripts (language\nlike R), may want separate subdirectories source code (“src”)\nversus compiled code scripts (“bin”).176Other researchers recommended “R” subdirectory used\ncode write reusable R functions, ones plan use\nseveral times across code scripts project.177 code runs data analysis, recommend \nseparate subdirectory named “model”178 “analysis.”179Report subdirectoriesYou can leverage standard structure ’ve created directory \ncreate report. can designed generate exploratory analysis \nvisualizations find typically want generate data. \ncan create using tools reproducible reports—R, key tool \nRMarkdown. , ’ll cover using tool creating report, \nmany details modules 18 20. Briefly, RMarkdown allows \ninclude code text meant humans within single, plain text\ndocument. document can rendered, process executes code\nformats text meant humans, producing document easy--read\nformat like Word PDF.Whether use tools , though, space \nproject directory keep documents create report findings.\ninclude initial reports, can also include documents like\npaper articles, conference abstracts, posters, presentations.use single subdirectory report files, named something like\n“doc.”180 Alternatively, using RMarkdown\nfiles, keep files (ones work \nedit reports) one subdirectory another subdirectory store \noutput RMarkdown files (generated reports format like PDF \nWord, treat read-generated \nRMarkdown file).181 two subdirectories named\n“analysis” “output”, respectively.182 Another article\nrecommends using separate subdirectories different types report outputs,\nexample “posters”, “manuscript”, “slides.”183Metadata subdirectories filesThe final major area cover project directory files metadata.\nfiles contain information describes project whole. \ncases might store information subdirectories, many cases,\ninformation might alternatively go single file main level \nproject directory.number pieces information may want include \nmetadata. include, example, information experiment, like\nmodel animal using treatment testing. \nalso include information related code analysis. One piece information\n’s important, example, list dependencies versions\nsoftware. example, used R analysis, version R \nuse, packages use supplement base R distribution?metadata can also provide information involved\nproject, role person , conditions reusing\nelements project, like code data. project directory \nshared complete information, details reuse \nparticularly helpful. might include information, example, \nlicense sharing code within project.Several articles suggest sharing metadocumentation type file\ncalled “README” file.184\nidea README file comes tradition software engineering.\ncode builds software system can large complex, many\nsource files must combined compiled “build” software.\nSince can hard navigate directory files, one\nlong-standing solution include README file. README file \nput top level directory’s hierarchy. way, someone opens\ndirectory, ’ll see file right away, discoverable\nfilename, since “README” tells exactly .file serves spot can help someone navigate rest \nfiles directory. can also use record metadata project:\nthings like involved research citation resulting\npaper. can write file plain text, ’re sharing project\ndirectory version control platform, might want explore\nwriting mark-language Markdown (see module 11 \nusing Markdown README file shared version control\nplatform).","code":""},{"path":"module7.html","id":"decide-on-file-name-conventions","chapter":"Module 7 Creating project directory templates","heading":"7.3 Decide on file name conventions","text":"final step designing conceptual framework create rules \n’ll name files project. create rules name\nfiles, first thing keep mind : use names balance\ngeneralizability discoverability.terms generalizability, want use file names generalize \nprojects. words, don’t make file names specific \nwon’t work next time project. may, example, want include\nname grant experiment filename metadata. \ninstinct good—can helpful include information \nexperiment somewhere filenames. try put type information\nhigh directory structure possible: specifically, put information \nname project directory . , within filenames \ndirectory, use names can used across many projects.type file always name project\ndirectories, team find easy find file use\nfile move one project next. even allow \nwrite code leverages fact certain files always \nname across projects.don’t, however, want make file names general aren’t\ndiscoverable (see module 6 idea discoverability).\nfilename, words, shouldn’t generic name doesn’t\ngive good idea contains.Say, example, used name “file_a” metadata file \nproject directory. filename generic work across many projects,\nunlike filename includes something like name experiment. ’s\ngeneric, though, hard someone figure \nfile contains just looking name. better name something like\n“experiment_metadata”—generic enough work across many projects, \ndetailed enough discoverable.Another thing consider, select file naming conventions, avoid\nspecial characters filenames. discussed idea module 4, \ncontext avoiding special characters column names cell entries\ndata collection spreadsheet. Similar considerations apply filenames.\nmany operating systems allow include things like spaces \nfilenames, special characters can make harder write code works\nfile. Try write filenames alphanumeric characters \nunderscores.One biggest culprits spaces. appealing include spaces\nfilename: ’s easier read words filename ’re\nseparated spaces. , though, get habit. move \ncoding file, spaces pain. Often, computer parses\ncode, thinks ’s gotten end something gets space.\ngets space filename, example, can think ’s gotten \nend filename contexts. , put space middle\nfilename, can confuse computer.ways help computer —ways “escape” special characters,\ncomputer treat literally rather attributing special\nmeaning special characters. However, ’s fun \nuse set files. ’s much simpler enforce rule \nuse underscores instead spaces filenames: “experiment_metadata.Md”,\nexample, rather “experiment metadata.Md”. underscores serve \npurpose legibility spaces , separating words within \nfilename. won’t confuse computer way, though.Another consideration good practice write code using relative pathnames start \ntop-level project directory.185 words, tell\ncomputer find files starting top level project\ndirectory. relative pathnames work equally well \nsomeone else’s computer, whereas use file pathnames absolute\n(.e., giving directions file root directory computer),\nsomeone else tries run code computer, won’t\nwork ’ll need change filepaths code, since everyone’s\ncomputer files organized differently. example, , \npersonal computer, project directory stored “Documents” folder,\ncolleague stored project directory “Desktop”\ndirectory, absolute filepaths file directory \ndifferent . relative pathnames, starting top level\nproject directory, , though, regardless\nstored project directory computer.comes code scripts project, ’s also one think \nmay want consider naming conventions. Often, divided key\ntasks (like data entry, pre-processing, analysis) separate scripts. \nscripts need follow specific order run recreate \nresults project. case, may want consider starting \nscript’s filename number, numbers indicate order \nscripts run.186 example, script files might look like:\n“01_reading_data.R”, “02_preprocessing_data.R”, “03_exploratory_analysis.R”, \n.cases ’ll one certain file type.\nexample, within raw data files, may one file per sample \nassay like flow cytometry, one file per timepoint ’re recording\ndata multiple timepoints.case, ’ll need develop rules name files,\nchosing system allows different filenames file. ,\ntwo things can keep mind: first, adhering standards \npossible, second designing filenames way can leverage\nsomething called regular expressions.module 2, talked standards terms recording\ndata. emphasized powerful standards can regularly\nfollowed practice. Just standards powerful tool recording\ndata, also powerful tool creating filenames.\nconventions discipline certain files named,\nfollow .One example may standard way piece laboratory\nequipment names file. example, may always include elements like\nsample name date sample run equipment.\ncase, don’t want change filenames. want keep \nstandard format, people may already built tools work \nstandard. change standard, tools wouldn’t available.\nStandards also tend help discoverability, change filenames\nstandard, may make harder someone else navigate files.standards don’t exist naming certain type file, can create \nstandards. , can think create filenames \nleverage regular expressions. coding tools can search \npatterns specify character strings, including filenames.example, say files record separate timepoints \nexperiment. pick naming convention always includes \ntimepoint filename, recorded using conventions always \nplace filename. collect timepoints call\n“day 7”, “day 14”, “day 21”, might incorporate within filenames\nusing “D” “day” two digits number. result \nfiles include something like “D07”, “D14”, “D21” name. \nstraightforward pull information back filenames \nalways put spot filename. example, ’re collecting\nbacterial loads measuring colony-forming units, might name files,\n“cfu_D07.xlsx”, “cfu_D14.xlsx”, “cfu_D21.xlsx”. \nalways put varying information (timepoint) spot, \neasy extract code using regular expressions.","code":""},{"path":"module7.html","id":"creating-and-using-a-project-template","chapter":"Module 7 Creating project directory templates","heading":"7.4 Creating and using a project template","text":"blueprint template project directory, can create\n“physical template” directory computer.\nprocess , designed template, easy. involves \nfancy tools—fact, ’s straightforward first might seem \nsimple useful. basic approach, create example file\ndirectory captures desired project directory structure. \ncreated templates, either data collection (module 4 5) \nreports (modules 18–20), can include within structure.words, create basic file directory desired template\nfiles file directory structure. ready start new project,\ncopy template, rename copy specific new project,\nuse directory store work data collect \nproject. Figure 7.1 gives example final\nresulting template directory might look like, well can copied,\nrenamed, used start new projects.\nFigure 7.1: research group can create file directory serve template experiments certain type laboratory. template can include templates files data recording generating reports. start recording data new experiment, researcher can copy rename template directory.\ntemplate restrictive—serves starting point, can\nadapted specific project. example, collecting\ndata assay used past experiments, can add\nnew data subdirectory project directory use storing new\ntype data. Figure 7.2 shows example \ncustomize basic template shown Figure 7.1.\nFigure 7.2: Example complex project directory structure created, directories added store data collected flow cytometry single cell RNA sequencing.\nKeep mind, though, want keep balance, avoid\nunneeded changes project template within specific project’s\ndirectory. many benefits standardizing (e.g.,\nknowing things , building tools leverage standardized\ndirectory structure) lost directories specific projects grow\ndifferent .Figure 7.3 gives basic walk-\nsimple steps ’ll use start new project directory ’ve created\ntype template (cover example much detail \nmodule 8, walk full example designing using \nproject template).\nFigure 7.3: Steps using basic project directory template created type study experiment.\n","code":""},{"path":"module7.html","id":"project-directories-as-rstudio-projects","chapter":"Module 7 Creating project directory templates","heading":"7.5 Project directories as RStudio Projects","text":"using R programming language data pre-processing, analysis,\nvisualization—well RMarkdown writing reports \npresentations—can use RStudio’s “Project” functionality make \neven convenient work files within research project’s directory.\ncan make file directory “Project” RStudio chosing “File” ->\n“New Project” RStudio’s menu. gives option create \nproject scratch make existing directory RStudio Project.make file directory RStudio Project, doesn’t change much \ndirectory except adding “.RProj” file. file keeps track \nthings file directory RStudio, including preferred settings\nRStudio use working project.working RStudio Project, RStudio automatically move \nworking directory top-level directory Project directory. \nmakes easy write code uses directory presumed working\ndirectory, using relative file paths identify files within directory.\ndiscussed value using relative pathnames earlier module, \ndiscussed design file naming conventions project directory.\nparticular, share project directory someone else, can\nsimilarly open RStudio Project version RStudio, \nrelative pathnames files work system without problems.\nfeature helps make code RStudio Project directory reproducible across\ndifferent people’s computers.advantages, well, turning research\nproject directories RStudio Projects. One easy \nconnect Projects GitHub, facilitates collaborative work\nproject across multiple team members tracking changes \nversion control. tracking project directory Git version\ncontrol system, open RStudio Project, special\ntab one panes help using Git project. tab provides\nvisual interface commit changes ’ve made, tracked\ncan reversed needed, also can easily push pull \ncommitted changes remote repository, like GitHub repository, \ncollaborating others. functionality described modules\n9 11.project directories set R Projects also makes easy \nnavigate among different projects. close RStudio reopen , \nautomatically open last Project open. small tab \ntop right hand corner RStudio window lists project \ncurrently . move different Project, can click arrow\nbeside project name. list recent projects, \nwell options open Project computer. want work \nRStudio, Projects, can choose “Close Project”.","code":""},{"path":"module8.html","id":"module8","chapter":"Module 8 Example: Creating a project template","heading":"Module 8 Example: Creating a project template","text":"module, ’ll show example creating project directory template\nlab group. walk process creating project\ndirectory template used manage analyze data \nspecific studies group. ’ll start discussing steps \nconceptual design—figuring blueprint standard subdirectories\nfile naming conventions. ’ll show blueprint can \ndeveloped physical implementation: file directory can copied \nrenamed initiate new project. full directory files example\ncan found https://github.com/geanders/example_for_improve_repro, \ncan download explore online.Objectives. module, trainee able :Examine files previous projects step developing project\ndirectory templateDevelop structure subdirectories project directory templateCreate project directory template initialize consistently-formatted\ndirectories lab group’s experimentsExplain report template can incorporated within project directory\ntemplate","code":""},{"path":"module8.html","id":"description-of-the-example-set-of-studies","chapter":"Module 8 Example: Creating a project template","heading":"8.1 Description of the example set of studies","text":"module, ’ll use example based set real immunology\nexperiments. example highlights research laboratory often\nconduct similar type experiment many times, lets us demonstrate \nproject directory template can reused across similar experiments. \nallow us show can move designing file directory \nsingle experiment designing one can used repeatedly, \ncan take advantage consistency directory structure across projects \nmake templates data collection can reused.example covers group studies explored novel\ntreatments tuberculosis. treatments exist tuberculosis, \ncurrent treatment regime lengthy involves combination multiple\ndrugs. treatment completed, can result development \nspread drug-resistant tuberculosis strains, treatment sometimes\nmust done observation.187 patient \nstrain tuberculosis resistant first-line drugs, \nneed treated second-line drugs, can serious side effects.188 critical need develop candidate drugs\ndisease, given limitations struggles current\ntreatment regime.study investigates mice challenged tuberculosis respond\ndifferent treatments, terms well handle treatment\n(assessed checking weight decreases notably treatment) \nalso well treatment manages limit growth tuberculosis \nmouse’s lungs.example studies conducted similar designs similar\ngoals—aimed test candidate treatments tuberculosis. studies \nset tested one treatments well one controls. \ncontrols include negative controls, like saline solution, positive\ncontrols, like drug already use treat disease (e.g., isoniazid). \nstudies tested controls, develop baseline expectations \nthings like bacterial load different mouse strains. set studies\ntested treatments monotherapies (one drug given \nanimal) well combinations two three drugs.\nmany drugs tested, tested different doses ,\ncases, different methods delivery different mouse models.treatments given several mice infected \nMycobacterium tuberculosis. treatment, mice weighed\nregularly. weight measurement helps determine particular treatment\nwell-tolerated animals—, may show treated mice\nlosing weight treatment. convenience, mice weighed\nindividually. Instead, mice treatment kept single cage,\nentire cage weighed, weight cage factored , \naverage weight mice determined dividing number mice \ncage. period time, mice sacrificed one lobe \nlungs used determine mouse’s bacterial load, plating \nmaterial lobe counting colony forming units (CFUs). One aim \ndata analysis compare bacterial load mice various\ntreatments bacterial load mice control group.full set studies included 19 studies. conducted \ndifferent times, data studies can collected using \ncommon format, ’ll talk data collection templates \nproject directory template designed accomodate experiments.","code":""},{"path":"module8.html","id":"step-1-survey-of-data-collected-for-the-projects","chapter":"Module 8 Example: Creating a project template","heading":"8.2 Step 1: Survey of data collected for the projects","text":"first step developing project template survey typical types\nfiles included research projects. give example \npart design process, let’s walk types data \ncollected example studies.First, metadata recorded study. Figure\n8.1 gives example. includes information strain\nmouse used study, treatment details (including method \ngiving drug drugs, often given week, many\nweeks), much bacteria animals exposed (measured terms \ninoculum given bacterial load one day \ngiven inoculum, based sacrificing one animal day \nchallenging animals bacteria), , study included \nnovel drug part tested treatment, batch number drug.\nFigure 8.1: Example recording metadata study set example studies module.\nNext, researchers recorded information treatment group\nwithin experiment. typically included least one negative control. \ncases, also positive control, animals treated\ndrug ’s standard use tuberculosis already (e.g.,\nisoniazid). studies also test one treatments, \ninclude monotherapies combined therapies. Figure 8.2\nshows example data recorded treatment study.\ndata include names doses three drugs treatment,\nwell column researcher can provide detailed specifications \ntreatment.\nFigure 8.2: Example recording treatment details study set example studies module.\nanimals challenged bacteria, treatment began, two\nmain types data measured recorded. First, mice weighed \nweek. convenience, mice weighed\nindividually. Instead, mice treatment kept single cage,\nentire cage weighed, weight cage factored , \naverage weight mice treatment determined dividing weight\nmice cage number mice cage. weights converted measure percent change \nweight since start treatment. animals’ weights decrease \ntreatment, marker treatment well-tolerated \nanimals. Figure 8.3 shows example data \nrecorded. animals within treatment group kept cage, \ncage measured week. dividing weight animals \ncage number animals, researchers estimate average weight\nanimals treatment group, recorded shown Figure\n8.3.\nFigure 8.3: Example recording weekly weights mice treatment group example set studies.\nFinally, treatment period, mice sacrificed portion \nmouse’s lung used estimate bacterial load mouse. Figure\n8.4 shows example data bacterial load\nmouse can recorded.\nFigure 8.4: Example recording bacterial load lungs mouse end treatment example set studies.\nexamples data researchers record entering \nspreadsheets. helpful stage ensure type data \nrecorded way separates data recording analysis (module 2).\nexample files ’ve shown —extra elements \nspreadsheets calculations create graphs. Later module, ’ll\ntalk bit templates can designed part \nprocess designing full project directory template. Earlier modules\n(modules 4 5) provide focused details designing data collection\ntemplates like .Another type files group’s studies typically generate ones\ngenerated directly laboratory equipment. example, \nexperiments may include flow cytometry assays, files output \nspecialized format directly flow cytometer. experiments might\nalso collect data single-cell RNA sequencing. ’ll want keep\nfiles mind design structure project directory\ntemplate.","code":""},{"path":"module8.html","id":"step-2-organizing-a-project-directory","chapter":"Module 8 Example: Creating a project template","heading":"8.3 Step 2: Organizing a project directory","text":"’ve determined types files ’ll normally include \nproject, need decide organize subdirectories \nproject file directory. case, ’ve organized project directory\ntemplate include just things top level (Figure\n8.5, also shown module 7):spreadsheet stores meta-data experimentA subdirectory named “raw_data”, ’ll store original raw files \ndata, pre-processed, focusing data generated laboratory\nequipmentA subdirectory named “data”, store experimental data \npre-processed, well recorded data require pre-processingA subdirectory named “R”, store code pre-processing analysisA subdirectory named “reports”, store files generate\nreports, well reports ultimately generated\nFigure 8.5: research group can create file directory serve template experiments certain type laboratory. template can include templates files data recording generating reports. start recording data new experiment, researcher can copy rename template directory.\nmay noticed structure captures main elements\ndiscussed including project template module 7:\ndata, code, reports, metadata.structure, ’ve selected subdirectory names generic enough\n(e.g., “data”, “reports”) can reused across many projects\nwithout modification. names also clear researcher \nexplores directory future, since names clear \nunambiguous. However, might make different choices—example, \nteam aren’t familiar R programming language, may\nwant use subdirectory name “code” rather “R”.Within subdirectories, can include subdirectories \norganize files (Figure8.6, repeated \nmodule 7). example, within “data” subdirectory, can \nsubdirectories different types data:subdirectory named “flow_data” data flow cytometryA subdirectory named “recorded_data”, data recorded “hand”\nlaboratory (example, weights animals)subdirectory named “sc_rna_seq_data” data single-cell RNA\nsequencing\nFigure 8.6: Example complex project directory structure created, directories added store data collected flow cytometry single cell RNA sequencing.\n, subdirectories named way generalize many\ndifferent experiments yet also clearly labels contents. Similar\nsubdirectory diversions also used within “raw_data” subdirectory,\ninclude files data need pre-processed \n’re used statistical analysis (modules 12–14). example, \nraw flow cytometry data need gated—process quantify\nimmune cell phenotypes sample—’s used statistical\nanalysis.exact combination subdirectories within “data” subdirectory might\nchange experiment experiment. example, experiments might\ninclude single-cell RNA sequencing assays, may . use \ntemplate, easy delete “data” subdirectories assays \nconducting, including template, can insure use\nconsistent name subdirectory include . ’re\ntrying consistent, ’s easier start everything might need\ndelete elements customize particular project rather \nstarting minimal framework adding.","code":""},{"path":"module8.html","id":"step-3-establishing-file-name-conventions","chapter":"Module 8 Example: Creating a project template","heading":"8.4 Step 3: Establishing file name conventions","text":"Next, decided ’d name files directory. First, \nfiles included every project. include file record\nmetadata experiment, well file record mouse weights \ncourse experiment bacterial load end experiment.files, selected filenames balance\ngeneralizability discoverability (see module 7 setting rules\nfilenames based principles). file experimental data, \nexample, named “experiment_metadata.xlsx”. name generic enough \nwork studies, also clear enough people\ngood idea ’s file explore project\ndirectory. Similarly, file recording weights bacterial load named\n“recorded_lab_data.xlsx”, balances considerations generalizability\ndiscoverability.may noticed names avoid special characters,\nincluding spaces. Instead, filenames use underscores help distinguish\ndifferent words filenames make easy read.experiments may read-outs laboratory equipment. Examples include\ndata assays like flow cytometry single-cell RNA sequencing. \ncases, ’ll keep filenames generated laboratory equipment,\n’ll maintain standard formats.","code":""},{"path":"module8.html","id":"step-4-designing-data-collection-templates","chapter":"Module 8 Example: Creating a project template","heading":"8.5 Step 4: Designing data collection templates","text":"next step create necessary data collection templates. ’ll create\nseparate spreadsheet type data, can group files\n’d like (e.g., one spreadsheet file several separate sheets). \nexample, created two files store type data, one metadata\nrecorded start experiment (overall experiment details \ndetails tested treatment) one data collected\ncourse experiment (mouse weights bacterial loads). Within\nfile, ’ve used separate sheets record different types data.\nallows us keep similar types data together file, \ntidy collection format specific type data (Figure\n8.7).\nFigure 8.7: Data collection templates example project directory template. templates created two files, one metadata, saved main directory project, one data collected laboratory experiment, saved ‘data’ subdirectory. file saved spreadsheet file, two sheets file store different types data.\ndata collection files designed using principles tidy\ndata collection. modules 4 5, showed can create tidy data\ncollection templates use, can paired \nreproducible reporting tools separate steps data collection \nreporting (modules ?? 20 go much depth reproducible\nreporting tools). decided types data \nusually collect type study template , can use\nprocess create tidy data collection templates type data.created template type data, added placeholder data\n(formatted red indicate placeholder, rather final\ndata). researcher can see example enter data \ntemplate start new project.Figure 8.8 gives example process.\nOne files included example template directory shown\nearlier spreadsheet record metadata experiment. spreadsheet\nfile two sheets, one records overall metadata study (\nexample, weeks treatment given strain mouse used) one \nrecords details treatments tested. file \ntemplate directory, spreadsheet pages include placeholder data. \nformatted red, visually can identified placeholders. \nincluding placeholder data, researcher can see example \nformat expect used recording data file. \nproject template copied, researcher replace data real\ndata, change font color black indicate placeholder\ndata replaced (Figure 8.8).\nFigure 8.8: template includes file experiment metadata, sheet recording overall details experiment. user can open file replace placeholder values (red) real values experiment. changing text color black, user can visual confirmation placeholder data replaced real study data.\nAnother sheet spreadsheet allows researcher record details \ntreatments tested experiment. , placeholder\ndata included template red font help show researcher \nrecord data, meant replaced real data \nspecific experiment (Figure 8.9). \nsimilar format used template file record data experiment,\nincluding weights animal week treatment final\nbacterial load animal end treatment. , \nplaceholder values template file, researcher replace \nreal data copying project template new experiment.\nFigure 8.9: template includes file experiment metadata, sheet recording details treatment. user can open file replace placeholder values (red) real values treatments experiment. changing text color black, user can visual confirmation placeholder data replaced real study data.\n","code":""},{"path":"module8.html","id":"step-5-designing-a-report-template","chapter":"Module 8 Example: Creating a project template","heading":"8.6 Step 5: Designing a report template","text":"final optional step create one template reports. can\ncreate report templates using tools reproducible reports—R, key tool\nRMarkdown. , ’ll cover using tool creating report\nbriefly, many details modules 18 20. \nexample files help develop template project report can input\ntype data typically collect type project.created Rmarkdown file analysis visualization \nincluded project template directory. means report file\ncopied available time someone copies project template\ndirectory start new project. However, obligated keep\nreport identical template. Instead, template report serves \nstarting point, can add adapt work study.file created using RMarkdown format,\ncombines text executable code. can create template \ninputs experimental data file formats created data recording\nfiles project template. , researcher able “knit”\nreport new experiment, recreate report based \ndata recorded experiment (Figure 8.10). knitting\ntemplate report, can create nicely formatted version report \nexperimental data (Figure 8.11).\nFigure 8.10: Example user can create report template. template includes example report, written using RMarkdown. user can open template report file use ‘Knit’ button RStudio render file. long experimental data recorded using data template files, code report can process data generate report data. user can also make changes additions template report.\n\nFigure 8.11: Example output ‘knitting’ report project template\nSpecifically, set studies preliminary report designed, \nexample shown Figure 8.12. report uses first page\nprovide nicely format version metadata study, including \ntable overall details table details specific treatment\ntested. second page provides graph shows percent weight\nchange mice treatment group compared weight group \nstart treatment. third page provides graph shows bacterial\nloads mouse, grouped treatment, well results running \nstatistical test, treatment group, hypothesis mean\ntransformed version measure bacterial load (log-10) group\nuntreated control group.\nFigure 8.12: Example preliminary report generated study set example studies module. first page includes metadata study, well details treatment tested. second page shows mouse weights treatment group changed course treatment, help identify treatment well-tolerated. third page graphs bacterial load mouse, grouped treatment, gives result statistical analysis test treatment groups outcomes significantly different untreated control group.\nLet’s take closer look elements. example, Figure\n8.13 shows tables first page report shown\nFigure 8.12. look back data collection \nstudy (e.g., Figures 8.1 8.2),\ncan see information tables pulled data\nrecorded start study.\nFigure 8.13: Example one element preliminary report generated study set example studies module. first page provides tables metadata study details treatment tested.\nFigure 8.14 shows second page report. \nfigure taken mouse weights—recorded one data\ncollection templates project (Figure 8.3)—used\ngenerate plot average mouse weight treatment group\nchanged course treatment.\nFigure 8.14: Example one element preliminary report generated study set example studies module. second page provides plot weights mice treatment changed course treatment.\nFigure 8.15 shows last page report. page\nstarts figure shows bacterial load lungs mouse \nstudy end treatment period. figure, measurement\nmouse shown point, points grouped \ntreatment group mouse. Boxplots added show distribution across\nmice group. color used show whether treatment \nnegative control, positive control, monotherapy, combined therapy. \nsecond part page gives table results running \nstatistical analysis compare bacterial load mice treatment\ngroup bacterial load mice untreated control group. Color \nadded table highlight treatments large difference \nbacterial load untreated control, well treatments \ndifference untreated control estimated statistically\nsignificant. data results, including labels plot,\ndata collected data collection templates shown earlier.\nFigure 8.15: Example one element preliminary report generated study set example studies module. third page provides results bacterial load lungs compares among treatments end treatment period.\nwrote code report way still run \nfewer observations data collection files, report\ntemplate flexibility terms study set studies\nmight vary. example, example set studies, experiments\nrun using control group mice, others run test\nseveral different treatment groups. report template can accommodate\ndifferences across studies set studies.","code":""},{"path":"module9.html","id":"module9","chapter":"Module 9 Harnessing version control for transparent data recording","heading":"Module 9 Harnessing version control for transparent data recording","text":"research project progresses, researchers often end many files\n(e.g., ‘draft1.doc’, ‘draft2.doc’). can result explosion files,\nbecomes hard track files represent “current” state \nproject. Version control allows researchers edit change research project\nfiles cleanly, including messages explain changes maintaining\npower backtrack previous versions.module, explain version control can used\nresearch projects improve transparency reproducibility \nresearch, particularly data recording. ’ll introduce \nbasic idea version control, using Git software program \nexample. later modules, ’ll explain version control platforms like GitHub,\nwell give tips use within research projects.Objectives. module, trainee able :Define “version”, “version control”, “version control software”,\n“repository”, “commit”, “commit message”Discuss challenges coordinating changes project files working\nteamsList downsides physical laboratory notebooksDistinguish “version control” “version control software”Identify examples versioning digital context (data, code, files)Discuss version control principles can improve collaboration \nscientific projects","code":""},{"path":"module9.html","id":"challenges-of-collaborating-on-evolving-research-materials","chapter":"Module 9 Harnessing version control for transparent data recording","heading":"9.1 Challenges of collaborating on evolving research materials","text":"research groups—professional teams—collaborate \npublications research, process can bit haphazard. Teams often use\nemails email attachments share updates project, sometimes\nuse email attachments pass around latest version document others\nreview edit.One fascinating example comes business world. implosion \nEnron, trove emails within company released. set \nemails become known Enron Corpus used variety \nresearch studies. One group researchers investigated emails corpus\ninvolved people work spreadsheets.189\nfound passing Excel files email attachments common\npractice, messages within emails suggested spreadsheets \nstored locally, rather location accessible team\nmembers.190 meant team members might often \nworking different versions spreadsheet file. note “\npractice emailing spreadsheets known result serious problems \nterms accountability errors, people access latest\nversion spreadsheet, need updated changes via email.”191 process collaboration often used \nscientific research: one study found, “Team members regularly pass data files\nback forth hand, email, using shared lab project servers,\nwebsites, databases.”192These practices make difficult keep track project files, \nparticular, track version file current. ,\nprocess constrains patterns collaboration—requires team member\ntake turns editing file, one team member attempt merge\nchanges made separate team members time \nversions collected.process also makes difficult keep track changes made, \noften requires one team member approve changes team members.\n“Track changes” “Comment” features software like Microsoft Word can\nhelp team communicate , features often lead \nmessy document stages editing, hard pick \ncurrent versus suggested wording, change accepted comment\ndeleted, conversations can lost forever. Finally, word processing tools\npoorly suited track changes add suggestions directly data code,\ndata code usually saved formats aren’t native word\nprocessing programs, copying format like Word can introduce\nproblematic hidden formatting can cause data code malfunction.","code":""},{"path":"module9.html","id":"recording-data-in-the-laboratoryfrom-paper-to-computers","chapter":"Module 9 Harnessing version control for transparent data recording","heading":"9.2 Recording data in the laboratory—from paper to computers","text":"scientific researchers tackling challenges \nprojects using something called version control. version\ncontrol—traditionally tool software engineers—relate collaborating\ncollect analyze scientific research data? Traditionally, experimental\ndata collected laboratory recorded paper laboratory notebook.\nlaboratory notebooks played role initial recording \ndata, also keep legal record data recorded lab.193 also resource collaborating across \nteam passing research project one lab member another.194However, paper laboratory notebooks number limitations. First, \ncan inefficient. time almost data analyses—even simple\ncalculations—done computer, recording research data paper rather\ndirectly entering computer inefficient. Also, stage \ncopying data one format another, especially done human rather\nmachine, introduces chance copying errors. Handwritten laboratory\nnotebooks can hard read,195 \nmay lack adequate flexibility handle complex experiments often\nconducted. , electronic alternatives can also easier search,\nallowing deeper comprehensive investigations data collected\nacross multiple experiments.196 one article notes, physical lab notebooks “usually\nchaotic always unsearchable.”197Given widespread recognition limitations paper laboratory notebooks,\npast couple decades, number efforts, formal\ninformal, move paper laboratory notebooks electronic\nalternatives. fields rely heavily computational analysis, \nresearch labs () use paper laboratory notebooks.198 fields, researchers traditionally\nused paper lab notebooks, companies working develop\nelectronic laboratory notebooks specifically tailored scientific research.199 early adapters pharmaceutical industrial\nlabs, companies budgets get customized versions \nauthority require use. academic laboratories, electronic lab\nnotebooks taken longer adapted.200 Indeed, widely adopted platform electronic laboratory\nnotebooks yet taken scientific community,201\ndespite clear advantages recording data directly computer rather \nfirst using paper notebook. Kwok notes 2018 commentary,“Since least 1990s, articles technology predicted imminent,\nwidespread adoption electronic laboratory notebooks (ELNs) researchers. \nyet happen”202Instead using customized electronic laboratory notebook software, \nacademics moving data recording online, using generalized\nelectronic alternatives, like Dropbox, Google applications, OneNote, \nEvernote.203\nscientists started using version control software, especially \ncombination Git GitHub, way improve laboratory data recording,\nparticular improve transparency reproducibility standards.\npieces software share pattern Google applications \nDropbox—generalized tools honed optimized ease\nuse role outside scientific research, can harnessed\npowerful tool scientific laboratory, well. also free—\nleast, GitHub, entry academic levels—, even better, one\n(Git) open-source.","code":""},{"path":"module9.html","id":"defining-version-and-version-control","chapter":"Module 9 Harnessing version control for transparent data recording","heading":"9.3 Defining “version” and “version control”","text":"scientific research today involves collaboration across team \nresearchers, rather individual scientist working alone. Collaboration\ndrives interdisciplinary science, also creates challenges. One\nchallenge comes coordinating versions research materials. \nmaterials can include data collection files, can also include \ndocuments like study protocols, well physical materials like cell lines,\nantibodies, model organisms.version one iteration research material evolving. \nexample, draft research paper one version paper. Research data\ncollect may also go several versions. example, \nidentify typo data record , may need correct typo\nadd note signature explain update. , \ncollecting data multiple timepoints, may new versions data file\ncomplete timepoint.materials evolve across versions, introduces challenges maintaining \nresearch process smooth, efficient, error-free. One challenge \nmake sure always clear version current, well \nversion used specific purposes. example, several coauthors\nediting paper draft, important ensure working \nrecent version.Another challenge coordinate changes different people make \nwork material time. Scientific collaboration often \noperate assembly line, one person finishes work \ndocument material hands next person. Instead, \noften several copies version different peoples’ hands, \nworking . One example paper draft—often coauthors\nedit latest draft time, rather one--one.\ncreates challenge taking contributions person \ncoordinating changes additions one primary copy.third challenge keep track changes made step, \ndocument moves version version. record can help auditing \nerrors bugs might introduced document evolves. Ideally, \nrecord also include information changes made \nstep.challenges can addressed process called version control.\nterm commonly used reference software development, \nidea version control widely relevant. process creates evolving\nversions document material can benefit idea version control,\naims record document changes material time, coordinate\ncontributions different members team, revert back older\nversions needed. module, ’ll focus version control applies\nresearch materials electronic (files directories), may\nalso find useful think principles elements version\ncontrol can applied research materials, like cell lines \nantibodies.","code":""},{"path":"module9.html","id":"what-are-the-key-elements-of-version-control","chapter":"Module 9 Harnessing version control for transparent data recording","heading":"9.4 What are the key elements of version control?","text":"term version version control refers one iteration state \ndocument set documents, example current version data file.\nword control captures idea allowing safe changes updates \nversion, especially one person working . Part \n“control” also include recording changes made one version \nnext annotating reasons changes.general term version control can refer method syncing\ncontributions several people file set files. Version control \ncomputer files can done “hand”, person manually logging change,\noriginally .204 However, ’s much efficient\nuse computer program handle tracking coordinate\ncontributions multiple people. Eric Raymond notes Art Unix\nProgramming, “tracking detail just sort thing computers \ngood humans .”205 goes describe version\ncontrol “suite programs automates away drudgery\ninvolved keeping annotated history project avoiding\nmodification conflicts.”206Software purpose—version control software—first developed \nsoftware programming projects. popular version control software today\ncomes roots. section, ’ll introduce key features \nversion control, ’ll use examples terminology common\nversion control software program called Git. terms derived\nparticular software program, represent ideas important\nimplementation version control. Later, ’ll touch \nideas incorporated software, like Google Docs.software available version control tracks electronic files. \nearliest version control software systems tracked single files, \nsystems quickly moved tracking sets files, called repositories. \nrepository almost identical file directory (may also know \nfile folder), indeed repository starts file directory. \ndifference repository enhanced additional overhead.207 overhead added record files \ndirectory changed time. can compare might track\ndocument changes documents paper rather electronic—\nstore documents paper folder add piece paper record\nlog change make documents folder. extra overhead\nchanges regular file directory repository similar log\nexample. repository, words, directory \nversion control.repository files version control, version control\nsoftware takes snapshots files look work . \nsnapshot called commit, provides record lines \nfile changed one snapshot another, well exactly changed.\nidea behind commits—recording differences, line--line, \nolder newer version file derives longstanding Unix command\nline tool called diff. tool, developed early history Unix \n&T’s Bell Labs,208 solid well-tested tool \nsimple important job generating list differences \ntwo plain text files. commit repository includes type \ninformation differences introduced files time \ncommit.working directory version control, explain \nchanges make —words, version control allows annotation\ndeveloping editing process.209 commit\nrequires enter commit message describing changes \ncommit made. commit messages can serve powerful tool \nexplaining changes team members reminding future\ncertain changes made. repository version control, ,\ncan include complete history files project directory \nchanged course project, also . feature used\nthoughtfully, commit history project provides well-documented\ndescription project’s full evolution. ’re working manuscript,\nexample, ’s time edit, can cut whole paragraphs, \never need get back, ’ll right commit history \nproject, commit message cut. make\ncommit message clear, make easy find commit ever\nneed paragraphs ., commits given ID tag (Git software,\ndone something called unique SHA-1 hash),210\nversion control systems number commands let “roll back”\nearlier versions. provides reversability within project files,\nallowing go back version certain commit made.211It turns functionality—able roll back earlier\nversions—wonderful side benefit comes working large\nproject. means don’t need save earlier versions file. \ncan maintain one one version project file project’s\ndirectory, confidence never “lose” old versions file.212 allows maintain clean \nsimple version project files, one copy , ensuring ’s\nalways clear version file “current” one (since ’s \none version).213 also provides reassurance can\ntry new directions project, always roll back old version \ndirection doesn’t work well.2011 commentary Nature Methods, Perkel tells story \nfunctionality helped one researcher keep project directories simpler:“Early graduate career, John Blischak found creating figures\nadvisor’s grant application. Blischak using programming language\nR generate figures, iterated optimized code, ran\nfamiliar problem: Determined lose work, gave new\nversion different filename—analysis_1, analysis_2, , \ninstance—failed document evolved. ‘idea \nchanged ,’ says Blischak… Using Git, Blischak says, longer\nneeded maintain multiple copies files. ‘just keep overwriting \nchanging saving snapshots. professor comes back says,\n’oh, sent email back March figure’, can say, ‘okay,\nwell, ’ll just bo back March version code can recreate\n’.”214A key strength, , using version control ability track every\nchange made files project, change made, made .\nVersion control creates full history evolution file \nproject. change committed, history records exact change made,\nincluding previous version file. change ever fully lost,\ntherefore, unless great deal extra work taken erase something \nproject’s commit history.’s also helpful understand version control programs handle\ncollaboration. earlier types version control programs, \ncalled centralized framework, one central repository file\nset files team working .215 team member wanted make\nchange “check ” file wanted work , make changes,\ncheck back newest main version.216 \none team member file checked , members locked \nmaking changes file—look , couldn’t make \nedits.217 meant \nchance two people trying change part file time.\nspirit, early system pretty similar idea sending file\naround team email, understanding one person works \ntime. slightly modern analogy idea single\nversion file Dropbox Google Docs, avoiding working file\nsee another team member working .assembly-line approach pretty clunky, though. particular, usually\nincreases amount time takes team finish project,\none person can work file time. Later types version\ncontrol programs moved toward different style, allowing distributed\nrather centralized collaborative work file set files.218 distributed model,\nteam members can version files, work \nmake records changes make files, occassionally sync \neveryone else share changes bring changes \ncopy files. functionality called concurrency, since allows\nteam members concurrently work set files.219This idea allowed development useful features styles \nworking, including branching forking. Branching allows try \nnew ideas ’re sure ’ll ultimately want go . Forking \nkey tool used open-source software development, , among things,\nfacilitates someone isn’t part original team getting copy \nfiles can work suggesting changes might helpful. ,\nbasic idea modern version control—project involves \nset computer files, everyone team copy directory\ncomputer, makes changes time spots files\nwant, regularly re-syncs local directory everyone\nelse’s share changes updates.distributed model also means copy full repository \nevery team member’s computer, side benefit providing additional\nbackup project files. Remote repositories—may server \ndifferent location—can added another copy project, can\nsimilarly synced regularly update changes made project files.","code":""},{"path":"module9.html","id":"comparing-git-to-other-tools","chapter":"Module 9 Harnessing version control for transparent data recording","heading":"9.5 Comparing Git to other tools","text":"number software systems version control, one \ncommon currently used scientific projects Git. program \ncreated Linus Torvalds, also created Linux operating system, 2005\nway facilitate team working Linux development. program \nversion control thrives large collaborative projects, example open-source\nsoftware development projects include numerous contributors, regular\noccasional.220 Target notes 2018 article \nversion control:“people sometimes grouse steep learning curve unintuitive\ninterface, Git become everyone’s go-version control.”221In recent years, complementary tools developed make \nprocess collaborating together using version control software easier. \ntools, bug trackers issue trackers, facilitate corroborative\nfile-based projects allow team keep running “-” list \nneeds done complete project. tools—discussed \nmodules 10 11—can used improve collaboration scientific\nprojects done teams. GitHub one popular version control platform\nadditional tools. created 2008 web-based platform \nfacilitate collaborating projects running Git version control. can\nprovide easier entry using Git version control trying learn \nuse Git command line.222 also interfaces well RStudio,\nmaking easy integrate collaborative workflow GitHub \nRStudio window computer otherwise analysis.223While Git version control software one best established ways\nimplementing version control, growing efforts enable level\nversion control platforms. example, Google Docs enables \nlevel version control Version History feature. feature\nallows name different versions document saved Google\nDocs. also allows restore document earlier versions, well \nsee changes made document made change.generalized tools like Google tools Dropbox might simpler \ninitially learn, powerful version control tools like Git offer key\nadvantages recording scientific data worth effort adopt. \nkey advantage ability track full history files \nevolve, including history changes file, also \nrecord change made.Git excels tracking changes made plain text files. files,\nwhether record code, data, text, Git can show line--line differences\ntwo versions file. makes easy go \nhistory “commits” plain text file Git-tracked repository see\nchange made time point, read commit\nmessages associated commits see change made. \nexample, value entered wrong row plain text file \nspreadsheet, researcher made commit correct data entry\nmistake, researcher explain problem resolution \ncommit message change., course, limitations using version control tools \nrecording experimental data. First, ideally laboratory data recorded \nplain text format, data may recorded binary file format. \nversion control tools, including Git, can used track changes binary\nfiles. However, Git take types files naturally. \nparticular, Git typically able show useful comparison \ndifferences two versions binary file.problems can arise binary file \nlarge,224 experimental research\ndata files (e.g., high-throughput output laboratory equipment\nlike mass spectrometer). However, emerging tools strategies \nimproving ability include track large binary files using Git \nGitHub.225Finally, tools techniques described book, \ninvestment required learn use Git,226 well\nextra overhead using version control tools project.227 However, Git can bring dramatic gains \nefficiency, transparency, organization research projects, even \nuse small subset basic functionality.228 module\n11, provide guidance getting started using Git GitHub track\nscientific research project.Third, combination Git GitHub can help way backup study data.229 Together, Git GitHub\nprovide structure project directory (repository) copied \nmultiple computers, users’ laptop desktop computers remote\nserver hosted GitHub similar organization. set-makes easy \nbring project files onto new computer—clone\nproject repository. also ensures copies full\nproject directory, including files, multiple places.230 , data backed across multiple\ncomputers, full history changes made data \nrecorded messages explaining changes, repositories commit\nmessages.231","code":""},{"path":"module9.html","id":"discussion-questions-3","chapter":"Module 9 Harnessing version control for transparent data recording","heading":"9.6 Discussion questions","text":"research, collect data paper laboratory notebooks, electronically, mixture two? found advantages disadvantages method typically use? ever cases choice must either record paper electronically (examples might include working behind secure barrier data recorded directly equipment digital format)?used following tools recording, sharing, versioning data research files (e.g., drafts research papers, code):\nElectronic laboratory notebooks\nDropbox\nGoogle Docs / Google Drive\nMicrosoft Teams\nLocal server drive run institution\nGitHub / GitLab\nElectronic laboratory notebooksDropboxGoogle Docs / Google DriveMicrosoft TeamsLocal server drive run institutionGitHub / GitLabDescribe tools helped version control, including tracking changes file helping coordinate several people working file . aspects tools ’ve used limited capacity?Can think examples times ’ve experienced failure version control? Examples might include case team members worked wrong version file, lost track changes made file. learn experience? developed methods avoid similar problems future? might version control problem like result problems rigor reproducibility scientific research?idea version control relate physical research materials, like model organisms, antibodies, cell lines? examples can share issues come research related version types physical research materials?steps think take research improve version control? see higher lower priority change take compared steps might improve rigor reproducibility research? Discuss reasoning.","code":""},{"path":"module10.html","id":"module10","chapter":"Module 10 Enhance the reproducibility of collaborative research with version control platforms","heading":"Module 10 Enhance the reproducibility of collaborative research with version control platforms","text":"researcher learned use Git computer local version\ncontrol, can begin using version control platforms (e.g., GitLab, GitHub)\ncollaborate others version control. module, \ndescribe research team can benefit using version control platform\nwork collaboratively. module 11, ’ll give detailed examples\ncan use version control platform even ’re familiar \ncoding.Objectives. module, trainee able :List benefits using version control platform collaborate\nresearch projects, particularly reproducibilityDescribe difference version control (e.g., Git) \nversion control platform (e.g., GitHub)Explain version control software version control platforms can\nhelp coordinate contributions different team membersDefine “merging”, “merge conflicts”, “issue trackers”Explain commit messages can improve project managementExplain -lists can help project managementDescribe version control platform provides additional back-\nstudy files","code":""},{"path":"module10.html","id":"what-are-version-control-platforms","chapter":"Module 10 Enhance the reproducibility of collaborative research with version control platforms","heading":"10.1 What are version control platforms?","text":"Module 9 introduced idea version control, including popular\nsoftware version control program Git. module, ’ll go \nstep , telling can expand idea version control\nleverage collaborating across research team, using version\ncontrol platforms. Version control platforms build functionality \nversion control software, can provide team tools\nsharing, tools visualization, tools project management.Version control platforms offer number advantages collaborating \nresearch project can help improve efficiency, rigor, \nreproducibility. , use become popular, \nresources help learn use platforms effectively.\nkey advantages using version control platform like GitHub \ncollaborate research projects include platform:Can track merge changes different collaborators made \ndocumentAllows create alternative versions project files (branches), merge main project desiredIncludes tools project management, including Issue TrackersProvides additional backup project filesAllows share project information online, including hosting websites related project supplemental files related manuscriptA number version control platforms available. Two currently \npopular scientific research GitHub (https://github.com/) GitLab\n(https://.gitlab.com/). provide free options scientific\nresearchers, including capabilities using public private\nrepositories collaboration researchers.Version control platforms always used conjunction version control\nsoftware, like Git software described module 9. version control\nplatform adds attractive visual interfaces working project, free \nlow-cost online hosting project files, team management tools \nproject. sense, can think Git engine version\ncontrol platform driver’s seat, dashboard, steering wheel, gears\nleverage power underlying Git software. One scientist, \narticle Git GitHub scientists, highlighted resources like\nGitHub “essential collaborative software projects enable\norganization sharing programming tasks different remote\ncontributors.”232A version control platform therefore combines strengths “Track changes”\nfeature file sharing platform like Dropbox. extent,\nGoogle Docs Google Drive also combine features, spreadsheet\nprograms moving toward rudimentary functionality version control.233 However, added advantages version control\nplatforms. example, version control platforms \nopen-source. GitLab one example. Since can set server \n, can used collaborate projects sensitive data, \nalso can operate directly server ’re using store large\nproject datasets run computationally-intensive pre-processing analysis.\nAlso, version control platforms include tools help manage \ntrack project. include “Issue Trackers”, tools exploring \nhistory file change, features assign project tasks \nspecific team members. next section describe features version\ncontrol platforms make helpful tool collaborating \nscientific research. systems leveraged scientists \nmanage research projects collaborate writing scientific manuscripts\ngrant proposals.234","code":""},{"path":"module10.html","id":"why-use-version-control-platforms","chapter":"Module 10 Enhance the reproducibility of collaborative research with version control platforms","heading":"10.2 Why use version control platforms?","text":"Let’s look detail advantages using version control\nprogram. first can provide easy--use interface \npower Git. Years ago, use version control required users \nfamiliar command line, send arcane commands track project\nfiles interface. However, version control platforms typically\nallow team members explore work functions Git easier\nway try use barebones version control software. \nrising popularity version control platforms, version control project\nmanagement can taught relatively quickly students months—\neven weeks—coding experience. fact, version control beginning \nused method turning grading homework beginning programming\nclasses, students learning techniques first weeks class.235 practically unimaginable without \nuser-friendly interface version control platform wrapper power\nversion control software .second advantage version control platform helps tracking \nmanaging contributions team members. proverb many cooks \nkitchen captures, time multiple people working project, \nintroduces chance conflicts—cases contributions different\npeople disagree. higher-level conflicts, like want \nfinal product look like jobs, can’t easily managed\ncomputer program, now complications integrating everyone’s\ncontributions—letting people work space bring\ntogether individual work one final project—can . \nprograms version control originally created help programmers\ndeveloping code, can used now coordinate group work numerous types\nfile-based projects, including scientific manuscripts, books, websites.236 Although can work projects include\nfiles saved binary (Word documents, example), thrive projects\nheavier concentration text-based files, fit nicely \nscientific research / data analysis workflow based data stored \nplain text formats data analysis scripts written plain text files, tools\ndiscuss modules.one key feature modern version control ’s critical making\nwork—resolving changes files started edited \ndifferent ways different people now need put back together. \nstep called merging. feature driven Git software\n, typically won’t use ’re collaborating project\nversion control platform like GitHub.can think merging changes two people made \nedited single file, file started identical\ncopies. Without version control, process can time-consuming \nfrustrating. one scientist notes:“likely share code multiple lab mates collaborators,\nmay suggestions improve . email code\nmultiple people, manually incorporate changes\nsends.”237The version control software can handle . Think file broken\nseparate lines. lines neither person\nchanged. easy handle “merge”—stay \noriginal copy file. Next, lines one person\nchanged, person didn’t. turns pretty\neasy handle, . one person changed line, use \nversion—’s --date, since people started \nversion, means person didn’t make changes \npart file. Finally, may lines people changed \ntime. called merge conflicts. ’re places \nfile ’s clear, easy--automate way computer can know\nversion put integrated, latest version file. Different\nversion control programs handle merge conflicts different ways.common version control program used today, Git, spots \nfile flagged special set symbols try integrate \ntwo updated versions file. Along special symbols denote \nconflict, also versions conflicting lines \nfile. Whoever integrating files must go pick version \nlines use integrated version file, write compromise\nversion lines brings elements people’s changes, \ndelete symbols denoting conflict save latest\nversion file.Another advantage version control platform , collaborate\nusing version control platform, commit messages provide way \ncommunicate across team members. example, one person key\nperson working certain file, run problem one spot \nasks another team member take go, second team member isn’t limited\njust looking file emailing suggestions. Instead, \nsecond person can make sure latest version file, make\nchanges think help,\ncommit changes message (commit message) think\nchange fix problem, push latest version file\nback first person. several places help \nchange file, can fixed several separate commits, \nmessage. first person, originally asked help, can\nread updates file (platforms using version control\nnow highlight changes file) read second\nperson’s message messages change might help. Even better, days\nmonths later, team members trying figure certain change\nmade part file, can go back read messages get \nexplanation.Even better, platforms using Git often include nice tools visualizing\ndifferences two files, providing visual way look \ndifferences files across time points project. example, GitHub\nautomatically shows changes using colors highlight additions subtractions\nplain text one file compared another version look\nrepository’s commit history. Similarly, RStudio provides new\n“Commit” window can used compare differences original \nrevised version plain text files particular stage commit history.\nmodule 9, ’ll walk examples navigating features.Another advantage version control platform often include extra\ntools project management. include issue trackers, allow \nteam keep running “-” list needs done complete \nproject.238 Sometimes best tools also happen \ncheap easy. case, tool might obvious don’t\neven think formalizing tool. “-” list excellent\nexample.-list allows take big task break specific steps\nneed done complete task. helps something key\nsolving big problems: able zoom big picture—big\nvague descriptions major steps solve problem—fine\ndetails tackle steps. Adam Savage \nTV show Mythbusters notes:“value list frees think creatively, \ndefining project’s scope scale page, brain doesn’t\nhold much information. beauty checkbox \nthing regard progress, allowing monitor status\nproject, without mentally keep track everything.”239The Issues section GitHub repository works type -list. \nlooking home Issues page, see overview tasks need \ncomplete finish project. tasks, can zoom \ndetails clicking Issue. take page \nteam can discuss details task, honing solve .’s tempting use emails discuss progress task talk \nsolve . Don’t. Use Issue instead. keep discussion one\nplace, won’t go back emails find old\ndiscussion solved . Also, Issues section GitHub doesn’t\ndelete Issue ’ve complete task. Instead, allows “close”\nIssue. moves Issue section closed\nissues—takes -list, saves full discussion\nsomewhere ’ll able find easily future ever want \nrevisit solved problem.Another advantage version control platforms , project uses \nversion control platform, easy share data recorded project\npublicly. GitHub, can set access project either public \nprivate, setting can converted easily one form \ncourse project.240 private project can viewed\nfellow team members, public project can viewed anyone.\ncan used share project data online associated manuscript\npublished, increasingly common request requirement journals \nfunding agencies.241 Sharing data allows complete\nassessment research reviewers readers makes easier \nresearchers build published results work,\nextending adapting code explore datasets ask \nresearch questions.242Further, Git tracks full history changes \ndocuments, includes functionality lets tag code data\nspecific point (example, date paper submitted) \nviewers can look specific version repository files, even \nproject team continues move forward improving files directory.\nadvanced end functionality, even ways assign \npersistent digital identifier (e.g., DOI, like assigned published articles)\nspecific version GitHub repository.243Version control platforms also help providing way backup study data.244 Together, Git GitHub\nprovide structure project directory (repository) copied \nmultiple computers. distributed model, collaborator \ncopy project files local computer. project\nfiles also stored remote repository push pull\ncommits. using GitHub platform, GitHub’s servers; \nuse GitLab, can set system server. time push\npull remote copy project repository, syncing \ncopy project files computers.set-makes easy bring project files onto new\ncomputer—clone project repository. also ensures\ncopies full project directory, including files, \nmultiple places.245 , data backed \nacross multiple computers, full history changes made \ndata recorded messages explaining changes, \nrepositories commit messages.246Leips highlights importance backup research data code:“Backup, backup, backup—main action can take care \ncomputers data. Many PIs assume backup systems inherently\npermanent foolproof, often takes loss remind one \nmaterials break, systems fail, humans make mistakes. Even data\nbacked work, least one backup system. Keep least\none backup site, case diaster lab (yes, fires floods\nhappen). doesn’t make much sense two separate backup systems stored\nnext drawer.”247Finally, version control platforms like GitHub can used number\nsupplementary tasks research project. include publishing\nwebpages web resources linked project otherwise improving\npublic engagement project, including allowing researchers\ncopy adapt project process called forking. Version\ncontrol platforms also provide supplemental backup project files.First, GitHub can used collaborate , host, publish websites \nonline content.248 Version control systems used \nlong time help writing longform materials like books (e.g.,\nRaymond249); new tools making process even easier. GitHub Pages\nfunctionality, example, now used host number books created\nR using bookdown package, including online version book.250 blogdown package similarly can used create websites,\neither individual researchers, research labs, specific projects\ncollaborations.251 , project includes creation \nscientific software, version control platform can used share \nsoftware—well associated documentation—format easy \nothers work .platform can also used share supplemental material manuscript,\nincluding code used pre-processing analyzing data. Perez highlights\nfunctionality:“traditional way promote scientific software publishing \nassociated paper peer-reviewed scientific literature, though, pointed\nBuckheir Donoho, just advertising. Additional steps can boost\nvisibility organization. example, GitHub Pages simple websites\nfreely hosted GitHub. Users can create host blog websites, help pages,\nmanuals, tutorials, websites related specific projects.”252With GitHub, collaborators public project can directly change\ncode, anyone else can suggest changes process copying \nversion project (forking ). allows someone make changes\nlike suggest directly copy code, ask \nproject’s owners consider integrating changes back main version\nproject pull request. GitHub therefore creates platform\npeople can explore, adapt, add people’s coding projects,\nenabling community coders.253 \nfunctionality, GitHub described “social network software\ndevelopment”254 “kind bazaar offers just \npiece code might want—much free.”255\nprocess can leveraged others copy adapt code—\nparticularly helpful ensuring software research project won’t \n“orphaned” main developer unavailable (e.g., retires, dies), \ninstead can picked continued interested researchers.\nCopyright statements licenses within code projects help clarify\nattribution rights cases.module 11, describe practical ways leverage resources\nwithin research group. include instructions team leaders—\nmay code may want use GitHub within projects help manage \nprojects—well researchers work directly data code \nresearch team. also number excellent resources now\navailable walk users set use version control\nplatform. process particularly straightforward research project\nfiles collected RStudio Project format, described earlier\nmodules.","code":""},{"path":"module11.html","id":"module11","chapter":"Module 11 Using Git and GitHub to implement version control","heading":"Module 11 Using Git and GitHub to implement version control","text":"chapter, give overview use Git GitHub\nlaboratory research projects. prefer open-source version\ncontrol platform, GitLab similar functionality can installed \nserver .’ll address two separate groups, separate sections. main focus,\n’ll overview can leverage use tools director \nmanager project, without knowing code language like R. \nfocusing audience module, see area \naren’t lot available resources provide guidance. GitHub provides \nnumber useful tools can used anyone, providing common space \nmanaging data recording, analysis reporting scientific research\nproject. case, need least one member team\ncomfortable programming language, set maintain \nGitHub repository, team members can participate many features \nGitHub repository regardless programming skill.audience information using Git GitHub researchers \ncomfortable coding. Fortunately, many good resources available \naudience. ’ll end module providing advice audience \npoint resources can go learn fully develop \nskills.examples, ’ll show different elements two real GitHub repository, used\nscientific projects papers. first repository available \nhttps://github.com/aef1004/cyto-feature_engineering. provides example data\ncode accompany published article pipeline flow cytometry\nanalysis.256 second repository available https://github.com/PodellLab/Granuloma_RSratio_ISH.\nprovides example data code accompany published article immune\ncell composition replication status Mycobacterium tuberculosis \nlevel granulomas.257Objectives. module, trainee able :Apply tools within version control platform manage research project,\neven members codersUtilize visualization tools version control platform explore \nevolution project filesUtilize Issues tracker version control platform break project\ntasks discuss details task teamDescribe roles ownership repository version control platformExplain repository GitHub can switched public private\nstates","code":""},{"path":"module11.html","id":"leveraging-git-and-github-as-a-non-coder","chapter":"Module 11 Using Git and GitHub to implement version control","heading":"11.1 Leveraging Git and GitHub as a non-coder","text":"Git history software development, \nintroductions quickly present arcane-looking code commands, may \nhesitations whether useful scientific research group.\nparticularly likely case , many research\ngroup, experience programming.case. Git traditionally used \ncommand-line interface (think black green computer screens shown \nmovies portray hackers), GitHub wrapped Git’s functionality \nattractive graphical user interface easy understand. \ninteract project repository online logged \nGitHub, rather exploring computer (although also\ngraphical user interfaces can use easily explore Git repositories\nlocally, computer).fact, combination Git GitHub can become secret weapon \nresearch group willing encourage group know\nprogramming (willing learn bit) take time learn \nset project environment project management. project \nset GitHub, number features can used \nteam members, whether code . features facilitate collaboration\ncoders non-coders data code evolve. major features\nadvantages Git GitHub described modules 9 \n10.mentioned modules 9 10, repositories \ntracked Git shared GitHub provide number tools \nuseful managing project, terms keeping track ’s \ndone project also planning needs done next, breaking\ngoals discrete tasks, assigning tasks team members, \nmaintaining discussion tackle tasks.can go long way just starting subset tools Git \nGitHub offer. module, ’ll focus :Exploring commits commit historyTracking making progress issuesManaging repository access ownershipProviding project documentation help others navigate project\nfilesAt end module, video demonstration walks \nelements ’ve highlighted.GitHub free join; paid plans, free plan adequate\ngetting started. create account, visit https://github.com/. \nfind need free plan provides, academic researchers can request\nfree use extensive versions needed, can explore \nopen-source alternative, GitLab.Even coding, need logged GitHub account\ncontribute repository. actions, need collaborator\nproject take action; later sections module, \ndescribe people can added collaborators GitLab repository.","code":""},{"path":"module11.html","id":"exploring-commits-and-the-commit-history","chapter":"Module 11 Using Git and GitHub to implement version control","heading":"11.1.1 Exploring commits and the commit history","text":"version control platform like GitHub can help managing projects\nproviding tools visually explore project evolved.\ntime team member makes change files GitHub repository, \nchange recorded commit, team member must include short\ncommit message describing change. file project \npage GitHub (Figure 11.1 shows example). can\nsee history changes files clicking “History” link \npage.\nFigure 11.1: Example file page within GitHub repository. file repository page. page, can see history changes made file looking ‘History’. can also make commit edit directly GitHub clicking ‘Edit’ icon.\nFigure 11.2 gives example can see \nfull history changes made file project. \nchange tracked commit, includes markers made \nchange message describing change. allows quickly pinpoint\nchanges file research project. Near commit message listings\nteam member made commit made. also helps \nsee team members contributed file evolves.\nFigure 11.2: Commit history GitHub. file repository ‘History’ page, can explore change commited file. commit unique identifier commit message describing change. can click entry commits see changes made file commit (see next figure).\nclick one commits listed file’s History page (Figure\n11.2 points one example click),\ntake page providing information changes made \ncommit (Figure 11.3). page provides \nline--line view change made project files \ncommit, well commit message commit. person\ncommitting change included longer description commentary,\ninformation also included.Within body page, can see changes made commit. Added\nlines highlighted green deleted lines highlighted red.\npart line changed, shown twice, red \nversion commit, green showing version following \ncommit. can visually compare two versions line see \nchanged commit.\nFigure 11.3: Commit history GitHub. commit page, can explore changes made commit, made , committed.\npage shown Figure 11.1 also allows make \nedits file commit . team members working lot \ncoding, usually make changes file locally, repository copy\ncomputers push latest changes GitHub version.\nworkflow allow test code locally update \nGitHub version.However, also possible make commit directly GitHub, may\nuseful option team members coding like make\nsmall changes writing files. file’s page GitHub, \n“Edit” icon (Figure 11.1). clicking , \nget page can directly edit file (Figure\n11.4 shows example page looks like). \nmade edits, need commit , along short\ndescription commit, “commit message”. like include \nlonger explanation changes, space , well, \nmake commit (Figure 11.4). commits show \nrepository’s history, attributed commit message\nattached change.\nFigure 11.4: Committing changes directly GitHub. click ‘Edit’ button file’s GitHub page (see previous figure), take page can edit file directly. save changes ‘commit’, including commit message describing made change. change tagged message name.\n","code":""},{"path":"module11.html","id":"tracking-and-making-progress-on-issues","chapter":"Module 11 Using Git and GitHub to implement version control","heading":"11.1.2 Tracking and making progress on issues","text":"Another way version control platform like GitHub can help manage \nproject “Issues” tracker. described module 10,\nIssues page can serve “-” list project whole.\nlets keep track tasks need done, well \ndetailed conversations team task.repository includes type tracker, can easily used \nteam members, whether comfortable coding . Figure\n11.5 gives example Issues tracker\npage \nrepository using example. main Issues page, like\none shown figure, well separate pages Issue.\nFigure 11.5: Issues tracker page example GitHub repository. Arrows highlight tab click get Issues tracker page repository, well go find open closed Issues repository.\nmain Issues tracker page provides clickable links open issues \nrepository. can open new issue using “New Issue” main\npage specific page repository’s issues. See Figure\n11.6 example button.\nFigure 11.6: Conversation Issue Issues tracker page example GitHub repository. example, can see GitHub Issues trackers allow discuss resolve issue across team. page, can read current conversation Issue #1 repository add comments. Issue resolved, can ‘Close’ Issue, moves list active issues, allows still re-read conversation , necessary, re-open issue later. can also open new issue page, using button highlighted top right.\npage specific issue (e.g., Figure 11.6), \ncan conversation team determine resolve issue.\nconversation can include web links, figures, even lists check boxes, \nhelp discuss plan resolve issue. issue numbered,\nallows track individually work project.resolved issue, close , using “Close” button \nIssue’s page (see Figure 11.6 example). moves issue\nactive list “Closed” list. closed issue still \npage, can read conversation describing \nresolved. need , can re-open closed issue later, \ndetermine fully resolved. Figure 11.5 shows\ngo see list closed Issues project.\nFigure 11.7: Labeling assigning Issues. GitHub Issues tracker allows assign issue one team members, clarifying take lead resolving issue. also allows tag issue one labels, can easily navigate issues specific type identify category specific issue.\nIssues tracker page includes advanced functionality, well\n(Figure 11.7). example, can assign issue one\nteam members, indicating responsible resolving \nissue. can also tag issue one labels, allowing \ngroup issues common categories. example, tag issues \ncover questions pre-processing data using “pre-processing” label,\nrelated creating figures final manuscript \n“figures” label.Managing repository access ownershipRepositories include functionality inviting team members, assigning\nroles, otherwise managing access repository. First, repository\ncan either public private. public repository, anyone \nable see full contents repository GitHub. can\nalso set repository private. case, repository can \nseen invited collaborate repository, \nlogged GitHub accounts. private / public\nstatus repository can changed time, want can\nmaintain repository project private publish results,\nswitch public, allow others explore code data\nlinked published results.can invite team members collaborate repository, long \nGitHub accounts. public repositories can seen anyone, \npeople can add change contents repository people \ninvited collaborate repository. person creates \nrepository (repository “owner”) can invite collaborators \n“Settings” tab repository, “Manage access” function \nrepositories maintainer. owner repository access\ntab repo. page, can invite collaborators \nsearching using GitHub “handle” (short name chose \nidentified GitHub). can also change access rights, example,\nallowing team members able make major changes \nrepository—like deleting —others can make smaller\nmodifications.[Add: Roles repository]owner repository, administrator rights \norganization repository, access additional page \nrepository—“Settings” page. Figure 11.8 shows\ntab ’ll click access Settings page. (\nsee tab ’re exploring repository, either \nowner-level rights repository aren’t logged GitHub\naccount.)\nFigure 11.8: owner-level rights repository, access additional page repository, called ‘Settings’. can use page several administrative tasks repository. example, can add remove collaborators repository.\ncan use Settings page manage collaborators project.\ngo “Collaborators” section Settings page\n(Figure 11.8), can add new collaborators using \n“Add people” button. allow search new collaborator\nusing either GitHub handle email used set \nGitHub account. invite someone, get email invitation,\ncan respond invitation join collaborator \nrepository. can also use area manage people already\ncollaborators. example, need remove collaborator \nproject, can “Collaborators” section Settings\npage repository.owner repository (administrative rights \norganization-style repository), able change repository\nvisibility, toggling private public vice-versa. ’ll \nSettings page repository. scroll page,\n’ll get area called “Danger Zone”, shown \nFigure 11.9. section, ’s line labeled\n“Change repository visibility”. can click button “Change\nvisibility”. repository currently private, brings option \n“Change public”. click , anyone able view \nrepository using repository’s web link. using repository\npaper, use functionality change repository \nprivate—’re working paper—public—’ve\npublished paper want share code data.\nFigure 11.9: owner-level rights repository, can change visibility repository. , go ‘Settings’ page repository scroll ‘Danger Zone’ section, shown . section, can change repository private public using option ‘Change repository visibility’.\n","code":""},{"path":"module11.html","id":"providing-project-documentation","chapter":"Module 11 Using Git and GitHub to implement version control","heading":"11.1.3 Providing project documentation","text":"[Add: README Markdown]planning use GitHub way share project directory, \nfind useful create README file using file format called\n“Markdown”. [Automatically renders nice format put GitHub]Module 7: metadata, READMEMarkdown renders nicely posted GitHubShow example Amy’s project","code":""},{"path":"module11.html","id":"leveraging-git-and-github-as-a-scientist-who-programs","chapter":"Module 11 Using Git and GitHub to implement version control","heading":"11.2 Leveraging Git and GitHub as a scientist who programs","text":"able leverage GitHub manage projects share data, need\nleast one person research group can set initial\nrepository. GitHub repositories can created easily starting \nRStudio Project, format organizing project files described \nmodule 7.many excellent resources provide instructions topic\nmeant researchers comfortable using R RStudio. \nexcellent place start online book written Jenny Bryan\nnamed Happy Git GitHub useR. book available free\nhttps://happygitwithr.com/. provides gentle yet thorough introduction\nusing Git, connecting RStudio Projects, connecting everything\nonline version control platform like GitHub. also includes \nhelpful section covers daily workflow look like \nusing Git GitHub conjunction projects include R code.’ve explored resource, others might find useful :Software Carpentry’s introduction version control Git, available \nhttps://swcarpentry.github.io/git-novice/Article Quick Introduction Version Control Git GitHub258Article Ten Simple Rules Taking Advantage Git GitHub259","code":""},{"path":"module12.html","id":"module12","chapter":"Module 12 Principles of pre-processing experimental data","heading":"Module 12 Principles of pre-processing experimental data","text":"experimental data collected biomedical research often requires\npre-processing can analyzed. scientific field, work\ndata, often take much time prepare data analysis\ntakes set run statistical analysis .260 certainly true complex biomedical data,\nincluding data flow cytometry, transcriptomics, proteomics, \nmetabolomics. worthwhile investment time learn strategies make\npre-processing data efficient reproducible, \ncritical—rigor entire experiment—ensure \npre-processing done correctly can repeated others.pre-processing steps, fact, clear practical follow\ntypes protocols follow wet lab procedure. Key \nreproducibility procedure described enough detail others\ncan follow exactly. Use point--click software /propritary\nsoftware can limit transparency reproducibility analysis stage\ntime-consuming repeated tasks.module, explain pre-processing can broken common\nthemes processes. module 13, explain scripted\npre-processing, especially using open-source software, can improve transparency\nreproducibility stage working biomedical data.Objectives. module, trainee able :Define “pre-processing” experimental data, “noise” data, “batch effects”,\n“normalization”, “dimension reduction”, “feature selection”List reasons pre-processing might necessaryUnderstand key themes processes pre-processing identify \nprocesses pipelines","code":""},{"path":"module12.html","id":"what-is-data-pre-processing","chapter":"Module 12 Principles of pre-processing experimental data","heading":"12.1 What is data pre-processing?","text":"conducting experiment involves work wet lab, \nlot work data ready analysis. may,\nexample, conducted extensive amount work involved laboratory\nanimals cell cultures. many cases, run samples \nadvanced equipment, like cytometers sequencers. completed\nlong hard process, may ask , “ran experiment, ran\nequipment… Aren’t done hard work?”.certain types data, may , may able proceed directly \nstatistical analysis. example, collected weights lab animals,\nprobably directly using data answer questions like whether weight\ndiffered treatment groups. However, lot biomedical data, \nable move directly analyzing data. Instead, need\nstart stage pre-processing data: , taking\ncomputational steps prepare data ’s appropriate format \nused statistical analysis.several reasons pre-processing often necessary. first \nmany biomedical data collected using extremely complex equipment \nscientific principles. pre-processing case used extract scientific\nmeaning data might collected using measurements \nclosely linked complex process final scientific question.\nNext, cases practical concerns made easier \ncollect data one way pre-process later get format \naligns scientific question. example, want average weight\nmice different treatment groups, may practical weigh \ncage contains mice treatment group rather weigh\nmouse individually. makes life lab easier, means ’ll need\ncomputational pre-processing data make sense \nlater. Third, now frequent cases assay generates\nlarge set measures—example, expression levels thousands \ngenes sample—pre-processing might help digesting \ncomplexity inherent type high-dimensional data. Finally, pre-processing\noften necessary check resolve quality control issues within \ndata. module, ’ll explore themes pre-processing \ndepth.","code":""},{"path":"module12.html","id":"common-themes-and-processes-in-data-pre-processing","chapter":"Module 12 Principles of pre-processing experimental data","heading":"12.2 Common themes and processes in data pre-processing","text":"Exactly pre-processing need vary depending way data\ncollected scientific questions hope answer, often \ntake lot work develop solid pipeline pre-processing data \nspecific assay. However, common themes drive need \npre-processing data across types data collection research\nquestions. common themes provide framework can help design\ndata pre-processing pipelines, interpret apply pipelines \ndeveloped researchers. rest module describe several \ncommon themes data pre-processing.","code":""},{"path":"module12.html","id":"extracting-scientifically-relevant-measurement","chapter":"Module 12 Principles of pre-processing experimental data","heading":"12.2.1 Extracting scientifically-relevant measurement","text":"One common purpose pre-processing translate measurements \ndirectly collect measurements meaningful scientific\nresearch question. Scientific research uses variety complex techniques \nequipment initially collect data. result inventions \nprocesses, data directly collected laboratory person \npiece equipment might require quite bit pre-processing translated\nmeasure meaningfully describes scientific process. key element\npre-processing data translate acquired data format can\ndirectly answer scientific questions.type pre-processing vary substantially assay assay, \nalgorithms tied methodology assay . ’ll describe\nexamples idea, moving example simple translation\nprocesses much complex (typical data collected\npresent many types biomedical research assays).basic example, assays use equipment can measure \nintensity color sample sample’s opacity. measures\nmight directly (least proportionally) interpretable. example,\nopacity might provide information concentration bacteria\nsample. Others might need interpretation, based scientific\nunderpinnings assay. example, enzyme-linked immunosorbent assay\n(ELISA), antibody levels detected measure intensity color \nsample various dilutions, interpret correctly, need know\nexact process used assay, well dilutions \nmeasured.complexity “translation” scales move data \ncollected using complex processes. Biomedical research today leverages\nextraordinarily complex equipment measurement processes learn \nhealth disease. invented processes measuring can\nprovide detailed informative data, allowing us “see”\nelements biological processes seen level .\nHowever, require steps translate data directly\nrecorded equipment data scientifically meaningful.One example flow cytometry. flow cytometry, immune cells characterized\nbased proteins present within surface cell,\nwell properties like cell size granularity.261 Flow cytometry identifies proteins complicated\nprocess involves lasers fluorescent tags leverages key\nbiological process—antibody can specific affinity one\nspecific protein.262The process starts identifying proteins can help \nidentify specific immune cell populations (e.g., CD3 CD4 proteins \ncombination can help identify helper T cells). collection proteins \nbasis panel ’s developed flow cytometry experiment. \nproteins panel, incorporate antibody \nspecific affinity protein. antibody sticks cell \nsubstantial number, indicates presence associated protein \ncell.able measure antibodies stick cells, type \nantibody attached specific fluorescent tag (often\nreferred “color” descriptions flow cytometry).263\nfluorescent tag included panel emit wavelength certain\nwell-defined range exposed light wavelengths certain\nrange. cell passes flow cytometer, lasers activate \nfluorescent tags, can measure intensity light emitted specific\nwavelengths identify proteins panel present \ncell.264This extraordinarily clever way identify cells, complexity \nprocess means lot pre-processing work must done resulting\nmeasurements. interpret data recorded flow cytometer\n(intensity light different wavelengths)—generate \ncharacterization immune cell populations data—need \nincorporate number steps translation. include steps \nincorporate information fluorescent tags attached \nantibodies, proteins cell antibodies attach , \nimmune cells proteins help characterize, wavelength fluorescent\ntag emits , . cases, measuring equipment provide\nsoftware performs pre-processing get first\nversion data, may need performed hand, especially \nneed customize based research question. , ’s critical \nunderstand process, decide ’s appropriate specific\nscientific question.Similarly complex processes used collect data many single-cell \nhigh throughput assays, including transcriptomics, metabolomics, proteomics,\nsingle cell RNA-sequencing. can require complex sometimes lengthy\nalgorithms pipelines extract direct scientifically-relevant measures\nmeasures laboratory equipment captures cases.\nDepending assay, pre-processing can include sequence alignment\nassembly (sequencing data collected) peak identification \nalignment (data collected using mass spectrometry, example).Paul Flicek Ewan Birney note article making sense \nsequence reads:“individual outputs sequence machines essentially worthless \n. … Fundamental creating biological understanding \nincreasing piles sequence data development analysis algorithms able\nassess success experiments synthesize data manageable\nunderstandable pieces.”265The discipline bioinformatics works develop types pre-processing\nalgorithms.266 Many available open-source,\nscripted software like R Python. types pre-processing algorithms\noften also available proprietary software, sometimes sold equipment\nmanufacturers sometimes separately.","code":""},{"path":"module12.html","id":"addressing-practical-concerns-and-limitations-in-data-collection","chapter":"Module 12 Principles of pre-processing experimental data","heading":"12.2.2 Addressing practical concerns and limitations in data collection","text":"Another common reason pre-processing address things \ncollecting data—specifically, things \npractical purposes practical limitations. need \nhandled, possible, computational pre-processing.type pre-processing often addresses something called\nnoise data. collect biomedical\nresearch data, collecting hope measure\nmeaningful biological variation two conditions. \nexample, may measure hope meaningful difference \ngene expression sample taken animal diseased versus\none healthy, aim finding biomarker disease., however, several sources variation data collect. first\nvariation comes meaningful biological variation \nsamples—type variation trying measure \nuse answer scientific questions. often call “signal” \ndata.267There sources variation, , though. sources irrelevant \nscientific question, often call “noise”—words,\ncause data change one sample next way might\nblur signal care . therefore often take steps \npre-processing try limit remove type variation, can see \nmeaningful biological variation clearly.two main sources noise: biological technical. Biological\nnoise data come biological processes, ones \nirrelevant process care particular experiment. \nexample, cells express different genes depending cell\ncycle. However, trying use single cell RNA-sequencing explore\nvariation gene expression cell type, might consider \ngrowth-related variation noise, even though represents biological\nprocess.second source noise technical. Technical noise comes variation\nintroduced process collecting data, rather \nbiological processes. introduction module, brought \nexample weighing mice cage rather individually; one example \ntechnical noise case differences across samples ’s\nbased number mice cage.another example, part process single-cell RNA-sequencing involves\namplifying complementary DNA developed messenger RNA \ncell sample. much complementary DNA amplified \nprocess, however, varies across cells.268 occurs ,\ndifferent fragments amplified sequences read,\nfragments amplified times others. two fragments \nexact abundence original cell, one amplified \n, one measured higher level sample \namplification bias accounted . isn’t addressed \npre-processing, amplification bias prevents meaningful comparison\nacross cells.Another source technical noise something called batch effects. \noccur data consistent differences based \nmeasuring, batch sample run , equipment used \nmeasure. example, two researchers working weigh mice \nexperiment, weights recorded one researchers might tend ,\naverage, lower recorded researcher, perhaps \ntwo scales using calibrated bit differently. Similarly,\nsettings conditions can change subtle ways different runs \npiece equipment, samples run different batches might \ndifferences output based batch.cases, ways reduce variation comes \nprocesses aren’t interest scientific question, either \nbiological technical sources. important consider , \nvariation might just lower statistical power \nanalysis, can go bias results.Batch effects, example, can often addressed statistical modeling,\nlong identified aligned difference \ntrying measure (words, samples control animals\nrun one batch treated animals another batch, \nable separate batch effect effect treatment).methods adjust batch effects fitting regression\nmodel includes batch factor, using residuals \nmodel next steps analysis (“regressing ” batch effects).269 can also incorporate directly statistical\nmodel used main statistical hypothesis testing interest.270 case, technical noise isn’t addressed\npre-processing phase, rather part statistical analysis.Another example process can help adjust unwanted variation \nnormalization. Let’s start simple example explain \nnormalization . Say wanted measure height three people,\ncan determine tallest shortest. However, rather \nstanding even surface, standing top ladders \ndifferent heights. measure height top person’s head\nground, able compare heights correctly,\nheight ladder incorporated measure. \nknew height person’s ladder, though, normalize \nmeasure subtracting ladder’s height total measurement, \nmeaningfully compare heights determine person tallest.Normalization plays similar role pre-processing many forms biomedical\ndata. One article defines normalization , “process accounting , \npossibly removing, sources variation biological interest.”271 One simple example comparing weights two groups\nmice. Often, group mice might measured collectively cage,\nrather taken weighed individually. Say three treated\nmice one cage four control mice another cage. can weigh cages\nmice, compare weights, need normalize \nmeasurement dividing total number mice cage (\nwords, taking average weight per mouse). type averaging \nsimple example normalizing data.normalization pre-processing might used adjust sequencing depth\ngene expression data, can meaningfully compare measures \ngene’s expression different samples treatment groups.\ncan done bulk RNA sequencing calculating adjusting \nglobal scale factor.272 One article highlights critical\nrole normalization RNA sequencing context reproducibility:“biggest, easiest way [biologist RNA-Seq tell \nbetter normalization data needed]—way discovered \nimportance normalization microarray context—lack \nreproducibility across different studies. can three studies \ndesigned study thing, just see basically \nreproducibility, terms differentially expressed genes. every time \nencountered , always traced back normalization. , ’d say\nbiggest sign biggest reason want use normalization\nclear signal ’s reproducible.”273In single-cell RNA sequencing, ’s also need normalization, \ncase procedures bit different. Difference processes \nneeded data tend noisier number \nzero-expression values.274 assays, therefore, new technologies \nnormalization developed. example, scRNA-seq, processes like \nuse unique molecular identifiers (UMIs) can allow later account \namplification bias.275","code":""},{"path":"module12.html","id":"digesting-complexity-in-datasets","chapter":"Module 12 Principles of pre-processing experimental data","heading":"12.2.3 Digesting complexity in datasets","text":"Biomedical research dramatically changed past couple decades \ninclude data higher dimensions: , data either includes many\nsamples many measures per sample, .Examples high-dimensional data biomedical data include data many\nmeasurements (also called features), often hundreds thousands \nterms measurements generated per sample. example, transcriptomics\ndata can include measurements sample expression level tens \nthousands different genes.276 Data metabolics,\nproteomics, “omics” similarly create data high-dimensional scales\nterms number features measured.also cases data large number \nobservations, rather (addition ) number measurements. One\nexample flow cytometry data, observations individual cells.\nCurrent experiments often capture range million cells. Another assay\ngenerates lots observations single cell RNA-sequencing. , \ntechnique, observations taken level cell, \norder least 10,000 cells processed per sample.Whether data large measures many features (e.g., transcriptomics)\nincludes many observations (e.g., single-cell data), sheer size \ndata can require digest somehow can use answer\nscientific questions. several pre-processing techniques can \nused . way digest size complexity depends \nwhether data large many features \nmany observations.data many measurements observation, different measurements\noften strong correlation structures across samples. example, large\ncollection genes may work concert, gene expression across \ngenes may highly correlated. another example, metabolite might break\nmultiple measured metabolite features, making measurements \nfeatures highly correlated. cases, data may even \nmeasurements samples. example, run assay measures \nlevel thousands metabolite features, twenty samples, \nend many measurements (columns dataset, tidy\nstructure) observations (rows tidy data structure).case data many measurements presents, first, technical issue. \ncase data measurements samples, may choice \nresolve later steps analysis. number \nstatistical techniques fail provide meaningless results datasets \ncolumns rows, algorithms run problems\nrelated singularity non-uniqueness.277 Chatfield\nnotes:“potentially dangerous allow number variables exceed \nnumber observations non-uniqueness singularity problems. Put\nsimply, unwary analyst may try estimate parameters \nobservations.”278Another concern data many measurements \namount information across measurements lower number \nmeasurement—words, measures partially fully\nredundant. get basic idea dimension reduction, consider example.\nSay conducted experiment includes two species research mice,\nC57 black 6 BALB/C. record information mouse, including\ncolumns record species mouse color coat .\nSince C57 black 6 mice always black, BALB/C mice always white, \ntwo columns data perfectly correlated. Therefore, one two\ncolumns adds information—one measurements mouse,\ncan perfectly deduce measurement . \ntherefore, without loss information, reduce number \ncolumns data ’ve collected choosing one \ntwo columns keep.idea scales much complex data—many high dimensional\ndatasets, many measurements (e.g., levels metabolite features \nmetabolomics data levels gene expression gene expression data) \nhighly correlated , essentially providing information\nacross different measurements. case, complexity dataset can\noften substantially reduced using something called dimension reduction.Dimension reduction helps collect information captured \ndataset fewer columns, “dimensions”—go, instance, columns\nmeasure expression thousands different genes fewer\ncolumns capture key sources variation across genes. One\nlong-standing approach dimension reduction principal components analysis\n(PCA).279 newer techniques developed, \nwell, t-distributed stochastic neighbor embedding (t-SNE).280 Newer techniques often aim improve limitations \nclassic techniques like PCA conditions current biomedical\ndata—example, may help address problems arise applying\ndimension reduction techniques large datasets.Another approach digest complexity high dimensional data remove\nfeatures measured entirely, approach \ngenerally called feature selection data science. One example \npre-processing single-cell RNA-sequencing data. case, common filter\ngenes whose expression measured. One filtering\ncriterion filter “low quality” genes. might genes low\nabundance average across samples high dropout rates (happens \ntranscript present cell either isn’t captured isn’t amplified\npresent sequencing reads) McCarthy et al.281 Another criterion filtering genes single cell\nRNA-sequencing focus genes vary substantially across different\ncell types, removing “housekeeping” genes similar expression regardless\ncell type.data lots observations, like single-cell data, sheer size\ndata can make difficult explore generate knowledge . \ncase, can often reduce complexity finding way group \nobservations summarizing size characteristics \ngroup.example, flow cytometry leverages different measures taken cell\nmake sense process referred gating. gating, \nmeasure taken cells considered one two time filter data.282 gating process steps many \n“gates”, filtering cells step retaining cells \nmarkers characteristics align certain cell type, \nresearcher satisfied identified cells certain\ntype sample (e.g., helper T cells sample). compresses \ndata counts different cell types, original data one observation\nper cell.Another way clustering techniques, can helpful \nexplore large-scale patterns across many observations. example, single\ncell RNA-sequencing measures messenger RNA expression cell sample\ncan 10,000 cells. One goal single-cell RNA-sequencing \nuse gene expression patterns cell identify distinct cell types \nsample, potentially including cell types known prior \nexperiment.283 , needs used measures \nexpression hundreds genes cell group thousands cells \nsimilar patterns gene expression. One use clustering techniques \ngroup cells cell types, based gene expression profiles, \nsingle-cell RNA-sequencing.284","code":""},{"path":"module12.html","id":"quality-assessment-and-control","chapter":"Module 12 Principles of pre-processing experimental data","heading":"12.2.4 Quality assessment and control","text":"Another common step pre-processing identify resolve quality control\nissues. cases error problem occurred data\nrecording measurement, samples poor quality need \ndiscarded.many reasons biomedical data might quality control\nissues. First, data recorded “hand” (including spreadsheet),\nperson recording data can miss number mis-type number. \nexample, recording weights mice experiment, may\nforget include decimal one recorded value, invert two numbers. \ntypes errors include recording errors (reading value instrument\nincorrectly), typing errors (making mistake entering value \nspreadsheet electronic record), copying errors (introduced \ncopying one record another).285While can hard identify later, many cases can\nidentify fix recording errors exploratory analysis data. \nexample, recorded mouse weights around 25 grams, one recorded\n252 grams, may able identify recorder missed decimal point\nrecorded one weight. case, identify error \nextreme outlier—fact, beyond value make physical sense.quality control issues may come form missing data (e.g., \nforget measure one mouse one time point), larger issues, like quality\nproblem whole sample. cases, important \nidentify missingness data, next step can try \ndetermine certain data points missing (e.g., missing random,\nprocess makes certain data points likely \nmissing, case missingness may bias later analysis), help \ndecide handle missing values.286Some quality control issues specific type data assay.\nexample, one common theme quality control repeats across methods\nmeasure data level single cell. examples type \nsingle-cell resolution measurement include flow cytometry single-cell\nRNA-seq. cases, measurements might made cells \nway problematic. can include cells dead damaged,287 can also include cases measurement\nmeant taken single cell instead taken two \ncells stuck together, piece debris , case \ndroplet-based single cell RNA-seq, empty droplet.Quality control steps can help identify remove problematic\nobservations. example, flow cytometry panels often include marker \ndead cells, can used data gated identify \nexclude cells, size measure (forward scatter) can identify cases\ntwo cells stuck together passed equipment \ntime. single-cell RNA-sequencing, low quality cells may \nidentified based relatively high mitochondrial DNA expression compared \nexpression genes, potentially cell ruptured \nlysed assay, much cytoplasm messenger RNA \nescaped, RNA mitochondria.288 Cells\ncan removed pre-processing scRNA-seq data based related\ncriteria (low number detected genes, small relative library size).289","code":""},{"path":"module13.html","id":"module13","chapter":"Module 13 Selecting software options for pre-processing","heading":"Module 13 Selecting software options for pre-processing","text":"Module 12 described common themes processes pre-processing\nbiomedical data. ’ve covered key processes pre-processing, \nhaven’t talked yet tools can use implement . often\ncombined together pipeline (also called workflow). pipelines can\nbecome fairly long complex need pre-process data \ncomplex.pre-processing pipelines run computer, software tools. \nexception might simple pre-processing tasks—one example \ngenerating average cage weight group mice based total cage\nweight number mice. However, even simple processes like , \ncan done hand, can also done computer, can help\navoid errors provide record calculation used \npre-processing.choice type software use pre-processing.\ntwo key dimensions separate choices—first, whether \nsoftware point--click versus script-based, , second, whether \nsoftware proprietary versus open-source. important note\n, cases, may make sense develop pipeline chains\ntogether different software programs complete required\npre-processing.module, ’ll talk advantages disadvantages \ndifferent types software. reproducibility rigor, many\nadvantages using software script-based open-source data\npre-processing, later modules, ’ll provide information \ncan use type software pre-processing biomedical data. also\nrecognize, however, cases software may \nviable option data pre-processing project.Objectives. module, trainee able :Describe software approaches pre-processing dataCompare advantages disadvantages Graphical User Interface–based\nversus scripted approaches open-source versus proprietary approaches\npre-processing","code":""},{"path":"module13.html","id":"gui-based-software-versus-script-based-software","chapter":"Module 13 Selecting software options for pre-processing","heading":"13.1 GUI-based software versus script-based software","text":"pick software pre-processing, first key dimension consider\nwhether software “point--click” script-based.\nLet’s start definition .Point--click software formally known GUI-based software, \nGUI stand “graphical user interface”. programs hand \nmouse time, use mouse select actions \noptions buttons widgets shown software \nscreen. type software also sometimes called “widget-based”, \nbuilt around widgets like drop-menus slider bars.290A basic example GUI-based software computer’s calendar application\n(“application” common synonym “software”). navigate across dates \ncalendar, use mouse click arrows dates.\nsoftware includes text entry—example, add something \ncalendar, can click textbox enter description \nactivity using keyboard. However, basic way navigate use\nsoftware via computer mouse.Script-based software uses script, rather clickable buttons graphics,\nmain interface. script, case, line--line set \ninstructions describing actions want software perform. \nscript-based software, typically keep keys keyboard often\nmouse. Many script-based software programs also allow \nalso send lines instructions one time area referred \nconsole, return result line run .\nScript-based software also sometimes called software “used\nprogramatically.”291 Several script-based software programs \ncommonly used biomedical data including R, Python, Unix bash scripts,\nwell less common emerging software programs like Julia.comparing point--click software script-based software \npre-processing, advantages point--click software, \nmany script-based software. terms code rigor reproducibility,\nscript-based software comes well ahead, especially used \nfull advantage.Let’s start, though, acknowledging appealing features point--click\nsoftware. features likely contribute wide popularity \nfact vast majority software use day--day life\noutside research probably point--click.First, GUI-based software often easier learn use, least terms \nbasic use. visual icons help navigate choices actions \nsoftware. GUI-based software programs designed take underlying\nprocesses make easier new user access use. \ninterface visual, rather language- script-based.\n, many people familiar point--click software, since \nmany everyday applications type, interface can feel \nfamiliar users. also easier new user pick \ntypically provide much smaller set options full programming language\n.contrast, script-based software can take investment time energy\ninitially learn use. coding languages, \none just —language. built (often large) set \nvocabulary must learn proficient, must learn names \noptions large set functions within language. , rules\nlogic must learn terms options structure access\ndata inputs outputs different functions can chained\ntogether build pipelines pre-processing analysis.Script-based software also requires precise language. Brian\nKernighan writes book D Digital:“computer ultimate sorcerer’s apprentice, able follow instructions\ntirelessly without error, requiring painstaking accuracy \nspecification .”292However, higher investment required learn script-based\nsoftware versus point--click software, also higher payoff \neffort. Script-based software creates full framework \ncombine tools interesting ways build new tools need .\npoint--click software, ’s always layer user \ncomputer logic, constrained use tools \ndesigned person programmed point--click software. \ncontrast, script-based software, direct access \nunderlying computer logic, many popular script-based languages\n(R, Python), extraordinary power flexibility can\nask program .analogy, think traveling country don’t yet speak \nlanguage. choices communicate. \nmemorize key phrases think ’ll need, get phrase book \nlists key phrases. Another choice try learn language,\nincluding learning grammar language, thoughts put\ntogether phrases. Learning language, even basic level, take\nmuch time. However, allow much greater ability express\n. know set phrases, may know ask someone \nbakery loaf bread, person wrote phrase book decided \ninclude , ask hotel extra blanket, wasn’t\nincluded. contrast, ’ve learned language, learned \nform question, can extrapolate express great variety \nthings.GUI-based software can like using phrase book foreign\nlanguage—person developed tool didn’t imagine something \nneed, ’re stuck. Scripted software like learning language—\nlearn rules (grammar) vocabulary (names functions \nparameters), , can combine address wide\nvariety tasks, including things one else yet thought .late 1990s, famous computer scientist named Richard Hamming wrote \nbook called, “Art Science Engineering”, talks lot \nprocess building things role programming can play \nprocess. predicted time 2020, experts \nparticular field programming field, rather experts \ncomputer programming trying build tools fields.293\nnotes:“wanted long run, course, man problem\nactual writing code human interface, often\ndays, person knows problem person \nknows programming language. date unfortunately far \nmuch good immediately, think year 2020 \nfairly universal practice expert field application \nactual program preparation rather experts computers (\nignorant field application) program preparation.”294The rise open-source, scripted programs like Python R rapidly helping \nachieve vision—scientists variety fields now write \nsmall software programs tools, building framework larger\nopen-source languages. Training programs many scientific fields recommend\nrequire least one course programming languages, often\ntaught conjunction data analysis data management.Another element helped make script-based software accessible \ndevelopment programming languages easier learn use. \nearly programming languages required programmer understand lot \ncomputer built organized, including thinking \ndata stored computer’s memory. programming languages \ndeveloped, “low-level” languages remained use, often allow\nunmatched speed processing. However, “higher-level” programming languages\nbecome common, might somewhat slower \ncomputational processing power, much faster humans learn \ncreate tools , abstract away many details make low-level\nprogramming difficult.development easier--learn high-level programming languages\nlike R Python, possible scientist become proficient one \nscript-based programs year. experience, \nfound often one semester dedicated course serious self-study,\nfollowed several months regularly applying software research\ndata, enough scientist become productive using script-based\nsoftware like R Python research. another year regular use,\nscientists can often start making small software extensions \nlanguage. However, 2017 article analyzing single-cell RNA-sequencing\ndata, author noted “relatively biologists comfortable working\nenvironments”, referring Unix R,295 noted\nbarrier using many available tools working \nsingle-cell RNA-sequencing data time.true substantially larger investment training \nshort course workshop, might adequate learning basics \nmany GUI-based software programs. Time can critical barrier, especially \nscientists advanced career may minimal time \ntraining. , ’s barrier analyzing types \nbiomedical data, due extreme size complexity data.296 However, much less time investment \ntakes become expert scientific field. takes years training \nbecome expert cellular biology immunology, example. Richard\nHamming’s vision experts can ask best creative\nquestions data, best remove barrier separate\nperson computer programmer, expert can directly create \nprogram leverage full capabilities computer. Higher-level\nprogramming languages now accessible enough vision playing \nacross scientific fields.Script-based approaches also encourage user learn underlying process\nworks. approach encourages user think like car owner gets\nhood time time like one drives car. \napproach take time learn develop, upside \nuser often much deeper understanding happenening \nstep, well fix adjust different steps fix pipeline \nadapt one pipeline meet another analysis need.Another advantage script-based software—one related idea\nexperts scientific field directly programming—often \ncutting edge algorithms pipelines available first scripted\nlanguages, later added point--click software programs. \nmeans may earlier access new algorithms approaches \ncomfortable coding script-based language.example, article single-cell RNA-sequencing 2017 noted ,\ntime, “, , ‘plug--play’ packages” working\nscRNA-seq data, available, “user-friendly \ndrawback extent ‘black box’, little\ntransparency precise algorithmic details parameters employed.”297 Similarly, another article year noted , \ntime, “scRNA-seq tools exist Unix programs packages \nprogramming language R”, although “ready--use pipelines \ndeveloped.”298Another key advantage script-based software , writing \nscript, thoroughly documenting steps took pre-process \ndata. create code script, script includes \nsteps details process. combination information \nversion software used raw data input pipeline, creates\nfully reproducible record data pre-processing analysis.means able re-steps \nfuture, need , also researchers can explore \nreplicate . may want share process others \nlaboratory group, example, can understand choices made \nsteps took pre-processing data. may also want share \nprocess readers articles publish, may fact \nrequired journal. Well-documented code also makes much easier write\nmethod section later manuscripts leveraged data collected \nexperiment.contrast, write steps took buttons\npressed using GUI-based software, ’s easy forget \nrecord step. GUI-based programs taking steps try \nameliorate , allowing user save download full record records\nsteps taken given pipeline allow user develop full,\nrecorded workflow (one example FlowJo Envoy’s workflow model analyzing\ndata flow cytometry). also movements towards\n“integrative frameworks”, can help improve reproducibility pipelines\nspan different types software (Galaxy, Gene Prof).299When use code script, run forget step detail \nstep. like writing recipe can applied . \nwriting script, encode process single time, can take time\ncheck recheck make sure ’ve encoded process correctly.\nhelps avoiding small errors pre-processing—\npunching numbers calculator , ’s easy mistype number\nforget step every now , code ensure \nprocess run every time faithfully uses numbers saved \ndata step, rather relying person correctly entering \nnumber calculation.Scripts can used across projects, well, can ensure consistency\ncalculation across projects. different people calculation \nlab different projects experiments, \ncalculations hand, might calculation slightly differently,\neven ’s small details like report rounded numbers. \nscript exact thing every time applied. can even share\nscript colleagues labs, want ensure data\npre-processing comparable experiments conducted different research\ngroups, many scientific journals allow supplemental material \ncode used data pre-processing analysis, links within manuscript\nrepository code posted online.also gains efficiency use script. often gain\nfully pays back investment learning software—can make data\npre-processing analysis much efficient long term. small\npre-processing steps, might seem small experiment, certainly\nfirst write script, likely take longer write test\nscript just calculation hand (even ’re\njust starting learn write code scripts). However, since script can\napplied , little extra work apply new data,\n’ll save time future, lot experiments \nprojects, can add . makes particularly useful write scripts\npre-processing tasks find \nlab.","code":""},{"path":"module13.html","id":"open-source-versus-proprietary-software","chapter":"Module 13 Selecting software options for pre-processing","heading":"13.2 Open-source versus proprietary software","text":"selecting software pre-processing, dimension consider \nwhether open-source proprietary. Open-source software software\ncan access, explore, build underlying code \nsoftware. also often free. contrast, code powers\nproprietary software typically kept private, can use product \nexplore way built extend way \ncan open-source software. biomedical research, many script-based languages\nopen-source, many GUI-based programs proprietary. However, \nhard fast rule, examples open-source GUI-based\nsoftware (example, Inkscape program vector graphic design) well\nproprietary script-based software (example, Matlab SAS). \nadvantages disadvantages types software, terms rigor\nreproducibility, open-source software often advantage.Transparency key element reproducibility.300 Gordon\nLithgow coauthors note commentary reproducibility, “Improved\nreproducibility comes pinning methods.”301 \nalgorithms software can investigated, scientists using two\ndifferent programs (example, one program Python one R) can\ndetermine choice program causing differences results. \ncontrast, two research groups use two different types proprietary\nsoftware, algorithms underlie processing often kept secret \ncompared. case, two groups conduct \nexperiment get different results, ’s impossible rule whether \ndifference caused choice software.Gordon Lithgow, Monica Driscoll, Patrick Phillips wrote commentary \nNature describing experiences replicating research. \ndescribe advice give students trying \nexperiment work:“nothing wrong reagents reproducibility still \nissue, like tell students, two options: (1) physical\nconstants universe hence laws physics state flux\nround-bottomed flask, (2) researcher something wrong\neither doesn’t know doesn’t want know . ask \nexplanation think ’m leaning towards.”302If get different results another group, critical \ndetailed description methods group used figure \ngroups getting different results. Open-source software provides \nlevel computational analysis, openness software\nmeans anyone can explore exact details algorithm runs.One key advantage open-source software code open, can\ndig figure exactly step program works. Futher, \nmany cases open-source scientific software, algorithms \nprinciples gone peer review part academic publication\nprocess. proprietary software, hand, details algorithms may\nconsidered protected intellectual property, may hard find \ndetails underlying algorithms work.303 Also,\nalgorithms may gone peer-review, especially \nconsidered private intellectual property.Another advantage open-source software older versions software\noften well-archived easily available reinstall need \nreproduce analysis done using earlier version software.\nAnother advantage open-source software often free. makes \neconomical test , means trainees lab \nproblem continuing use software move new positions.\ncost open-source software, , comes price buy\nsoftware, investment required learn .One facet proprietary software advantage often \ncomprehensive company-based user support open-source software. \ncompanies make sell proprietary software usually user\nsupport team answer questions help develop pipelines may also offer\ntraining programs materials.open-source software also robust user support, although sometimes bit\nless organized common source. cases, developed \nresult large community users help . Message boards like\nStackOverflow provides forum users ask respond questions. \ncompanies also exist provide, business model, user support \nopen-source software. open-source software usually free, \ncompanies make money providing support software.User support sparser smaller software packages developed\nextensions open-source software. example, many packages \npre-processing types biomedical data built small bioinformatics teams\nindividuals academic research institutions. Often software \ndeveloped single person small team one part job\nprofile, limited resources user support providing training.\nextensions build larger, supported open-source software (e.g., R\nPython), extension built maintained small\nteam may capacity respond quickly user questions. Many\nopen-source software developers try create helpful documentation form\nhelp files package vignettes (tutorials use software \ncreated), practical point view difficult small\nopen-source developers provide level user support large\nproprietary software company can.often case cutting-edge open-source software biomedical\npre-processing. just-developed software packages less likely \ncomprehensively documented longer-established software. , can\ntake community software users develop software \navailable, limitation new software open-source\nproprietary languages, can represent problem open-source\nsoftware, typically company-based helpline \ncommunity users often represents one main sources help \ntroubleshooting.","code":""},{"path":"module14.html","id":"module14","chapter":"Module 14 Introduction to scripted data pre-processing in R","heading":"Module 14 Introduction to scripted data pre-processing in R","text":"Learning code can seem daunting, ’s difficult \nlearning new language. Many people variety disciplines \nlearned code help research. can pay big dividends \nterms reproducibility efficiency.module, ’ll provide tips make easier get started.\nnew coding, can give framework tackle \ncan seem daunting task learning code, well help see \napproachable techniques.module meant researchers yet used code scripts either\ninterested starting supervising researchers working\ncode biomedical analysis. aim module provide enough\ninformation someone without coding experience can gain comfort \nnavigating R code scripts, example help understand paper includes\nscripts part supplemental materials help understand work \ntrainee incorporating code research. researchers \nalready using code scripts, recommend next module (module 15), provides\nadvice steps can improve reproducibility writing scripts \nbiomedical data pre-processing.module, provide introduction scripted pre-processing \nexperimental data R scripts. introduce basic elements \nR code script well basics creating running script. end\nmodule, video exercise, demonstrate create,\nsave, run R code script simple data pre-processing task.Objectives. module, trainee able :Describe R code script differs interactive\ncoding RExplain code scripts can increase reproducibility data pre-processingCreate save R script perform simple data pre-processing taskRun R scriptWork example R script using video exerciseDefine “code script”, “assignment operator”, “function”, “function call”,\n“package”, “batch execution”, “keyboard shortcut”","code":""},{"path":"module14.html","id":"what-is-a-code-script","chapter":"Module 14 Introduction to scripted data pre-processing in R","heading":"14.1 What is a code script?","text":"simplest method working R something called interactive\ncoding. style coding, enter single command function call\ncursor console, tell program execute one element \ncode (example, pressing Return key), wait executes\nenter next command function call.script, hand, longer document gives steps \nprocess. can think code script like script play—’s\nrecord everything happens course event. play,\nscript records dialogue stage directions play, \ndata pre-processing task, records steps inputting data\npre-processing steps finally saving data processed form \nanalysis, visualization, statistical testing.can run code whether ’re using script typing \ncommands one time console interactive coding. However, \ncode interactively console, ’re making record \nsteps (note, ways save history commands typed \nconsole, can messy reproduce later, consider\ncommands typed console recorded purposes \nreproducibility). write code script, hand, \nrecord can later reopen see repeat \nsteps. broad way, can visualize process walking wet\nsand—making record (footsteps) path took \nmaking path.code script typically written plain text document, can create,\nedit, save code scripts interactive development environment (like\nRStudio, programming R). program (R example) can read\nrun script “batch” time. words, can walk\nexecute piece code recorded script, rather\nneeding enter line code one time console. \nmany programming languages, can also run code script smaller\nsections, executing just one lines time explore ’s\nhappening line code. combination functionality, \nwell recording code future reference reproduction, code scripts\nprovide excellent method building using pipelines code \npre-process biomedical data.later sections module, ’ll walk practical steps \nwriting one code scripts. video exercise end, ’ll look \nexample script simple task biomedical data pre-processing,\ncalculating rate growth bacteria different growing conditions.\nexercise, ’ll walk open, run, explore \nscript RStudio.","code":""},{"path":"module14.html","id":"how-code-scripts-improve-reproducibility-of-pre-processing","chapter":"Module 14 Introduction to scripted data pre-processing in R","heading":"14.2 How code scripts improve reproducibility of pre-processing","text":"introduction book, provided definition computational\nreproducibility. Specifically, computational reproducibility means\nanother researcher get exact results original study \noriginal data collected study.304 Computational\nreproducibility, , requires two main things: original data \nthorough instructions describe data processed analyzed.305Neither elements trivial provide thorough way complex\nbiomedical experiment. Raw datasets often extremely large complex. \nprovide thorough instructions processing analysis requires “access \n… source code binaries exact versions software used carry \ninitial analysis (includes helper scripts used convert\nformats, groom data, ) knowing parameter settings exactly \nused.”306By using code script data pre-processing (data analysis \nvisualization), can often substantially improve computational\nreproducibility experiment. code script \ndocuments exact precise instructions data processed \nanalyzed. example, R script include instructions \ndata loaded file, even include file name data\nsaved, must reference input data. , provides \nlist function calls run order \nrun. function call, provides details parameter settings\nused function. Since R open-source language, packages \nlargely open-source well, know version R package used\nscript, can find read underlying code \ndefines functions used script. words, open-source\nnature code means can, want, dig algorithms\nunderlying step process, consider step\nscript “black box”.course writing executable script pre-process data, , \nthoroughly documenting step take process, creating one \nkey components (clear instructions data processed \nanalyzed) necessary make experiment computationally reproducible.\nscript, two elements required\nmake experiment fully computationally reproducible: first, original,\nraw data, second, information versions software used \ncode (include version R used, well versions\nR packages used supplement base R functions).","code":""},{"path":"module14.html","id":"how-to-write-an-r-code-script","chapter":"Module 14 Introduction to scripted data pre-processing in R","heading":"14.3 How to write an R code script","text":"section, ’ll go basics help get started writing \ncode script R. process writing code script similar many \ninterpreted languages, like Python Julia. familiar writing\ncode scripts R, may find module 15—provide tips \nimproving reproducibility writing scripts—helpful.’ll start basic conventions R programming language.\nnever used R , critical understand basic\npieces understand R code script put together\nrun. Specifically, ’ll cover:R object?R functions function calls?R library?R script?later modules, ’ll go detail helpful tools R,\nincluding suite “tidyverse” tools now taught beginner R\nprogramming courses. , course, room provide full course\nprogram R, aiming give enough overview \ncan understand R programming can fit data pre-processing analysis\npipeline laboratory-based biomedical research projects, well \ncan navigate R script someone else written. module 16, ’ll\nprovide directions resources like continue developing\nexpertise R programming beyond basics covered modules.","code":""},{"path":"module14.html","id":"what-is-an-r-object","chapter":"Module 14 Introduction to scripted data pre-processing in R","heading":"14.3.1 What is an R object?","text":"First, ’ll need understand R keeps data ’re working \n. work R, piece data work available\nsomething called object. simplest way think R object \nsimply container data. Different objects can structured different\nways, terms arrange data—implications \naccess data object—regardless structure, \nR objects share purpose storing data way ’s available \nwork R.One first steps R scripts, therefore, create \nobjects. data available, ’s much interesting\nstuff can R. want work data stored \nfile—example, data recorded laboratory saved \nExcel file—can create R object data reading \ndata using specific R function (’ll cover minute). read\ndata R store object can access later.keep track objects R session, typically assign\nobject name. time want use data object, work\nobject way, can refer name, rather \nneeding repeat code used initially create . can assign\nobject name using special function R called gets arrow \nassignment operator. ’s arrow made less hyphen keys, \nspaces two (<-). ’ll put name want give object\nleft arrow code create object (example, read\ndata file) right. Therefore, beginning R script\noften one lines code look like :example, line code reading data Excel file named\n“my_recorded_data.xlsx” storing R object assigned name\nmy_data. want work data later code pipeline, \ncan name my_data, now stores data file.addition creating objects data initially read , \nlikely create intermediate objects along way. example, take\ninitial data filter subset, might assign version\ndata separate object name, can work version later \ncode. Alternatively, cases ’ll just overwrite original object\nnew version, using object name (example, creating subset \nmy_data object assigning name my_data). reassigns \nobject name—refer my_data point , contain \nsubsetted version. However, cases can useful helps keep\ncollection R objects session bit smaller simpler. ’s\n, can make changes simplify version data ’re working\nR without worrying changing raw data. read data\noutside file, like Excel file, R work copy data, \noriginal data. can make many changes want data object R\nwithout changing anything raw data.","code":"\nmy_data <- read_excel(\"my_recorded_data.xlsx\")"},{"path":"module14.html","id":"what-are-r-functions-and-an-r-function-calls","chapter":"Module 14 Introduction to scripted data pre-processing in R","heading":"14.3.2 What are R functions and an R function calls?","text":"next key component R programming language idea R functions\nR function calls. parts R things (whereas objects R\n“things” functions operate ). R function tool can\ntake one R objects inputs, something based inputs, return \nnew R object output. Occasionally ’ll also “side effects” beyond returning\nR object—example, functions make plot show plotting\nwindow RStudio.R objects input can ones ’ve assigned name (\nexample, my_data). can also simple objects make fly,\njust input function. example, ’re reading data\nfile, one R object inputs ’ll need give function \npath file, either save object (e.g.,\nmy_data_filepath <- \"my_recorded_data.xlsx\" reference\nmy_data_filepath call function) create object fly\ncall function (e.g., just put \"my_recorded_data.xlsx\" directly \nfunction call, shown example ).function tool, encapsulates code something \ninput objects. use tool, ’s called calling function. Therefore,\nlines code script give function calls, \nasking R run specific function (, cases, linked set functions)\nbased specified inputs.example, following function call read data Excel file\n“my_recorded_data.xlsx”:line code calling function read_excel, tool inputting\ndata Excel file R object specific data structure. running\nline code, either console R script, asking R input\ndata file named “my_recorded_data.xlsx”, R object ’re\ngiving input function. particular call read data —\nwon’t assign resulting object name, instead just print data\nR console.’d like read data save object use later, ’ll\nwant add another function call, assign output object\nname. , ’ll use gets arrow described earlier. \nspecial type function R. R functions consist function’s name,\nfollowed parentheses inside put objects input \nfunction (e.g., read_excel(\"my_recorded_dat.xlsx\"). gets arrow \ndifferent type function called operator. functions go two\nobjects, input operator function. ’re used often\narithmetic (example, + operator adds values objects\n, can call 1 + 2 add one two). \ngets arrow, go name want assign object\n(e.g., my_data) function call creates object (e.g.,\nread_excel(\"my_recorded_data.xlsx\")):case, line R execute include two functions, \noutput one gets linked straight second, result \noutput second function (data Excel file stored \nobject assigned name my_data).write R script, use function calls work \nsteps pipeline. can use different function calls things like\napply transformation, average values across groups, reduce dimensions \nhigh-dimensional dataset. ’ve pre-processed data, can also use\nfunction calls run statistical tests data visualize results\nfigures tables.process writing script normally iterative—’ll write \ncode first steps (e.g., read data), look ’ve\ngot, plan next steps, try write code steps, run \ncheck output, . process similar drafting \npaper. can try things early steps—steps won’t work \nfirst, turn don’t need . continue, ’ll\nrefine script, editing essential steps making sure \nfunction call within steps operating intend. can \nintimidating start blank file develop code—just like \nblank piece paper writing manuscript—just like writing,\ncan start something rough iterate arrive \nversion want.process might seem bit overwhelming first learn , \nsuffices point understand , R code, ’ll working \nobjects (materials) functions (tools). look R\nscripts video exercise module, ’ll see two pieces—objects \nfunctions—used scripts. building blocks\nR scripts.","code":"\nread_excel(\"my_recorded_data.xlsx\")\nmy_data <- read_excel(\"my_recorded_data.xlsx\")"},{"path":"module14.html","id":"what-is-an-r-library","chapter":"Module 14 Introduction to scripted data pre-processing in R","heading":"14.3.3 What is an R library?","text":"’s one last component R helpful understand move \nrest module next modules. ’s idea R package, \nfortunately, ’s pretty straightforward one.just talked functions R tools, can use interesting\nthings data (including pre-processing steps talked \nmodule 12). However, version R initially install computer\n(available free major operating systems https://cran.r-hub.io/) doesn’t\ninclude tools likely want use. initial download gives \nbase programming language, called base R, well \nextensions common tasks, like fitting common statistical models.R open-source software, people use R can build \nsimple base. R users can create new functions combine rudimentary\ntools base R create customized tools suited tasks. R users\ncan create tools personal use, often , \nalso mechanism share new tools others ’d like.\ncan bundle set R functions ’ve created R package \npost package public repository others can download \nuse functions . examples modules, ’ll \nusing tools packages, ’s rare someone uses R without using\nleast supplementary packages, ’s good get idea \nget use .people make packages can share number repositories, \nstandard repository sharing R packages widely Comprehensive R\nArchive Network (CRAN). package shared CRAN, can get \nusing function install.packages along package’s name. \nexample, code showed earlier, read_excel function come\nbase R, instead part package called readxl, shared\nCRAN. download package can use functions, can\nrun:download code package unpack special part \ncomputer R can easily find . need install package\n, least get new computer update version base R.\nHowever, use functions package, ’ll need load package\ncurrent R session. makes functions package available \nwork R session. , use library function,\nalong name package. example, load readxl package \nR session, ’d need run:need install package , need load every\ntime open new R session work, want use functions \nR session. Therefore, ’ll often see lot calls library\nfunction R scripts. can use call anywhere script long \nput code use library’s functions, ’s great \nget habit putting library function calls start \nR script. way, share script someone else, can\nquickly check see ’ll need install new packages can\nrun code script.","code":"\ninstall.packages(\"readxl\")\nlibrary(\"readxl\")"},{"path":"module14.html","id":"what-is-an-r-script","chapter":"Module 14 Introduction to scripted data pre-processing in R","heading":"14.3.4 What is an R script","text":"Based points ’ve just discussed, hopefully can envision now\nR script ultimately include number lines code, covering \nnumber R function calls work data stored objects. can expect\nlots calls assign objects names (<-), \nfunction calls typically include function called name \nobjects input function, contained inside parentheses \nfunction name.type script written plain text, best way \ncreate R script using text editor. computer likely came \ntext editor one pieces utility software installed \ndefault. However, R scripts, can easier use text editor \ncomes part RStudio. allows open edit scripts \nnice environment, one includes console area can test pieces\ncode, pane viewing figures, .RStudio, can create new R script going “File” menu top\nscreen, choosing “New File” choosing “R Script”. open\nnew plain text file , default, file extension “.R” (e.g.,\n“my_file.R”), standard file extension R scripts. ’ve\ncreated R script file, can begin writing script. next\nsection, ’ll walk can run code ’ve put \nscript. However, think ’s worth mentioning , get started \nprocess, might find easiest start writing R script\nscratch, instead starting someone else’s walking \n. can explore works (reverse engineer ). can try\nchanging small parts, see acts expect . process\nhelp get feel scripts organized \noperate. video exercise module, ’ll provide R script \nbasic laboratory data pre-processing task walk , can use\nstarting point understand work create, edit, \nrun R script.","code":""},{"path":"module14.html","id":"how-to-run-code-in-an-r-script","chapter":"Module 14 Introduction to scripted data pre-processing in R","heading":"14.4 How to run code in an R script","text":"’ve written code R script, can run (execute) code \nnumber ways. First, can run code script , \nknown batch execution. , code script \nexecuted R, ’s executed R one line time, won’t \nchance make changes along way. compare idea \ncode script play script, can think like \nplay performed audience—start play, don’t \nchance stop work ’s going. Instead, go straight\nend. error somewhere along way, code\nstop running point ’ll get error message, otherwise\nrun code batch, R won’t stop executing lines gets\nend. mode running code great ’ve developed \npipeline ’re happy —quickly runs everything provides \noutput.way can execute code running single line, \nsmall set lines, code time. play analogy, similar\nmight happen rehearsals, go part play\nscript stop get comments director, either re-try \npart changes move next small part. mode running\ncode great ’re developing pipeline. Just like \nplay’s rehearsals, ’ll want lot chances explore change things \ndevelop final product, mode running code excellent \nexploration editing. Often, time code spent\nstyle code execution. Running batch mode get lot work\ndone, quick programmer—developing code takes\ntime, just like writing manuscript, time comes drafting \nrough draft editing arrive clean clear final\nversion.methods code execution easy RStudio. Since ’ll\nusually start using line--line execution, ’ll start talking \ncan . RStudio, can open code script (file ending “.R”),\nstill able see console, space submitting\nfunction calls R. execute code script one line time,\n’s quick ways can tell RStudio send line \nscript console run . Start putting cursor line code.\nOne way now execute line (.e., send console run) \nclick “Run” button top right-hand corner script file. \ntry , see line code gets sent console\npane RStudio, results running line shown \nconsole.Even quicker keyboard shortcut thing. (Keyboard\nshortcuts short control sequences type keyboard run \ncommand. ’re faster clicking buttons can without\ntaking hands keyboard. Ctrl-C one common one might\nused , programs copy current selection.) \nrunning line R code, cursor line function call \nwant execute, use keyboard shortcut Ctrl-Return (depending \noperating system, may need use Command rather Ctrl).can use similar method run lines code . \nhighlight code want run, can use either \ntwo methods (click “Run” button use Ctrl-Return keyboard shortcut).\nshow examples video exercise end\nmodule.execute R script batch mode, ways can \n. First, “Source” button top right R script file \nopen RStudio. can click button run entire\nscript batch. also R command can use source file\nbased file name, source. file working directory\nnamed “my_pipeline.R”, example, can execute code batch \nrunning source(\"my_pipeline.R).get started, ’s probably easiest just use buttons “Run” “Source”\nRStudio provides window R script file. work, \nmay find methods help work faster, allow \ninteresting things, ’s good know ’re , don’t need\ntry navigate learn run code R script.","code":""},{"path":"module15.html","id":"module15","chapter":"Module 15 Tips for improving reproducibility when writing R scripts","heading":"Module 15 Tips for improving reproducibility when writing R scripts","text":"biomedical researchers already worked quite bit programming\nlanguage like R, either role primarily computational, \nway understand data ’ve collected wet lab. \nmodule 14 focused scientists new coding, help give \nentry point write run code script R, module\nfocuses different audience—scientists familiar coding \nlike take steps improve practice.worked number scientists situation. module\nprovides series tips can improve coding practice \nmake rigorous reproducible. tips based \nexperiences things —real regular practice—get \nway code rigorous reproducible.’ll provide advice three main areas:Write code computers, edit humansModify rather start scratchDo repeat yourselfThis module meant researchers using R already part \nresearch. meant complement alternative module 14,\nfocused readers new creating code scripts.Objectives. module, trainee able :Improve reproducibility scripts leveraging tips \nresearchers already codersImplement advice editing code make clear humansPractice editing code include better names data objects columnsPractice breaking monolithic code code structured \norganizedList examples dead-end codePractice editing scripts remove dead-end codeDefine “package vignette”List steps adapt example code make reproducible easier\nmaintain pipelineExplain can improve rigor reproducibility code \navoid repetition","code":""},{"path":"module15.html","id":"write-code-for-computers-but-edit-it-for-humans","chapter":"Module 15 Tips for improving reproducibility when writing R scripts","heading":"15.1 Write code for computers, but edit it for humans","text":"key requirement project computationally reproducible \ncode used pre-processing analysis available. However, even code\nproject available, can hard understand reproduce \nanalysis. One common culprit code unclear. One way improve \nreproducibility code, therefore, make sure edit \n’s clear humans, just computers.World War World War II, British US used special type \ncamouflage ships called “dazzle camouflage”. type \ncamouflage uses large geometric shapes, often black white, makes\nships look bit like zebras. Unlike types camouflage, type\ndoesn’t conceal ship—’s still clear ’s . However, \nable hit ship sea, people needed know , also\ngoing. ship moving: time \nballistic fired lands changed location. People\nneeded calibrate aim ship time ballistic got\n. Dazzle camouflage makes much harder determine ship \nheaded.Often, people write code research projects looks like ’s using\ndazzle camouflage. ’s easy see ’s something look\ncode script, ’s hard figure ’s \n’s trying go. words, ’s hard human quickly digest. \ntype code hard others figure , also hard \nfigure come back code future.best way avoid type code get practice editing\ncode. first write code, don’t want write slowly \ncarefully—rather, ’ll usually best figuring get something\nwork get flow get code without worrying \nlegible humans.fine, get habit thinking just first step:\ninitial coding, ’re getting code work computer, \nlater need go back clean code ’s clear humans,\n. Editing code make easier understand (others \n) also make code easier maintain extend future.idea similar writing. Many writing experts recommend break\nwriting process several stages. First, write drafting\nprocess, get ideas paper without editing much. \nstage getting ideas . separate stage, edit, \nstage audience clearly mind, editing make writing clear\n. separating stages, can use mind \ncreative, less constrained way create ideas, critical\nway refine ideas audience.practice familiar many writers, ’s less well known \nscientists also coders. don’t already, try incorporating editing\nstages develop code. helpful take time edit code \n’re still within day two writing , ’s helpful work \nediting stage fairly frequently. Since often requires less energy brain\npower initial stage (getting code work computer), \ncan helpful incorporate editing time times day energy\notherwise low. example, taking ten fifteen minutes edit existing\ncode can good way start coding day, get \nheavier lifting writing new code.edit code, specific things can make\nclearer humans read. editing steps cover \nmodule :Improve names ’re using within codeBreak monolithic codeAdd useful commentsRemove dead-end codeLet’s take closer look can steps.","code":""},{"path":"module15.html","id":"improve-names-within-the-code","chapter":"Module 15 Tips for improving reproducibility when writing R scripts","heading":"15.1.1 Improve names within the code","text":"’re initially coding, might often use “placeholder” types names\ndata objects. example, coder might tend name objects “df” (\n“dataframe”) “ex” (“example”) ’re first getting code work.’s problem using types generic names initially\ndevelop code. fact, ’s rich history placeholder\nobject names. even fancy name, metasyntactic variables.\nDifferent coding languages developed different ones popular,\ncoders different countries. example, many C programmers \nname things “foo” “bar” initially work code, \nItalians often use Italian words different Disney characters\n(“pippo”, “pluto”, “paperino”).problem isn’t using placeholder names; problem \ndon’t later edit code use better names. generic names\ntell nothing ’s stored object go back\nread code later. better names object, can read \ncode ways document , without even needing \nread code comments figure ’s going .style guide focused tidyverse approach available\nhttps://style.tidyverse.org/syntax.html. includes guidance \nselect good names objects R within section “Syntax”. Generally,\ngood principles include name object give \nidea ’s contained object. example, dataframe\nweights mice experiment, ’s much better name\n“mouse_weights” rather something generic like “foo”. \nguidance help make life easier coder, including things\nlike using lowercase letters.Similar principles apply column names: ideally, want\nnames describe contain. also rules \nmake easier work column names. example, column names can\ninclude spaces, , makes using within R harder. time\nrefer column name, surround name backticks R\nprocess full name single name, rather thinking name ends\nfirst space. becomes pain write lot code refers\ncolumn. ’s also helpful keep column names fairly short, can\nsee full name work dataset resulting output.comes column names, editing might improve names\nquickly wrote coded. However, common reason \nungainly column names ’ve read data file format like Excel,\neasy person entered data include spaces \nspecial characters column names. case, tools \nR can help quickly improve column names. particular, \njanitor package function called clean_names lot \nwork , including converting name lowercase, removing special\ncharacters (like “*” “&”), replacing spaces underscores. \nneed make targeted changes column names, can using \nrename function dplyr package.","code":""},{"path":"module15.html","id":"break-up-monolithic-code","chapter":"Module 15 Tips for improving reproducibility when writing R scripts","heading":"15.1.2 Break up monolithic code","text":"Next, can edit break “monolithic” code—, code isn’t\nclearly divided show sections steps process. ’re first\ncreating code, won’t want take time nicely organize \nlogical sections. However, ready edit code, find\nbreaking clear sections labeling help others\nnavigate code higher level (understanding big picture \nworks looking major steps takes), diving details \nsection big picture clear., process mimics process used many writers. ’s common \ncreate drafts notes lack clear organization, instead just\ncollecting raw material shaped final article book.\nHowever, raw material needs organized edited make \nsomething others can navigate make sense .similar way, ’ve gotten code work, make sure \nclear picture whole process tackles problem \n“big picture” level. One big steps might something like reading \ncleaning data, another step might identifying addressing\noutliers data. ’ve identified big steps, try much \npossible group code big steps, can use code comments\nblank lines code separate sections label \ndescribe ’re ., might find move code around script.\nfine long doesn’t affect computer able process \nscript. example, one big steps might loading packages ’ll\nneed. Rather lot library calls sprinkled throughout \ncode, can group together start script section\ncalled something like “Loading packages”. clean parts \nscript, library calls start code script,\nanother person immediately see packages ’ll need \ninstalled run code.Another way can break monolithic code split lines.\nR process code whether ’s one line split separate\nlines: R just keeps reading gets end function call\neither way. means can use “Return” key break code\nlines ’re always able see full line code without scrolling.One common standard keep lines code 80 characters \nfewer. RStudio functionality reformat code meet \nstandard. RStudio menus, go “Code” menu, can select\n“Reformat Code”. can help clean long lines code editing\nprocess.","code":""},{"path":"module15.html","id":"add-useful-comments","chapter":"Module 15 Tips for improving reproducibility when writing R scripts","heading":"15.1.3 Add useful comments","text":"breaking monolithic code, helpful add code comments \ncertain things. R, can add code comment #;\nR won’t read anything comes symbol line. can use \nadd small messages humans describe code.add comments, keep mind ’s often useful describe\n’re something rather ’re . lot R\ncode—especially tidyverse approach—functions names \nclearly describe . example, function rename column \ncalled rename, function select certain columns called\nselect. Therefore, code fairly good job self-documenting\nterms describing ’s .Instead, can use code comments remind others \n’re implementing certain steps. example, rather code\ncomment says “Rename columns”, say, “columns come\nExcel file generated cytometer include lot special\ncharacters, need remove make easier work data \nR.” explaining ’re something, ’ll also help ’s\ntime maintain extend code. ’ll able tell, example,\nwhether changing deleting certain line code cause big problem\nareas code.","code":""},{"path":"module15.html","id":"remove-dead-end-code","chapter":"Module 15 Tips for improving reproducibility when writing R scripts","heading":"15.1.4 Remove dead-end code","text":"Another useful step edit code edit pieces ’ll call\n“dead-end code”. pieces code aren’t contributing \nprocess script.two main types dead-end code often see. First, ’s\ncode use interactive coding process check things. \nexample, might use View function take look data frame \ncertain step process, use functions like summary str \nexplore ’s different objects.’s great kind exploration code; fact, one \nadvantages interactive software like R can explore \ndevelop scripts. However, tools help develop script,\nones necessary final script run. Instead, just\ngunk code ’s real work.two things can regarding type dead-end code. first\ncan get habit running console, rather \nscript, even ’re developing code. However, \nrequire switching console script write code,\ncan interrupt flow. alternative run script \nwrite code, delete exploratory calls \nedit script.’s also second type dead-end code. code wrote try\nsolve particular problem, ultimately didn’t work (\nreplaced better approach). Often, may worked long time\npiece code, might contain really clever approach \n’re proud . However, leaving script, isn’t contributing \nultimate process ended , get way understanding\nprimary code. lead reader rabbit hole, rather allowing\nmove step step logic.writing, similarly areas aren’t contributing forward\nmovement piece authors reluctant remove love\none reason another. resulted famous advice \nauthors (Stephen King, among others) “murder darlings”. \nwords, brave enough edit anything isn’t contributing \nnecessary progress piece. Coders take advice similar\nway comes pieces code scripts don’t ultimately\ncontribute pipeline ’ve developed.","code":""},{"path":"module15.html","id":"modify-rather-than-start-from-scratch","chapter":"Module 15 Tips for improving reproducibility when writing R scripts","heading":"15.2 Modify rather than start from scratch","text":"get started solving problem? Science engineering long\ntraditions starting modifying something exists, rather starting\nscratch. example, clear important penicillin \nhuman health, time hard work producing scale. Guru\nMadhavan National Academy Engineering tells story book\nApplied Minds: Engineers Think, focusing critical role adapting\nexisting technology get foothold problem:“Extracting penicillin mold child’s play… Instead \ndesigning building reactor chemical reactions scratch—\nmeant time, money, uncertainty—[Margaret] Hutchinson opted \nsomething already functional. researchers found mold \ncantaloupe effective source penicillin, started .\nteam revised fermentation process Pfizer using produce\nfood additives like citric acid gluconic acid sugars, help \nmicrobes. Hutchinson swiftly helped convert run-Brooklyn ice factory \nproduction facility. deep-tank fermentation process produced great\nquantities mold mixing sugar, salt, milk, minerals, fodder \nchemical separation process Hutchinson knew well refinery\nbusiness.”307Similarly, code, keep mind shouldn’t reinvent wheel.\nInstead, ’s often useful start existing script, pipeline, piece\ncode.find starting scripts learn , tactics can\ntry. First, check around colleagues see R code data\npre-processing tasks lab. work similar types\ndata, use R, ’re likely come scripts \nachieve tasks also need .Another excellent source example R code vignettes examples \ncome many R packages. using functions R package,\nlikely vignette comes package, may also \nexamples within helpfiles package’s functions. package\nvignette tutorial walks major functionality \npackage, showing use key functions package extended\nexample. packages multiple vignettes, showing range things\ncan package.find vignette package ’re using, can\ngoogle package name “vignette”. can also find console\nR using function vignette. example, find package\nreadxl, helps read data Excel files, vignettes, can\nrun vignette(package = \"readxl\"). tell package two,\none called “cell--column-types” one called “sheet-geometry”. open one\n, can use vignette function. example,\nvignette(\"cell--column-types\", package = \"readxl\") open first \ntwo vignettes within R session.open helpfile function R, console type question mark\nfunction name. example, ?read_excel open helpfile\nread_excel function (need make sure ’ve run\nlibrary(\"readxl\") load package function). helpfile\nprovides useful information running function, one useful\nparts “Examples” section. Scroll bottom helpfile \nfind section. includes several examples can copy R\nscript console try , figure types inputs \nfunction needs different options function modify .Online resources like StackOverflow also provide advice example code \nmany challenges might come ’re coding. Google can also \nused help solve coding problems, especially become familiar \nspecial operators, can help refine search. can\nfind Google special operators \nhttps://support.google.com/websearch/answer/2466433?hl=en.’s problem using starting points develop\npipeline. However, ’s often tempting coder leave\nexample code “-” ’ve found example solution works. Instead,\n’s critical make sure fully understand line code \nscript works way . , ’re adapting example code \nproblem, edit possible use set tools ’re\nfamiliar . section, ’ll go steps \ntake start example code adapt pipeline way\nrigorous reproducible.find piece example code think help something\nneed code, ’ll first want make sure can get\nwork example data came , try \ndata. won’t run data, trouble-shooting\nsteps can take. First, make sure required packages\ninstalled loaded. Second, make sure ’ve saved example data \nright place computer example code reads data file.\nFinally, make sure versions packages R \nused example. code still doesn’t work ’ve resolved \nissues, may want move finding example code.’ve gotten code run example data, walk line\nline understand . step, make sure understand\ninput looks like output looks like. code nested\n(function calls placed within function calls), sure understand\ncode level nesting. code uses piping move output\none call input next, make sure ’ve worked \nlines pipe individually.two tools can help dissect code way. First,\nwork R study, can highlight code script use \n“Run” button run highlighted code. functionality allows \nrun nested function call without running whole line code, run\npart series piped calls (highlighting everything \npiping symbol line running ). tool ’s useful \nfunction dplyr package called pull. function allows \nextract column dataframe vector. helpful ’re\ndissecting nested calls piped code, often function operate \nsingle column dataframe. function allows pull column\ntest function call see ’s column.figure example code data comes , \ncan adapt work data. , pay close attention \ndata similar different example data. stage, \ngoal get example code work data.Many researchers stop step—’ve gotten example code work \ndata (hopefully worked understand ). However,\nexample code often follows different style code write .\nexample, may use tidyverse approach, example might use\ncode written base R style. , different coders think \ntackle problems different ways, can lead case \nexample code ’ve adapted script feels different usual\ncode.can result spots code later worried \nchange, works, don’t understand well enough feel\ncomfortable making change. can make code fragile hard \nmaintain. Instead, take moment adapt logic ’ve learned \nexample set tools. example, example code \nwritten base R prefer tidyverse tools, rewrite code’s logic\nuse tidyverse tools.gain several advantages adapt code use tools ’re familiar\n. example, bugs less likely, bugs, ’ll\ncatch quickly, since ’re familiar tools code \nusing. code also much easier understand maintain \nfuture ’s written using tools know well.","code":""},{"path":"module15.html","id":"do-not-repeat-yourself","chapter":"Module 15 Tips for improving reproducibility when writing R scripts","heading":"15.3 Do not repeat yourself","text":"become familiar programming R, can start evolve \nstyle writing scripts advanced ways. key one learn limit\noften repeat code. write data pre-processing pipelines, ’ll\nfind often need thing, variations thing, \n. example, may need read clean several files \ntype structure. likely, first least, find copying \npasting code several parts script, minor changes \ncode (e.g., changing R object input time).Don’t worry much start learn write R scripts. \nnormal part drafting process. However, get better using R, ’ll\nwant learn techniques can help avoid repetition.reasons ’ll want avoid repetition code \npossible. First, repeated copies similar code make\ncode script much longer harder figure later. Second, hard\nkeep copies code sync . example, \nseveral copies code use check outliers data, \ndecide want change , ’ll need find every copy\npiece code script make sure make change \nplace. Instead, less repetition code, can make\nchange single place ensure change place\neverywhere process.tools useful develop help avoid repetition. \nfirst learn write R functions. R user can write new\nfunction. can collect packages plan share others, \ncan also just write personal use. create function, \nencapsulates code something need , allows \nthing anywhere else code just calling function, rather\ncopying lines original code. excellent way \nwrite code one place need use often, rather copying \npasting code throughout R script.Since need run code defines function use , \noften makes sense write code creates functions near top \ncode script. find ’ve written lot functions, \n’ve written functions ’d like use one data\npre-processing scripts, can even save code creates functions \nseparate R script just source separate script top \nscript uses function, using source call. “Sourcing” file \nway simply runs code file. Eventually, \neven think creating package functions.one excellent set tool avoiding repetition want \nmention. , likely complex ’ll want start \nlearn write R scripts, comfortable \nbasics, ’s powerful tool creating code scripts short \nsimple possible powerful things. set tools \nfocus iteration. include loops, allow step \nelements data structure apply code . also include\nset tools purrr library allow apply code,\nfunction, element larger data structure. \nexcellent tools something like reading lot similar\nfiles combining single R object pre-processing.go details write R functions iteration\ntools modules, aim get started give \noverview might want go next. want learn write\nR functions, ’s chapter describing process free\nonline book “R Data Science” guidance topic\n(https://r4ds..co.nz/functions.html).308 ’d like learn\ntools iteration, book also chapter \n(https://r4ds..co.nz/iteration.html).","code":""},{"path":"module16.html","id":"module16","chapter":"Module 16 Simplify scripted pre-processing through R’s “tidyverse” tools","heading":"Module 16 Simplify scripted pre-processing through R’s “tidyverse” tools","text":"learn code, , good strategy start collecting “tools” \ntoolbox R—functions learned use well \nunderstand thoroughly. make proficient R quickly, \nalso limit chance bugs errors code, making data\nwork robust rigorous. ask good programmers, find\nlarge amount code relies fairly small set general-use\ntools, specialized tools used , specific\nalgorithm necessary. first start , though, hard know\ntools important add early learn well. section,\n’ll cover tools found helpful pre-processing biological\ndata. exhaustive, may help identify sets tools\nfocus learning well data pre-processing analysis biological\ndata.key tools pre-processing laboratory data :Tools data inputTools changing columns creating new columnsTools working character stringsTools working dates timesTools statistical modelingWe concentrate tools drawn collection tools called \n“tidyverse”. “tidyverse” approach approach using R grown\nenormously popularity recent years. R courses workshops \nbeginning programmers now structured around approach. provides \npowerful yet flexible approach working data R, one \neasier ways start learning R. module 3 described\ntidyverse approach conjunction talking power tidy\ndata format. module, ’ll go deeper specific tools \napproach can used common data pre-processing tasks working \nbiomedical data, well provide information resources can \nused continue learning approach.tidyverse functions come base R, rather available\nextensions base R, commonly referred “packages”. Like base\nR, open-source free. Many available \nrepository called CRAN, can download directly R using \ninstall.packages function.heart tidyverse functions available umbrella\npackage called “tidyverse”. package includes number key tidyverse\npackages (e.g., “dplyr”, “tidyr”, “stringr”, “forcats”, “ggplot2”) allows \nquickly install set packages computer. coding\nR, need load package R session, can\nusing library call (e.g., library(\"tidyverse\")).addition packages come umbrella “tidyverse” package,\nnumerous packages build tidyverse approach.\ncreated creator tidyverse approach (Hadley Wickham)\nothers team, others created R programmers \nfollow standards tidyverse approach. example one \nextensions specifically created working biomedical data\ntidybulk package,309 working \ntranscriptomics data.Objectives. module, trainee able :Define “tidyverse”, “character string”, “factor”,Explain tidyverse collection packages can user-friendly\npowerful solving many complex tasks dataDescribe difference base R R’s tidyverseList two key tidyverse packages file inputList key tidyverse package changing adding columnsList key tidyverse packages working character strings factorsList key tidyverse package working datesList two key tidyverse packages related statistical modelingLocate resources learning tidyverse approach","code":""},{"path":"module16.html","id":"tools-for-data-input","chapter":"Module 16 Simplify scripted pre-processing through R’s “tidyverse” tools","heading":"16.1 Tools for data input","text":"able work data R, first must load data R\nsession. Data typically saved type file files, \nmust instruct R find data read \nfile R session.several key tidyverse tools inputting data file. \nimportant package tidyverse called readr. package\nallows read data plain text files. Data often stored \nplain text files, including formats like CSV (“comma-separated\nvalues”), tab-separated values, fixed width files. files\ncan open computer text editor (example,\nNotepad, Wordpad, TextEdit).readr package includes various functions read data \ntypes files, different functions different formats \nfiles. example, CSV files separate different pieces data \nfile commas, can read R readr function\nread_csv.equipment laboratory may allow save results plain\ntext format. export data laboratory equipment, can\ncheck see option outload format like “CSV”\n“txt”, allow use readr functions \nread data R.packages tidyverse allow read data\ntypes file formats. example, may data \nrecorded Excel spreadsheet. Excel files bit complex\nstructure plain text files, functions read\nplain text files R work Excel files. Instead, \nseries functions package called readxl can use \nread data Excel files R. functions even allow \nspecify sheet Excel file read data , well \ncells sheet, allow fine control data input\nExcel spreadsheet.cases, may collecting data laboratory equipment \nexport data standard format, like plain text file \nbasic spreadsheet file. Instead, equipment save data file\nformat standardized certain type data (e.g., mzML\nfile metabolomics data) file type proprietary \ncompany manufactures equipment. chance someone \ncreated R package can input data specialized types \nfiles. fact, common file types biomedical research, chance\nhigh (example, several packages available functions \ninput data mzML file). One best ways find appropriate\ntool input data specialized formats searching Google \n“R data input” name file format. use file\nformat often laboratory, worth research determine\nR package good fit inputting data file format\nworking vignettes helpfiles package \nlearn use well.can learn readr readxl packages \nvignettes, provide tutorials walking functionality \npackage. can find :readr: https://readr.tidyverse.org/readxl: https://readxl.tidyverse.org/","code":""},{"path":"module16.html","id":"tools-for-changing-or-creating-columns","chapter":"Module 16 Simplify scripted pre-processing through R’s “tidyverse” tools","heading":"16.2 Tools for changing or creating columns","text":"many pre-processing tasks require creating columns \nmathematical functions existing columns. Therefore, ’ll want \ntools changing existing columns making new column.One example scaling normalizing data. Scaling often\nrequired using techniques dimension reduction (e.g.,\nprincipal components analysis) clustering, ensure unit \nmeasurement column influence weight later analysis. \nexample, clustering observations using measurements subject\nincluded weight, don’t want get different results depending \nwhether weight measured grams versus pounds, type \nscaling can help avoid differences based units used \nmeasurements.range ways standardize normalize different types \nbiomedical data, ranging simple much complex. simpler\nend method called z-score normalization, observations \nfeature (.e., column) changed overall mean 0 standard\ndeviation 1. can done taking value column \nsubtracting column-wide mean, dividing standard\ndeviation. also complex methods scaling normalization.\nsimilarly require mathematical algorithms functions applied \noriginal data create new column data scaled normalized\nversion original.R, functions come base installation R (\nwords, don’t require installing extra packages) can used basic\nprocesses standardization normalization. example, scale\nfunction can used basic scaling described previous paragraph.\ncan also directly use math functions (like - subtraction / \ndivision) basic functions (like mean calculate mean \nvector numbers sd calculate standard deviation) make \ntypes calculations scratch. apply , though, ’ll need know\nfunctions work columns dataframe.dplyr package key package learn tidyverse, forms\nheart tools cleaning exploring data stored tidy\ndataframes. package includes functions making changes \nsingle column (e.g., mutate function), also functions can used\nperform calculation across many columns (e.g., across\nfunction). efficient way something like scale data \nmultiple columns .functions can also used basic cleaning operations dataframe.\nexample, data recorded colony-forming units may include “TNTC”\ncells spreadsheet many bacteria grown individual\ncolonies “numerous count”. read data, may want\nchange values missing values can run numerical\ncalculations cells include colony counts. type conversion\ncan easily done using functions dplyr package. also\ncritical performing processes like scaling / normalization— mutate\nfunction, example, can used create new column scaled data \napplying scaling function existing column.can learn dplyr package vignette, \navailable : https://dplyr.tidyverse.org/.","code":""},{"path":"module16.html","id":"tools-for-working-with-character-strings","chapter":"Module 16 Simplify scripted pre-processing through R’s “tidyverse” tools","heading":"16.3 Tools for working with character strings","text":"learned basic tools inputting data, well basic\nmanipulations columns dplyr tools, take time \nlearn tools can often used make coding pipelines\nmuch efficient. One learn work well character\nstrings.Character strings strings alphanumerical symbols stored inside\nquotation marks, like “Mouse-01” “Control group”. Several tidyverse packages\nhelp work type data efficiently, either finding\nusing regular patterns data (e.g., number “01” stored \n“Mouse-01”) treating data marker set number groups\n(e.g., “Control group” versus “Treated group”). tools can help \nprocessing exploring data, also extremely important \ncreating figures tables data clear labels. start\nlearning work character string data, realize can also\ntreat file names directory names project character strings,\nuse tools embed use useful information filenames.stringr package, part tidyverse, includes simple \npowerful tools working vectors composed character strings. \nexample, package includes function let extract subset \ncharacter string based position characters string, \nfunction lets replace every instance pattern something\nelse, function tell character strings vector\nmatch certain pattern. also includes function can change\ncase letters string, either uppercase, lowercase,\n“title case” (first letter word capitalized).likely realize powerful many tools need\none tasks, ’ll find make life much easier.\nexample, say column data provides ID \nstudy subject (e.g., “Mouse 1A”). IDs entered using\nupper case (e.g., “MOUSE 1A”), lower case (“mouse 1a”), \nmixture (e.g., “Mouse 1A”), may find hard write code\nrecognizes “Mouse 1A” “mouse 1a” “MOUSE 1A”. \nfunctions stringr package let quickly convert everything \ncase work around issue.another example, may want extract certain elements subject\nID—example, might want create column changed\n“Mouse 1A” just “1A” “Mouse 2B” just “2B”. stringr package \nfunctions let several ways. example, \nfunction let remove “Mouse” character string, \nanother function let extract part string \nstarts first number. types tools can invaluable \nneed pre-process clean data format first enters R.Sometimes, want treat character strings discrete categories\nvalues. example, part data records subject IDs\n(e.g., “Mouse 1A”, “Mouse 2B”), may want able link \nobservations recorded subject. Similarly, \nmay want treat variable records treatment (e.g., “treated” / “control”)\nset specific categories observation belongs .R, can treating column something called “factor”.\ndata type looks like character string (e.g., “treated”), R \nrecorded set values values column can \n(e.g., “treated” “control”), summarize plot data, can\ngroup variable get summaries within category, align \ncolor shape plotted points.forcats package includes helpful tools working factor type\ndata. column changed factor, possible levels \nfactor (words, possible values can take) given order,\noften alphabetical. won’t notice order many processing\nmight , control order categories mentioned \nsummarize plot data. forcats package includes function \nlets rearrange order, rearrange order category\npresented summaries plots. package also includes numerous \ntools working type data. example, factor\ntakes many different possible values, let convert \nspecify common (can specify many categories),\npool rest “” category.vignettes stringr forcats packages available :stringr: https://stringr.tidyverse.org/forcats: https://forcats.tidyverse.org/","code":""},{"path":"module16.html","id":"tools-for-working-with-dates-and-times","chapter":"Module 16 Simplify scripted pre-processing through R’s “tidyverse” tools","heading":"16.4 Tools for working with dates and times","text":"Another handy set tools working dates times. Often, \nrecord date observation collected, date time \ndata collected fine time scale. Although record \ncharacter string (e.g., “August 1, 2019”), ’ll want able use \nquantitative information within date. example, may want able\ntell date observation certain date, determine\nmany days two date.tidyverse includes package working dates times called\nlubridate. package includes functions allow change column\ndata date date-time data type. allow \noperations values dates—words, things like determine\nnumber days two dates. lubridate package also includes\nfunctions operations dates, including determining one date \nlarger smaller another whether ’s within interval two dates,\nwell determining difference two dates finding \ndate certain number days given date. also functions\nextract certain elements date, like day week \nmonth year.functions lubridate package can useful pre-processing\ndata. example, may record date measurement take,\nalso need determine much time passed start \nexperiment measurement. lubridate package function \nallow calculate time since recorded start time, \nallows record date time measurement, \ndetermine time since start experiment within reproducible code\nread recorded data R.find lubridate package, can read vignette\nhttps://lubridate.tidyverse.org/.","code":""},{"path":"module16.html","id":"tools-for-statistical-modeling","chapter":"Module 16 Simplify scripted pre-processing through R’s “tidyverse” tools","heading":"16.5 Tools for statistical modeling","text":"Often, analysis biomedical data include statistical hypothesis\ntesting model building. example, collected bacterial\nloads two groups animals different treatment assignments\n(treated control), may want test hypothesis average\nbacterial load two groups . treatment successful\nexperiment adequate power, data hopefully show \nnull hypothesis rejected.R number functions can run common statistical\nhypothesis tests (e.g., Student’s t-test) well fit commonly-used\nstatistical models (e.g., linear regression models). Many tools\ncommon tests model building included initial installation\nR. means can use without installing loading additional\npackages., many additional packages available run\nless common statistical tests fit less common statistical model frameworks.\nPart R’s strength deep availability packages \nstatistical analysis. can often use Google search determine \nfunction package statistical analysis \nlike perform R, rare find least one package \nappropriate algorithm. help select among different packages, check\narticle “Ten Simple Rules Finding Selecting R Packages.”310In addition learning tools types statistical analysis\noften research, also helpful learn tools\nhelp incorporate statistical analysis workflow.\nMany tools R statistical analysis originally focused \nendpoint code pipeline. example, many result\nprint-summary results statistical test model fit.\nfine want record result, often \nwant use results R code, example add plots \ntables combine results.couple packages can help . First, \npackage called broom can conver output many statistical\ntests models tidy dataframe. focused learning\ntidyverse tools, functionality makes much easier \ncontinue working output. tidymodels\npackage extends idea creating common interface fitting\nvariety statistical models extracting results tidy format.can read vignettes broom tidymodels packages :broom: https://cran.r-project.org/web/packages/broom/vignettes/broom.htmltidymodels: https://www.tidymodels.org/","code":""},{"path":"module16.html","id":"resources-to-learn-more-on-tidyverse-tools","chapter":"Module 16 Simplify scripted pre-processing through R’s “tidyverse” tools","heading":"16.6 Resources to learn more on tidyverse tools","text":"tidyverse approach now widely taught, -person courses \nuniversities variety online resources.\nSince many excellent resources available—many free—learn\ncode R using tidyverse approach, consider beyond scope\nmodules go deeply instructions. Rather, ’ll\npoint excellent references go deeply tidyverse\napproach coding, set tools, can applied \nworking biomedical data.","code":""},{"path":"module16.html","id":"classes-and-workshops","chapter":"Module 16 Simplify scripted pre-processing through R’s “tidyverse” tools","heading":"16.6.1 Classes and workshops","text":"R programming classes universities, well workshops conferences\nvenues, now focus tidyverse approach, especially \ngeared new R users. R programming class can worthwhile investment \ntime resource available , head research group \ntime take one , instead encourage\ntrainees research group take type class. Programming \nscripted languages, like Python Julia, provides similar skills, although \ncollection extension packages available biomedical data tends \nextensive R (least time). Classes programming\nlanguages like Java C++, hand, less immediate\nrelevance biologists bench scientists, \nlike become better working biomedical data, worthwhile \nfocus programming languages scripted.","code":""},{"path":"module16.html","id":"online-books","chapter":"Module 16 Simplify scripted pre-processing through R’s “tidyverse” tools","heading":"16.6.2 Online books","text":"number excellent free online books available help\nlearn R (many can also purchased hard copy, \nprefer format). typically include lots examples code \nhelp try concepts learn .One key resource learning tidyverse approach R book R \nData Science Hadley Wickham (primary developer tidyverse) \nGarrett Grolemund.311 book available print edition\nO’Reilly Media. also freely available online \nhttps://r4ds..co.nz/. book geared beginners R, moving \nget readers intermediate stage coding expertise, level\nallow scientific researchers powerfully work \nexperimental data. book includes exercises practicing concepts, \nseparate online book available solutions exercises\nhttps://jrnold.github.io/r4ds-exercise-solutions/.Another online book excellent tool—particularly using R\nbiomedical research—Modern Statistics Modern Biology, Susan\nHolmes Wolfgang Huber.312 book shows tidyverse\napproach can combined tools Bioconductor custom-built \nwork bioinformatics data. also provides excellent overview \nstatistical methods working biomedical data can \napplied using R. book available online \nhttps://www.huber.embl.de/msmb/.","code":""},{"path":"module16.html","id":"cheatsheets","chapter":"Module 16 Simplify scripted pre-processing through R’s “tidyverse” tools","heading":"16.6.3 Cheatsheets","text":"many key tidyverse packages, two-page “cheatsheets” \ndeveloped package creators help users learn remember\nfunctions available package. available \nhttps://posit.co/resources/cheatsheets/.cheatsheet includes numerous working examples. One excellent way \nfamiliarize tools package, , work \nexamples cheatsheet one time, making sure understand \ninputs outputs function function created output.\nworked cheatsheet way, can keep close\ndesk serve quick reminder names uses different\nfunctions package, used enough don’t need\nmemory jog.deeper tutorials tidyverse package, can explore \npackage’s vignette. ’ve provided links several throughout \nmodule.","code":""},{"path":"module17.html","id":"module17","chapter":"Module 17 Complex data types in experimental data pre-processing","heading":"Module 17 Complex data types in experimental data pre-processing","text":"Raw data many biomedical experiments, especially use\nhigh-throughput techniques, can large complex. Vince Buffalo\nnotes article bioinformatics data skills:“Right now, labs across world, machines sequencing genomes \nlife earth. Even rapidly decreasing costs huge technological\nadvancements genome sequencing, ’re seeing glimpse biological\ninformation contained every cell, tissue, organism, ecosystem. However,\nsmidgen total biological information ’re gathering amounts mountains\ndata biologists need work . point human history \nability understand life’s complexities dependent skills \nwork analyze data.”313In previous modules, gone lot detail advantages \ntidyverse approach. However work biomedical data, may find \nunreasonable start tidyverse approach first steps \npre-processing data. particularly case working \ndata complex research equipment, like mass spectrometers flow\ncytometers.can frustrating realize can’t use standard tools\nsteps working data collect experiments.\nexample, may taken R course workshop, \npoint starting feel pretty comfortable use\nR work standard datasets. can feel like ’re starting \nsquare one realize approach won’t work steps \nworking data ’re collecting research.module aims help navigate process. particular, helpful\nunderstand Bioconductor approach differs tidyverse approach,\nstart developing framework tools navigating approaches.primary difference two approaches data objects \nstructured. work data R, kept “object”, \ncan think structured container data. tidyverse approach,\nprimary data container dataframe. dataframe made set \nobject types called vectors: column dataframe vector.\nTherefore, navigate tidyverse approach, data structures need\nunderstand well dataframe structure vector structure. Tools\ntidyverse use simple structures .contrast, Bioconductor approach uses collection complex data\ncontainers. number reasons , ’ll discuss \nmodule.note, possible near future, steps even\ncomplex pipelines manageable tidyverse approach. \nR developers embracing tidyverse approach making tools packages\nwithin framework. areas complex data, \nmajor inroads allow tidyverse approach throughout pipeline even\nworking complex data. ’ll end module discussing \nprospects.Objectives. module, trainee able :Explain R software pre-processing biomedical data often stores\ndata complex, “untidy” formatsDescribe complex data formats can create barriers \nlaboratory-based researchers seeking use reproducibility tools \ndata pre-processingExplain tidyverse Bioconductor approaches differ \ndata structures use","code":""},{"path":"module17.html","id":"how-the-bioconductor-and-tidyverse-approaches-differ","chapter":"Module 17 Complex data types in experimental data pre-processing","heading":"17.1 How the Bioconductor and tidyverse approaches differ","text":"heart difference tidyverse Bioconductor approaches\ncomes data structured within pipelines. \ndifferences one, differences result \nmain one.’ve described detail modules 3 16, \ntidyverse approach, data stored throughout pipeline dataframe\nstructure. dataframes composed data structure called vector.\nVectors make columns dataframe. Almost every function \ntidyverse, therefore, designed input either dataframe vector. \nalmost every function designed output type data container\n(dataframe vector) inputs. result, tidyverse approach allows\ncombine functions different orders tackle complex processes \nchain many small steps.contrast, packages Bioconductor use complex data structures\nstore data. Often, Bioconductor pipeline use different data\nstructures different points pipeline. example, data might\nstored one type data container ’s read R, \nanother type ’ve done pre-processing.result, Bioconductor approach, types data\nstructures understand learn use. Another result\nfunctions use pipeline often work \nspecific data structure. therefore need keep track type \ndata structure required input function.also means constrained chain together\ndifferent functions make pipeline. clear, pipeline R \nincludes complex Bioconductor data structures typically still \nmodular, sense can adapt separate specific parts \npipeline. However, tend much less flexible pipelines developed\ntidyverse approach. data structure changes often, certain\nfunctions outputing data structure needed next step, \nfunction next step outputting data different structure, \n. changing data structure means functions step often\nconstrained always put order. comparison, small\ntools make tidyverse functions can often combined many different\norders, letting build much larger variety pipelines . Also,\nmany functions work complex data types many things\nwithin one function, can harder learn understand, \noften much customized specific action, means \nlearn functions (since one specific thing).difference also make difference work modify \npipeline code. tidyverse approach, change functions \ninclude order call , rearranging small tools \ncreate different pipelines. Bioconductor pipeline, ’s common \ncustomize , adjust parameter settings within functions, \nstill call standard series functions standardized order.differences, can hard pick Bioconductor\napproach ’re used tidyverse approach. However, Bioconductor \ncritical learn working many types biomedical data, many\nkey tools algorithms genomic data shared \nproject. means , many biomedical researchers now generating\ncomplex, high-throughput data, worth learning use complex data\nstructures R.","code":""},{"path":"module17.html","id":"why-is-the-bioconductor-approach-designed-as-it-is","chapter":"Module 17 Complex data types in experimental data pre-processing","heading":"17.2 Why is the Bioconductor approach designed as it is?","text":"begin learning Bioconductor approach, can helpful understand \n’s designed way . First, characteristics complex\ndata can make data unsuitable tidyverse approach, including data\nsize complexity. next section module, ’ll discuss \ncharacteristics, well provide examples biomedical data can\n. However, also historical cultural reasons \nBioconductor design. helpful introduction , can\nhelp navigate work within Bioconductor framework.Bioconductor predates tidyverse approach. fact, around almost\nlong R —first version R first released 2000, \nBioconductor started 2003. Bioconductor project inspired \nambitious aim—allow people around world coordinate make tools \npre-processing analyzing genomic high-throughput data. Anyone \nallowed make extension R package, including Bioconductor\npackage.Imagine complex try harness contributions. Within \nBioconductor project, challenge managed using general design\nprinciples, centered standard data structures. different Bioconductor\ndata structures, , implemented help many people coordinate make\nsoftware extensions R handle complex biomedical data. Susan Holmes \nWolfgang Huber note book Modern Statistics Modern Biology,\n“specialized data containers … help keep data consistent, safe \neasy use.”314 Indeed, article software \ncomputational biology, Robert Gentleman—one developers R \nfounders Bioconductor project—quoted saying:“defined handful data structures expected people use. \ninstance, everybody puts gene expression data kind \nbox, doesn’t matter data came , box can\nused analytic tools. Really, think ’s data structures drive\ninteroperability.” — Robert Gentlemen quoted Stephen Altschul et al.315Each person writes code Bioconductor can use data structures,\nwriting functions input output data within defined structures. \nworking something isn’t yet defined structure, \ncan define new ones within package, others can use \npackages.result design, number complex data structures \nuse within Bioconductor packages. might come across start work\nusing approach include:316ExpressionSetSummarizedExperimentGRangesVCFVRangesBSgenome","code":""},{"path":"module17.html","id":"why-is-it-sometimes-necessary-to-use-a-bioconductor-approach-with-biomedical-data","chapter":"Module 17 Complex data types in experimental data pre-processing","heading":"17.3 Why is it sometimes necessary to use a Bioconductor approach with biomedical data","text":"data collected complex laboratory equipment like flow cytometers \nmass spectrometers, two main features make useful use \ncomplex data structures R earlier stages pre-processing data\nrather directly using tidy data structure. First, data often \nlarge, cases large difficult read R. Second,\ndata might combine various elements ’d like keep together \nmove steps pre-processing data, elements\nnatural structures, making hard set data \ntwo-dimensional dataframe. Let’s take detailed look .First, large datasets common biomedical data, including genomics\ndata. book Modern Statistics Modern Biology, Holmes Huber\ndescribe size biological data exploded:“Biology, formerly science sparse, often qualitative data, \nturned field whose production quantitative data par high\nenergy physics astronomy whose data wildly heterogeneous \ncomplex.”317When datasets large, can cause complications computers. computer\nseveral ways can store data. primary storage closely\nconnected computer’s processing unit, calculations made, \ndata stored primary storage can processed code quickly.\nstorage called computer’s random access memory, RAM. R uses \napproach, load data R stored one traditional\ndata structures, data moved part computer’s RAM.318Data can also stored devices computer, including hard drives\nsolid state drives built computer even onto storage\ndevices can removed computer, like USB drives external hard\ndrives. size available storage devices tends much, much\nlarger storage size computer’s RAM. However, takes longer \naccess data secondary storage devices aren’t directly\nconnected processor, instead require data move RAM \ncan accessed processor, part computer \ncan things analyze, modify, otherwise process data.traditional dataframe structure R built \nreading data RAM. However, many biological experiments now create\ndata much large read memory R reasonable way.319 try read dataset\n’s large RAM, R can’t handle . Roger Peng notes \nR Programming Data Science:“Reading large dataset enough RAM one easy\nway freeze computer (least R session). usually \nunpleasant experience usually requires kill R process, \nbest case scenario, reboot computer, worst case.”320More complex data structures can allow sophisticated ways handle massive\ndata, often necessary working massive biological\ndatasets, particularly early pre-processing, data can \nsummarized efficient way. example, complex data structure \nallow much data left disk, read memory demand,\nspecific portions data needed.321 approach can used iterate across subsets \ndata, reading parts data memory time.322 structures can designed work way ,\nuser, won’t notice difference data kept\n(disk versus memory)—means won’t worry \nmemory management issues, instead can just gain everything going\nsmoothly, even datasets get large.323The second reason tidy dataframes aren’t always best container \nbiomedical data complexity data. Dataframes \nclearly simply organized. However, can restrictive \ncases. Sometimes, might data fit well within \ntwo-dimensional, non-ragged structure characteristic dataframe\nstructure.example, biomedical data may data records characteristics \nseveral levels data. may records levels gene expression\nwithin sample, separate information gene measured, \nanother set information characterizes samples.\ncritical keep “like” measurements aligned data like\n—words, insure can connect data \ncharacterizes gene data provides measures level \nexpression gene sample—data naturally \ntwo-dimensional structure fit naturally dataframe\nstructure.Finally, one advantages complex data structures biomedical\ndata pre-processing can leveraged develop powerful\nalgorithms working complex biomedical data. include reading data\nspecialized file formats often output laboratory\nequipment.324","code":""},{"path":"module17.html","id":"navigating-bioconductor-packages-and-data-structures","chapter":"Module 17 Complex data types in experimental data pre-processing","heading":"17.4 Navigating Bioconductor packages and data structures","text":"[install BioC packages; biocLite()]CRAN common spot sharing general-purpose packages, \nspecialized repository used many genomics biology-related\nR packages called Bioconductor. packages can also easily installed\ncall R, case requires installation function \nBiocManager package. Many functions useful \npreprocessing biological data laboratory experiments available \nBioconductor.[find BioC data structures]resources learning use specific Bioconductor packages, well\ngeneral resources Bioconductor, like R Programming \nBioinformatics [ref].","code":""},{"path":"module17.html","id":"combining-bioconductor-and-tidyverse-approaches-in-a-workflow","chapter":"Module 17 Complex data types in experimental data pre-processing","heading":"17.5 Combining Bioconductor and tidyverse approaches in a workflow","text":"Work research data typically require series steps \npre-processing, analysis, exploration, visualization. Collectively, \nform workflow pipeline data analysis. large, complex\nbiological data, early steps workflow might require Bioconductor\napproach, given size complexity data, ’d like \nuse method algorithm available Bioconductor. However, doesn’t\nmean must completely give power efficiency tidyverse\napproach described earlier modules.Instead, can combine two approaches workflow like shown \nFigure 17.1. combined approach, start \nworkflow Bioconductor approach transition possible \ntidyverse approach, transitioning “tidying” complex data\nstructure simpler dataframe data structure along way. useful\napproach, workflow advanced stage \nstraightforward store data dataframe, large advantages\nshifting tidyverse approach compared using complex\nobject-oriented classes storing data, particular comes \ndata analysis visualization later stages workflow. \nsection, describe can make transition create combined\nworkflow.\nFigure 17.1: overview workflow moves Bioconductor approach—pre-processing data—tidyverse approach one pre-processing created smaller, simpler data can reasonably stored dataframe structure.\nKey combined pipeline tools can convert specialized data\nstructures Bioconductor tidy dataframes. set tools \navailable biobroom package.325 biobroom package\nincludes three generic functions (also called “methods”), can used\nnumber Bioconductor object classes. applied object stored one\nBioconductor classes, functions extract part data\ntidy dataframe format. format, easy use tools \ntidyverse explore, analyze, visualize data.three generic functions biobroom called tidy, augment,\nglance. function names mimic names three main functions\nbroom package, general purpose package extracting\ntidy datasets complex R object containers.326 \nbroom package focuses output functions R statistical\ntesting modeling, newer biobroom package replicates idea,\nmany common object classes used store data \nBioconductor packages workflows.327As example, can talk biobroom package can used \nconvert output generated functions edgeR package. edgeR\npackage popular Bioconductor package can used gene expression\ndata, explore genes expressed differently across experimental\ngroups (approach called differential expression analysis).328 \nusing functions package, data must pre-processed align\nsequence reads raw data create table counts \nread gene across sample. edgeR package includes\nfunctions pre-processing data, including filtering genes low\nread counts across samples applying model-based normalization across\nsamples help handle technical bias, including differences sequencing depth.329The edgeR package operates data stored special object class defined \npackage called DGEList object class.330 object\nclass includes areas storing table read counts, form \nmatrix appropriate analysis functions package, well \nspots storing information sample , needed, space \nstore annotations genes.331 functions edgeR\npackage can perform differential expression analysis data \nDGEList class. result object DGEExact class, also\ndefined edgeR package. extract data class tidy\nformat, can use tidy glance functions biobroom.","code":""},{"path":"module17.html","id":"outlook-for-a-tidyverse-approach-to-biomedical-data","chapter":"Module 17 Complex data types in experimental data pre-processing","heading":"17.6 Outlook for a tidyverse approach to biomedical data","text":"Finally, tools continuing evolve, ’s quite possible \nfuture might tidy dataframe formats adaptable enough handle\nearlier stages data pre-processing genomics data. tidyverse\ndataframe approach already adapted enable tidy dataframes include\ncomplex types data within certain columns data frame special\nlist-type column.functionality leveraged sf\npackage, example, enable tidy approach working geographical\ndata. allows working geographical data, example data\nshapefiles creating maps, use standard tidyverse approaches\nstill containing complex data needed geographical information\nwithin tidy dataframe. Another example tidymodels package, …seems possible similar approaches may adapted near future\nallow biomedical genomic data stored way accounts\ncomplexity early pre-processing data also allows \nnatural integration wealth powerful tools available \ntidyverse approach.","code":""},{"path":"module18.html","id":"module18","chapter":"Module 18 Introduction to reproducible data pre-processing protocols","heading":"Module 18 Introduction to reproducible data pre-processing protocols","text":"Reproducibility tools can used create reproducible data pre-processing\nprotocols—documents combine code text “knitted” document. \ndocuments can re-used ensure data pre-processing consistent \nreproducible across research projects. module, describe \nreproducible data pre-processing protocols can improve reproducibility \npre-processing experimental data, well ensure transparency,\nconsistency, reproducibility across research projects conducted \nresearch team.Objectives. module, trainee able :Define “reproducible data pre-processing protocol”Explain protocols improve reproducibility data pre-processing\nphaseList benefits, including improving efficiency consistency data\npre-processingUnderstand “knitted” document can used combine text \nexecutable code create reproducible data pre-processing protocol","code":""},{"path":"module18.html","id":"introducing-reproducible-data-pre-processing-protocols","chapter":"Module 18 Introduction to reproducible data pre-processing protocols","heading":"18.1 Introducing reproducible data pre-processing protocols","text":"ever worked laboratory, likely familiar protocols.\nwet lab, protocols used “recipes” conducting certain\nexperiments processes. written clear enough everyone \nlab follow steps process following protocol. \nway, help standardize processes done laboratory, \ncan also play role improving safety quality data collection.\nProtocols similarly used medical procedures \ntests, well clinical trials. cases, help define \ndetail steps procedure, can done way \ncomparable one case next high precision.\nKathy Thomas Mary Beth Farrell note article write\nclinical imaging protocols:“one composing protocol, helpful\nimagine technologists facility won \nlottery quit. newly hired technologist\nneed know image patient exact manner\npast produce results?”332You can apply similar idea pre-processing analyzing data \ncollect laboratory. Just wet lab protocol can help standardize \ndata collection point data recorded, separate protocol\ncan help define manage work data.\nbasic content data-focused protocol include description type data\nexpect input, type data expect end process,\nsteps take get input output.\ndata-focused protocol can include steps quality control \ncollected data, well pre-processing steps like transformations \nscaling data.module 20, ’ll walk example creating data pre-processing\nprotocol focuses data collected plating samples estimate\nbacterial load. case, key step pre-processing data \nidentify “good” dilution used estimating bacterial load \nsample—sample plated several dilutions, work data,\nmust identify dilution sample enough bacteria grew \ncountable, many many colonies count. high\nthroughput experiments, like RNA-seq experiments, may important steps\ndata pre-processing help check batch effects across samples, \nsigns poor-quality sample, normalizing scaling data \npreparation applying algorithms, like algorithms estimate\ndifferential expression across samples identify clusters within data.data-focused protocol brings many advantages wet lab protocols.\ncan help standardize process data pre-processing across members \nlaboratory, well experiment experiment. can also help\nensure quality data collection, defining clear rules,\nsteps, guidelines completing data pre-processing. Finally, can\nhelp ensure someone else recreate process later time, \ncan improve reproducibility experiment.\ndata-focused protocols help improving quality reproducibility,\nalso help improve efficiency. protocols include\nclearly defined steps, well explanations step, illustrate\nexample data. “recipe”, new lab member can quickly\nlearn data pre-processing, long-term lab member remember\nexact steps quickly.can create data pre-processing protocol using document processing\nprogram ’d like. example, write one Google Docs \nWord. However, better format. programming languages like R \nPython, can created type document called knitted document. \nknitted document interweaves two elements: first, text written humans \nsecond, executable code meant computer. documents can \n“rendered” R another programming language, executes code \nadds output code appropriate place text. end\nresult document format easy share read (PDF, Word, \nHTML), includes text, example code, output. can use \ndocuments record data pre-processing process type data \nlaboratory, using knitted document, ensure code \n“checked” every time render document. module,\ngive overview knitted documents work, well \ncan improve reproducibility efficiency experimental work. \nmodule 19, ’ll show can make free RStudio software.\nFinally, module 20, ’ll walk full example writing data pre-processing\nprotocol way—can take look now get idea downloading \nexample protocol\n.\nalso excellent data-focused protocols published \njournals like Nature Protocols. recent examples protocols include\nNadine Schrode et al.333, Katrien Quintelier et al.334, Erica L-W Majumder et al.335 \nmay find useful take look one get idea \ndata-focused protocols can useful.","code":""},{"path":"module18.html","id":"using-knitted-documents-for-protocols","chapter":"Module 18 Introduction to reproducible data pre-processing protocols","heading":"18.2 Using knitted documents for protocols","text":"comes protocols focused data pre-processing \nanalysis, big advantages creating \nknitted documents. section, ’ll walk knitted\ndocument , next section ’ll cover advantages using format \ncreate data-focused protocols.knitted document one written plain text way “knits”\ntogether text executable code. written document, can\nrender , executes code, adds document results \nexecution (figures, tables, code output, example), formats text\nusing formatting choices ’ve specified. end result nicely format\ndocument, can one several output formats, including PDF, Word, \nHTML. Since code executed create document, can ensure \ncode worked intended.coded using scripting language like R Python, likely \nalready seen many examples knitted documents. languages, \nmany tutorials available created knitted documents. Figure\n18.1 shows example start vignette \nxcms package R. package helps pre-processing \nanalyzing data liquid chromatography–mass spectrometry (LC–MS)\nexperiments. can see document includes text explain package\nalso example code output code.\nlarger example, modules online book written knitted\ndocuments.\nFigure 18.1: example knitted document. shows section online vignette xcms package Bioconductor. two types content highlighted: formatted text humans read, executable computer code.\ncan visualize full process creating rendering knitted document\nfollowing way. Imagine write document hand sheets \npaper. parts need team member add data run\ncalculation, include notes square brackets telling team member\nthings. , use editing marks show \ntext italicized text section header:send document team member Kristina first, \ncalculations adds results indicated spot paper, \nnote gets replaced results. focuses notes \nsquare brackets ignores rest document. Next, Kristina sends \ndocument, additions, assistant, Tom, type document. Tom\ntypes full document, paying attention indications included\nformatting. example, sees “Results” meant section\nheading, since line starts “#”, team’s convention \nsection headings. therefore types line larger font. \nalso sees “Mycobacterium tuberculosis” surrounded asterisks, \ntypes italics.Knitted documents work way, computer steps \nKristina Tom toy example. way document written \nexample analogous writing knitted document plain text \nappropriate “executable” sections, designated special markings, \nmarkings used show text formatted final\nversion. Kristina looked section marked , generated\nresults section, replaced note results, \nanalogous first stage rendering knitted document, document\npassed software looks executable code ignores everything\nelse, executing code adding results right place.\nTom took output used formatting marks text create \nnicely formatted final report, step analogous second stage \nrendering formatted document, software program takes output \nfirst stage formats full document attractive, easy--read final\ndocument, using markings include format document.Knitted documents therefore build two key techniques. first \nability include executable code document, way computer can\ngo document, find code, execute , fill results \nappropriate spot document. second set conventions \nformatting marks can put plain text document indicate\nformatting added, like headers italic text. Let’s take closer\nlook necessary techniques.first technique ’s needed create knitted documents ability \ninclude executable code within plain text version document.\nidea can use special markers indicate document\ncode starts ends. markings, computer program can\nfigure lines document run code, ones \nignore ’s looking executable code. toy example ,\nnotes Kristina put square brackets, content started \nname colon. “process” document, , just scan\nsquare brackets name inside ignore everything else\ndocument.idea happens knitted documents, computer program takes \nplace Kristina example. markings place indicate executable\ncode, document run two separate programs rendered.\nfirst program look code execute ignore lines \nfile. execute code place results, like figures,\ntables, code output, document right piece code. \ntalk second program just minute, talk \nsomething called “markup languages”.technique comes idea include code executed \ndocument otherwise easy humans read. incredibly\npowerful idea. originated famous computer scientist named Donald\nKnuth, realized one key making computer code sound make sure\nclear humans code . Computers faithfully \nexactly tell , ’re hoping \nlong provide correct instructions. greatest room error,\n, comes humans giving right instructions computers. \nwrite sound code, code easy others maintain \nextend, must make sure humans understand \nasking computer . Donald Knuth came system called “literate\nprogramming” allows programmers write code way focuses \ndocumenting code humans, also allowing computer easily\npull just parts needs execute, ignoring text\nmeant humans. process flips idea documenting code including\nplain text comments code—instead code heart \ndocument, documentation code heart, code provided\nillustrate implementation. used well, technique results \nbeautiful documents clearly comprehensively document intent \nimplementation computer code. knitted documents can build\nR Python systems like RMarkdown Jupyter Notebooks build\nliterate programming ideas, applying ways complement\nprogramming languages can run interactively, rather needing \ncompiled ’re run.second technique required knitted documents one allows \nwrite text plain text, include formatting specifications plain text,\nrender attractive output document PDF, Word, HTML. \npart process uses tool set tools called Markup languages.\n, use markup language called Markdown. one easiest\nmarkup languages learn, fairly small set formatting indicators\ncan used “markup” formatting document. small set,\nhowever, covers much formatting might want , \nlanguage provides easy introduction markup languages still providing\nadequate functionality purposes.Markdown markup language evolved starting spaces people \ncommunicate plain text , without point--click methods adding\nformatting like bold italic type.336 example,\nearly versions email allowed users write using plain text. \nusers eventually evolved conventions “mark-” plain text,\nserve purposes normally served things like italics bold formatted text\n(e.g., emphasis, highlighting). example, emphasize word, user \nsurround asterisks, like:early prototype markup language, reader’s mind \n“rendering”, interpreting markers sign part text\nemphasized. Markdown, text can rendered attractive\noutput documents, like PDF, rendering process actually\nchanged words asterisks print italics.Markdown language developed set types marks—like\nasterisks—used “mark ” plain text formatting \napplied text rendered. marks can use\nnumber formatting specifications, including: italics, bold, underline,\nstrike-, bulleted lists, numbered lists, web links, headers different\nlevels (e.g., mark sections subsections), horizontal\nrules, block quotes. Details examples Markdown syntax can \nfound Markdown Guide page https://www.markdownguide.org/basic-syntax/,\n’ll cover examples using Markdown modules 19 20. \ndocument run program execute code, run\nprogram interprets formatting markup (markup renderer),\nformat document based mark indications \noutput attractive document format like PDF, Word, HTML.","code":"# Results\n\nWe measured the bacterial load of \n*Mycobacterium tuberculosis* for each \nsample. \n\n[Kristina: Calculate bacterial loads for \neach sample based on dilutions and\nadd table with results here.]I just read a *really* interesting article!"},{"path":"module18.html","id":"advantages-of-using-knitted-documents-for-data-focused-protocols","chapter":"Module 18 Introduction to reproducible data pre-processing protocols","heading":"18.3 Advantages of using knitted documents for data-focused protocols","text":"several advantages using knitted documents writing code \npre-process analyze research data. include improvements terms \nreliability, efficiency, transparency, reproducibility.First, written code within knitted document, code \nchecked every time render document. words, checking\ncode ensure operates intend throughout process writing\nediting document, checking code time render document\nformatted version. helps increase reliability rigor \ncode written. Open-source software evolves time, \ncontinuing check code work protocols reports data,\ncan ensure quickly identify adapt changes.\n, can identify updates research data introduce issues\ncode. , checking code frequently, can identify \nissues quickly, often allow easily pinpoint fix \nissues. contrast, identify problem writing lot code,\noften difficult identify source issue. including code\nchecked time document rendered, can quickly identify \nchange open-source software affects analysis conducting\npre-processing work adapt changes quickly.Second, write document includes executable code, allows \neasily rerun code update research data set, adopt code\nwork new data set. using knitted document write\npre-processing protocols research reports, workflow probably \nrun code—either script command line—copy \nresults document word processing program like Word Google Docs.\n, must recopy results every time adapt part\ncode add new data. contrast, use knitted document, \nrendering process executes code incorporates results directly \nautomatically nicely formatted final document. use knitted\ndocuments therefore can substantially improve efficiency \npre-processing analyzing data generating reports summarize\nprocess.Third, documents created knitted format created using plain\ntext. Plain text files can easily tracked well clearly using version\ncontrol tools like Git, associated collaboration tools like GitHub, \ndiscussed modules 9–11. substantially increases\ntransparency data pre-processing analysis. allows \nclearly document changes others make document, step--step. \ncan document made change, person can include message \nmade change. full history changes recorded can \nsearched explore document evolved .final advantage using knitted documents, especially pre-processing\nresearch data, allows code clearly thoroughly\ndocumented. can help increase reproducibility process. \nwords, can help ensure another researcher repeat \nprocess, making adaptations appropriate data set, ensuring\narrive results using original data. also ensures \ncan remember exactly , especially useful plan \nreuse adopt code work data sets, often case\npre-processing protocol. using knitted document, \nusing code pre-processing, alternative may documenting \ncode comments code script. code script allow include\ndocumentation code code comments, demarcated\ncode script special symbol (# R). However code\ncomments much less expressive harder read nicely formatted text,\nhard include elements like mathematical equations literature\ncitations code comments. knitted document allows write \ndocumentation format clear attractive humans read, \nincluding code clear easy computer execute.","code":""},{"path":"module18.html","id":"how-knitted-documents-work","chapter":"Module 18 Introduction to reproducible data pre-processing protocols","heading":"18.4 How knitted documents work","text":"Now ’ve gotten top-level view idea knitted documents \ndiscussed advantages, let’s take closer look work. ’ll\nwrap module covering mechanics knitted\ndocuments work, module 19 ’ll look closely \ncan leverage techniques RMarkdown system specifically.seven components documents work. helpful \nunderstand understand begin creating adapting knitted\ndocuments. Knitted documents can created number programs, \nlater focus RMarkdown, seven components play\nregardless exact system used create knitted document, therefore\nhelp gaining general understanding type document. listed\nseven components following paragraphs describe \nfully:Knitted documents start plain text;special section start document (preamble) gives \noverall directions document;Special combinations characters indicate executable code starts;special combinations show regular text starts (\nexecutable code section ends);Formatting rest document specified markup\nlanguage;create final document rendering plain text document. \nprocess runs two software programs; andThe final document attractive read-—never make\nedits output, initial plain text document.First, knitted document written plain text. earlier module,\ndescribed advantages using plain text file formats, rather\nproprietary /binary file formats, especially context saving\nresearch data (e.g., using csv file formats rather Excel file formats).\nPlain text can also used write documentation, including knitted\ndocuments. Figure 18.2 shows example plan text might look like \nstart xcms tutorial shown Figure 18.1.\nFigure 18.2: example plain text used write knitted document. shows section plain text used write online vignette xcms package Bioconductor. full plain text file used vignette can viewed GitHub .\nthings keep mind writing plain text. First, \nalways use text editor rather word processor \nwriting document plain text. Text editors can include software programs\nlike Notepad Microsoft operating systems TextEdit Mac operating\nsystems. can also use advanced text editor, like vi/vim emacs.\nRStudio can also serve text editor, work \nRStudio, often obvious option text editor use write\nknitted documents.must use text editor write plain text knitted documents \nreasons must use one write code scripts. Word processors often\nintroduce formatting saved underlying code rather clearly\nevident document see type. hidden formatting can\ncomplicate written text. Conversely, text written text editor \nintroduce hard--see formatting. Word processing programs also tend \nautomatically convert symbols slightly fancier versions symbol.\nexample, may change basic quotation symbol one shaping,\ndepending whether mark comes beginning end quotation. \nsubtle change formatting can cause issues code formatting\nspecifications include knitted document., writing plain text, typically use characters\nAmerican Standard Code Information Interchange, ASCII. \ncharacter set early computing includes 128 characters. small\ncharacter set enforces simplicity: character set mostly includes \ncan see keyboard, like digits 0 9, lowercase uppercase\nalphabet, symbols, including punctuation symbols like exclamation point\nquotation marks, mathematical symbols like plus, minus, division,\ncontrol codes, including ones new line, tab, even ringing \nbell. full set characters included ASCII can found number \nsources including thorough Wikipedia page character set (https://en.wikipedia.org/wiki/ASCII).character set available plain text files small, \nfind becomes important leverage limited characters \navailable. One example white space. White space can created ASCII\nspace character new line command. important\ncomponent can used make plain text files clear humans read. \nbegin discussing convention markdown languages, find \nwhite space often used help specify formatting well.second component knitted documents work knitted document\nspecial section start called preamble. preamble\ngive overall directions regarding document, like title \nauthors format rendered. Knitted documents \ncreated using markup language specify formatting document, \nnumber different markup languages including HTML, LaTeX, \nMarkdown. specifications document’s preamble depend \nmarkup language used.RMarkdown, focusing Markdown, preamble \nspecified using something called YAML (short YAML Ain’t Markup Language).\nexample YAML sample pre-processing protocol created\nusing RMarkdown:YAML preamble specifies information document keys \nvalues. example, title specified using YAML key title,\nfollowed colon space, desired value \ncomponent document, \"Pre-processing Protocol LC-MS Data\".\nSimilarly, author specified author key desired\nvalue component, date date key associated\ncomponent.Different keys can take different types values YAML\n(similar different parameters function can take different values). example, keys author, title, date take\ncharacter string desired character combination, quotation\nmarks surrounding values keys denote character strings. contrast, output key—specifies format \nknitted document rendered —can take one \nset values, specified without surrounding\nquotation marks (pdf_document case, render document\nPDF report).rules keys can included preamble depend \nmarkup language used. , showing example Markdown, \ncan also use markup languages like LaTeX HTML, \nconvention specifying preamble. module 19, \ntalk specifically RMarkdown, give resources \ncan find customize preamble RMarkdown specifically. \nusing different markup language, numerous websites,\ncheatsheets, resources can use find keywords \navailable preamble markup language, well possible\nvalues keywords can take.next characteristic knitted documents need clearly\ndemarcate executable code starts regular formatted text starts\n(words, executable code section ends). , knitted\ndocuments two special combination characters, one can used \nplain text indicate executable code starts one indicate\nends. example, Figure 18.3 shows plain text\nused RMarkdown document write regular text, \nexecutable code, indicate start regular text:\nFigure 18.3: example special combinations characters used demarcate code RMarkdown file. color formatting applied automatically RStudio; text example written plain text.\ncombination indicates start executable code vary depending\nmarkup language .\nmay noticed markers, indicate beginning end \nexecutable code, seem like odd character combination. good\nreason . making character combination unusual, \nless chance shows regular text. way fewer\ncases writer unintentionally indicate start new section \nexecutable code trying write regular text knitted document.next characteristic knitted documents formatting regular\ntext document—, everything executable code—\nspecified using called markup language. writing \nplain text, buttons click formatting, example, \nspecify words phrases bold italics, font size, headings,\n. Instead use special characters character combinations \nspecify formatting final document. character combinations \ndefined based markup language use. mentioned earlier, RMarkdown\nuses Markdown language; knitted documents can created using LaTeX\nHTML. example special character combinations work, \nMarkdown, place two asterisks around word phrase make bold. \nwrite “” final document, words, ’ll write\n**\"\"** plain text initial document.can start see works looking example xcms\nvignette shown earlier Figures 18.1 \n18.2. Figure 18.4, ’ve\nrecreated two parts side--side, ’re easier compare.\nFigure 18.4: original plain text knitted document final output, side side. examples xcms package vignette, package available Bioconductor. left part figure shows plain text written create output, shown left part figure. can see elements like sections headers different font styles indicated original plain text special characters combinations charaters, using Markdown language syntax.\ncan look several formatting elements . First, section headed\n“Initial data inspection”. can see original plain text document,\nmarked using # start line text header. \ncan also see words phrases formatted computer-style font\nfinal document—indicate values computer code,\nrather regular English words—surrounded backticks plain\ntext file.final characteristics knitted documents , create final\ndocument, render plain text document. process \ncreate attractive final document. visualize , rendering \nprocess takes document plain text format, shown left\nFigure 18.4, final format, shown right\nfigure.render document, run two software programs, \ndescribed earlier. first look sections executable code,\nbased character combination used mark executable code\nsections. first software execute code take \noutput—including data results, figures, tables—insert \nrelevant spot document’s file. Next, output file software\nrun another software program. second program look\nformatting instructions render final document \nattractive format. final output can number file formats,\ndepending specify preamble, including PDF document, HTML\nfile, Word document.consider final document, regardless output format, \nread-. means never make edits changes final\nversion document. Instead make changes initial\nplain text file. rendering process overwrite \nprevious versions final document. Therefore changes \nmade final document overwritten anytime re-render \noriginal plain text document.","code":"  ---\n  title: \"Pre-processing Protocol for LC-MS Data\"\n  author: \"Jane Doe\"\n  date: \"1/25/2021\"\n  output: pdf_document\n  ---"},{"path":"module19.html","id":"module19","chapter":"Module 19 RMarkdown for creating reproducible data pre-processing protocols","heading":"Module 19 RMarkdown for creating reproducible data pre-processing protocols","text":"R extension package RMarkdown can used create documents combine\ncode text ‘knitted’ document, become popular tool \nimproving computational reproducibility efficiency data analysis\nstage research. tool can also used earlier research process,\nhowever, improve reproducibility pre-processing steps. module, \nprovide detailed instructions use RMarkdown RStudio create\ndocuments combine code text. show RMarkdown document\ndescribing data pre-processing protocol can used efficiently apply \ndata pre-processing steps different sets raw data.Objectives. module, trainee able :Define RMarkdown documents can createExplain RMarkdown can used improve reproducibility research\nprojects data pre-processing phaseCreate document RStudio using RMarkdownDescribe advanced features RMarkdown can find \n","code":""},{"path":"module19.html","id":"creating-knitted-documents-in-r","chapter":"Module 19 RMarkdown for creating reproducible data pre-processing protocols","heading":"19.1 Creating knitted documents in R","text":"module 18, described knitted documents , well \nadvantages using knitted documents create data pre-processing protocols\ncommon pre-processing tasks research group. also described \nkey elements creating knitted document, regardless software system\nusing. module, \ngo detail can create documents using R \nRStudio, module 20 walk example data\npre-processing protocol created using method. strongly recommend \nread previous module (module 18) working one.R special format creating knitted documents called RMarkdown. \nmodule 18, talked elements knitted document, later\nmodule ’ll walk apply RMarkdown. However, \neasiest way learn use RMarkdown try example, ’ll start\nbasic one. ’d like try , ’ll need download\nR RStudio. RStudio IDE can downloaded installed free\nsoftware, long use personal version (Posit, company \ncreated RStudio, creates higher-powered versions corporate use).Like plain text documents, RMarkdown file edited using text\neditor, rather word processor like Word Google Docs. easiest \nuse RStudio IDE text editor creating editing R markdown\ndocument, IDE incorporated helpful functionality working\nplain text documents RMarkdown. RStudio, can create number \ntypes new files “File” menu. create new R markdown file,\nopen RStudio choose “New File”, choose “RMarkdown” \nchoices menu. Figure 19.1 shows example \nmenu option looks like.\nFigure 19.1: RStudio pull-menus help navigate open new RMarkdown file.\nopen window options can specify overall\ninformation document (Figure 19.2), including\ntitle author. can specify output format \nlike. Possible output formats include HTML, Word, PDF. able \nuse HTML Word output formats without additional software, ’ll start\nexample. \nlike use PDF output, need install one piece \nsoftware: Miktex Windows, MacTex Mac, TeX Live Linux. \npieces software underlying TeX engine open-source \nfree. example module 20 created PDF using one tools.\nFigure 19.2: Options available create new RMarkdown file RStudio. can specify information go document’s preamble, including title authors format document output (HTML, Word, PDF).\nselected options menu can choose “Okay” button\n(Figure 19.2). open new document. \ndocument, however, won’t blank. Instead include example document\nwritten RMarkdown (Figure 19.3). example\ndocument helps navigate RMarkdown process works, letting test\nsample document. also gives starting point—understand\nexample document works, can edit change convert \ndocument like create.\nFigure 19.3: Example template RMarkdown document see create new RMarkdown file RStudio. can explore template try rendering (knitting) . familiar example works, can edit text code adapt document.\nused RMarkdown , helpful try knitting \nexample document making changes, explore pieces document\nalign elements rendered output document. \nfamiliar line-elements file output document,\ncan delete parts example file insert text code.Let’s walk explore example document, aligning \nformatted output document (Figure 19.4).\nFirst, render RMarkdown document, RStudio can\nuse “Knit” button top file, shown Figure\n19.5. click button, render \nentire document output format ’ve selected (HTML, PDF, Word). \nrendering process run executable code apply formatting.\nfinal output (Figure 19.4, right) pop \nnew window. start RMarkdown, useful look \noutput see compares plain text RMarkdown file (Figure\n19.4, left).\nFigure 19.4: Example template RMarkdown document see create new RMarkdown file RStudio. can explore template try rendering (knitting) . familiar example works, can edit text code adapt document.\n\nFigure 19.5: Example template RMarkdown document, highlighting buttons RStudio can use facilitate working document. ‘knit’ button, highlighted top figure, render entire document. green arrow, highlighted lower figure within code chunk, can used run code specific code chunk.\nalso notice, first render document, working\ndirectory new file output document. example, \nworking create HTML document using RMarkdown file called\n“my_report.Rmd”, knit RMarkdown file, notice new file\nworking directory called “my_report.html”. new file output\nfile, one share colleagues report. \nconsider output document “read-”—words, can read \nshare document, make changes directly \ndocument, since overwritten anytime re-render original\nRMarkdown document.Next, let’s compare example RMarkdown document (one given \nfirst open RMarkdown file RStudio) output file \ncreated render example document (Figure\n19.4). look output document (Figure\n19.4, right), can notice different elements\nalign pieces original RMarkdown file (Figure\n19.4). example, output document includes \nheader text “R Markdown”. second-level header created \nMarkdown notation original file :header formatted larger font text, separate\nline—exact formatting specified within style file RMarkdown\ndocument, applied second-level headers document. \ncan also see formatting specified things like bold font word\n“Knit”, Markdown syntax **Knit**, clickable link specified\nsyntax <http://rmarkdown.rstudio.com>. beginning \noriginal document, can see elements like title, author, date, \noutput format specified YAML. Finally, can see special\ncharacter combinations demarcate sections executable code.Let’s look little closely next part module \nelements RMarkdown document work.","code":"## R Markdown"},{"path":"module19.html","id":"formatting-text-with-markdown-in-rmarkdown","chapter":"Module 19 RMarkdown for creating reproducible data pre-processing protocols","heading":"19.2 Formatting text with Markdown in RMarkdown","text":"remember module 18, one element knitted documents \nwritten plain text, formatting specified\nusing markup language.\nmain text RMarkdown document, formatting done using\nMarkdown markup language. Markdown popular markup language, part\ngood bit simpler markup languages like HTML LaTeX.\nsimplicity means quite expressive markup\nlanguages. However, Markdown probably provides adequate formatting least 90% \nformatting typically want research report \npre-processing protocol, staying simpler, much easier learn \nMarkdown syntax quickly compared markup languages.markup languages, Markdown uses special characters combinations characters indicate formatting within plain text original document. document rendered, markings used software create formatting specified final output document. example formatting symbols conventions Markdown include:format word phrase bold, surround two asterisks (**)format word phrase italics, surround one asterisk (*)create first-level header, put header text line, starting line #create second-level header, put header text line, starting line ##separate paragraphs empty linesuse hyphens create bulleted listsOne thing keep mind using Markdown, terms formatting, \nwhite space can important specifying formatting. example \nspecify new paragraph, must leave blank line previous\ntext. Similarly use hash (#) indicate header, must leave \nblank space hash word phrase want used \nheader. create section header, write:hand, forgot space hash sign, like :ouput document get :#Initial Data InspectionSimilarly, white space needed separate paragraphs. example, create two paragraphs:Meanwhile create one:syntax Markdown fairly simple can learned quickly. \ndetails syntax, can refer RMarkdown reference guide \nhttps://rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf. \nbasic formatting rules Markdown also covered \nextensive resources RMarkdown point later \nmodule.","code":"# Initial Data Inspection#Initial Data InspectionThis is a first paragraph. \n\nThis is a second.This is a first paragraph.\nThis is still part of the first paragraph."},{"path":"module19.html","id":"preambles-in-rmarkdown-documents","chapter":"Module 19 RMarkdown for creating reproducible data pre-processing protocols","heading":"19.3 Preambles in RMarkdown documents","text":"module 18, explained knitted documents include \npreamble specify metadata document, including elements\nlike title, authors, output format. R, preamble \ncreated using YAML. subsection, provide details\nusing YAML section RMarkdown documents.RMarkdown document, YAML special section\ntop RMarkdown document (original, plain text file, \nrendered version). set rest document using special\ncombination characters, using process similar executable code\nset text special set characters can easily\nidentified software program renders document. YAML, \ncombination characters three hyphens (---) line \nstart YAML section another three line end .\nexample YAML might look like top RMarkdown\ndocument:Within YAML , can specify different options document.\ncan change simple things like title, author, date, can\nalso change complex things, including output document rendered.\nthing want specify, specify special\nkeyword option valid choice keyword. idea\nsimilar setting parameter values function call R. \nexample, title: keyword valid one RMarkdown YAML. allows \nset words printed title space, using title formatting,\noutput document. can take string characters, can put \ntext title ’d like, long surround quotation\nmarks. author: date: keywords work similar ways. output:\nkeyword allows specify output document rendered .\ncase, keyword can take one set values, including\nword_document output Word document, pdf_document output pdf\ndocument (see later section set-required make \nwork), html_document output HTML document.start using RMarkdown, able lot without messing \nYAML much. fact, can get long way without ever changing values\nYAML default values given first create \nRMarkdown document. become familiar R, may want learn\nYAML works can use customize \ndocument—turns quite lot can set YAML \ninteresting customizations final rendered document. book R\nMarkdown: Definitive Guide,337 available free online, \nsections discussing YAML choices HTML pdf output, \nhttps://bookdown.org/yihui/rmarkdown/html-document.html \nhttps://bookdown.org/yihui/rmarkdown/pdf-document.html, respectively. \nalso talk Yihui Xie, creator RMarkdown, gave topic \npast Posit conference, available \nhttps://rstudio.com/resources/rstudioconf-2017/customizing-extending-r-markdown/.","code":"---\ntitle: \"Laboratory report for example project\"\nauthor: \"Brooke Anderson\"\ndate: \"1/12/2020\"\noutput: word_document\n---"},{"path":"module19.html","id":"executable-code-in-rmarkdown-files","chapter":"Module 19 RMarkdown for creating reproducible data pre-processing protocols","heading":"19.4 Executable code in RMarkdown files","text":"module 18, described knitted documents use special markers\nindicate sections executable code start stop. RMarkdown,\nmarkers use indicate executable code look like :RMarkdown, following combination indicates\nstart executable code:```{r}combination indicates end executable code (\nwords start regular text):```example , shown basic\nversion markup character combination used specify start \nexecutable code (```{r}). character combination can expanded,\nhowever, include specifications want code section\nfollowing run, well want output shown. example,\nuse following indications specify code run,\ncode printed final document, specifying\necho = FALSE, well created figure centered \npage, specifying fig.align = \"center\":```{r echo = FALSE, fig.align = \"center\"}numerous options can used specify code run.\nspecifications called\nchunk options, specify special character combination\nmark start executable code. example, can specify \ncode printed document, executed, setting \neval parameter FALSE ```{r eval = FALSE} marker \nstart code section.chunk options also include echo, can used specify whether \nprint code code chunk document rendered. \ndocuments, useful print code executed, \ndocuments may want printed. example, pre-processing\nprotocol, aiming show others pre-processing \ndone. case, helpful print code, \nfuture researchers read protocol can clearly see step. \ncontrast, using RMarkdown create report article \nfocused results analysis, may make sense instead hide\ncode final document.part code options, can also specify whether messages warnings\ncreated running code included document output, \nnumber code chunk options specify tables figures\nrendered code shown. details possible options\ncan specified code evaluated within executable chunk \ncode, can refer RMarkdown cheat sheet available \nhttps://rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdfRStudio functionality useful working code \nRMarkdown documents. Within code chuck buttons can used \ntest code chunk executable code. One green right arrow\nkey right top code chunk, highlighted Figure\n19.5. button run code \nchunk show output output field open directly \ncode chunk. functionality allows explore code \ndocument build , rather waiting ready render \nentire document. button directly left button, looks\nlike upward-pointing arrow rectangle, execute code comes\nchunk document. can helpful making sure \nset environment run particular chunk code.","code":"```r{}\nmy_object <- c(1, 2, 3)\n```"},{"path":"module19.html","id":"more-advanced-rmarkdown-functionality","chapter":"Module 19 RMarkdown for creating reproducible data pre-processing protocols","heading":"19.5 More advanced RMarkdown functionality","text":"details resources covered far focus basics \nRMarkdown. can get lot done just basics. However, RMarkdown\nsystem rich allows complex functionality beyond basics. \nsubsection, highlight just ways RMarkdown can used\nadvanced way. Since topic broad, focus elements\nfound particularly useful biomedical researchers \nbecome advanced RMarkdown users. part, go \nextensive detail use advanced features module,\ninstead point resources can learn ready. \njust learning RMarkdown, point helpful just know \nadvanced features available, can come back explore\nbecome familiar basics. However, provide \ndetails one advanced element find particularly useful creating data\npre-processing protocols: including bibliographical references.","code":""},{"path":"module19.html","id":"including-bibliographical-references","chapter":"Module 19 RMarkdown for creating reproducible data pre-processing protocols","heading":"19.5.1 Including bibliographical references","text":"include references RMarkdown documents, can use something called\nBibTeX. software system free open-source works \nconcert LaTeX markup languages. allows save bibliographical information plain\ntext file—following certain rules—reference information \ndocument. way, can serve role bibliographical reference\nmanager (like Endnote Mendeley) free keeping information\nplain text files, can easily tracked version control like\nGit. using BibTeX RMarkdown, can include bibliographical references\ndocuments create, RMarkdown handle creation \nreferences section numbering documents within text.use BibTeX add references RMarkdown document, ’ll need take\nthree steps:Create plain text file listings \nreferences (BibTeX file). Save file \nextension .bib. listings need follow special format, \n’ll describe just minute.RMarkdown document, include filepath\nBibTeX file, RMarkdown able find bibliographical\nlistings.text RMarkdown file, include key special character\ncombination anytime want reference paper. referencing also\nfollows special format, ’ll describe .Let’s look steps bit detail. first step \ncreate plain text file listing documents \n’d like cite. plain text document saved file\nextension .bib (example, “mybibliography.bib”), listings \ndocument file must follow specific rules.Let’s take look one explore rules. ’s example BibTeX\nlisting scientific article:can see listing article, starts \nkeyword @article. BibTeX can record number different types documents,\nincluding articles, books, websites. start specifying document\ntype different types documents need include different elements\nlistings. example, website include date \nlast accessed, article typically .Within curly brackets listing shown , key-value\npairs—elements type value given keyword (e.g.,\ntitle), value element given equals sign.\nexample, specify journal article published, \nlisting journal={Scientific Reports}. Finally, listing key\nuse identify listing main text. case,\nlisting given key fox2020, combines first author \npublication year. can use keys like items bibliography,\nlong different every listing, computer can\nidentify bibliographical listing referring use key.format may seem overwhelming, fortunately rarely \ncreate listings hand. Instead, can get directly Google\nScholar. , look paper Google Scholar (Figure\n19.6). see , look small quotation mark\nsymbol bottom article listing (shown top red arrow \nFigure 19.6). click , open pop-\ncitation article. bottom pop-link \nsays “BibTeX” (bottom red arrow Figure 19.6). click\n, take page gives full BibTex listing \narticle, can just copy paste plain text BibTeX file.\nFigure 19.6: Example using Google Scholar get bibliographical information BibTeX file. look article Google Scholar, option (quotation mark icon article listing) open pop-window bibliographical information. bottom pop-box, can click ‘BibTeX’ get plain text version BibTeX entry article. can copy paste BibTeX file.\nplain text BibTeX file, tell computer \nfind including path YAML. example, created BibTeX\nfile called “mybibliography.tex” saved directory \nRMarkdown document, use following indicate file \nRMarkdown document:shows YAML document—part goes beginning \nRMarkdown document gives metadata overall instructions \ndocument. example, ’ve added extra line: bibliography: mybibliography.bib. says ’d like link BibTeX file \ndocument rendered, well find file (file named\n“mybibliography.bib” directory RMarkdown file).Now created BibTeX file told RMarkdown file \nfind , can connect two. write RMarkdown file, can\nrefer BibTeX listings using key set \ndocument. example, wanted reference Fox et al. paper \nused example listing , use key set \nlisting, fox2020.\nfollow special convention reference key: ’ll use \n@ symbol directly followed key. Typically, surround\nsquare brackets. Therefore, reference Fox et al. paper,\n’d use [@fox2021].’s might look practice. write RMarkdown document:output rendering RMarkdown document ’d get:``technique follows earlier work (Fox et al. 2020).”full paper details included end document, reference\nsection.","code":"@article{fox2020,\n  title={Cyto-feature engineering: A pipeline for flow cytometry \n    analysis to uncover immune populations and associations with \n    disease},\n  author={Fox, Amy and Dutt, Taru S and Karger, Burton and Rojas, \n    Mauricio and Obreg{\\'o}n-Henao, Andr{\\'e}s and \n    Anderson, G Brooke and Henao-Tamayo, Marcela},\n  journal={Scientific Reports},\n  volume={10},\n  number={1},\n  pages={1--12},\n  year={2020}\n}  ---\n  title: \"Reproducible Research with R\"\n  author: \"Brooke Anderson\"\n  date: \"1/25/2021\"\n  output: beamer_presentation\n  bibliography: mybibliography.bib\n  ---This technique follows earlier work [@fox2020]."},{"path":"module19.html","id":"other-advanced-rmarkdown-functionality","chapter":"Module 19 RMarkdown for creating reproducible data pre-processing protocols","heading":"19.5.2 Other advanced RMarkdown functionality","text":"number advanced things can RMarkdown, \nmastered basics. First, can use RMarkdown build different\ntypes documents, just reports Word, PDF, HTML. example, can\nuse bookdown package create entire online print books using \nRMarkdown framework. book modules created using system.\ncan also create websites web dashboards, using blogdown \nflexdashboard packages, repectively. blogdown package allows \ncreate professionally-styled websites, including blog sections can\ninclude R code results. Figure 19.7 gives example \nwebsite created using blogdown—can see full website\n’d like check \nfeatures framework provides. flexdashboard package lets\ncreate “dashboards” data, similar dashboards many public\nhealth departments using COVID-19 pandemic share case numbers \nspecific counties states.\nFigure 19.7: Example website created using blogdown, leveraging RMarkdown framework.\nRMarkdown, can also create reports customized \ndefault style explored . First, can create templates add\ncustomized styling document. fact, many journals created\njournal-specific templates can use RMarkdown. templates,\ncan write research results reproducible way, using RMarkdown,\nsubmit resulting document directly journal, correct\nformat. example first page article created RMarkdown using\none article templates shown Figure 19.8.338 rticles package R provides templates several\ndifferent journal families.\nFigure 19.8: Example manuscript written RMarkdown using templat. figure shows first page article written submission PLoS Computation Biology, written RMarkdown using PLoS template rticles package (Wendt Anderson 2022).\nRMarkdown also features make easy run code \ncomputationally expensive code written another programming\nlanguage. code takes long time run, options RMarkdown \ncache results—, run code render document,\nre-run later renderings inputs changed. RMarkdown\nsaving intermediate results, well using system \nremember pieces code depend earlier code. computationally\nexpensive code, can big time saver, although can also use storage,\nsince saving results. include code languages R, can\nchange something called engine code chunk. Essentially, \nlanguage computer use run code chunk. can change\nengine certain chunks code run using Python, Julia, \nlanguages specifying engine ’d like use marker document\nindicates start piece executable code. Earlier module,\nshowed executable code normally introduced RMarkdown \n```{r}. r string specifying R engine \nused run code.Finally, RMarkdown allows create customized formatting, move \nadvanced ways use framework.\nmentioned earlier, Markdown fairly simple markup language. Occasionally,\nsimplicity means might able create fancier formatting\nmight desire. method allows work around \nconstraint RMarkdown.RMarkdown documents, need complex formatting, can shift\ncomplex markup language part document. Markup languages\nlike LaTeX HTML much expressive Markdown, many \nformatting choices possible. However,\ndownside—include formatting specified \ncomplex markup languages, limit output formats can\nrender document . example, include LaTeX formatting within\nRMarkdown document, must output document PDF, \ninclude HTML, must output HTML file. Conversely, stick \nsimpler formatting available Markdown syntax, can easily\nswitch output format document among several choices.One area customization particularly useful simple implement \ncustomized tables. Markdown syntax can create simple tables, \nallow creation complex tables. R package called\nkableExtra allows create attractive complex tables\nRMarkdown documents.\npackage leverages power underlying markup languages, rather\nsimpler Markdown language.\n\nkableExtra package extensively documented two vignettes come\npackage, one output pdf\n(https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_pdf.pdf)\none HTML\n(https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html).","code":""},{"path":"module19.html","id":"learning-more-about-rmarkdown.","chapter":"Module 19 RMarkdown for creating reproducible data pre-processing protocols","heading":"19.6 Learning more about RMarkdown.","text":"learn RMarkdown, can explore number excellent resources.\ncomprehensive shared Posit, RMarkdown’s developer\nmaintainer, Yihui Xie, works. resources freely available\nonline, also available buy print books, prefer \nformat.First, check online tutorials provided Posit \nRMarkdown. available Posit’s RMarkdown page:\nhttps://rmarkdown.rstudio.com/. page’s “Getting Started” section\n(https://rmarkdown.rstudio.com/lesson-1.html) provides nice introduction \ncan work try RMarkdown practice overview provided \nlast subsection module. “Articles” section\n(https://rmarkdown.rstudio.com/articles.html) provides number \ndocuments help learn RMarkdown. Posit’s RMarkdown page also includes \n“Gallery” (https://rmarkdown.rstudio.com/gallery.html). resource allows \nbrowse example documents, can get visual idea \nmight want create access example code similar document.\ngreat resource exploring variety documents can\ncreate using RMarkdown.go deeply RMarkdown, two online books \nteam available online. first R Markdown: Definitive\nGuide Yihui Xie, J. J. Allaire, Garrett Grolemund.339 book \navailable free online https://bookdown.org/yihui/rmarkdown/. moves \nbasics advanced functionality can implement \nRMarkdown, including several topics highlight later \nsubsection.second online book explore team R Markdown Cookbook, \nYihui Xie, Christophe Dervieux, Emily Riederer.340 book available\nfree online https://bookdown.org/yihui/rmarkdown-cookbook/. book \nhelpful resource dipping specific section want learn \nachieve specific task. Just like regular cookbook recipes \ncan explore use one time, book require comprehensive\nend--end read, instead provides “recipes” advice instructions \nspecific things. example, want figure align \nfigure create center page, rather left, can\nfind “recipe” book .","code":""},{"path":"module20.html","id":"module20","chapter":"Module 20 Example: Creating a reproducible data pre-processing protocol","heading":"Module 20 Example: Creating a reproducible data pre-processing protocol","text":"walk example creating reproducible data pre-processing\nprotocol. example, look pre-process analyze data\ncollected laboratory estimate bacterial load samples.\ndata come plating samples immunological experiment serial\ndilutions, using data experiment lead one coauthors. data\npre-processing protocol created using RMarkdown allows efficient,\ntransparent, reproducible pre-processing plating data experiments\nresearch group. go RMarkdown techniques can \napplied develop type data pre-processing protocol laboratory\nresearch group.Objectives. module, able :Explain reproducible data pre-processing protocol can developed \nreal research projectUnderstand design implement data pre-processing protocol \nreplace manual point--click data pre-processing toolsApply techniques RMarkdown develop reproducible data\npre-processing protocols","code":""},{"path":"module20.html","id":"introduction-and-example-data","chapter":"Module 20 Example: Creating a reproducible data pre-processing protocol","heading":"20.1 Introduction and example data","text":"module, ’ll provide advice example can use \ntools knitted documents create reproducible data pre-processing\nprotocol. module builds ideas techniques introduced\nlast two modules (modules 18 19), help put practical use \ndata pre-processing repeatedly research data \nlaboratory.module, use example common pre-processing task \nimmunological research: estimating bacterial load samples plating \ndifferent dilutions. type experiment, laboratory researcher\nplates samples several dillutions, identifies good dilution \ncounting colony-forming units (CFUs), back-calculates estimated\nbacterial load original sample based colonies counted “good”\ndilution. experimental technique dates back late 1800s, Robert\nKoch, continues widely used microbiology research applications\ntoday.341 data originally experiment one\nauthors’ laboratory also available example data R\npackage called bactcountr, currently development \nhttps://github.com/aef1004/bactcountr/tree/master/data.data representative data often collected immunological research.\nexample, may testing drugs infectious bacteria\nwant know successful different drugs limiting bacterial load.\nrun experiment samples animals treated different drugs\ncontrol want know much viable (.e., replicating)\nbacteria samples.can find plating sample different dilutions \ncounting colony-forming units (CFUs) cultured plate.\nput sample plate medium can grow give \ntime grow. idea individual bacteria original sample end\nrandomly around surface plate, viable (able \nreproduce) form new colony , , ’ll able see.get good estimate bactieral load process, need count\nCFUs “countable” plate—one “just right” dilution (\ntypically won’t know dilution sample plating).\nhigh dilution (.e., one viable bacteria),\nrandomness play big role CFU count, ’ll estimate \noriginal bacterial load variability. low dilution (.e., one\nlots viable bacteria), difficult identify separate\ncolonies, may complete resources. translate diluted\nconcentration original concentration, can back-calculation,\nincorporating number colonies counted dilution \ndilute sample . therefore pre-processing required (although\nfairly simple) prepare data collected get estimate \nbacterial load original sample. estimate bacterial load can used \nstatistical testing combined experimental data explore\nquestions like whether candidate vaccine reduces bacterial load research\nanimal challenged pathogen.use example common data pre-processing task show \ncreate reproducible pre-processing protocol module. like,\ncan access components example pre-processing protocol \nfollow along, re-rendering computer. example data\navailable csv file, downloadable\n.\ncan open file using spreadsheet software, look directly \nRStudio. final pre-processing protocol data can also \ndownloaded, including original RMarkdown\nfile\noutput PDF\ndocument.\nThroughout module, walk elements document, \nprovide example explain process developing data pre-processing\nmodules common tasks research group. recommend go ahead \nread output PDF document, get idea example protocol \n’re creating.example intentionally simple, allow basic introduction \nprocess using pre-processing tasks familiar many laboratory-based\nscientists easy explain anyone used plating experimental\nwork. However, general process can also used create\npre-processing protocols data much larger complex \npre-processing pipelines much involved.\nexample, process used create data pre-processing protocols\nautomated gating flow cytometry data pre-processing data\ncollected single cell RNA sequencing.","code":""},{"path":"module20.html","id":"advice-on-designing-a-pre-processing-protocol","chapter":"Module 20 Example: Creating a reproducible data pre-processing protocol","heading":"20.2 Advice on designing a pre-processing protocol","text":"write protocol knitted document, decide \ncontent include protocol. section provides tips design\nprocess. section, ’ll describe key steps designing \ndata pre-processing protocol:Defining input output data protocol;Setting project directory protocol;Outlining key tasks pre-processing input data; andAdding code pre-processing.illustrate design steps using example protocol \npre-processing plating data.","code":""},{"path":"module20.html","id":"defining-input-and-output-data-for-the-protocol","chapter":"Module 20 Example: Creating a reproducible data pre-processing protocol","heading":"20.2.1 Defining input and output data for the protocol","text":"first step designing data pre-processing protocol decide \nstarting point protocol (data input) ending point (data\noutput). may make sense design separate protocol major type \ndata collect research laboratory. input data \nprotocol, design, might data output specific\ntype equipment (e.g., flow cytometer) certain type sample \nmeasurement (e.g., metabolomics run mass spectrometer), even \nfairly simple type data (e.g., CFUs plating data, used example\nprotocol module). example, say working three types \ndata research experiment: data flow cytometer, metabolomics data\nmeasured mass spectrometer, bacterial load data measured plating\ndata counting colony forming units (CFUs). case, may want \ncreate three pre-processing protocols: one flow data, one \nmetabolomics data, one CFU data. protocols modular can\nre-used experiments use three types data.example dataset, can begin create pre-processing protocol\ncollect research data new experiment. \nformat initial data similar format anticipate \ndata, can create code explanations key steps \npre-processing type data. Often, able adapt \nRMarkdown document change inputting example data inputting\nexperimental data minimal complications, data comes .\nthinking researching data pre-processing options data\ncollected, can save time analyzing presenting project results\n’ve completed experimental data collection project. ,\nexample dataset, can get good approximation format \noutput data pre-processing steps. allow begin\nplanning analysis visualization use combine \ndifferent types data experiment use investigate important\nresearch hypotheses. , data follow standardized formats across steps \nprocess, often easy adapt code protocol input\nnew dataset created, without major changes code developed\nexample dataset.pre-processing protocols types data might complex,\nothers might fairly simple. However, still worthwhile develop \nprotocol even simple pre-processing tasks, allows pass along\ndetails pre-processing data might become “common\nsense” longer-tenured members research group. example, \npre-processing tasks example protocol fairly simple. protocol\ninputs data collected plain-text delimited file (csv file, \nexample). Within protocol, steps convert initial measurements\nplating different dilutions estimate bacterial load \nsample. also sections protocol exploratory data\nanalysis, allow quality assessment control collected data \npart pre-processing. output protocol simple data object (\ndataframe, example) bacterial load original sample.\ndata now ready used tables figures research report\nmanuscript, well explore associations experimental design\ndetails (e.g., comparing bacterial load treated versus untreated animals) \nmerged types experimental data (e.g., comparing immune cell\npopulations, measured flow cytometry data, bacterial loads, \nmeasured plating counting CFUs).identified input data type use protocol, \nidentify example dataset laboratory can use create \nprotocol. dataset currently need pre-process, \ncase development protocol serve second purpose, allowing\ncomplete task time. However, may new set\ndata type currently need pre-process, case\ncan build protocol using dataset previous experiment \nlaboratory. case, may already record steps \nused pre-process data previously, can helpful starting\npoint draft thorough pre-processing protocol. may want \nselect example dataset already published getting ready\npublish, won’t feel awkard making data available people\npractice . don’t example dataset laboratory,\ncan explore example datasets already available, either data\nincluded existing R packages open repositories, including \nhosted national research institutions like NIH. case, \nsure cite source data include available information \nequipment used collect , including equipment settings used\ndata collected.example protocol module, want pre-process data \ncollected “hand” counting CFUs plates laboratory. counts\nrecorded plain text delimited file (csv file) using spreadsheet\nsoftware. spreadsheet set ensure data can easily converted\n“tidy” format, described module 3. first rows input\ndata look like :row represents number bacterial colonies counted plating \ncertain sample, sample represents one experimental animal several\nexperimental animals (replicates) considered experimental group.\nColumns included values experimental group sample\n(group), specific ID sample within experimental group\n(replicate, e.g., 2-mouse experimental group 2), \ncolony-forming units (CFUs) counted several dilutions. cell \nvalue “TNTC”, indicates CFUs numerous count \nsample dilution.identified input data type use protocol,\nwell selected example dataset type use create \nprotocol, can include section protocol\ndescribes input data, file format , \ncan read R pre-processing (Figure 20.1).\nFigure 20.1: Providing details input data pre-processing protocol. example data file type data input protocol, can add section provides code read data R. can also add code show first rows example dataset, well description data. figure shows examples elements can added RMarkdown file pre-processing protocol, associated elements final pdf protocol, using example protocol module.\ndata output, often makes sense plan data format \nappropriate data analysis merging types data collected\nexperiment. aim pre-processing get data \nformat collected format meaningful \ncombining types data experiment using statistical\nhypothesis testing.example pre-processing protocol, ultimately\noutput simple dataset, one row original samples. \nfirst rows output data :original sample, estimate CFUs Mycobacterium\ntuberculosis full spleen given (cfu_in_organ). data\ncan now merged data collected animal experiment.\nexample, joined data provide measures \nimmune cell populations animal, explore certain immune\ncells associated bacterial load. also joined \nexperimental information used hypothesis testing. example,\ndata merged table describes groups \ncontrols versus used certain vaccine, test \nconducted exploring evidence bacterial loads animals given \nvaccine lower control animals.","code":"## # A tibble: 6 × 6\n##   group replicate dilution_0 dilution_1 dilution_2 dilution_3\n##   <dbl> <chr>     <chr>      <chr>           <dbl>      <dbl>\n## 1     2 2-A       26         10                  0          0\n## 2     2 2-B       TNTC       52                 10          5\n## 3     2 2-C       0          0                   0          0\n## 4     3 3-A       0          0                   0          0\n## 5     3 3-B       TNTC       TNTC               30         10\n## 6     3 3-C       0          0                   0          0## # A tibble: 6 × 3\n##   group replicate cfu_in_organ\n##   <dbl> <chr>            <dbl>\n## 1     2 2-A                260\n## 2     2 2-B               2500\n## 3     2 2-C                  0\n## 4     3 3-A                  0\n## 5     3 3-B               7500\n## 6     3 3-C                  0"},{"path":"module20.html","id":"setting-up-a-project-directory-for-the-protocol","chapter":"Module 20 Example: Creating a reproducible data pre-processing protocol","heading":"20.2.2 Setting up a project directory for the protocol","text":"decided input output data formats, next want\nset file directory storing inputs needed protocol.\ncan include project files protocol RStudio Project (see\nmodule 6) post either publicly privately GitHub (see modules\n9–11). creates “packet” everything reader needs use \nrecreate —can download whole GitHub repository \nnice project directory computer everything need try\nprotocol.Part design protocol involves deciding files \nincluded project directory. Figure 20.2\nprovides example initial files included project directory \nexample protocol module. left side figure shows \nfiles initially included, left side shows files \nproject code protocol run.Generally, project directory include file input\nexample data, whatever file format usually collect type \ndata. also include RMarkdown file protocol written. \nplanning cite articles references, can include BibTeX\nfile, bibliographical information source plan cite (see module 19).\nFinally, like include photographs graphics, can include\nimage files project directory. Often, might want group \ntogether subdirectory project named something like “figures”.run RMarkdown file protocol, generate additional\nfiles project. Two typical files generate output\nfile protocol (example, output pdf file). Usually,\ncode protocol also result output data, pre-processed\nprotocol code written file used \nanalysis.\nFigure 20.2: Example files project directory data pre-processing protocol. left files initially included project directory example protocol module. include file input data (cfu_data.csv), BibTeX file bibliographical information references (example_bib.bib), RMarkdown file protocol (example_protocol.Rmd), subdirectory figures include protocol (figures). right shown directory code protocol RMarkdown document run, creates output pdf protocol (example_protocol.pdf) well output data (processed_cfu_estimates.csv).\n","code":""},{"path":"module20.html","id":"outlining-key-tasks-in-pre-processing-the-input-data","chapter":"Module 20 Example: Creating a reproducible data pre-processing protocol","heading":"20.2.3 Outlining key tasks in pre-processing the input data","text":"next step outline key tasks involved moving \ndata input desired data output. plating data using \nexample, key tasks included pre-processing protocol :Read data RExplore data perform quality checksIdentify “good” dilution sample—one \ncountable plateEstimate bacterial load original sample based CFUs counted\ndilutionOutput data estimated bacterial load sampleOnce basic design, can set RMarkdown file \npre-processing protocol include separate section task, well \n“Overview” section beginning describe overall protocol, \ndata pre-processed, laboratory procedures used collect \ndata. RMarkdown, can create first-level section headers putting \ntext header line beginning line #, followed\nspace. include blank line line \nheader text. Figure 20.3 shows done \nexample protocol module, showing text plain text RMarkdown\nfile protocol align section headers final pdf output \nprotocol.\nFigure 20.3: Dividing RMarkdown data pre-processing protocol sections. shows example creating section headers data pre-processing protocol created RMarkdown, showing section headers example pre-procotcol module.\n","code":""},{"path":"module20.html","id":"adding-code-for-pre-processing","chapter":"Module 20 Example: Creating a reproducible data pre-processing protocol","heading":"20.2.4 Adding code for pre-processing","text":"many steps, likely code—can start drafting \ncode—required step.\nRMarkdown, can test code write . insert piece\nexecutable code within special section, separated regular\ntext special characters, described previous modules.pre-processing steps straightforward (e.g., calculating \ndilution factor example module, requires simple mathematical\noperations), can directly write code required step.\npre-processing steps, however, algorithm may bit \ncomplex. example, complex algorithms developed steps like\npeak identification alignment required \npre-processing data mass spectrometer.complex tasks, can start explore available R packages \nperforming task. thousands packages available extend \nbasic functionality R, providing code implementations algorithms \nvariety scientific fields. Many R packages relevant biological\ndata—especially high-throughput biological data—available \nrepository called Bioconductor. packages open-source (can\nexplore code want ) free. can use vignettes package\nmanuals Bioconductor packages identify different functions can\nuse pre-processing steps. identified function \ntask, can use helpfile function see use . help\ndocumentation allow determine function’s parameters \nchoices can select .can add piece code RMarkdown version protocol using\nstandard method RMarkdown (module 11). Figure 20.4\nshows example example protocol module. , using\ncode help identify “good” dilution counting CFUs sample. \ncode included executable code chunk, run time\nprotocol rendered. Code comments included code provide\nfiner-level details code .\nFigure 20.4: Example including code data pre-processing protocol created RMarkdown. figure shows code can included RMarkdown file pre-processing protocol (right), corresponding output final pdf protocol (left), code identify ‘good’ dilution counting CFUs sample. Code comments included provide finer-level details code.\nstep protocol, can also include potential problems\nmight come specific instances data get \nfuture experiments. can help adapt code protocol \nthoughtful ways apply future new data collected\nnew studies projects.","code":""},{"path":"module20.html","id":"writing-data-pre-processing-protocols","chapter":"Module 20 Example: Creating a reproducible data pre-processing protocol","heading":"20.3 Writing data pre-processing protocols","text":"Now planned key components pre-processing protocol,\ncan use RMarkdown’s functionality flesh full pre-processing\nprotocol. gives chance move beyond simple code script, \ninstead include thorough descriptions ’re step \n’re . can also include discussions potential limitations\napproach taking pre-processing, well areas\nresearch groups might use different approach. details can\nhelp time write Methods section paper describing \nresults experiment using data. can also help research\ngroup identify pre-processing choices might differ research\ngroups, opens opportunity perform sensitivity analyses regarding\npre-processing choices ensure final conclusions robust\nacross multiple reasonable pre-processing approaches.Protocols common wet lab techniques, provide “recipe” \nensures consistency reproducibility processes. Computational tasks,\nincluding data pre-processing, can also standardized creation \nuse protocol research group. code scripts becoming \ncommon means recording data pre-processing steps, often \nclear traditional protocol, particular terms providing \nthorough description done step \ndone way. Data pre-processing protocols can provide thorough\ndescriptions, creating RMarkdown similar types\n“knitted” documents (modules 18 19), can combine executable code\nused pre-process data extensive documentation. \nadvantage, creation protocols ensure research\ngroup thought carefully step process, rather \nrelying cobbling together bits pieces code ’ve found don’t\nfully understand. Just creation research protocol \nclinical trial requires careful consideration step ultimate\ntrial,342 creation data pre-processing protocols ensure\nstep process carefully considered, helps \nensure step process conducted carefully \nsteps taken designing experiment whole wet lab technique\nconducted experiment.data pre-processing protocol, sense use , essentially \nannotated recipe step preparing data initial, “raw”\nstate output laboratory equipment (collected hand) \nstate useful answering important research questions. exact\nimplementation step given code can re-used adapted\nnew data similar format. However, code script often enough\nhelpfully understand, share, collaborate process. Instead, ’s\ncritical also include descriptions written humans humans. \nannotations can include descriptions code certain parameters \nstandardized algorithms code. can also used justify\nchoices, link characteristics data equipment\nexperiment well scientific principles underlie \nchoices. Protocols like critical allow standardize \nprocess use across many samples one experiment, across different\nexperiments projects research laboratory, even across different\nresearch laboratories.begin adding text pre-processing protocol, keep \nmind general aims. First, good protocol provides adequate detail \nanother researcher can fully reproduce procedure.343 \nprotocol trial wet lab technique, means protocol \nallow another researcher reproduce process get results \ncomparable results;344 data pre-processing\nprotocol, protocol must include adequate details another researcher,\nprovided start data, gets identical results (short \npre-processing steps include element sampling random-number\ngeneration, e.g., Monte Carlo methods). idea—able exactly\nre-create computational results earlier project—referred \ncomputational reproducibility considered key component \nensuring research fully reproducible.creating data pre-processing protocol knitted document\nusing tool like RMarkdown (modules 18 19), can ensure protocol \ncomputationally reproducible. RMarkdown document, include code\nexamples executable code—means code run every time \nrender document. therefore “checking” code every time \nrun . last step pre-processing protocol, output \ncopy pre-processed data use analysis \nproject. can use functions R output plain text format,\nexample comma-separated delimited file (modules 4 5). time render\nprotocol, re-write output file, provides assurance\ncode protocol can used reproduce output data (since\n’s created form data).Figure 20.5 provides example example protocol\nmodule. RMarkdown file protocol includes code write \nfinal, pre-processed data comma-separated plain text file called\n“processed_cfu_estimates.csv”. code writes output file \ndirectory ’ve saved RMarkdown file. time RMarkdown file \nrendered create pdf version protocol, input data \npre-processed scratch, using code throughout protocol, \nfile overwritten data generated. guarantees code\nprotocol can used anyone—researchers—reproduce\nfinal data protocol, guarantees data \ncomputationally reproducible.\nFigure 20.5: Example using code pre-processing protocol output final, pre-processed data used analysis research project. example comes example protocol module, showing executable code included RMarkdown file protocol (right) code included final pdf protocol. Outputting pre-processed data plain text file last step protocol helps ensure computational reproducibility step working experimental data.\ndata pre-processing protocol, show code use implement\nchoice also explain clearly text made choice \nalternatives considered data characteristics different.\nWrite explaining new research group member (\nfuture self) think step pre-processing, ’re\nway , code used way. \nalso include references justify choices \navailable—include using BibTeX (module 19). , make much\neasier write Methods section papers report \ndata pre-processed, ’ll already draft information \npre-processing methods protocol.Good protocols include (data pre-processing protocols, \ncode), also step taken. includes explanations higher-level\n(.e., larger question asked) also fine level, \nstep process. protocol include background, \naims work, hypotheses tested, materials methods, methods \ndata collection equipment analyze samples.345This step documentation explanation important creating \nuseful data pre-processing protocol. Yes, code allows someone else \nreplicate . However, , familiar \nsoftware program, including extension packages include, can\n“read” code directly understand ’s . , even \nunderstand code well create , unlikely \nstay level comprehension future, tasks \nchallenges take brain space. Explaining humans, text \naugments accompanies code, also important function names \nparameter names code often easy decipher. excellent\nprogrammers can sometimes create functions clear transparent names,\neasy translate determine task , difficult \nsoftware development rare practice. Human annotations, written \nhumans, critical ensure steps clear \nothers future revisit done data \nplan future data.process writing protocol way forces think \nstep process, certain way (include parameters choose\ncertain functions pipeline code), include justifications \nliterature reasoning. done well, allow quickly\nthoroughly write associated sections Methods research reports \nmanuscripts help answer questions challenges reviewers. Writing\nprotocol also help identify steps uncertain \nproceed choices make customizing analysis research\ndata. areas can search deeply literature \nunderstand implications certain choices , needed, contact \nresearchers developed maintained associated software packages get\nadvice.example, example protocol module explains pre-process\ndata collected counting CFUs plating serial dilutions samples. One\nsteps pre-processing identify dilution sample \n“countable” plate. protocol includes explanation \nimportant identify dilution countable plate also gives\nrules used pick dilution sample, including \ncode implements rules. allows protocol provide research\ngroup members logic behind pre-processing, can adapt\nneeded future experiments. example, count range CFUs used \nprotocol find good dilution quarter typically\nsuggested range process, experiment plated\nsample quarter plate, rather using full plate. \nexplaining reasoning, future protocol adapted \nusing full plate rather quarter plate sample.One tool RMarkdown helpful process built-\nreferencing system. previous module, showed can include\nbibliographical references RMarkdown file. write protocol\nwithin RMarkdown, can include references way provide background\nsupport explain conducting step \npre-processing. Figure 20.6 shows example \nelements use , showing element example protocol \nmodule.\nFigure 20.6: Including references data pre-processing protocol created RMarkdown. RMarkdown built-referencing system can use, based BibTeX system LaTeX. figure shows examples example protocol module elements used referencing. create BibTeX file information reference, use key reference within text cite reference. cited references printed end document; can chose header want reference section RMarkdown file (‘References’ example). YAML RMarkdown file, specify path BibTeX file (‘bibliography:’ key), can linked RMarkdown file rendered.\nhelpful tools RMarkdown tools creating equations tables. \ndescribed module 19, RMarkdown includes number formatting\ntools. can create simple tables basic formatting, complex\ntables using add-packages like kableExtra. Math can typeset using\nconventions developed LaTeX mark-language. module 19\nprovided advice links resources using types tools. Figure\n20.7 gives example use within example\nprotocol module.\nFigure 20.7: Example including tables equations RMarkdown data pre-processing protocol.\ncan also include figures, either figures created R outside figure files.\nfigures created code RMarkdown document automatically\nincluded protocol. graphics, can include image files\n(e.g., png jpeg files) using include_graphics function \nknitr package. can use options code chunk options specify\nsize figure document include figure caption. \nfigures automatically numbered order appear protocol.Figure 20.8 shows example external figure\nfiles included example protocol. case, functionality\nallowed us include overview graphic created PowerPoint \nsaved image well photograph taken member research\ngroup.\nFigure 20.8: Example including figures image files RMarkdown data pre-processing protocol.\nFinally, can try even complex functionality RMarkdown \ncontinue build data pre-processing protocols research group. Figure\n20.9 shows example using R code within YAML \nexample protocol module; allows us include “Last edited” date\nupdated day’s date time protocol re-rendered.\nFigure 20.9: Example using advanced RMarkdown functionality within data pre-processing protocol. example, R code incorporated YAML document include date document last rendered, marking pdf output Last edited date protocol.\n","code":""},{"path":"module20.html","id":"applied-exercise-1","chapter":"Module 20 Example: Creating a reproducible data pre-processing protocol","heading":"20.4 Applied exercise","text":"wrap module, try downloading source file output example\ndata pre-processing protocol. , can find source code (RMarkdown file)\noutput file . like try re-running file, can get additional files\n’ll need (original data file, figure files, etc.) . See \ncan compare elements RMarkdown file output produce \nPDF file. Read descriptions protocol. think recreate\nprocess laboratory ran new experiment involved plating samples \nestimate bacterial load?","code":""}]
