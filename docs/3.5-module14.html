<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="3.5 Simplify scripted pre-processing through R’s ‘tidyverse’ tools | Improving the Reproducibility of Experimental Data Recording and Pre-Processing" />
<meta property="og:type" content="book" />

<meta property="og:description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing." />
<meta name="github-repo" content="rstudio/bookdown-demo" />

<meta name="author" content="Brooke Anderson, Michael Lyons, Mercedes Gonzalez-Juarrero, Marcela Henao-Tamayo, and Gregory Robertson" />



<meta name="description" content="Online book with modules for improving the reproducibility of experimental data recording and preprocessing.">

<title>3.5 Simplify scripted pre-processing through R’s ‘tidyverse’ tools | Improving the Reproducibility of Experimental Data Recording and Pre-Processing</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="toc.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#rigor-and-reproducibility-in-computation" id="toc-rigor-and-reproducibility-in-computation"><span class="toc-section-number">1</span> Rigor and reproducibility in computation</a></li>
<li><a href="2-experimental-data-recording.html#experimental-data-recording" id="toc-experimental-data-recording"><span class="toc-section-number">2</span> Experimental Data Recording</a></li>
<li><a href="3-experimental-data-preprocessing.html#experimental-data-preprocessing" id="toc-experimental-data-preprocessing"><span class="toc-section-number">3</span> Experimental Data Preprocessing</a></li>
<li><a href="4-references.html#references" id="toc-references"><span class="toc-section-number">4</span> References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="module14" class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Simplify scripted pre-processing through R’s ‘tidyverse’ tools</h2>
<p>The R programming language now includes a collection of ‘tidyverse’ extension
packages that enable user-friendly yet powerful work with experimental data,
including pre-processing and exploratory visualizations. The principle behind
the ‘tidyverse’ is that a collection of simple, general tools can be joined
together to solve complex problems, as long as a consistent format is used for
the input and output of each tool (the ‘tidy’ data format taught in other
modules).</p>
<p>Once data are in the “tidy” data format, you can create a pipeline of code that
uses small tools, each of which does one simple thing, to work with the data.
This work can include cleaning the data, adding values that are functions of the
original values for each observation (e.g., adding a column with BMI based on
values for each observation on height and weight), applying statistical models to
test hypotheses, summarizing data to create tables, and visualizing the data.</p>
<p>In this module, we will explain why this ‘tidyverse’ system is so
powerful and how it can be leveraged within biomedical research, especially for
reproducibly pre-processing experimental data.</p>
<p><strong>Objectives.</strong> After this module, the trainee will be able to:</p>
<ul>
<li>Define R’s ‘tidyverse’ system</li>
<li>Explain how the ‘tidyverse’ collection of packages can be both user-friendly
and powerful in solving many complex tasks with data</li>
<li>Describe the difference between base R and R’s ‘tidyverse’.</li>
</ul>
<div id="an-overview-of-the-tidyverse-approach" class="section level3" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> An overview of the “tidyverse” approach</h3>
<p>The “tidyverse” approach is an approach to using R that has grown enormously
in popularity in recent years. Most R courses and workshops for beginning
programmers are now structured around this approach. It provides a powerful
yet flexible approach for working with data in R, and it one of the easier
ways to start learning R. In this section, we’ll cover the philosophy that
provides a framework for this approach. In the following sections of the
modules, we’ll go deeper into specific tools under this approach that can
be used for common data preprocessing tasks when working with biomedical
data, as well as provide information on more resources that can be used to
continue learning this approach.</p>
<p>The term “elegance” often captures styles and approaches that are beautiful and
functional without unneeded extras or complexity. Engineers and scientists
sometimes use this term to capture approaches that achieve a desired result with
minimal complexity and friction. A coding problem, for example, could be solved
by an average coder with a hundred lines of code that get the job done, but a
very good coder might be able to solve the same problem with five lines of code
that are easy to follow. The second approach would be applauded as the “elegant”
solution. In mathematics, similarly, proofs can be complex and unwieldy, or they
can be simple and elegant—this idea was beautifully captured by the Hungarian
mathematician Paul Erdos, who famously described very elegant mathematical proofs
as being from “The Book”—that is, God’s own version of the proof of the
mathematical idea.</p>
<blockquote>
<p>“Paul Erdos liked to talk about The Book, in which God maintains the perfect
proofs for mathematical theorems, following the dictum of G. H. Hardy that
there is no permanent place for ugly mathematics. Erdos also said that you
need not believe in God but, as a mathematician, you should believe in
The Book.” [Proofs from the Book, Third Edition, Preface]</p>
</blockquote>
<p>The “tidyverse” approach in R is elegant. It is powerful, and gives you immense
flexibility once you’ve gotten the hang of it, but it’s also so straightforward
that the basics can be quickly taught to and applied by beginning coders. It
focuses on keeping data in a simple, standard format called “tidy” dataframes.
By keeping data in this format while working with it, common tools can be applied
that work with the data at any stage of a “tidy” coding pipeline. These tools take
a “tidy” dataframe as their input, and they also output a “tidy” dataframe, with
whatever change the function implements applied. Because each of these “tidyverse”
tools input and output data in the same standard format, they can be strung together
in order you want. By contrast, when functions input and output data in different
object types, they can only be joined in a specified order, because you can only
apply certain functions to certain object types.</p>
<p>Since the “tidyverse” tools can be strung together in any order, they can be
used very flexibly to build up to do interesting tasks. The tidyverse tools
generally each do very small and simple things. For example, one function
(<code>select</code>) just limits the data to a subset of its original columns; another
(<code>mutate</code>) adds or changes values in columns of the dataset, while another
(<code>distinct</code>) limits the dataframe to remove any rows that are duplicates. These
small, simple steps can be combined together in different patterns to add up to
complex operations on the data, while keeping each step very simple and clear.
Since the data stays in a standard and simple object type, it is easy to check
in on your data at any stage, as the common visualization tools for this
approach (from the <code>ggplot2</code> package and its extensions) can be always be
applied to data stored in a tidy dataframe.</p>
<blockquote>
<p>“After they’ve been built in, mechanical fasteners make everything after that
easier. They allow for disassembly, reconfiguration, as well as replacement.”
<span class="citation">(Savage 2020)</span></p>
</blockquote>
<blockquote>
<p>“That’s the reason I prefer mechanical solutions. They can be undone. Whatever
I’m putting together can be pulled apart again without damaging the construction.
… it takes more engineering, more fiddling, and definitely more time. But the
trade-off is more options. And I want options. That’s the space I like to exist in
as a maker.” <span class="citation">(Savage 2020)</span></p>
</blockquote>
<blockquote>
<p>“Standards are for products what grammar is for language. People sometimes
criticize standards for making life a matter of routine rather than inspiration.
Some argue that standards hinder creativity and keep us slaves to the past.
But try imagining a world without standards. From tenderloin beef cuts to
the geometric design of highways, standards may diminish variety and
authenticity, but they improve efficiency. From street signs to nutrition
labels, standards provide a common language of reason. From Internet
protocols to MP3 audio formats, standards enable systems to work together.
From paper sizes … to George Laurer’s Universal Product Code, standards
offer the convenience of comparability.” <span class="citation">(Madhavan 2015)</span></p>
</blockquote>
<blockquote>
<p>“The lawmakers of the growing nation [Canada] were eager to establish
a coast-to-coast railway system within the decade. … Throughout his
surveys, Fleming relied to crude geometric calculations based on
longitude, as there was no uniform time across the regions. ‘There was
no system. Like the rail lines, the different times touched or overlapped
at 300 points in the country.’ … Even regionally, timekeeping was in
disarray. If it was 12:13 in Boston, it was 12:27 in Philadelphia and
12:32 in Buffalo. in 1832 the United States had about 229 miles of
railroads. By 1880, the country had increased its rail infrastructure
to close to 95,000 miles. To perserve the sanity of the train driver,
each railroad company began to maintain its own time. Clocks had up
to six dials, and train stations displayed the time in various
citis. A train going from Baltimore, Maryland, to Scranton, Pennsylvania,
in those days might follow Baltimore time, creating the danger of collisions
when trains operated on a single track.” <span class="citation">(Madhavan 2015)</span></p>
</blockquote>
<blockquote>
<p>“Simplicity is not about stripping features down to a bare minimum.
It’s about achieving elegance while maintaining performance.” <span class="citation">(Madhavan 2015)</span></p>
</blockquote>
<blockquote>
<p>“The essence of good technology is that it’s intuitive, and it evolves.
Ideally, you don’t even want to know it’s there.” <span class="citation">(Madhavan 2015)</span></p>
</blockquote>
<p><strong>Functions available through packages</strong></p>
<p>The tidyverse functions do not come with base R, but rather are available
through extensions to base R, commonly referred to as “packages”. Like base
R, these are all open-source and free. Many are available through a
repository called CRAN, and you can download them directly from R using the
<code>install.packages</code> function.</p>
<p>The heart of the tidyverse functions are available through an umbrella
package called “tidyverse”. This package includes a number of key tidyverse
packages (e.g., “dplyr”, “tidyr”, “stringr”, “forcats”, “ggplot2”) and allows you
to quickly install this set of packages on your computer. When you are coding
in R, you will then need to load the package in your R session, which you can
do using the <code>library</code> call (e.g., <code>library("tidyverse")</code>).</p>
<p>In addition to the packages that come with the umbrella “tidyverse” package,
there are numerous other packages that build on the tidyverse approach.
Some are created by the creator of the tidyverse approach (Hadley Wickham)
or others on his team, while others are created by other R programmers but
follow the standards of the tidyverse approach. An examples of one of
these extensions that is specifically created for working with biomedical data
is the <code>tidybulk</code> package <span class="citation">(Mangiola et al. 2021)</span>, for working with
transcriptomics data.</p>
<p><strong>Uses the same data structure throughout</strong></p>
<p>The centralizing principal of the tidyverse approach is the format in which data
is stored throughout “tidyverse” coding—the tidy dataframe. Briefly, you can
think of this format in two parts. First, there’s the R object type that the
data should be stored in—a basic “dataframe” object. The dataframe object type
is a very basic two-dimensional format for storing data in R. When you print it
out, it will remind you of looking at data in a spreadsheet. The two
dimensions—rows and columns—allow you to include data for one or more
observations, with different values that were measured for each.</p>
<p>The tidyverse approach hinges on using the same data structure throughout your
coding pipeline—specifically, the tidy dataframe structure (see module 2.3 for
more details on this structure). By insisting on the same data structure
throughout, this approach is able to offer small functions that can be chained
together to solve complex problems.</p>
<p>This idea rests on the idea of the power of modularity. You can think of this
in terms of children’s toys—building bricks like Legos are powerful because
they are modular, while a toy like a stuffed animal is not. Each individual
Lego is small and simple, and would be pretty boring by itself. However,
because the blocks can be combined in different ways, they can be used to
create very complex and interesting structures. By contrast, something that is
not modular, like a stuffed animal, always retains the same structure. While it
might be more interesting and complex to start with than a single Lego block,
it will not evolve or contribute to something more interesting.</p>
<p>This modularity works in the same way that it does for Lego bricks. Lego
bricks can be combined in interesting ways because they all take the same input
and give the same output—the shape of the tubes on the bottom of each brick
accept the shape of the studs at the top of each brick, so they can be
put together in essentially infinite combinations. The tidyverse approach
in R works in a similar way—the functions in this approach almost all
input data that are in a dataframe structure (or in a vector structure, for
functions that operate on columns in the dataframe) and they almost all output
data in the same structure that they input it. As a result, the functions can
be chained together in interesting ways, where the output of one function
can feed directly into the input of another.</p>
<blockquote>
<p>“A common observation is that more of the data scientist’s time is occupied
with data cleaning, manipulation, and ‘munging’ than it is with actual
statistical modeling (Rahm and Do, 2000; Dasu and Johnson, 2003). Thus, the
development of tools for manipulating and transforming data is necessary for
efficient and effective data analysis. One important choice for a data scientist
working in R is how data should be structured, particularly the choice of
dividing observations across rows, columns, and multiple tables. The concept of
‘tidy data,’ introduced by Wickham (2014a), offers a set of guidelines for
organizing data in order to facilitate statistical analysis and visualization. … This framework makes it easy for analysts to reshape, combine, group and otherwise manipulate data. Packages such as ggplot2, dplyr, and many built-in R modeling and plotting functions require the input to be in a tidy form, so keeping the data in this form allows multiple tools
to be used in sequence in a seamless analysis pipeline (Wickham, 2009; Wickham and Francois,
2014).”
<span class="citation">(D. Robinson 2014)</span></p>
</blockquote>
<p><strong>Small, simple tools</strong></p>
<p>Since the functions in the tidyverse approach are designed to work in a modular
way—in other words, to be combined in interesting ways to solve larger
problems, rather than solving a large problem with a single function—each of
the functions tends to be a small, simple tool. In other words, each function
tends to do one thing simply but well. This makes it fairly easy to start
learning the tidyverse approach, as each of the functions that you learn as you
begin does one thing that is fairly straightforward. As you get better and
better as coding using this approach, you often find that it is not because you
are using more complex functions, but rather that you’re becoming more clever at
combining sets of simple functions in interesting ways.</p>
<blockquote>
<p>“Even though the Unix system introduces a number of innovative programs and techniques,
no single program or idea makes it work well. Instead, what makes it effective is an approach
to programming, a philosophy of using the computer. Although that philosophy can’t be written
down in a single sentence, at its heart is the idea that the power of a system comes more from
the relationships among programs than from the programs themselves. Many Unix programs do
quite trivial things in isolation, but, combined with other programs, become general and
useful tools.” <span class="citation">(Kernighan and Pike 1984)</span></p>
</blockquote>
</div>
<div id="useful-tidyverse-tools-for-data-preprocessing" class="section level3" number="3.5.2">
<h3><span class="header-section-number">3.5.2</span> Useful tidyverse tools for data preprocessing</h3>
<p>As you learn to code, a good strategy is to start collecting “tools” for
your “toolbox” in R—functions that you have learned to use very well and
that you understand thoroughly. This will help make you proficient in R more
quickly, and it will also limit the chance of bugs and errors in your code,
making your data work more robust and rigorous. In this section, we’ll cover
some tidyverse tools that we have found helpful for preprocessing biological
data. These are not exhaustive, but may help you to identify some sets of
tools to focus on learning well for data preprocessing and analysis of
biological data.</p>
<blockquote>
<p>“Similar to early many, beginner makers start with a rudimentary set of tools for
basic creative tasks: a hammer (of course), a set of screwdrivers, scissors,
some pliers, maybe a crescent wrench, and some kind of cutting device. Almost
everyone who has strived to make things has some combination of this list. Then,
as we get more experienced, we seek out better versions of the tools we already
have as well as new tools that can facilitate the learning of new techniques—new
ways of cutting things apart, and new ways of putting them back together.” <span class="citation">(Savage 2020)</span></p>
</blockquote>
<blockquote>
<p>“Once we start to expand past the basic complement of tools, what to add to our
collections becomes a multifactor calculus based on reliability, cost, space, time,
repairability, skill, and need. These choices are nontrivial, because the tools we use
are extensions of our hands and our minds. The best tools ‘wear in’ to fit you based
on how you use them, they get smooth where you grab them. They tell the story of their
utility with their patina of use. A toolbox of tools you know well and use lovingly is
a magnificent thing.” <span class="citation">(Savage 2020)</span></p>
</blockquote>
<blockquote>
<p>“The reality is that tool choice is both less important and more important than you
think it is. It is less important to the extent that tool usage is entirely
subjective, which means there is no one right way to do things. But it is more
important, because the best tool for any job is the one you’re most comfortable with,
the one that you can make do what you want it to do, whose movements you fully
understand.” <span class="citation">(Savage 2020)</span></p>
</blockquote>
<p>Some key tools from the tidyverse for pre-processing laboratory data:</p>
<ul>
<li>Tools for data input</li>
<li>Tools for changing columns or creating new columns</li>
<li>Tools for working with character strings</li>
<li>Tools for working with dates and times</li>
<li>Tools for statistical modeling</li>
</ul>
<p><strong>Tools for data input</strong></p>
<p>To be able to work with data in R, you first must load it into your R
session. Data will typically be saved in some type of file or files, and
so you must instruct R in how to find that data and then read it from
the file into the R session.</p>
<p>There are several key tidyverse tools for inputting data from a file. The
most important is a package in the tidyverse called, <code>readr</code>. This package
allows you to read data from plain text files. Data are often stored in
these plain text files, including in formats like CSV (“comma-separated
values”), tab-separated values, and fixed width files. These are all files
that you can open on your computer with a text editor (for example,
Notepad, Wordpad, or TextEdit).</p>
<p>The <code>readr</code> package includes various functions to read in data from these
types of files, with different functions for different formats of those
files. For example, CSV files separate different pieces of data in the
file with commas, and these can be read into R with the <code>readr</code> function
<code>read_csv</code>.</p>
<p>Some equipment in the laboratory may allow you to save results in a plain
text format. When you export your data from laboratory equipment, you can
check to see if there is an option to outload it to a format like “csv”
or “txt”, which would allow you to use these <code>readr</code> functions to then
read the data into R.</p>
<p>There are other packages in the tidyverse that allow you to read in data
from other types of file formats. For example, you may have data that
you recorded into an Excel spreadsheet. Excel files are a bit more complex
in their structure than plain text files, and the functions that read
plain text files into R will not work for Excel files. Instead, there are
a series of functions in a package called <code>readxl</code> that you can use to
read in data from Excel files into R. These functions even allow you to
specify which sheet of an Excel file to read data from, as well as which
cells on that sheet, so they allow for very fine control of data input
from an Excel spreadsheet.</p>
<p>In some cases, you may be collecting data with laboratory equipment that does
not export its data to a standard format, like a plain text file or a
basic spreadsheet file. Instead, some equipment will save data into a file
format that has been standardized for a certain type of data (e.g., an mzML
file for metabolomics data) or to a file type that is proprietary to the
company that manufactures the equipment. There is a chance that someone has
created an R package that can input data from these more specialized types of
files. In fact, for common file types from biomedical research, that chance
is high (for example, there are several packages available with functions that
input data from an mzML file). One of the best ways to find an appropriate
tool to input data from more specialized formats is by searching Google for
“R data input” and then the name of the file format. If you use that file
format often in your laboratory, it is worth some research to determine
which R package is a good fit for inputting data from that file format
and then working through vignettes and other helpfiles for that package to
learn how to use it well.</p>
<p>You can learn more about the <code>readr</code> and <code>readxl</code> packages through their
vignettes, which provide tutorials walking through the functionality of
each package. You can find those at:</p>
<ul>
<li><code>readr</code>: <a href="https://readr.tidyverse.org/" class="uri">https://readr.tidyverse.org/</a></li>
<li><code>readxl</code>: <a href="https://readxl.tidyverse.org/" class="uri">https://readxl.tidyverse.org/</a></li>
</ul>
<p><strong>Tools for changing or creating columns</strong></p>
<p>There are many preprocessing tasks that require creating columns that are
mathematical functions of existing columns. One example is when you are
scaling or normalizing data.</p>
<p>There are a range of ways to standardize and normalize different types of
biomedical data, ranging from very simple to much more complex. At the simpler
end is a process called “scaling”, where the observations for each feature or
column are changed to have an overall mean of 0 and standard deviation of 1.
This can be done by taking each value in a column and subtracting from it
the column-wide mean, then dividing by the column-wide standard deviation [?].
This type of scaling is often required before using some of the techniques
for dimension reduction (e.g., principal components analysis) or clustering,
to ensure that the unit of measurement of each column does not influence
its weight in later analysis. For example, if you were clustering observations
using measurements for each subject that included their weight, you don’t
want to get different results depending on whether their weight is measured
in grams versus pounds, and this type of scaling can help avoid any of those
differences based on the units used for measurements.</p>
<p>There are also more complex methods for scaling and normalization. All similarly
require mathematical algorithms or functions to be applied to the original
data to create a new column of data that is the scaled or normalized version of
the original.</p>
<p>In R, there are functions that come with the base installation of R (in other
words, don’t require installing extra packages) that can be used for more basic
processes of standardization and normalization. For example, the <code>scale</code>
function can be used for the basic scaling described in the previous paragraph.
You can also directly use math functions (like <code>-</code> for subtraction and <code>/</code> for
division) and very basic functions (like <code>mean</code> to calculate the mean of a
vector of numbers and <code>sd</code> to calculate the standard deviation) to make these
types of calculations from scratch. For more complex processes, there are
often specialized functions available in separate packages that you can
install.</p>
<p>The <code>dplyr</code> package is a key package to learn from the tidyverse, as it forms
the heart of the tools for cleaning and exploring data that are stored in tidy
dataframes. These functions can also be used for basic cleaning operations in a
dataframe. For example, data that are recorded for colony-forming units may
include “TNTC” in cells of the spreadsheet where so many bacteria had grown that
the individual colonies were “too numerous to count”. When you read in the data,
you may want to change these values to missing values so that you can run
numerical calculations on the cells that include colony counts. This type of
conversion can easily be done using functions from the <code>dplyr</code> package. They are
also critical for performing processes like scaling / normalization— the
<code>mutate</code> function, for example, can be used to create a new column of scaled
data by applying a scaling function to an existing column.</p>
<p>The package includes not only functions for making changes to a single column
(e.g., the <code>mutate</code> function), but also functions that can be used to perform
the same calculation across many columns (e.g., the <code>across</code> function). This
is an efficient way to do something like scale the data in multiple columns at
once.</p>
<p>You can learn more about the <code>dplyr</code> package through its vignette, which is
available at: <a href="https://dplyr.tidyverse.org/" class="uri">https://dplyr.tidyverse.org/</a>.</p>
<p><strong>Tools for working with character strings</strong></p>
<p>Once you have learned the basic tools for inputting data, as well as basic
manipulations on columns with the <code>dplyr</code> tools, you should take some time to
learn a few other tools that can often be used to make your coding pipelines
much more efficient. One of these is to learn how to work well with character
strings.</p>
<p>Character strings are strings of alphanumerical symbols that are stored
inside quotation marks, like “Mouse-01” or “Control group”. Several tidyverse
packages help you work with this type of data more efficiently, either through
finding and using regular patterns in the data (e.g., the number “01” stored in
“Mouse-01”) or in treating these data as a marker of a set number of groups
(e.g., “Control group” versus “Treated group”). These tools can help you in
processing and exploring the data, and they are also extremely important in
creating figures and tables from the data with clear labels. Once you start
learning to work with character string data, you will realize that it’s not just
within the data, but also that you can treat the file names and directory names
of your project as character strings, and use these tools to embed and use
useful information in them.</p>
<p>The <code>stringr</code> package, which is part of the tidyverse, includes simple but
powerful tools for working with vectors composed of character strings. For
example, the package includes a function that let you extract a subset of each
character string based on the position of the characters in the string, a
function that lets you replace every instance of a pattern with something
else, and a function that will tell you which character strings in the vector
have a match to a certain pattern. It also includes a function that can change
the case of all the letters in each string, either to uppercase, to lowercase,
or to “title case” (the first letter in each word is capitalized).</p>
<p>You likely will not realize how powerful many of these tools are
until you have a time when you need to do one of these tasks! For example,
say that you have a column in your data that provides the ID of each study
subject (e.g., “Mouse 1A”). If some of the IDs were entered using upper
case (e.g., “MOUSE 1A”), some with lower case (“mouse 1a”), and some with
a mixture (e.g., “Mouse 1A”), then you may find that it is hard
to write code that recognizes that “Mouse 1A” is the same as “mouse 1a” and
“MOUSE 1A”. The functions in the <code>stringr</code> package would let you quickly
convert everything to the same case and so work around this issue.
As another example, you may want to extract certain elements from each
subject ID—for example, you might want to create a column where you
have changed “Mouse 1A” to just “1A” and “Mouse 2B” to just “2B”. The
<code>stringr</code> package has functions that will let you do this in several
ways. For example, it has a function that would let you remove “Mouse”
from each character string, and another function that would let you
extract only the part of the string that starts from the first number.
These types of tools can be invaluable when you need to preprocess or
clean data from the format that it first enters R.</p>
<p>Sometimes, you will want to treat character strings as discrete categories
or values. For example, if part of your data records subject IDs
(e.g., “Mouse 1A”, “Mouse 2B”), you may want to be able to link up all
of the observations that are recorded for each subject. Similarly, you
may want to treat a variable that records treatment (e.g., “treated” / “control”)
as a set of specific categories that each observation belongs to.</p>
<p>In R, you can do this by treating that column as something called a “factor”.
This data type looks like a character string (e.g., “treated”), but R has
recorded that there are only a few set values that values in the column can have
(e.g., “treated” or “control”), and when you summarize or plot the data, you can
group by this variable to get summaries within each category, or align it with
the color or shape of plotted points.</p>
<p>The <code>forcats</code> package includes helpful tools for working with this factor type
of data. When a column is changed into a factor, the possible levels of the
factor (in other words, the possible values it can take) will be given an order,
often alphabetical. You won’t notice this order with many of the processing
you might do, but it will control the order that categories are mentioned when
you summarize or plot the data. The <code>forcats</code> package includes a function that
lets you rearrange this order, and so rearrange the order that each category
is presented in summaries and plots. The package also includes numerous other
tools for working with this type of data. For example, if you have a factor
that takes many different possible values, it will let you to convert to
specify only those that are most common (you can specify how many categories),
and then pool the rest into an “Other” category.</p>
<p>The vignettes for the <code>stringr</code> and <code>forcats</code> packages are available at:</p>
<ul>
<li><code>stringr</code>: <a href="https://stringr.tidyverse.org/" class="uri">https://stringr.tidyverse.org/</a></li>
<li><code>forcats</code>: <a href="https://forcats.tidyverse.org/" class="uri">https://forcats.tidyverse.org/</a></li>
</ul>
<blockquote>
<p>“In bioinformatics, we often need to extract data from strings. R has several
functions to manipulate strings that are handy when working with bioinformatics data in
R. Note, however, that for most bioinformatics text-processing tasks, R is <em>not</em>
the preferred language to use for a few reasons. First, R works with all data stored
in memory; many bioinformatics text-processing tasks are best tackled with the
stream-based approaches…, which explicityly avoid loading all data in memory at
once. Second, R’s string processing functions are admittedly a bit clunky compared
to Python’s.” <span class="citation">(Buffalo 2015)</span></p>
</blockquote>
<p><strong>Tools for working with dates and times</strong></p>
<p>Another handy set of tools are those for working with dates and times. Often, you
will record the date that an observation is collected, or the date and time if
data are being collected at a fine time scale. Although you record these as a
character string (e.g., “August 1, 2019”), you’ll want to be able to use the
quantitative information within the date. For example, you may want to be able
to tell if the date of each observation is before a certain date, or determine
how many days there are between two date.</p>
<p>The tidyverse includes a package for working with dates and times called
<code>lubridate</code>. This package includes functions that allow you to change a column
in your data to have a date or date-time data type. This will allow you to
do operations on those values as dates—in other words, do things like determine
the number of days between two dates. The <code>lubridate</code> package also includes
functions for these operations on dates, including determining if one date is
larger or smaller than another and whether it’s within an interval of two dates,
as well as determining the difference between two dates or finding out which
date is a certain number of days after a given date. There are also functions
to extract certain elements from each date, like the day of the week or the
month of the year.</p>
<p>The functions in the <code>lubridate</code> package can be very useful for preprocessing
data. For example, you may record the date of each measurement that you take,
but also need to determine how much time has passed between the start of the
experiment and that measurement. The <code>lubridate</code> package has a function that
will allow you to calculate the time since a recorded start time, and so this
allows you to record only the date and time of each measurement, and then
determine the time since the start of the experiment within reproducible code
once you read the recorded data into R.</p>
<p>To find out more about the <code>lubridate</code> package, you can read its vignette
at <a href="https://lubridate.tidyverse.org/" class="uri">https://lubridate.tidyverse.org/</a>.</p>
<p><strong>Tools for statistical modeling</strong></p>
<p>Often, analysis of biomedical data will include some statistical hypothesis
testing or model building. For example, if you have collected bacterial
loads in two groups of animals with different treatment assignments
(treated and control), you may want to test the hypothesis that the average
bacterial load in the two groups is the same. If the treatment was successful
and the experiment had adequate power, then the data will hopefully show that
this null hypothesis should be rejected.</p>
<p>R has a number of functions that can run the most common statistical
hypothesis tests (e.g., Student’s t-test) as well as fit commonly-used
statistical models (e.g., linear regression models). Many of the tools
for common tests and model building are included with your initial installation
of R. This means that you can use them without installing or loading additional
packages.</p>
<p>Further, there are many additional packages that are available that run
less common statistical tests or fit less common statistical model frameworks.
Part of R’s strength is in it’s deep availability of these packages for
statistical analysis. You can often use a Google search to determine if
there is a function or package for a statistical analysis that you would
like to perform in R, and it is rare to not find at least one package with
the appropriate algorithm. To help you select among different packages, check
out the article “Ten Simple Rules for Finding and Selecting R Packages”
<span class="citation">(Wendt and Anderson 2022)</span>.</p>
<p>In addition to learning the tools for the types of statistical analysis
that you do often in your research, it is also helpful to learn some tools
that help you incorporate that statistical analysis into your workflow.
Many of the tools in R for statistical analysis were originally focused on
being an endpoint of a code pipeline. For example, many of them will result
in a print-out summary of the results of the statistical test or model fit.
This is fine if you only want to record that result, but often you will
want to use the results in further R code, for example to add to plots or
tables or to combine with other results.</p>
<p>There are a couple of packages that can help with this. First, there is a
package called <code>broom</code> that can conver the output of many statistical
tests and model fits into a tidy dataframe. If you have focused on learning
tidyverse tools, then this functionality makes it much easier for you to
continue working with the output. The <code>tidymodels</code>
package extends on this idea by creating a common interface for fitting
a variety of statistical models and extracting results in a tidy format.</p>
<p>You can read the vignettes for the <code>broom</code> and <code>tidymodels</code> packages at:</p>
<ul>
<li><code>broom</code>: <a href="https://cran.r-project.org/web/packages/broom/vignettes/broom.html" class="uri">https://cran.r-project.org/web/packages/broom/vignettes/broom.html</a></li>
<li><code>tidymodels</code>: <a href="https://www.tidymodels.org/" class="uri">https://www.tidymodels.org/</a></li>
</ul>
</div>
<div id="resources-to-learn-more-on-tidyverse-tools" class="section level3" number="3.5.3">
<h3><span class="header-section-number">3.5.3</span> Resources to learn more on tidyverse tools</h3>
<p>Here we have introduced the tidyverse approach, as well as covered some key
tools within it for biomedical data preprocessing. However, we strongly
recommend that you continue to learn more in this approach. In this section,
we’ll point you to resources that you can use to continue to learn this approach
to working with data in R.</p>
<p>The tidyverse approach is now widely taught, both in in-person courses at
universities and through a variety of online resources.
Since there are so many excellent resources available—many for free—to learn
how to code in R using the tidyverse approach, we consider it beyond the scope
of these modules to go more deeply into these instructions. Rather, we’ll
point you to some excellent references that go deeply into the tidyverse
approach to coding, its set of tools, and how they can be applied when
working with biomedical data.</p>
<p><strong>Classes and workshops</strong></p>
<p>Most R programming classes at universities, as well as workshops at conferences
and other venues, now focus on the tidyverse approach, especially if they are
geared to new R users. An R programming class can be a worthwhile investment of
time if this resource is available to you, and if you head a research group and
do not have time to take one yourself, you could instead consider encouraging
trainees in your research group to take this type of class. Programming in other
scripted languages, like Python and Julia, provides similar skills, although the
collection of extension packages that are available for biomedical data tends to
be most extensive for R (at least at this time). Classes in programming
languages like Java or C++, on the other hand, would have less immediate
relevance for most biologists and other bench scientists, and so if you would
like to become better at working with biomedical data, it would be worthwhile to
focus on programming languages that are scripted.</p>
<p><strong>Online books</strong></p>
<p>There are a number of excellent free online books that are available to help
you learn more about R (many of which can also be purchased as a hard copy, if
you prefer that format). These typically include lots of examples of code that
help you try out concepts as you learn them.</p>
<p>One key resource for learning the tidyverse approach for R is the book <em>R for
Data Science</em> by Hadley Wickham (the primary developer of the tidyverse) and
Garrett Grolemund. This book is available as a print edition through O’Reilly
Media. It is also freely available online at <a href="https://r4ds.had.co.nz/" class="uri">https://r4ds.had.co.nz/</a>. This book
is geared to beginners in R, moving through to get readers to an intermediate
stage of coding expertise, which is a level that will allow most scientific
researchers to powerfully work with their experimental data. The book includes
exercises for practicing the concepts, and a separate online book is available
with solutions for the exercises
(<a href="https://jrnold.github.io/r4ds-exercise-solutions/" class="uri">https://jrnold.github.io/r4ds-exercise-solutions/</a>).</p>
<p>Another online book that is an excellent tool—particularly for those using
R for biomedical research—is <em>Modern Statistics for Modern Biology</em>, by
Susan Holmes and Wolfgang Huber. These book shows how the tidyverse approach
can be combined with tools from Bioconductor that are custom built to
work with bioinformatics data. It also provides an excellent overview of
statistical methods for working with biomedical data and how those can be
applied using R. The book is available online at <a href="https://www.huber.embl.de/msmb/" class="uri">https://www.huber.embl.de/msmb/</a>.</p>
<p><strong>Cheatsheets</strong></p>
<p>For many of the key tidyverse packages, there are two-page “cheatsheets” that
have been developed by the package creators to help users learn and remember
the functions that are available in the package. These are available here:
<a href="https://posit.co/resources/cheatsheets/" class="uri">https://posit.co/resources/cheatsheets/</a>.</p>
<p>Each cheatsheet includes numerous working examples. One excellent way to
familiarize yourself with the tools in a package, then, is to work through the
examples on the cheatsheet one at a time, making sure that you understand the
inputs and outputs to the function and how the function has created the output.
Once you have worked through a cheatsheet in this way, you can keep it close
to your desk to serve as a quick reminder of the names and uses of different
functions in the package, until you have used them enough that you don’t need
this memory jog.</p>
<p>For deeper tutorials of each tidyverse package, you can explore the
package’s vignette. We’ve provided links to several of these throughout this
module.</p>
</div>
<div id="practice-quiz" class="section level3" number="3.5.4">
<h3><span class="header-section-number">3.5.4</span> Practice quiz</h3>

</div>
</div>
<p style="text-align: center;">
<a href="3.4-module13a.html"><button class="btn btn-default">Previous</button></a>
<a href="3.6-module15.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
