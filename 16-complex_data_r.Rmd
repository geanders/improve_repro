## Complex data types in R and Bioconductor

Many R extension packages for pre-processing experimental data use complex
(rather than 'tidy') data formats within their code, and many output data in
complex formats. Very recently, the *broom* and *biobroom* R
packages have been developed to extract a 'tidy' dataset from a complex data
format. These tools create a clean, simple connection between the complex data
formats often used in pre-processing experimental data and the 'tidy' format
required to use the 'tidyverse' tools now taught in many introductory R courses.
In this module, we will describe the 'list' data structure, the common backbone
for complex data structures in R and provide tips on how to explore and extract
data stored in R in this format, including through the *broom* and
*biobroom* packages.

**Objectives.** After this module, the trainee will be able to:

- Describe the structure of R's 'list' data format 
- Take basic steps to explore and extract data stored in R's complex, list-based
structures
- Describe what the *broom* and *biobroom* R packages can do 
- Explain how converting data to a 'tidy' format can improve reproducibility

### Subsection 1

[Comparison of complexity of biological systems versus complexity of code and 
algorithms for data pre-processing---for the later, nothing is unknowable or even 
unknown. Someone somewhere is guaranteed to know exactly how it works, what it's
doing, and why. By contrast, with biological systems, there are still things
that noone anywhere completely understands. It's helpful to remember that all 
code and algorithms for data pre-processing is knowable, and that the details 
are all there if and when you want to dig to figure out what's going on.]

[There are ways to fully package up and save the computer environment used to 
run a pipeline of pre-processing and analysis, including any system settings, 
all different software used in analysis steps, and so on. Some of the approaches
that are being explored for this include the use of "containers", including 
Docker containers. This does allow, typically, for full reproducibility of 
the workflow. However, this approach isn't very proactive in emphasizing the 
robustness of a workflow or its comprehensibility to others---instead, it 
makes the workflow reproducible by putting everything in a black box that must
be carefully unpackaged and explored if someone wants to understand or adapt
the pipeline.]

> "Object-oriented design doesn't have to be over-complicated design, but we've 
observed that too often it is. Too many OO designs are spaghetti-like tangles of 
is-a and has-a relationships, or feature thick layers of glue in which many of the
objects seem to exist simply to hold places in a steep-sided pyramid of abstractions. 
Such designs are the opposite of transparent; they are (notoriously) opaque and 
difficult to debug." [@raymond2003art]

> "Unix programmers are the original zealots about modularity, but tend to go about it
in a quiter way [that with OOP]. Keeping glue layers thin is part of it; more generally, 
our tradition teaches us to build lower, hugging the ground with algorithms and structures
that are designed to be simple and transparent." [@raymond2003art]

> "A *standard* is a precise and detailed description of how some artifact is built
or is supposed to work. Examples of software standards include programming 
languages (the definition of syntax and semantics), data formats (how information is
represented), algorithmic processing (the steps necessary to do a computation), and
the like. Some standards, like the Word `.doc` file format, are *de facto* standards---they
have no official standing but everyoen uses them. The word 'standard' is best reserved for
formal descriptions, often developed and maintained by a quasi-neutral party like a 
government or a consortium, that define how something is built or operated. The definition 
is sufficiently complete and precise that separate entities can interact or provide independent
implementations. We benefit from hardware standards all the time, though we may not notice 
how many there are. If I buy a new television set, I can plug it inot the electrical outlets
in my home thanks to standards for the size and shape of plugs and the voltage they provide. 
The set itself will receive signals and display pictures because of standards for broadcast
and cable television. I can plug other devices into it through standard cables and connectors 
like HDMI, USB, S-video and so on. But every TV needs its own remote control and every cell
phone needs a different charger because those have not been standardized. Computing has plenty
of standards as well, including character sets like ASCII and Unicode, programming languages
like C and C++, algorithms for encryption and compression, and protocols for exchanging
information over networks." [@kernighan2011d]

> "Standards are important. They make it possible for independently created things to cooperate, 
and they open an area to competition from multiple suppliers, while proprietary systems tend
to lock everyone in. ... Standards have disadvantages, too---a standard can impede progress if
it is inferior or outdated yet everyone is forced to use it. But these are modest drawbacks
compared to the advantages." [@kernighan2011d]

> "A *class* is a blueprint for constructing a particular package of code and data; each 
variable created according to a class's blueprint is known as an *object* of that class. 
Code outside of a class that creates and uses an object of that class is known as a *client*
of the class. A *class declaration* names the class and lists all of the *members*, or 
items inside that class. Each item is either a *data member*---a variable declared within the
class---or a *method* (also known as a *member function*), which is a function declared within
the class. Member functions can include a special type called a *constructor*, which has the 
same name as the class and is invoked implicitly when an object of the class is declared. 
In addition to the normal attributes of a variable or function declaration (such as type, and 
for functions, the parameter list), each member has an *access specifier*, which indicates 
what functions can access the member. A *public member* can be accessed by any code using the 
object: code inside the class, a client of the class, or code in a *subclass*, which is a class
that 'inherits' all the code and data of an existing class. A *private member* can be 
accessed only by the code inside the class. *Protected members* ... are similar to private 
members, except that methods in subclasses can also reference them. Both private and 
protected members, though, are inaccessible from client code." [@spraul2012think]

> "An object should be a meaningful, closely knit collection of data and code that operates
on the data." [@spraul2012think]

> "Recognizing a situation in which a class would be useful is essential to reaching the 
higher levels of programming style, but it's equally important to recognize situations in 
which a class is going to make things worse." [@spraul2012think]

> "The word *encapsulation* is a fancy way of saying that classes put multiple pieces of 
data and code together in a single package. If you've ever seen a gelatin medicine capsule
filled with little spheres, that's a good analogy: The patient takes one capsule and swallows
all the individual ingredient spheres inside. ... From a problem-solving standpoint, 
encapsulation allows us to more easily reuse the code from previous problems to solve current
problems. Often, even though we have worked on a problem similar to our current project, 
reusing what we learned before still takes a lot of work. A fully encapsulated class can 
work like an external USB drive; you just plug it in and it works. FOr this to happen, 
though, we must design the class correctly to make sure that the code and data is truly 
encapsulated and as independent as possible from anything outside of the class. For example, 
a class that references a global variable can't be copied into a new project without 
copying the global variable, as well." [@spraul2012think]

> "Beyond reusing classes from one program to the next, classes offer the potential for
a more immediate form of code reuse: inheritance. ... Using inheritance, we create parent
classes with methods common to two or more child classes, thereby 'factoring out' not 
just a few lines of code [as with helper functions in procedural code] but whole methods."
[@spraul2012think]

> "One technique we're returned to again and again is dividing a complex problem into 
smaller, more manageable pieces. Classes are great at dividing programs up into functional
units. Encapsulation not only holds data and code together in a reusable package; it also
cordons off that data and code from the rest of the program, allowing us to work on that 
class, and everything else separately. The more classes we make in a program, the greater
the problem-dividing effect." [@spraul2012think]

> "Some people use the terms *information hiding* and *encapsulation* interchangeable, but
we'll separate the ideas here. As described previously ..., encapsulation is 
packaging data and code together. Information hiding means separating the interface of a 
data structure---the definition of the operations and their parameters---from the implementation
of a data structure, or the code inside the functions. If a class has been written with 
information hiding as a goal, then it's possible to change the implementation of the methods
without requiring any changes in the client code (the code that uses the class). Again, we
have to be clear on the term *interface*; this means not only the name of the methods and
their parameter list but also the explanation (perhaps expressed in code documentation) of 
what the different methods do. When we talk about changing the implementation without 
changing the interface, we mean that we change *how* the class methods work but not 
*what* they do. Some programming authors have referred to this as a kind of implicit contract
between the class and the client: The class agrees never to change the effects of 
existing operations, and the client agrees to use the class strictly on the basis of its
interface and to ignore any implementation details." [@spraul2012think]

> "So how does information hiding affect problem solving? The principle of information hiding 
tells the programmer to put aside the class implementation details when working on the 
client code, or more broadly, to be concerned about a particular class's implementation
only when working inside that class. When you can put implementation details out of your
mind, you can eliminate distracting thoughts and concentrate on solving the problem at hand."
[@spraul2012think]

> "A final goal of a well-designed class is expressiveness, or what might be broadly called
writability---the ease with which code can be written. A good class, once written, makes
the rest of the code simpler to write in the way that a good function makes code simpler to
write. Classes effectively extend a language, becoming high-level counterparts to basic
low-level features such as loops, if statements, and so forth. ... With classes, programming
actions that previously took many steps can be done in just a few steps or just one."
[@spraul2012think]

> "Right now, in labs across the world, machines are sequencing the genomes of the life 
on earth. Even with rapidly decreasing costs and huge technological advancements in 
genome sequencing, we're only seeing a glimpse of the biological information contained
in every cell, tissue, organism, and ecosystem. However, the smidgen of total biological 
information we're gathering amounts to mountains of data biologists need to work with. At 
no other point in human history has our ability to understand life's complexities been so 
dependent on our skills to work with and analyze data." [@buffalo2015bioinformatics]

> "Bioinformaticians are concerned with deriving biological understanding from large
amounts of data with specialized skills and tools. Early in biology's history, the 
datasets were small and manageable. Most biologists could analyze their own data after
taking a statistics course, using Microsoft Excel on a personal desktop computer.
However, this is all rapidly changing. Large sequencing datasets are widespread, and will 
only become more common in the future. Analyzing this data takes different tools, new skills,
and many computers with large amounts of memory, processing power, and disk space."
[@buffalo2015bioinformatics]

> "In a relatively short period of time, sequencing costs dropped drastically, allowing 
researchers to utilize sequencing data to help answer important biological questions. 
Early sequencing was low-throughput and costly. Whole genome sequencing efforts were
expensive (the human genome cost around $2.7 billion) and only possible through large
collaborative efforts. Since the release of the human genome, sequencing costs have 
decreased explonentially until about 2008 ... With the introduction of next-generation
sequencing technologies, the cost of sequencing a megabase of DNA dropped even more 
rapidly. At this crucial point, a technology that was only affordable to large collaborative
sequencing efforts (or individual researchers with very deep pockets) became affordable
to researchers across all of biology. ... What was the consequence of this drop in 
sequencing costs due to these new technologies? As you may have guessed, lots and lots
of data. Biological databases have swelled with data after exponential growth. Whereas once
small databases shared between collaborators were sufficient, now petabytes of useful
data are sitting on servers all over the world. Key insights into biological questions are
stored not just in the unanalyzed experimental data sitting on your hard drive, but also
spinning around a disk in a data center thousands of miles away." [@buffalo2015bioinformatics]

> "To make matters even more complicated, new tools for analyzing biological data are 
continually being created, and their underlying algorithms are advancing. A 2012 review
listed over 70 short-read mappers ... Likewise, our approach to genome assembly has 
changed considerably in the past five years, as methods to assemble long sequences
(such as overlap-layout-consensus algorithms) were abandoned with the emergence of 
short high-throughput sequencing reads. Now, advances in sequencing chemistry are 
leading to longer sequencing read lengths and new algorithms are replacing others that
were just a few years old. Unfortunately, this abundance and rapid development of 
bioinformatics tools has serious downsides. Often, bioinformatics tools are not adequately
benchmarked, or if they are, they are only benchmarked in one organism. This makes it
difficult for new biologists to find and choose the best tool to analyze their data.
To make matters more difficult, some bioinformatics programs are not actively developed
so that they lose relevance or carry bugs that could negatively affect results. All of 
this makes choosing an appropriate bioinformatics program in your own research difficult.
More importantly, it's imperative to critically assess the output of bioinformatics
programs run on your own data." [@buffalo2015bioinformatics]

> "With the nature of biological data changing so rapidly, how are you supposed to 
learn bioinformatics? With all of the tools out there and more continually being 
created, how is a biologist supposed to know whether a program will work appropriately 
on her organism's data? The solution is to approach bioinformatics as a bioinformatician
does: try stuff, and assess the results. In this way, bioinformatics is just about having
the skills to experiment with data using a computer and understanding your results. 
The experimental part is easy: this comes naturally to most scientists. The limiting
factor for most biologists is having the data skills to freely experiment and work with
large data on a computer." [@buffalo2015bioinformatics]

> "Unfortunately, many of the biologist's common computational tools can't scale to the
size and complexity of modern biological data. Complex data formats, interfacing 
numerous programs, and assessing software and data make large bioinformatics datasets 
difficult to work with." [@buffalo2015bioinformatics]

> "In 10 years, bioinformaticians may only be using a few of the bioinformatics 
software programs around today. But we most certainly will be using data skills and
experimentation to assess data and methods of the future." [@buffalo2015bioinformatics]

> "Biology's increasing use of large sequencing datasets is changing more that the tools 
and skills we need: it's also changing how reproducible and robust our scientific 
findings are. As we utilize new tools and skills to analyze genomics data, it's 
necessary to ensure that our approaches are still as reproducible and robust as
any other experimental approaches. Unfortunately, the size of our data and the complexity
of our analysis workflows make these goals especially difficult in genomics."
[@buffalo2015bioinformatics]

> "The requisite of reproducibility is that we share our data and methods. In the pre-genomics
era, this was much easier. Papers coule include detailed method summaries and entire
datasets---exactly as Kreitman's 1986 paper did with a 4,713bp Adh gene flanking sequence
(it was embedded in the middle of the paper). Now papers have long supplementary methods, 
code, and data. Sharing data is no longer trivial either, as sequencing projects can include
terabytes of accompanying data. Reference genomes and annotation datasets used in analyses are
constantly updated, which can make reproducibility tricky. Links to supplemental materials, 
methods, and data on journal websites break, materials on faculty websites disappear when
faculty members move or update their sites, and software projects become stale when 
developers leave and don't update code. ... Additionally, the complexity of bioinformatics
analyses can lead to findings being susceptible to errors and technical confounding. 
Even fairly routine genomics projects can use dozens of different programs, complicated
input paramter combinations, and many sample and annotation datasets; in addition, work
may be spread across servers and workstations. All of these computational data-processing
steps create results used in higher-level analyses where we draw our biological conclusions.
The end result is that research findings may rest on a rickety scaffold of numerous 
processing steps. To make matters worse, bioinformatics workflows and analyses are usually 
only run once to produce results for a publication, and then never run or tested again. 
These analyses may rely on very specific versions of all software used, which can make it
difficult to reproduce on a different system. In learning bioinformatics data skills, it's
necessary to concurrently learn reproducibility and robust best practices." 
[@buffalo2015bioinformatics]

> "When we are writing code in a programming language, we work most of the time with RAM, 
combining and restructuring data values to produce new values in RAM. ... The computer memory
in RAM is a series of 0's and 1's, just like the computer memory used to store files in mass
storage. In order to work with data values, we need to get those values into RAM in some
format. At the basic level of representing a single number or a single piece of text, the 
solution is the same as it was in Chapter 5 [on file formats for mass storage]. Everything
is represented as a pattern of bits, using various numbers of bytes for different sorts of
values. In R, in an English locale, and on a 32-bit operating system, a single character
usually takes up one byte, an 

### Subsection 2

### Applied exercise


