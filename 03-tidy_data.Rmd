## The 'tidy' data format {#module3}

In the previous module, we explained the benefits of saving data in a structured
format, and in particular one that follows standards for your discipline. In
this section, we'll talk about the "tidy" data format. The tidy data format is
one implementation of a tabular, two-dimensional structured data format that has
quickly gained popularity among statisticians and data scientists since it was
defined in a 2014 paper [@wickham2014tidy]. These principles cover some basic
rules for ordering the data, and even if you haven't heard the term *tidy data*,
you may already be implementing many of its standards in your own datasets.
Datasets in this format tend to be very easily to work with, including to
further clean, model, and visualize the data, as well as to integrate the data
with other datasets. In particular, this data format is compatible with a
collection of open-source tools on the R platform called the
*tidyverse*. These characteristics mean that, if you are planning to use a
standardized data format for recording experimental data in your research group,
you may want to consider creating one that adheres to the tidy data format.

**Objectives.** After this module, the trainee will be able to:

- List characteristics defining the "tidy" structured data format 
- Understand how to reformat a dataset to make it follow the "tidy" format
- Explain the difference between the a structured data format (general concept)
and the "tidy' data format (one popular implementation)
- Understand benefits of recording data in a "tidy" format

Adam Savage has built a career out of making things. He became famous as the
host of the TV show *Mythbusters*, where a crew builds contraptions to test
urban myths. For many years before that, he created models and special effects
for movies. He has thought a lot about how to effectively work in teams to make
things, and in 2019 he published a book about his life as a maker called *Every
Tool is a Hammer* [@savage2020every].

Among many insights, Savage focuses on the importance
of tidying up as part of the creation process, saying "It's time, when taken,
that you might feel is slowing you down in the moment, but in fact is saving you
time in the long run." [@savage2020every] He introduces a new word for the
process of straightening up tools and materials---"knolling". He borrowed the
term from an artist, Tom Sachs, whose rules for his own workshop include,
"Always Be Knolling".

The idea of "knolling" includes a few key principles. First, only have what you
need out. Put everything else somewhere else. Removing any extras makes it
faster to find what you need when you need it. Second, for things you need, make
sure they're out and available. "Drawers are where things go to die," Savage
says, highlighting inefficiency when you have to look
for things that are hidden from site as you work. Finally, organize the things
that you have out. Put like things together, and arrange everything neatly,
aligning things in parallel or perpendicular patterns, rather than piling it
haphazardly.

Just as organizing tools and materials improves efficiency in a workshop,
organizing your data can dramatically improve the efficiency of data
preprocessing, analysis, and visualization. Indeed, "tidying up" your data
can give such dramatic improvements that a number of researchers have 
developed systems and written papers that describe good organization schemes
to use to tidy up data (e.g., [@wickham2014tidy]). 

The principles for tidying up data follow some of the principles for knolling.
For example, you want to make sure that you're saving data in a file or
spreadsheet that only includes the data, removing any of the extras. Lab groups
will sometimes design spreadsheets for data collection that include a space for
recording data, but also space for notes, embedded calculations, and plots.
These extra elements can make it hard to extract and use the data itself. One
way to tidy up a dataset is to remove any of these extra elements. While you can
do this after you've collected your data, it's more efficient to design a way to
record your data in the first place without extra elements in the file or
spreadsheet. You can further tidy up your data format by reformatting it to 
follow the rules of a data format called the "tidy data" format.

We'll start this module by describing rules a dataset format must follow for it
to be "tidy" and clarifying how you can set up your data recording to follow
these rules. In later parts of this module, we'll talk more about why it's
helpful to use a tidy data format, as well as a bit about the tidyverse tools
that you can use with data in this format. 

### What makes data "tidy"?

The "tidy" data format describes one way to structure tabular data. The name
follows from the focus of this data format and its associated set of tools---the
"tidyverse"---on preparing and cleaning ("tidying") data, in contrast to sets of
tools more focused on other steps, like data analysis [@wickham2014tidy]. The
word "tidy" is not meant to apply that other formats are "dirty", or that they
include data that is incorrect or subpar. In fact, the same set of datapoints
could be saved in a file in a way that is either "tidy" (in the sense of
[@wickham2014tidy]) or untidy, depending only on how the data are organized
across columns and rows.

Wickham notes in his article, where he first describes the tidy data format, 
that his ideas about this format evolved from seeing many examples of 
different ways that data could be organized within a two-dimensional structure. 
He notes: 

> "The development of tidy data has been driven by my experience from working
with real-world datasets. With few, if any, constraints on their organization,
such datasets are often constructed in bizarre ways. I have spent countless
hours struggling to get such datasets organized in a way that makes data
analysis possible, let alone easy." [@wickham2014tidy]

To help you understand the tidy data format that Wickham developed, let's start
with a checklist of rules that make a dataset tidy. Some of these are drawn
directly from the journal article that originally defined the data format
[@wickham2014tidy]. Other rules are based on common untidy patterns that show
up in data recording templates for laboratory research. The checklist is:

- Data are recorded in a tabular, two-dimensional format
- The data collection file or spreadsheet avoids extra elements 
like plots or embedded equations in the file
- Each observation forms a row
- Each variable forms a column
- Column headers are variable names, not values
- Each type of observational unit forms its own table
- A single variable is in a single column, not spread across multiple columns
- A column contains only one variable; multiple variables are not stored in one column
- Data types are consistent within a column

In module 2.1, we discussed the first two principles, highlighting how
important it is to separate data collection from further steps of data
processing and analysis. In this section of the module, we'll go through other
items in this checklist, to help you understand what makes a dataset follow the
tidy data format. If so, you'll be able to set up your data recording template
to follow this template, and you'll be able to tell when you work with data that
others collect if it is in this format, and restructure it if not. In the next
part of this module, we'll explain why it's so useful to have your data in this
format.

Tidy data, first, must be in a tabular (i.e., two-dimensional, with columns and
rows, and with all rows and columns of the same length---nothing "ragged"). If
it's in a spreadsheet, it should be stored without any "extras", like embedded
plots and calculations.  If you record data in a spreadsheet using a very basic
strategy of saving a single table per spreadsheet, with the first row giving the
column names, then your data will be in a tabular format. In general, if your
recorded data looks "boxy", it's probably in a two-dimensional tabular format.

There are some additional criteria for the tidy data format, though, and so
not every structured, tabular dataset is in a tidy format. As Wickham notes
in his paper defining the format, 

> "Most statistical datasets are rectangular tables made up of rows and columns
... [but] there are many ways to structure the same underlying data. ... 
Real datasets can, and often do, violate the three precepts of tidy data in
almost every way imaginable." [@wickham2014tidy]

The first of these rules are that each row of a tidy dataset records the
values for a single observation, and that each column records values of a
variable: that is, characteristics or measurements of a certain type, in the
order of the observations given by the rows [@wickham2014tidy].

To figure out if your data format follows these rules, it's important to
determine the *unit of observation* of that data, which is the unit at which you
take measurements [@sedgwick2014unit]. This idea is different than the *unit of
analysis*, which is the unit that you're focusing on in your study hypotheses
and conclusions (this is sometimes also called the "sampling unit" or "unit of
investigation") [@altman1997units]. In some cases, these two might be equivalent
(the same unit is both the unit of observation and the unit of measurement), but
often they are not [@sedgwick2014unit]. Sedgwick notes: 

> "The unit of observation and unit of analysis are often confused.
The unit of observation, sometimes referred to as the unit of
measurement, is defined statistically as the 'who' or 'what'
for which data are measured or collected. The unit of analysis
is defined statistically as the 'who' or 'what' for which
information is analysed and conclusions are made." [@sedgwick2014unit]

As an example, say you are testing how the immune system of mice responds to a
certain drug over time. In this case, the unit of analysis might be the drug,
or a combination of drug and dose---ultimately, you may want to test something
like if one drug is more effective than another. To answer this research
question, you likely have several replicates of mice in each treatment group. If
a separate mouse (replicate) is used to collect each observation, and a mouse is
never measured twice (i.e., at different time points, or for a different
infection status), then the unit of measurement---the level at which each data
point is collected---is the mouse. This is because each mouse is providing a
single observation to help answer the larger research question. 

As another example, say you conducted a trial on human subjects, to see how 
a certain treatment affects the speed of recovery, where each study
subject was measured at different time points. In this case, the unit of
observation is the combination of study subject and time point (while the unit
of analysis is the treatment). That means that Subject 1's measurement at Time 1 would be one
observation, and the same person's measurement at Time 2 would be a separate
observation. For a dataset to comply with the tidy data format, these two
observations would need to be recorded on separate lines in the data. If the
data instead had different columns to record each study subject's measurements
at different time points, then the data would still be tabular, but it would not
be tidy. 

Once you have divided your data into separate datasets based on the level of
observation, and structured each row to record data for a single observation
based on the unit of observation within that dataset, each column should be used
to measure a separate characteristic or measurement (a *variable*) for each
measurement [@wickham2014tidy].  A column could either give characteristics of
the data that were pre-defined by the study design---for example, the treatment
assigned to a mouse (a type of variable called a *fixed variable*, since its
value was fixed before the start of the experiment) or observed measurements,
like the level of infection measured in an animal (a type of variable called a
*measured variable*, since its value is determine through the experiment)
[@wickham2014tidy].

In the example of human subjects measured at repeated time points, you may
initially find the tidy format unappealing, because it seems like it would
lead to a lot of repeated data. For example, if you wanted to record each study
subject's sex, it seems like the tidy format would require you to repeat that
information in each separate line of data that's used to record the measurements
for that subject for different time points. This isn't the case---instead, with
a tidy data format, different "levels" of data observations should be recorded
in separate tables [@wickham2014tidy]. In other words, you should design a
separate table for each unit of observation if you have data at several of these
units for your experiment. For example, if you have some data on each study
subject that does not change across the time points of the study---like the
subject's ID, sex, and age at enrollment---those form a separate dataset, one
where the unit of observation is the study subject, so there should be just one
row of data per study subject in that data table, while the measurements for
each time point should be recorded in a separate data table. A unique
identifier, like a subject ID, should be recorded in each data table so it can
be used to link the data in the two tables. If you are using a spreadsheet to
record data, this would mean that the data for these separate levels of
observation should be recorded in separate sheets, and not on the same sheet of
a spreadsheet file. Once you read the data into a scripting language like R or
Python, it will be easy to link the larger and smaller tidy datasets as needed
for analysis, visualizations, and reports.

### Why make your data tidy?

This may all seem like a lot of extra work to make a dataset tidy, and why 
bother if you already have it in a structured, tabular format? It turns out 
that, once you get the hang of what gives data a tidy format, it's pretty 
simple to design recording formats that comply with these rules. What's more, 
when data is in a tidy format, it can be directly input into a collection 
of tools in R that belong to something called the "tidyverse". 

R's *tidyverse* framework enables powerful and user-friendly data management,
processing, and analysis by combining simple tools to solve complex, multi-step
problems [@ross2017declutter; @silge2016tidytext; @wickham2016ggplot2;
@wickham2016r]. Since the *tidyverse* tools are simple and share a common
interface, they are easier to learn, use, and combine than tools created in the
traditional base R framework [@ross2017declutter; @lowndes2017our;
@mcnamara2016state]. This *tidyverse* framework is quickly becoming the standard
taught in introductory R courses and books [@hicks2017guide; @baumer2015data;
@kaplan2018teaching; @stander2017enthusing;
@mcnamara2016state], ensuring ample training resources for researchers new to
programming, including books (e.g., [@baumer2017modern; @lifesciencesR;
@wickham2016r]), massive open online courses (MOOCs), on-site university courses
[@baumer2015data; @kaplan2018teaching; @stander2017enthusing], and Software
Carpentry workshops [@wilson2014software; @pawlik2017developing]. Further, tools
that extend the *tidyverse* have been created to enable high-quality data
analysis and visualization in several domains, including text mining
[@silge2017text], microbiome studies [@mcmurdie2013phyloseq], natural language
processing [@RJ-2017-035], network analysis [@RJ-2017-023], ecology
[@hsieh2016inext], and genomics [@yin2012ggbio]. 

This collection of tools is very straightforward to use and so powerful that
it's well worth making an effort to record data in a format that works directly
with the tools, if possible. Outside of cases of very complex or very large
data, it should be possible. As Jeff Leek notes in a blog post on tidy data
analysis,

> "Tidy data is great for a huge fraction of data analyses you might
be interested in. It makes organizing, developing, and sharing data a lot
easier. It's how I recommend most people share data." [@leek2017toward]

The tidyverse is a collection of tools united by a common philosophy: very complex
things can be done simply and efficiently with small, sharp tools that share a
common interface. Zev Ross, in an article about tidy tools and how they can 
declutter a workflow, notes: 

> "The philosophy of the tidyverse is similar to
and inspired by the “unix philosophy”, a set of loose principles that ensure
most command line tools play well together. ...  Each function should solve one
small and well-defined class of problems. To solve more complex problems, you
combine simple pieces in a standard way." [@ross2017declutter]

The tidyverse isn't the only popular system that follows this
philosophy---one other favorite is Legos. Legos are small, plastic bricks, with
small studs on top and tubes for the studs to fit into on the bottom. The studs
all have the same, standardized size and are all spaced the same distance apart.
Therefore, the bricks can be joined together in any combination, since each
brick uses the same *input format* (studs of the standard size and spaced at the
standard distance fit into the tubes on the bottom of the brick) and the same
*output format* (again, studs of the standard size and spaced at the standard
distance at the top of the brick). Because of this design, bricks can be joined
regardless of whether the bricks are different colors or different heights or
different widths or depths. With Legos, even though each "tool" (brick) is very
simple, the tools can be combined in infinite variations to create very complex
structures.

The tools in the tidyverse operate on a similar principle. They all input a
tidy dataset (or a column from a tidy dataset) and they (almost) all output data
in the same format they input it. For most of the tools, their required format
for input and output is the tidy data format [@wickham2014tidy], called a tidy
*dataframe* in R---this is a dataframe that follows the rules detailed earlier
in this section. 

This common input / output interface, and the use of small tools that follow
this interface and can be combined in various ways, is what makes the tidyverse
tools so powerful. However, there are other good things about the tidyverse that
make it so popular. One is that it's fairly easy to learn to use the tools, in
comparison to learning how to write code for other R tools [@robinson2017teach;
@peng2018teaching]. The developers who have created the tidyverse tools have
taken a lot of effort to try to make sure that they have a clear and consistent
*user interface* [@wickham2017tidy; @bryan2017data]. Wickham highlights how this
standardization makes an approach focused on tidy data so powerful: 

> "A standard makes initial data cleaning easier because you do not need to
start from scratch and reinvent the wheel every time. The tidy data standard has
been designed to facilitate initial exploration and analysis of the data, and to
simplify the development of data analysis tools that work well together."
[@wickham2014tidy]

To help understand a user interface, and how having a consistent user interface
across tools is useful, let's think about a different example---cars. When you
drive a car, you get the car to do what you want through the steering wheel, the
gas pedal, the break pedal, and different knobs and buttons on the dashboard.
When the car needs to give you feedback, it uses different gauges on the
dashboard, like the speedometer, as well as warning lights and sounds.
Collectively, these ways of interacting with your car make up the car's *user
interface*. In the same way, each function in a programming language has a
collection of parameters you can set, which let you customize the way the
function runs, as well as a way of providing you output once the function has
finished running and the way to provide any messages or warnings about the
function's run. For functions, the software developer can usually choose design
elements for the function's user interface, including which parameters to
include for the function, what to name those parameters, and how to provide
feedback to the user through messages, warnings, and the final output.  

If a collection of tools is similar in its user interfaces, it will make it
easier for users to learn and use any of the tools in that collection once
they've learned how to use one. For cars, this explains how the rental car
business is able to succeed. Even though different car models are very different
in many characteristics---their engines, their colors, their software---they are
very consistent in their user interfaces. Once you've learned how to drive one
car, when you get in a new car, the gas pedal, brake, and steering wheel are
almost guaranteed to be in about the same place and to operate about the same
way as in the car you learned to drive in. The exceptions are rare enough to be
memorable---think how many movies have a laughline from a character trying to
drive a car with the driver side on the opposite side of what they're used to. 

The tidyverse tools are similarly designed so that they all have a very similar
user interface. For example, many of the tidyverse functions use a parameter
named ".data" to refer to the input data. Similarly, parameters
named ".vars" and ".funs" are repeatedly used over tidyverse functions, with the
same meaning in each case. What's more, the tidyverse functions are typically given names
that very clearly describe the action that the function does, like `filter`,
`summarize`, `mutate`, and `group`. As a result, the final code is very clear
and can almost be "read" as a natural language, rather than code. As Jenny 
Bryan notes, in an article on data science: 

> "The Tidyverse
philosophy is to rigorously (and ruthlessly) identify and obey common
conventions. This applies to the objects passed from one function to another
and to the user interface each function presents. Taken in isolation, each
instance of this seems small and unimportant. But collectively, it creates 
a cohesive system: having learned one component you are more likely to be
able to guess how another different component works."
[@bryan2017data]

As a result, the tidyverse collection of tools is pretty easy to learn, compared
to other sets of functions in scripting languages, and pretty easy to expand
your knowledge of once you know some of its functions. Wickham notes: 

> "The goal of [the tidy tools] principles is to provide a uniform interface so
that tidyverse packages work together naturally, and once you’ve mastered one,
you have a head start on mastering the others." [@wickhem2017tidy]

Many people who teach
R programming now focus on first teaching the tidyverse, given these
characteristics [@robinson2017teach; @peng2018teaching], and it's often a
first focus for online courses and workshops on R programming. Since its main
data structure is the tidy data structure, it's often well worth recording
data in this format so that all these tools can easily be used to explore and
model the data.   

### Using tidyverse tools with data in the tidy data format

The tidyverse includes tools for many of the tasks you might need to
do while managing and working with experimental data. When you download
R, you get what's called *base R*. This includes the main code that drives
anything you do in R, as well as functions for doing many core tasks. 
However, the power of R is that, in addition to base R, you can also add
onto R through what are called *packages* (sometimes also referred to 
as *extensions* or *libraries*). These are kind of like "booster packs"
that add on new functions for R. They can be created and contributed
by anyone, and many are collected through a few key repositories like
CRAN and Bioconductor. 

All the tidyverse tools are included in R extension packages, rather than base
R, so once you download R, you'll need to download these packages as well to use
the tidyverse tools. The core tidyverse functions include functions to read in
data (the `readr` package for reading in plain text, delimited files, `readxl`
to read in data from Excel spreadsheets), clean or summarize the data (the
`dplyr` package, which includes functions to merge different datasets, make
new columns as functions of old ones, and summarize columns in the data, either
as a whole or by group), and reformat the data if needed to get it in a tidy
format (the `tidyr` package). The tidyverse also includes more precise tools,
including tools to parse dates and times (`lubridate`) and tools to work with
character strings, including using regular expressions as a powerful way to find
and use certain patterns in strings (`stringr`).  Finally, the tidyverse
includes powerful functions for visualizing data, based around the `ggplot2`
package, which implements a "grammar of graphics" within R. We cover some
tidyverse tools you may find helpful for pre-processing biomedical data in 
module 3.5.

You can install and load any of these tidyverse packages one-by-one using the
`install.packages` and `library` functions with the package name from within R.
If you are planning on using many of the tidyverse packages, you can also
install and load many of the tidyverse functions by installing a package called
tidyverse, which serves as an umbrella for many of the tidyverse packages.

In addition to the original tools in the tidyverse, many people have developed
*tidyverse extensions*---R packages that build off the tools and principles in
the tidyverse. These often bring the tidyverse conventions into tools for
specific areas of science. For example, the `tidytext` package provides tools to
analyze large datasets of text, including books or collections of tweets, using
the tidy data format and tidyverse-style tools. Similar tidyverse extensions
exist for working with network data (`tidygraph`) or geospatial data (`sf`).
Extensions also exist for the visualization branch of the tidyverse
specifically. These include *ggplot extensions* that allow users to create
things like calendar plots (`sugrrants`), gene arrow maps (`gggene`), network
plots (`igraph`), phytogenetic trees (`ggtree`) and anatogram images
(`gganatogram`). These extensions all allow users to work with data that's in a
tidy data format, and they all provide similar user interfaces, making it
easier to learn a large set of tools to do a range of data analysis and
visualization, compared to if the set of tools lacked this coherence.



<!-- ### Practice quiz -->

<!-- Question 1: What is a fundamental characteristic of tidy data?  -->

<!-- a) All data are stored in a single column  -->

<!-- b) Each variable is stored in a separate column [Correct answer]  -->

<!-- c) Observations are mixed within a single column  -->

<!-- d) Redundant information is encouraged for completeness  -->



<!-- Question 2: In tidy data, how should missing values be handled?  -->



<!-- a) Replace them with zeros for consistency  -->

<!-- b) Remove all observations with missing values; tidy data cannot have missing values  -->

<!-- c) Keep them as-is, with no special treatment  -->

<!-- d) Use a consistent representation (e.g., “NA”, or consistently leave them as an empty cell)  -->



<!-- Question 3: Which of the following is the only statement that does not describe one of the advantages of using a tidy data format?  -->



<!-- a) This data format is commonly taught in beginning data analysis courses, and so there are numerous resources to help learn to use it.  -->

<!-- b) This format will always make the data easy to read and interpret when printed out in a paper document [Correct answer]  -->

<!-- c) Many functions and packages in the R programming environment are available to work with data stored in this format  -->

<!-- d) This format is appropriate for chaining together simple commands to create a more complex pipeline to analyze and process data  -->



<!-- Question 4:  -->

<!-- In a data collection template, which of the following are considered extra elements and should be removed to help make the template meet the standards for “tidy” data? Select all that apply.  -->



<!-- a) Embedded formulas that make calculations between cells [Should be selected]  -->

<!-- b) An area to record the observed data from the experiment  -->

<!-- c) Plots [Should be selected]  -->



<!-- Question 5: Which is not one of the guidelines for the tidy data format?  -->



<!-- a) A single variable is not spread across multiple columns  -->

<!-- b) Each observation forms a row  -->

<!-- c) All data from an experiment are included in the same table, even if data are collected at different units of observation [Correct answer]  -->

<!-- d) Each variable forms a column  -->



<!-- Question 6: What is a tabular data format?  -->



<!-- a) One in which the data are hierarchical, with different numbers of observations for each study subject  -->

<!-- b) One in which the data form a matrix, with the same number of rows and columns and the same type of data in each table cell  -->

<!-- c) One in which the data are arranged in a rectangular, two-dimensional format, with rows and columns [Correct answer]  -->

<!-- d) One in which the data adhere to all rules of the tidy data format  -->



<!-- Question 7: You are conducting a study that compares a drug to a placebo. You take measurements on human subjects over time, recording measurements at five timepoints for each study subject. Which of the following is true of this dataset?  -->



<!-- a) The unit of observation is the combination of study subject and timepoint, while the unit of analysis is the treatment (drug or placebo) [Correct answer]  -->

<!-- b) The unit of observation is the study subject, while the unit of analysis is the treatment (drug or placebo)  -->

<!-- c) The unit of observation is the timepoint, while the unit of analysis is the human subject  -->

<!-- d) The unit of observation and the unit of analysis are both the treatment (drug or placebo)  -->



<!-- Discussion questions -->

<!-- What are your main considerations when you decide how to record your data?   -->

<!-- Based on the reading, can you define the tidy data format? Were you familiar with this format before preparing for this discussion? Do you use some of these principles when recording your own data?  -->

<!-- Describe advantages, as well as potential limitations, of storing data in a tidy data format  -->

<!-- In data that you have collected, can you think of examples when the data collection format included extra elements, beyond simply space for recording the data? Examples might include plots, calculations, notes, and highlighting. What were some of the advantages of having these extra elements in the template? Based on the reading or your own experience, what are some disadvantages to including these extra elements in a data collection template?  -->

<!-- In research collaborations, have you experienced a case where the data format for one researcher created difficulties for the other?  -->
