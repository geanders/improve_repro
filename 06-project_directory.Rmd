## Power of using a single structured 'Project' directory for storing and tracking research project files {#module6}

To improve the computational reproducibility of a research project, researchers
can use a single 'Project' directory to collectively store all research data,
meta-data, pre-processing code, and research products (e.g., paper drafts,
figures). We will explain how this practice improves the reproducibility and
list some of the common components and subdirectories to include in the
structure of a 'Project' directory, including subdirectories for raw and
pre-processed experimental data.

**Objectives.** After this module, the trainee will be able to:

- Describe a 'Project' directory, including common components and subdirectories 
- List how a single 'Project' directory improves reproducibility

In previous modules, we've discussed how you can separate the steps of data
collection from steps of data cleaning, management, pre-processing, and 
analysis. In the case of data recorded in a laboratory, this separation 
will often result in moving from a single file (for example, a spreadsheet), 
that combines all the steps to using separate files for the different 
steps. As you move to separate data recording and analysis, then, it 
is very important to store the set of files for a project in a way that 
is clear and easy to manage. If the file directory for the project is 
well-designed, it can even allow you to create software tools and report templates
that can be reused over many experiments. In the next few modules, we'll 
discuss how you can organize the files for an experiment using R's "Project"
system. The modules will discuss the advantages of well-designed project
directories, tips for arranging files within a project directory, 
and how to create templates for R Projects that allow you to use consistent
file organization across many experiments.

### Organizing project files 

In earlier modules, we discussed how to separate data collection from data
analysis. By separating data collection and analysis into separate files, we can
make the file for each step simpler. Further, by separating, we can save the
files in plain text, which makes it easier to track them using version
controlled (discussed in later modules), which helps create a record of changes
made to the data or analysis code during the research process. As we continue
work on a project, we'll likely generate more types of files. These can include
other data files (both "raw" data---directly output from measurement equipment
or directly recorded from observations, as well as any "cleaned" version of this
data, after steps have been taken to preprocess the data to prepare it for
visualization and analysis in papers and reports). These files also include the
files with writing and presentations (posters and slides) associated with the
project, as well as code scripts for preprocessing data, for conducting data
analysis, and for creating and sharing final figures and tables.

While this process helps in reproducibility, it results in more files being
collected for an experiment. Instead of data and its analysis collected within a
single spreadsheet file, you may end up with multiple files of data collected
from the experiment, as well as separate files with scripts for processing,
analyzing, and visualizing the data. With more complex experiments, there may be
different data files with the data collected from different assays. For example,
you may run an experiment where you collect data from each research animal on
bacterial load, as well as flow cytometry data, as well as a measure of antibody
levels through ELISA. As a result, you may have one raw data file each from the
bacterial load (CFUs) assay and the ELISA analysis, as well as one file per
animal from the flow cytometry analysis.

As the files for a project accumulate, do you have a clear plan for keeping them
organized? Based on one analysis, many biomedical researchers do not. One study,
for example, surveyed over 250 biomedical researchers at the University of
Washington. They noted that, "a common theme surrounding data management and
analysis was that may researchers preferred to utilize their own individual
methods to organize data. The varied ways of managing data were accepted as
functional for most present needs. Some researchers admitted to having no
organizational methodology at all, while others used whatever method best suited
their individual needs" [@anderson2007issues]. One respondent answered, "They're
not organized in any way---they're just thrown into files under different
projects," while another said "I grab them when I need them, they're not
organized in any decent way," and another, "It's not even organized---a file on
a central computer of protocols that we use, common lab protocols but those are
just individual Word files within a folder so it's not searchable per se"
[@anderson2007issues].

> "Virtually all researchers use computers as a central tool in their 
workflow. However, our formal education rarely includes any training in 
how to organise our computer files to make it easy to reproduce results
and share our analysis pipeline with others. Without clear instructions, 
many researchers struggle to avoid chaos in their file structures, and so 
are understandable reluctant to expose their workflow for others to see. 
This may be one of the reasons that so many requests for details about 
method, including requests for data and code, are turned down or go 
unanswered." [@marwick2018packaging]

Computer files and directories are the workspace that you use when you are
working with data, including preprocessing and analyzing it. There are many
reasons why you will benefit from keeping this "workspace" well-organized. Adam
Savage, famous from the TV show *Mythbusters*, talks about why it is so critical
to keep an organized work space for any project when you are trying to complete a
substantial or complex project. He has a specific name for the process of 
organizing and cleaning up his work space, "knolling". He notes that, 

> "Beyond giving my brain the space to more easily take in what I am working with, 
knolling my work space throughout the day also reduces the likelihood that I'll 
lose things, and increases the likelihood that I will find them quickly when they 
do go missing. ... By forcing me to slow down (which I've learned actually allows
me to work faster), the whole process saves me time on the other end of my work 
process." [@savage2020every]

He goes even further, to discuss how important work space organization is in 
other cases where teams are trying to do substantial or complex projects: 

> "There's one group of makers who understand intuitively, perhaps more than
anyone else, the multifaceted value proposition that knolling represents: chefs.
They call it *mise en place*. Coined in the late nineteenth century by famed
French chef August Escoffier, *mise en place* translates roughly to 'everything
in its place'. The principle behind it, cribbed from Escoffier's military
service during the Franco-Prussian War, is really about order and discipline."
[@savage2020every]

> "Kitchens are pressure cookers in which wasted movement and hasty technique
can ruin a dish, slice an artery, burn a hand, land you in the weeds, and
ultimately kill a restaurant. *Mise en place* is the only way to reliably create
a perfect dish, to exact specifications, over and over again, night after night,
for paying customers who demand nothing less." [@savage2020every]

> "For all the alchemy that goes into building something, the magic of making is
only possible because of the many repetitive processes we endure in preparation
for final assembly, and then the deliberate way we put it all together. And the
only way to get that prep right, to get your *mise* in its proper *place* is to
slow down, address your work properly, and lock your shit down."
[@savage2020every]

You gain in many ways when you organize your work space, especially when you do
it consistently. You know where to find things. You know where to find things
when you come back to a project after a while away from it (for example, while
the paper was out for review). You can teach someone else how to find things
quickly and consistently across your multiple projects, as well as where to put
things if they're contributing to one of them. You have a place for everything.
An organized work space can help foster organized thinking. 

> "Your working environment, whether it's a supermarket, office, studio, or 
building site, persuades you to work and think in certain ways. The more aware
you are of that, and the more you understand your medium, the more you can use 
it to your advantage." [@judkins2016art]

> "I started to clean up before heading upstairs at the end of the day, and lo and 
behold the shop became a far more efficient and well-oiled maching to work in. The
freed-up space in my mind and the open work space at my fingertips allowed me a lot 
of room, both mental and physical, to pursue a wide variety of projects, and I finally 
started to understand how much benefit was to be gained by taking the time to clean."
[@savage2020every]

> "A research compendium should organize its files according to the prevailing
conventions of the scholarly community, whether that be an academic discipline
or a lab group. Following these conventions will help other people recognize the
structure of the project, and also support tool building which takes advantage
of the shared structure." [@marwick2018packaging]

```{marginfigure, echo = TRUE}
"Just as a well-organized laboratory makes a scientist's life easier, a well-organized
and well-documented project makes a bioinformatician's life easier. Regardless of the 
particular project you're working on, your project directory should be laid out in a 
consistent and understandable fashion. Clear project organization makes it easier
for both you and collaborators to figure out exactly where and what everything is. 
Additionally, it's much easier to automate tasks when files are organized and 
clearly named. For example, processing 300 gene sequences stored in separate FASTA
files with a script is trivial if these files are organized in a single directory and 
are consistently named." 

[@buffalo2015bioinformatics]
```

> "Using research compendia has benefits both for you as a researcher, and for the community that you work in. Among the most important of these benefits is that a research compendium is a convenient way to publicly share data and code (McKiernan et al. Citation2016). Papers with publicly available datasets may receive a higher number of citations than similar studies without available data (Piwowar, Day, and Fridsma Citation2007; Piwowar and Vision Citation2013; Henneken and Accomazzi Citation2011; Dorch Citation2012; Sears Citation2011; Gleditsch and Strand Citation2003)." [@marwick2018packaging]

> "In our own experience as researchers, we enjoy benefits of increased efficiency through simplified file management and streamlined analytical workflows. These help us to work more transparently and efficiently. While reproducibility is no guarantee of correctness, making our results reproducible lets us inspect our own work for errors. Because we follow the same pattern in multiple projects, the startup and reentry costs for each project are significantly reduced. The result is that we save time, and minimize the risk of errors that might result from results which cannot be reproduced. A compendium makes it easier to communicate our work with others, including collaborators and (not least of all) our future selves. As members of research communities, by including compendia with our published papers we benefit from increased impact and visibility of our research among our peers."
[@marwick2018packaging]

> "There are many barriers to sharing reproducible code and corresponding computational results14. One barrier is simply that
keeping code and results sufficiently organized and documented
is difficult---it is burdensome even for experienced programmers who are well-trained in relevant computational tools
such as version control (discussed later), and even harder for
the many domain scientists who write code with little formal
training in computing and informatics15" [@blischak2019creating]

> "I will not describe profound issues such as how to formulate hypotheses, design experiments, or draw conclusions. Rather, I will focus on relatively mundane issues such as organizing files and directories and documenting progress. These issues are important because poor organizational choices can lead to significantly slower research progress." [@noble2009quick]

### How to organize project files

First, and at a minimum, you should get in the habit of storing all of the files
for an experiment in the same place. First, they should all be on the same
computer. The files should **not** be spread across multiple computers. Second,
they should all be in a single directory within the file system of that
computer. The files should not be spread across the fille system of a computer.
If you are collaborating across a team, you can all share copies of this single
project directory across your computers, using cloud services like Google Drive
or Microsoft teams. In other words, you can have multiple copies of the
directory, so that it can be accessed from different computers, but you
shouldn't store some of the files on one computer and some on another computer.
Ideally, if you're sharing access to the directory, you should do it in a way
that the changes that one person makes are synced by other users, so you're all
working from the same version of the project directory. In later modules, we
will discuss how this can be done with some version control systems.

> "When you begin a new project, you will need to decide upon some organizational structure for the relevant directories. It is generally a good idea to store all of the files relevant to one project under a common root directory. The exception to this rule is source code or scripts that are used in multiple projects. Each such program might have a project directory of its own." [@noble2009quick]

```{marginfigure, echo = TRUE}
"All files and directories used in your project should live in a single project directory
with a clear name. During the course of a project, you'll have amassed data files, notes, 
scripts, and so on---if these were scattered all over your hard drive (or worse, across 
many computers' hard drives), it would be a nightmare to keep track of everything. Even 
worse, such a disordered project would later make your research nearly impossible to 
reproduce." 

[@buffalo2015bioinformatics]
```

> "Perhaps the most complex undertaking so far has been developing practices for
curating and preserving all the data that underpin a paper, including
replicates. This took about a year, working with senior faculty members, the
information-technology team and another research manager. We trialled our
data-archiving system with a couple of groups, implemented it across our
institute for a year and then amended it on the basis of feedback. Instead of
squirrelling away data in individual folders and lab books, researchers now
archive all published data in a designated central drive, so that the
information is accessible for the long haul. Initially, people thought the
process was just extra bureaucratic work, or that it had been invented so I
could police their data. Now, it has become the norm, and researchers tell me
they save time and worry by having their data organized and archived."
[@winchester2018give]

There are a number of advantages to keeping all files related to a single
project inside a dedicated file directory on your computer. First, this provides
a clear and obvious place to search for all project files throughout your work
on the project, including after lulls in activity (like waiting for reviews from
a paper submission). 

By keeping all project files within a single directory, you also make it easier
to share the collection of files for the project. The directory acts as a
sharable unit that can be easily shared as a collected unit (for example,
through posting on Google Drive or compressing the directory and emailing the
resulting file). There are several reasons you might want to share these files.
An obvious one is that you likely will want to share the project files across
members in your research team, so they can collaborate together on the project.
However, there are also other reasons you'd need to share files, and one that is
growing in popularity is that you may be asked to share files (data, code
scripts, etc.) when you publish a paper describing your results.

When files are all stored in one directory, the directory can be compressed and
shared as an email attachment or through a file sharing platform like Google
Drive. As you learn more tools for reproducibility, you can also share the
directory through some more dynamic platforms, that let all those sharing access
continue to change and contribute to the files in the directory in a way that is
tracked and reversible. In later modules in this book, we will introduce `git`
version control software and the GitHub platform for sharing files under this
type of version control---this is one example of this more dynamic way of
sharing files within a directory. To gain the advantages of directory-based
project file organization, all the files need to be within a single directory,
but they don't all have to be within the same "level" in that directory.
Instead, you can use subdirectories to structure and organize these files, while
still retaining all the advantages of directory-based file organization. This
will help limit the number of files in each "level" of the directory, so none
becomes an overwhelming slew of files of different types. It can help you
navigate the files in the directory, and also help someone you share the
directory with figure out what's in it and where everything is.

When all the materials for a project are stored in a single directory, it makes
it easier to share the set of files through version control and online 
version control platforms [@vuorre2021sharing]. (This also helps you to track
changes to files across the project [@vuorre2021sharing]). 

Next, you should organize the files within that directory in a meaningful 
way. Files can be separated into subdirectories by type or other organizing 
principle. Computer file systems are well-structured to use a hierarchical 
design, with subdirectories nested inside directories. You can leverage this 
to manage the complexity and breadth of files for your project.

> "The core guiding principle is simple: Someone unfamiliar with your project should be able to look at your computer files and understand in detail what you did and why. This 'someone' could be any of a variety of people: someone who read your published article and wants to try to reproduce your work, a collaborator who wants to understand the details of your experiments, a future student working in your lab who wants to extend your work after you have moved on to a new job, your research advisor, who may be interested in understanding your work or who may be evaluating your research skills. Most commonly, however, that 'someone' is you. A few months from now, you may not remember what you were up to when you created a particular set of files, or you may not remember what conclusions you drew. You will either have to then spend time reconstructing your previous experiments or lose whatever insights you gained from those experiments." [@noble2009quick]

> "This leads to the second principle, which is actually more like a version of Murphy's Law: Everything you do, you will probably have to do over again. Inevitably, you will discover some flaw in your initial preparation of the data being analyzed, or you will get access to new data, or you will decide that your parameterization of a particular model was not broad enough. This means that the experiment you did last week, or even the set of experiments you've been working on over the past month, will probably need to be redone. If you have organized and documented your work clearly, then repeating the experiment with the new data or the new parameterization will be much, much easier." [@noble2009quick]

> "As a project grows you might start with a single file, then add subdirectories which have consistent and readily meaningful names such as “analysis,” “data,” and so on. There are few strict rules here. The key principle is to organize the compendium so that another person can know what to expect from the plain meaning of the file and directory names. Using widely held conventions, such as the R package structure, will help other people to understand how your files relate to each other without having to ask you. Naming objects is difficult to do well, so it is worth to put some effort into a logical and systematic file naming convention if you have a complex project with many files and directories (e.g., a multi-experiment study where each experiment has numerous data and code files)." [@marwick2018packaging]

Marwick's three generic principles of research compendia: 

> "it organizes the files according to a convention, separates data and code, and specifies the computational environment" [@marwick2018packaging]

For example, you can use one level of generic subdirectories to organize the
files into sections for raw (unprocessed) data, processed data, data processing
scripts, data analysis scripts, output reports, output figures, and so on.
Within each of those subdirectories, you might have more subdirectories. For
example, the raw data subdirectory might include subdirectories for data from
different types of assays (immune cell populations measured through flow
cytometry, bacterial load measured through CFUs, antibodies measure through
ELISA, etc.)

Subdirectory organizations can also, it turns out, be used in clever ways within
code scripts applied to files in the directory. For example, there are functions
in all scripting languages that will list all the files in a specified
subdirectory. If you keep all your raw data files of a certain type (for
example, all output from running flow cytometry for the project) within a single
subdirectory, you can use this type of function with code scripts to list all
the files in that directory and then apply code that you've developed to
preprocess or visualize the data across all those files. This code would
continue to work as you added files to that directory, since it starts by
looking in that subdirectory each time it runs and working with all files there
as of that moment.

```{marginfigure, echo = TRUE}
"Organizing data files into a single directory with consistent filenames prepares us to 
iterate over *all* of our data, whether it's the four example files used in this example, 
or 40,000 files in a real project. Think of it this way: remember when you discovered you 
could select many files with your mouse cursor? With this trick, you could move 60 files
as easily as six files. You could also select certain file types (e.g., photos) and attach
them all to an email with one movement. By using consistent file naming and directory 
organization, you can do the same programatically using the Unix shell and other 
programming languages." 

[@buffalo2015bioinformatics]
```

Even within a subdirectory, it is worthwhile to give a lot of thought to how you
name each file. First, special tools in languages like R can use
thoughtfully-designed files name to extract and use information in the file
names themselves, based on a technique called *regular expressions*. Second, the
file name is a valuable way to be very clear about exactly what is stored in the
file. The less murky the organization and naming of subdirectories and files
within the project directory is, the more likely that all research team members
will be able to correctly navigate and use the data stored for the project.

```{marginfigure, echo = TRUE}
"In addition to simplifying working with files, consistent naming is an often overlooked
component of robust bioinformatics. Bad naming schemes can easily lead to switched samples.
Poorly chosen filenames can also cause serious errors when you or collaborators think you're
working with the correct data, but it's actually outdated or the wrong file. I guarantee 
that out of all the papers published in the past decade, at least a few and likely many 
more contain erroneous results because of a file naming issue." 

[@buffalo2015bioinformatics]
```

Reasons to design and use a project template include: 

- Keep your project files organized (and so have more "brain space" for 
other parts of the work, also make it easier for everyone to find
everything, even after not looking at the project for a while)
- Use the same organization for all projects ("I have a system"; this 
way you can make tools that work across the projects based on the 
organization, this also helps you navigate projects when you revisit 
them, since they're organized the same way as your most recent projects, 
also this means that you only have to think once about organization, not
every time you start a project)
- Be able to share and publish project files easily 

### What is a project template?

Here is how a project template will work when it is used: You will have a 
template directory saved on your computer, which will include subdirectories
and example templates for data collection and the project report. When you 
start a new project, you will copy that template directory, rename it, 
and then start filling it with your data for that particular project. 
As you use it for a project, you can customize it as you need. For example, 
if you had included a subdirectory for flow cytometry data, but are not 
running that assay in this experiment, you can remove that subdirectory. 
Similarly, you can customize the report as you go to help it work well for 
this specific experiment. However, you will aim to keep to the standard
format as much as possible, since it's the standardization across projects
that provides many of the advantages.

The next step is to not only use a structured directory for each project / 
experiment, but to start using the same, standardized structure for every 
one of your experiments. This takes more work, in particular to design a 
structure that can be used across many projects. However, the gains in terms
of organization and efficiency can be extraordinary. We have not run across
many people who have taken the time to set this up. However, among the 
people we know who have, we haven't run across many who ever went back 
once they started using this type of system. 

This involves first designing a common template for the directory structure for
your projects. Once you have decided on a structure for this template, you can
create a version of it on your computer---a file directory with all the
subdirectories included, but without any files (or only template files you'd
want to use as a starting point in each project). When you start a new project,
you can then just copy this template and rename it. If you are using R and begin
to use R Projects (described in the next section), you can also create an R
Studio Project template to serve as this kind of starting point each time you
start a new project.

In other areas of science and engineering, this idea of standardized directory
structures has allowed the development of powerful techniques for open-source
software developers to work together. For example, anyone may create their own
extensions to the R programming language and share these with others through
GitHub or several large repositories. This is coordinated by enforcing a common
directory structure on these extension "packages"---to create a new package, you
must put certain types of files in certain subdirectories within a project
directory. With these standardized rules of directory structure and content,
each of these packages can interact with the base version of R, since there are
functions that can tap into any of these new packages by assuming where each
type of file will be within the package's directory of files. In a similar way,
if you impose a common directory structure across all the project directories in
your research lab, your collaborators will quickly be able to learn where to
find each element, even in projects they are new to, and you will all be able to
write code that can be easily applied across all project directories, allowing
you to improve reproducibility and comparability across all projects by assuring
that you are conducting the same preprocessing and analysis across all projects
(or, if you are conducting things differently for different projects, that you
are deliberate and aware that you are doing so).

In fact, some go so far as to suggest bundling research project
data and results as an R package [@vuorre2021sharing; @marwick2018packaging].

> "Our contribution to this drive to improve the openness and reproducibility 
of research is to show how R packages can be used as a research 
compendium for organising and sharing files. Although the R packaging 
system is traditionally a method for sharing statistical methods, we claim 
that R packages are suitable for use as research compendia that can help
improve computational reproducibility. Our focus is on the R programming 
language because the R package structure is uniquely suitable to being 
easily adapted to solve problems of organising files and sharing them with 
other researchers." [@marwick2018packaging]

> "Borrowing standards and best practices from software development, such as 
the R package standard discussed here, may serve to dramatically improve 
reproducibility. By organizing research assets in a standard format, their 
reuse will be less time consuming and error-prone. Standard formats also 
facilitate large-scale meta-scientific investigations (Gorgolewski et al., 
2016), and inclusion of data sets for meta-analysis." [@vuorre2021sharing]

```{marginfigure, echo = TRUE}
"Because automating file processing tasks is an integral part of bioinformatics, 
organizing our projects to facilitate this is essential. Organizing data into subdirectories
and using clear and consistent file naming schemes is imperative---both of these practices
allow us to *programmatically* refer to files, the first step to automating a task. 
Doing something programatically means doing it through code rather than manually, using
a method that can effortlessly scale to multiple objects (e.g., files). Programatically
referring to multiple files is easier and safer than typing them all out (because it's 
less error prone.)" 

[@buffalo2015bioinformatics]
```


There have been burgeoning efforts to standardize the way data are 
organized in some research groups over time: 

> "Some of the earliest examples of these principles [of research compendia] in
action can be found in the work of Claerbout (Citation1992) and his colleagues
in the early 1990s. Claerbout introduced these practices in his Stanford
geophysics research group (Buckheit and Donoho Citation1995). Other early
examples of compendia appeared in econometrics (Koenker Citation1996), (Vinod
Citation2001), epidemiology (Peng, Dominici, and Zeger Citation2006), and signal
processing (Vandewalle, Kovacevic, and Vetterli Citation2009)."
[@marwick2018packaging]

> "A notable effort to promote reproducible research at the undergraduate level is Project TIER (Teaching Integrity in Empirical Research) for economics majors at Haverford College. Project TIER provides a detailed file-structure protocol for students working on their senior thesis, aiming to teach students to document their data management and analysis to enable an independent researcher to reproduce the student’s data processing and analysis (Ball and Medeiros Citation2012)." 
[@marwick2018packaging]

You can have different templates to use when you create a new project. A
template will provide a basic set-up for the project. For example, R packages
are created within a directory, and package developers now often use an R
Project for that directory. Since R packages have a consistent set of usual
subdirectories---including subdirectories for R code, data, and documentation
and a file for metadata---there is now a Project Template specificially for R
package development, and this sets up a directory with some of the typical
subdirectories and files needed for these projects. 

In the next module, we will walk through the steps of designing a project
template that you can use across experiments for your laboratory group. In
module 2.8, we'll walk through an example of creating and using this kind of
project template for an example set of studies.

An even broader step would be to standardize formats across research groups, 
to have standard formats within a larger scientific discipline. 

> "More generally, however, tools and descriptions, such as Psych-DS, 
rrtools, vertical, and workflowr are pointing to the same general idea: 
Psychological sciences would benefit from standardizing, as much as 
possible, the ways in which computational work is curated. We think 
that these tools and principles are in their infancy, and their importance
is only now being recognized. Therefore, we expect these tools to 
evolve and mutate to best suit the practical researchers' needs."
[@vuorre2021sharing]

Efforts to standardize across a discipline or multiple disciplines: 

- workflowr: An R package that sets up a project directory and provides
functionality to automate analysis [@blischak2019creating]. It has been
used for example biomedical studies that include scRNA-seq analysis, 
DNA sequencing data, genomic studies [@blischak2019creating].

> "Workflowr aims to instill a particular "workflow"---a sequence of
steps to be repeated and integrated into research practice---that
helps make projects more reproducible and accessible. This workflow
integrates four key elements: (1) version control (via Git); (2) literate
programming (via R Markdown); (3) automatic checks and safeguards
that improve code reproducibility; and (4) sharing code and results via
a browsable website. These features exploit powerful existing tools,
whose mastery would take considerable study. However, the
workflowr interface is simple enough that novice users can quickly
enjoy its many benefits. By simply following the workflowr
 "workflow", R users can create projects whose results, figures, and
development history are easily accessible on a static website---
thereby conveniently shareable with collaborators by sending them a
URL---and accompanied by source code and reproducibility
safeguards." [@blischak2019creating]

**Alternatives**

Other alternatives might include electronic notebooks or project systems. 
Another tool of use in this area is cloud-based file management (Google, 
Teams). The most common alternative might be to not have any organizing
system.

--------------------------------------------------------------------------

> "We also needed to cope with a large amount of data going from each lab [involved
in trying to reproduce results] to a single database. We wrote an iPad app so that
measurements were entered directly into the system and not jotted on paper to 
be entered later. The app prompted us to include a full description for each 
plate of worms, and ensured that data and metadata for each experiment were proofread
(the strain names MY16 and my16 are not the same). This simple technology removed
small recording errors that could disproportionately affect statistical analyses."
[@lithgow2017long]

> "Any scientist adopting a QA system has to wager that the up-front hassle will
pay off in the future. 'It is very difficult to get people to check and annotate
everything, because they think it is nonsense,' says Carmen Navarro-Aragay, head
of the University of Barcelona quality team that worked with Cirera. 'They
realize the value only when they get results that they do not understand and
find that the answer is lurking somewhere in their notebooks.' Even when
experiments go as expected, quality systems can save time, says Murtaugh.
Methods and data sections in papers practically write themselves, with no time
wasted in frenzied hunting for missing information. There are fewer questions
about how experiments were done and where data are stored, says Murtaugh. 'It
allows us to concentrate on biological explanations for results.'"
[@baker2016quality]

> "Having data that are traceable — down to who did what experiment on which
machine, and where the source data are stored — has knock-on benefits for
research integrity, says Nett. 'You can't pick out the data that you want.'
Researchers who must provide strong explanations about why they chose to leave
any information out of their analysis will be less tempted to cherry-pick data.
QA can also weed out digital meddling: popular spreadsheet programs such as
Microsoft Excel can be vulnerable to errors or manipulation if not properly
locked, but QA teams can set up instruments to store read-only files and prevent
researchers from tampering with data accidentally or intentionally."
[@baker2016quality]

> "Data should be logged in a lab notebook, not scribbled onto memo paper or 
other detritus and carelessly transcribed. Notebooks should be bound or digital; 
loose paper can too easily be lost or removed." [@baker2016quality]

> "Protocols should be followed to the letter or deviations documented." [@baker2016quality]

> "Heres how to [knoll:] 1. Examine your work space for all items not in use---tools, 
materials, books, coffee cups, it doesn't matter what it is. 2. Remove those unused
items from your space. When in doubt, leave it on the table. 3. Group all like items---pens
with pencils, washers with O-rings, nuts with bolts, etc. 4. Align (parallel) or square
(90-degree angle) all objects within each group to each other and then to the surface
upon which they sit." [@savage2020every]

> "Kitchens are pressure cookers in which wasted movement and hasty technique can 
ruin a dish, slice an artery, burn a hand, land you in the weeds, and ultimately kill 
a restaurant. *Mise en place* is the only way to reliably create a perfect dish, to 
exact specifications, over and over again, night after night, for paying customers
who demand nothing less." [@savage2020every]

> "A shop is not simply a place to make things. Yes, it's where we collect our 
materials, our tools, our notes, and our half-completed ideas, but it's also a 
manifestation of how we think about organization, project management, and working
priorities." [@savage2020every]

> "It often seems like our first work spaces are an energized mess of chaos and creation
that we will insist at the time we have our arms around, but with some distance---both
physical and temporal---we will realize was retarding our creative output in one way
or another." [@savage2020every]

> "Not all organizational methodologies are created equal. One could be spotlessly 
organized, with everything put away and labeled and color coded, and it could feel 
like a prison with the walls closing in around you. Another could be equally organized
but a bit more open and exposed, and it could untap creative genius like no other space 
you've worked in." [@savage2020every]

> "Luck favors the prepared mind." ---Louis Pasteur

> "With a science like molecular biology, we inevitably have an image in our heads of the
scientist alone in the lab, hunched over a microscope, and stumbling across a major new
finding. But Dunbar's study showed that those isolated eureka moments were rarities. 
Instead, most important ideas emerged during regular lab meetings, where a dozen or 
so researchers would gather and informally present and discuss their latest work. If 
you looked at the map of idea formation that Dunbar created, the ground zero of 
innovation was not the microscope. It was the conference table." [@johnson2011good]

> "Even with all the advanced technology of a leading molecular biology lab, the most
productive tool for generating good ideas remains a circle of humans at a table, talking 
shop." [@johnson2011good]

> "You get a feeling that there's an interesting avenue to explore, a problem
that might someday lead you to a solution, but then you get distracted by more
pressing matters and the hunch disappears. So part of the secret of hunch
cultivation is simple: write everything down." [@johnson2011good]

> "We can track the evolution of Darwin's ideas with such precision because he 
adhered to a rigorous practice of maintaining notebooks where he quoted
other sources, improvised new ideas, interrogated and dismissed false leads, 
drew diagrams, and generally let his mind roam on the page. We can see Darwin's 
ideas evolve because on some basic level the notebook platform creates a 
cultivating space for his hunches; it is not that the notebook is a mere
transcription of the ideas, which are happening offstage somewhere in Darwin's 
mind. Darwin was constantly rereading his notes, discovering new implications.
His ideas emerge as a kind of duet between the present-tense thinking brain and all 
those past observations recorded on paper." [@johnson2011good]


<!-- ### Practice quiz -->

