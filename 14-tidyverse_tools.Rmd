## Simplify scripted pre-processing through R's 'tidyverse' tools {#module14}

The R programming language now includes a collection of 'tidyverse' extension
packages that enable user-friendly yet powerful work with experimental data,
including pre-processing and exploratory visualizations. The principle behind
the 'tidyverse' is that a collection of simple, general tools can be joined
together to solve complex problems, as long as a consistent format is used for
the input and output of each tool (the 'tidy' data format taught in other
modules). In this module, we will explain why this 'tidyverse' system is so
powerful and how it can be leveraged within biomedical research, especially for
reproducibly pre-processing experimental data.

**Objectives.** After this module, the trainee will be able to:

- Define R's 'tidyverse' system 
- Explain how the 'tidyverse' collection of packages can be both user-friendly
and powerful in solving many complex tasks with data
- Describe the difference between base R and R's 'tidyverse'.

### What are data structures / containers?

**Data types versus data structures**

You can think of *data structures* as containers that hold your data in R, 
holding it in a way that lets you access and work with the data. 

One important distinction is between data structures and data types. A 
*data type* refers to the characteristic of the data. Is it a date, for
example, or a number, or a character string? R can do different types of
things with different types of data---for example, R can add together 
two pieces of data that are numbers, while for two pieces of data that are
dates, it can tell which is the later date. For character strings, R can 
look for patterns in the string (for example, does it include any capital 
letters? Does is start with "b"?). All pieces of data are, at the deepest
level, stored as a string of 0s and 1s. By assigning a data type like 
"character string" or "numeric" to each piece of data, R can make more 
sense of each piece of data in terms of what operations are reasonable to 
perform on the data, helping to translate those 0s and 1s into something
more meaningful. 

In R, your data can be stored as different *types* of data: whole numbers can be
stored as an *integer* data type, continuous [?] numbers through a few types of
*floating* data types, character strings as a *character* data type, and logical
data (which can only take the two values of "TRUE" and "FALSE") as a *logical*
data type. More complex data types can be built using these---for example,
there's a special data type for storing dates that's based on a combination of
an [integer?] data type, with added information counting the number of days [?]
from a set starting date (called the [Unix epoch?]), January 1, 1970. (This
set-up for storing dates allows them to be printed to look like dates, rather
than numbers, but at the same time allows them to be manipulated through
operations like finding out which date comes earliest in a set, determining the
number of days between two dates, and so on.) R uses these different data types
for several reasons. First, by using different data types, R can improve its 
efficiency [?] in storing data. Each piece of data must---as you go deep in the
heart of how the computer works---as a series of binary digits (0s and 1s). 
Some types of data can be stored using fewer of these *bits* (*bi*nary dig*its*).
Each measurement of logical data, for example, can be stored in a single bit, 
since it only can take one of two values (0 or 1, for FALSE and TRUE, respectively).
For character strings, these can be divided into each character in the string 
for storage (for example, "cat" can be stored as "c", "a", "t"). There is a set
of characters called the ASCII character set that includes the lowercase and 
uppercase of the letters and punctuation sets that you see on a standard 
US keyboard [?], and if the character strings only use these characters, they 
can be stored in [x] bits per character. For numeric data types, integers can 
typically be stores in [x] bits per number, while continuous [?] numbers, 
stored in single or double floating point notation [?], are stored in [x] 
and [x] bits respectively. When R stores data in specific types, it can be
more memory efficient by packing the types of data that can be stored in less
space (like logical data) into very compact structures.

Data structures, on the other hand, allow you to keep together, as well as
refer to, data that you have loaded into R. A single object can contain 
pieces of data with different data types, and the object's data structure
defines how all the pieces of data in the object are organized and how you 
can access and work with the data in the structure. For example, one of the
simplest data structures in R is the vector---this data structure requires
that all the data stored in it have the same data type (e.g., all be 
numeric), and it holds together a one-dimensional string of pieces of data
of that type. For example, a vector of the numbers one to five would be 
the string of those numbers---1, 2, 3, 4, 5---while a 
vector of the letters a to e would be the string of data with the "character"
type---"a", "b", "c", "d", "e". 

One of the "building block"
data structures in R is the vector. This data structure is one dimensional and
can only contain data that have the same data type---you can think of this as a
bead string of values, each of the same type. For example, you could have a
vector that gives a series of names of study sites (each a character string), or
a vector that gives the dates of time points in a study (each a date data type),
or a vector that gives the weights of mice in a study (each a numeric data
type). You cannot, however, have a vector that includes some study site names
and then some dates and then some weights, since these should be in different
data types. Further, you can't arrange the data in any structure except a
straight, one-dimensional series if you are using a vector. The dataframe
structure provides a bit more flexibility---you can expand into two dimensions, 
rather than one, and you can have different data types in different columns of
the dataframe (although each column must itself have a single data type). 

A second key data structure in R, the 
dataframe, provides a structure that stores one or more of these vectors, 
and so it allows you to store data with different types in the same object. 
For example, you could create an object with a dataframe structure that 
contains one column with a vector of the numbers one to five and another 
with the letters a through e. This structure would look something like this:

```{r echo = FALSE}
letters_numbers <- data.frame(numbers = 1:5, letters = c("a", "b", "c", "d", "e"))
letters_numbers
```

While the dataframe structure can combine vectors with data in different types
(in the example, the `numbers` column has a numeric data type and the `letters`
column has a character data type), it does have a rule for combining different
vectors. They all must be the same length. This means that the dataframe 
structure is always rectangular, with each column having the same number of 
rows. In the example above, both the `numbers` and `letters` columns are 
vectors with five values, so the dataframe ends up having two columns and 
five rows. 

The
dataframe object type is a very basic two-dimensional format for storing data in
R. When you print it out, it will remind you of looking at data in a
spreadsheet. The two dimensions---rows and columns---allow you to include data
for one or more observations, with different values that were measured for each.
For example, if you were conducting a study of children's BMI and blood sugar,
you might have an observation for each child in the study, and values measured
for each child of height, weight, a blood sugar measure, study ID, and date of
the observation. 

The two-dimensional structure of a dataframe keeps the values
measured for each observation lined up with each other, and lets you keep them
aligned as you work with the data. You could also store data for each value as
separate objects, in one-dimensional vectors, which you can visualize as strings
of values of the same data type, like the dates that each observation was made,
or the weight of each study subject. However, when the data is in separate
vectors, it is easy to make coding mistakes, and coding is often less efficient.
If you want to remove one observation, for example, because you find it is a
duplicate, you would need to carefully make sure you remove it correctly from
each vector. When data are stored in a dataframe, you can remove the row for
that observation with one command, and you can be sure that you've removed the
value you meant to from each of the measured values. 

To be able to understand some key differences in the Bioconductor approach and the tidyverse approach, you first need to understand how programming uses **data structures** to store data, and that there can be numerous different data structures available within a programming language to handle different types of data. 

When you process data using a programming language, there will be different
structures that you can use to store data as you work with it. You can think of
these data structures as containers where you keep your data in the programming
environment while you work with it, and different structures organize the data
in different ways.

If you've read the earlier modules, you've already seen one example of a data
structure. In other modules, we've discussed the "tidyverse" approach to
processing data in R---this approach emphasizes the *dataframe* as a way to
store data while you're working with it (in other words, a data structure). In
fact, the use of the dataframe as data structure for data storage is one of the
defining features of the "tidyverse" approach. We mentioned in earlier modules
that the tidyverse approach is based on using a common interface, so that you
can mix and match small functions in different ways---the common interface is
the dataframe. The tidyverse approach is built on the use of a common structure
for storing data, the dataframe---almost all functions take data in this
structure and almost all return data in this structure.

Figure \@ref(fig:dfdatastructure) shows an annotated example of a dataframe,
highlighting some of the key elements of its structure. A dataframe stores data
in a two-dimensional structure, combining rows and columns. Each column is
constrained to have data of the same type---in other words, all values in a
column could be numeric (e.g., 1, 4, 10), or all could be character strings
(e.g., "Mouse 1", "Mouse 3"), but the same column cannot combine some values
that are numeric and some that are character strings. Across the dataframe, all
columns must have the same length (i.e., if you printed out the full dataframe,
it would look like a rectangle). All the column values should be lined up, so
that as you are reading across a row, the values in the column cells are from
the same observation or unit.

```{r dfdatastructure, echo = FALSE, out.width = "\\textwidth", fig.cap = "An example of the dataframe data structure. This data structure is the most frequently used data structure within the tidyverse approach, and its use is in fact a defining element of the approach.", fig.fullwidth = TRUE}
knitr::include_graphics("figures/dataframe.png")
```



**Common R data structures**

We just covered two of the most common data structures in R: the vector and 
the dataframe. You will come across a number of other structures as you work 
in R. 

One other very common data structure is the list. This is a very flexible 
data structure, and it allows enormous flexibility in collecting other types
of data structures (including other lists) into a single R object. 

In addition to dataframes, there are a number of other simple, general purpose
data structures that are often used to store data in R, and that you're likely
to come across as you work in R. These include **vectors**, which are used to
store one-dimensional strings of data of a single type (e.g., all numeric, or
all character strings; as a note, you can think of each column in a dataframe as
a vector), **matrices**, which are also used to store data of a single type, but
with a two-dimensional structure, and **arrays**, which, like matrices and
vectors, store data of a single type, but in three dimensions. Another common
general purpose data structure in R is the *list*, which allows you to combine
data stored in any type of structure to create a single R object, giving
enormous flexibility (but minimal set structure from one object to another).
This data structure is the building block for some of the more complex specific
data structures, which we'll cover next. 
The list structure in R has enormous
flexibility in terms of storing lots of data in lots of possible places. This
data can have different types and even different substructures. Some data
structures in R are very constrained in what type of data they can store and
what structure they use to store it. 

There are a number of simple,
general purpose data structures that are often used to store data in R. These
include **vectors**, which are used to store one-dimensional strings of data of
a single type (e.g., all numeric, or all character strings), **matrices**, which
are also used to store data of a single type, but with a two-dimensional
structure, and **dataframes**, which are used to store multiple vectors of the
same length, and so allow for storing measurements of different data types for
multiple observations.

[Figure: examples of these three structures]

Other data structures are more customized for specific types of data. For 
example, there are a number of specialized data structures that are commonly 
used in Bioconductor packages to store specific types of genomic [?] data. 
These structures have been specially designed to meaningfully arrange the 
types of data that are commonly generated in certain types of experiments [?]. 
While the common types of data structures the we mentioned before (vectors, 
dataframes, and lists) are all provided as part of base R, many of these more
specialized data structures are defined in R extensions (packages that you 
install once you've installed base R), and so you cannot access those data
structures until you have installed additional packages. 

In a later module, we will go into more depth about some of these specialized
data structures in R, and how in certain cases they are powerful tools for 
complex analysis or for working with complex data.  

**Link between functions and object classes**

In a language like R, you will find that data structures can be closely tied
to data structures and data types. In many cases, a function is designed to 
only work with data that is of a certain type or that is stored in a certain 
data structure. 

For example, the package named `lubridate` provides helpful functions for 
working with data that represent dates. Most of the functions in this package
will only work on a vector (the data structure) that contains pieces of 
data that are dates (the data type). There are functions in this package 
that can do things like extract the year, month, or day of month from a date, 
but only if you input a vector of data in with the date data type. 

Many other functions require that you input a dataframe. For example, there 
are functions in the `dplyr` package that allow you to extract a subset of 
a dataframe---for example, to extract a smaller dataframe with only certain 
rows or certain columns from the original. These functions require that you 
input data in a dataframe structure. 

For more complex or customized data structures, like the data structures
within the Bioconductor project, there will often be a whole suite of 
customized data structures, and functions associated with those data structures, 
that are used during a pipeline of analyzing data from a certain type of 
experiment. For example, there are data structures, and functions associated
with those structures, that are customized to work with flow cytometry data, 
and other collections of data structures and functions for working with 
metabolomics data, and so on. 

As a note, there are a few functions that are "generics"---they have been coded
in such a way that they will work with a variety of data structures. For 
these functions, they will often output different things depending on the type
of data structure that is input when the function is called. For example, 
the `summary` function is a generic function---you can input objects with a
variety of data structures and the function will work, providing some type
of summary of the object in each case. If you input an object with a vector
data structure, the `summary` function will provide a summary of the contents
of that vector---if the data type is numeric, it will give values like the 
median, the range, and the number of missing values, while if the data type
is logical (TRUE or FALSE for each data piece), it will give a count of the 
number of TRUE and FALSE values in the vector. If you input an object instead
that is a dataframe, the `summary` function will output a summary of each of
the columns in the dataframe. Although there are such generic functions that
work with different data structures, the general rule in R is that a 
function is typically designed to work with a certain data structure (in 
fact, the generic functions are coded to have different functions that work 
with each type of data structure, and so a structure-specific function is 
called "under the hood" once one of the generic functions is called). 

**"Tidy" data and data structures**

In this module, we aim to introduce you to something called the "tidyverse" 
approach to programming in R. This is a powerful approach, and we will 
detail it more extensively in the next section. Here, we want to introduce 
a key element of the approach---it is based on storing data in a single
data structure throughout your pipeline of code. Specifically, it focuses
on tools and techniques that work when you use a dataframe structure to 
store data as you work with the data, with a set of functions that both 
input and output objects that use the dataframe structure (as well as 
functions that input and output vectors, which are used to perform 
operations on the data in the columns within the dataframe---if you recall, 
each column in a dataframe has a vector data structure). 

The tidyverse approach requires one step beyond the data structure, and that
is how the data are arranged within that structure. You can store data in 
a dataframe structure as long as you can put it in two dimensions (rows and 
columns), with the same type of data in each column. With the same set of 
data, there are often many different arrangements you could make that satisfy 
these constraints. The "tidy" part of the "tidy data" structure---which is 
at the heart of the tidyverse approach---are a set of standards describing
exactly how you arrange the data within the dataframe structure. 

The R object class---dataframe, and more specifically, tibble---of the standard
format for data for a tidyverse approach is just the first part of the standard 
data format for the tidyverse approach. The second part of the standard format is 
how you organize your data in this format. To easily work with tidyverse functions, 
you'll want to make sure that your data is stored within that dataframe following
"tidy" data principals. These are fully described in an earlier module in this
book [which module]. If you use this data format to initially collect your
data, as described in an earlier module, you will find it very easy to read the
data into R and work within the tidyverse approach. When working with larger and
more complex data collected from laboratory equipment, you may find you need to 
do some preprocessing of the data using an object-oriented approach before you 
can move the data into this tidy format, but at that point, you can continue with
analysis and visualization of your data using a tidyverse approach. 

**The tibble data structure**

As a final note, there is a specialized data structure that is often 
associated with the tidyverse approach. It is called the "tibble", and it is 
a specialized version of the dataframe. What do we mean by it being a 
"specialized" version of a more common data structure? This means that there 
are a few functions that will give different results if the data are stored
in a tibble structure rather than the more generic dataframe structure. 
However, if a function does not have a specialized method for the tibble 
structure (and most functions don't), then the function will treat the object
as a regular data frame. Some of the few functions that have specialized
methods for tibbles include the `print` function, which is also the default
function that is run if you just type an object's name at the console and 
press "Enter". The `print` function, when called with an object in the tibble
structure, will only print out the first few rows (whereas, with a generic 
dataframe, it will print out all rows, sometimes resulting in a very long
print-out). Similarly, if there are many columns, it will only print out 
a certain number and just provide summaries for the rest (again, with a 
generic dataframe, everything would be printed). It also provides some nice
summaries of the dataframe as a whole, as well as of the type of data in each 
column. Overall, the `print` method for a tibble structure provides a clearer
and typically more useful overview of the data in the object than does the
`print` method for a generic dataframe structure. All this being said, as 
you first begin to work in the tidyverse approach, you often may not notice
whether your data is stored in a more generic dataframe structure or the 
more specialized tibble version, and it often won't matter much which of the
two structures is being used, since the tibble structure in most cases 
(i.e., for most functions) is just treated as the more generic dataframe
structure. 

Sometimes, you'll see that data in a tidyverse approach are stored in a special
type of dataframe called a "tibble"---this isn't very different from a
dataframe, and in fact is a special type of dataframe. It's only differences in
practice are that it has a slightly different `print` method. The `print` method
is the method that's run, by default, when you just type the R object's name 
at the console. A tibble prints more nicely than a basic dataframe. By default, 
it will only print the first few lines. By contrast, a dataframe will, by default, 
print everything---if you have a lot of data, this can create an overwhelming
amount of output when you just want to check out what the data looks like. The
printout of a tibble will also include some interesting annotations to help you 
see what's in the data, including the dimensions of the full dataframe and the 
data type of each column in the data. 

Sometimes, you'll see that data in a tidyverse approach are stored in a special
type of dataframe called a "tibble"---this isn't very different from a
dataframe, and in fact is a special type of dataframe. It's only differences in
practice are that it has a slightly different `print` method. The `print` method
is the method that's run, by default, when you just type the R object's name 
at the console. A tibble prints more nicely than a basic dataframe. By default, 
it will only print the first few lines. By contrast, a dataframe will, by default, 
print everything---if you have a lot of data, this can create an overwhelming
amount of output when you just want to check out what the data looks like. The
printout of a tibble will also include some interesting annotations to help you 
see what's in the data, including the dimensions of the full dataframe and the 
data type of each column in the data. 

### An overview of the "tidyverse" approach

Once data are in the "tidy" data format, you can create a pipeline of code that 
uses small tools, each of which does one simple thing, to work with the data. 
This work can include cleaning the data, adding values that are functions of the 
original values for each observation (e.g., adding a column with BMI based on 
values for each observation on height and weight), applying statistical models to 
test hypotheses, summarizing data to create tables, and visualizing the data. 

The "tidyverse" approach is an approach to using R that has grown enormously 
in popularity in recent years. Most R courses and workshops for beginning 
programmers are now structured around this approach. It provides a powerful
yet flexible approach for working with data in R, and it one of the easier
ways to start learning R. In this section, we'll cover the philosophy that
provides a framework for this approach. In the following sections of the 
modules, we'll go deeper into specific tools under this approach that can 
be used for common data preprocessing tasks when working with biomedical 
data, as well as provide information on more resources that can be used to
continue learning this approach. 

The term "elegance" often captures styles and approaches that are beautiful and
functional without unneeded extras or complexity. Engineers and scientists
sometimes use this term to capture approaches that achieve a desired result with
minimal complexity and friction. A coding problem, for example, could be solved
by an average coder with a hundred lines of code that get the job done, but a
very good coder might be able to solve the same problem with five lines of code
that are easy to follow. The second approach would be applauded as the "elegant"
solution. In mathematics, similarly, proofs can be complex and unwieldy, or they 
can be simple and elegant---this idea was beautifully captured by the Hungarian
mathematician Paul Erdos, who famously described very elegant mathematical proofs
as being from "The Book"---that is, God's own version of the proof of the 
mathematical idea. 

> "Paul Erdos liked to talk about The Book, in which God maintains the perfect
proofs for mathematical theorems, following the dictum of G. H. Hardy that
there is no permanent place for ugly mathematics. Erdos also said that you
need not believe in God but, as a mathematician, you should believe in
The Book." [Proofs from the Book, Third Edition, Preface]

The "tidyverse" approach in R is elegant. It is powerful, and gives you immense
flexibility once you've gotten the hang of it, but it's also so straightforward
that the basics can be quickly taught to and applied by beginning coders. It 
focuses on keeping data in a simple, standard format called "tidy" dataframes. 
By keeping data in this format while working with it, common tools can be applied
that work with the data at any stage of a "tidy" coding pipeline. These tools take
a "tidy" dataframe as their input, and they also output a "tidy" dataframe, with 
whatever change the function implements applied. Because each of these "tidyverse"
tools input and output data in the same standard format, they can be strung together
in order you want. By contrast, when functions input and output data in different
object types, they can only be joined in a specified order, because you can only 
apply certain functions to certain object types. 

Since the "tidyverse" tools can be strung together in any order, they can be
used very flexibly to build up to do interesting tasks. The tidyverse tools
generally each do very small and simple things. For example, one function
(`select`) just limits the data to a subset of its original columns; another
(`mutate`) adds or changes values in columns of the dataset, while another
(`distinct`) limits the dataframe to remove any rows that are duplicates. These
small, simple steps can be combined together in different patterns to add up to
complex operations on the data, while keeping each step very simple and clear.
Since the data stays in a standard and simple object type, it is easy to check
in on your data at any stage, as the common visualization tools for this
approach (from the `ggplot2` package and its extensions) can be always be
applied to data stored in a tidy dataframe.

**Uses the same data structure throughout**

The centralizing principal of the tidyverse approach is the format in which data
is stored throughout "tidyverse" coding---the tidy dataframe. Briefly, you can think of this format in two parts. First, there's the R
object type that the data should be stored in---a basic "dataframe" object.

As we mentioned in the last section, the tidyverse approach hinges on 
using the same data structure throughout your coding pipeline---specifically, 
the dataframe structure (or its more specialized version, the tibble structure).
By insisting on the same data structure throughout, this approach is able to
offer small functions that can be chained together to solve complex problems. 

This idea rests on the idea of the power of modularity. You can think of this 
in terms of children's toys---building bricks like Legos are powerful because
they are modular, while a toy like a stuffed animal is not. Each individual 
Lego is small and simple, and would be pretty boring by itself. However, 
because the blocks can be combined in different ways, they can be used to 
create very complex and interesting structures. By contrast, something that is 
not modular, like a stuffed animal, always retains the same structure. While it
might be more interesting and complex to start with than a single Lego block, 
it will not evolve or contribute to something more interesting. 

This modularity works in the same way that it does for Lego bricks. Lego 
bricks can be combined in interesting ways because they all take the same input
and give the same output---the shape of the holes on the bottom of each brick
accept the shape of the pins [?] at the top of each brick, so they can be
put together in essentially infinite combinations. The tidyverse approach 
in R works in a similar way---the functions in this approach almost all 
input data that are in a dataframe structure (or in a vector structure, for 
functions that operate on columns in the dataframe) and they almost all output
data in the same structure that they input it. As a result, the functions can 
be chained together in interesting ways, where the output of one function 
can feed directly into the input of another. 

**Small, simple tools**

Since the functions in the tidyverse approach are designed to work in a 
modular way---in other words, to be combined in interesting ways to solve
larger problems, rather than solving a large problem with a single function---each
of the functions tends to be a small, simple tool. In other words, each 
function tends to do one thing simply but well. This makes it fairly easy 
to start learning the tidyverse approach, as each of the functions that you 
learn as you begin does one thing that is fairly straightforward. As you 
get better and better as coding using this approach, you often find that it 
is not because you are using more complex functions, but rather that you're 
becoming more clever at combining sets of simple functions in interesting 
ways. 

**Functions available through packages**

The tidyverse functions do not come with base R, but rather are available 
through extensions to base R, commonly referred to as "packages". Like base 
R, these are all open-source and free. Many are available through a 
repository called CRAN, and you can download them directly from R using the 
`install.packages` function. 

The heart of the tidyverse functions are available through an umbrella 
package called "tidyverse". This package includes a number of key tidyverse
packages (e.g., "dplyr", "tidyr", "stringr", "forcats" [?], "ggplot2") and allows you
to quickly install this set of packages on your computer. When you are coding
in R, you will then need to load the package in your R session, which you can
do using the `library` call (e.g., `library("tidyverse")`). 

In addition to the packages that come with the umbrella "tidyverse" package, 
there are numerous other packages that build on the tidyverse approach. 
Some are created by the creator of the tidyverse approach (Hadley Wickham)
or others on his team, while others are created by other R programmers but 
follow the standards of the tidyverse approach. Some of the most helpful of 
these for working with biomedical data include ...

### Useful tidyverse tools for data preprocessing

As you learn to code, a good strategy is to start collecting "tools" for
your "toolbox" in R---functions that you have learned to use very well and 
that you understand thoroughly. This will help make you proficient in R more
quickly, and it will also limit the chance of bugs and errors in your code, 
making your data work more robust and rigorous. In this section, we'll cover
some tidyverse tools that we have found helpful for preprocessing biological 
data. These are not exhaustive, but may help you to identify some sets of 
tools to focus on learning well for data preprocessing and analysis of 
biological data. 

Some key tools from the tidyverse for pre-processing laboratory data: 

- Tools to "tidy" data: pivoting, etc. Allows you to design a data collection
template that is more convenient in the lab, but quickly move it into the "tidy" 
format once you read it into R
- Visualization: Not just for final product, also for exploratory analysis
- Tools to work with dates, character strings (regular expressions), numbers
- Tools to map/apply: Reading in lots of files, doing the same thing to all of 
them. 
- Working with lists, converting from lists to tidy dataframes
- Examples from code in earlier modules

**Tools for data input**

**Tools for standardization and normalization**

**Tools for working with character strings**

**Tools for working with dates and times**

**Tools for statistical modeling**

### Resources to learn more on tidyverse tools

Here we have introduced the tidyverse approach, as well as covered some key 
tools within it for biomedical data preprocessing. However, we strongly 
recommend that you continue to learn more in this approach. In this section, 
we'll point you to resources that you can use to continue to learn this approach
to working with data in R. 

The tidyverse approach is now widely taught, both in in-person courses at
universities and through a variety of online resources. 
Since there are so many excellent resources available---many for free---to learn
how to code in R using the tidyverse approach, we consider it beyond the scope
of these modules to go more deeply into these instructions. However, we do think
it is critical that biological researchers learn how to connect this approach to
the type of coding that is often necessary for pre-processing large and complex
data that is output from laboratory equipment. Through many of the modules in this book, 
we provide advice on how to make these connections, so that data from different
sources---including different types of laboratory equipment and hand-recorded data
collected by personnel in the lab, like colony forming units measured from plating
samples---can all be connected in a tidyverse pipeline by recording hand-recorded
data following a tidy format and by pre-processing data with the aim of moving 
data toward a tidy dataframe that can be integrated with other "tidy" data for 
analysis and visualization.

**Classes and workshops**

Most R programming classes at universities, as well as workshops at conferences
and other venues, now focus on the tidyverse approach, especially if they are
geared to new R users. An R programming class can be a worthwhile investment of
time if this resource is available to you, and if you head a research group and
do not have time to take one yourself, you could instead consider encouraging
trainees in your research group to take this type of class. Programming in other
scripted languages, like Python and Julia, provides similar skills, although the
collection of extension packages that are available for biomedical data tends to
be most extensive for R (at least at this time). Classes in programming
languages like Java or C++, on the other hand, would have less immediate
relevance for most biologists and other bench scientists, and so if you would
like to become better at working with biomedical data, it would be worthwhile to
focus on programming languages that are scripted.

[Software Carpentry]

**Cheatsheets**

For many of the key tidyverse packages, there are two-page "cheatsheets" that
have been developed by the package creators to help users learn and remember
the functions that are available in the package. These are available here [?]. 

Each cheatsheet includes numerous working examples. One excellent way to 
familiarize yourself with the tools in a package, then, is to work through the 
examples on the cheatsheet one at a time, making sure that you understand the 
inputs and outputs to the function and how the function has created the output. 
Once you have worked through a cheatsheet in this way, you can keep it close
to your desk to serve as a quick reminder of the names and uses of different 
functions in the package, until you have used them enough that you don't need
this memory jog. 

**Online books**

There are a number of excellent free online books that are available to help
you learn more about R (many of which can also be purchased as a hard copy, if 
you prefer that format). These typically include lots of examples of code that 
help you try out concepts as you learn them. Two excellent books for biomedical
data preprocessing and analysis are *R for 
Data Science* by ... and *Modern Statistics for Modern Biology* by ... 

*R for Data Science*, which is available at ..., covers ...

One key resource for
learning the tidyverse approach for R is the book *R for Data Science* by Hadley
Wickham (the primary developer of the tidyverse) and Garrett Grolemund. This
book is available as a print edition through O'Reilly Media. It is also freely
available online at https://r4ds.had.co.nz/. This book is geared to beginners in
R, moving through to get readers to an intermediate stage of coding expertise,
which is a level that will allow most scientific researchers to powerfully 
work with their experimental data. The book includes exercises for practicing the
concepts, and a separate online book is available with solutions for the 
exercises (https://jrnold.github.io/r4ds-exercise-solutions/). 

*Modern Statistics for Modern Biology*, which is available at ..., covers ...

------------------------------------------------------------------------------------

### Subsection 1

> "There is a now-old trope in the world of programming. It's called the 'worse is 
better' debate; it seeks to explain why the Unix operating systems (which include
Mac OS X these days), made up of so many little interchangeable parts, were so much
more successful in the marketplace than LISP systems, which were ideologically pure, 
based on a single languagae (again, LISP), which itself was exceptionally simple, 
a favorite of 'serious' hackers everywhere. It's too complex to rehash here, but one
of the ideas inherent within 'worse is better' is thata systems made up of many 
simple pieces that can be roped together, even if those pieces don't share a consistent
interface, are likely to be more successful than systems that are designed with consistency 
in every regard. And it strikes me that this is a fundamental drama of new technologies. 
Unix beat out the LISP machines. If you consider mobile handsets, many of which run
descendants of Unit (iOS and Andriod), Unix beat out Windows as well. And HTML5 beat out
all of the various initiatives to create a single unified web. It nods to accessibility: 
it doesn't get in the way of those who want to make something huge and interconnected.
But it doesn't enforce; it doesn't seek to change the behavior of page creators in the
same way that such lost standards as XHTML 2.0 (which eremged from the offices of 
the World Wide Web Consortium, and then disappeared under the weight of its own
intentions) once did. It's not a bad place to end up. It means that there is no 
single framework, no set of easy rules to lear, no overarching principles that, 
once learned, can make the web appear like a golden statue atop a mountain. There
are just components: HTML to get the words on the page, forms to get people to
write in, videos and images to put up pictures, moving or otherwise, and 
JavaScript to make everything dance." [@ford2015on]

> "One of the fundamental contributions of the Unix system [is] the idea of a *pipe*.
A pipe is a way to connect the output of one program to the input of another program
without any temporary file; a *pipeline* is a connection of two or more programs through
pipes. ... Any program that reads from a terminal can read from a pipe instead; any program
that writes on the terminal can write to a pipe. ... The programs in a pipeline actually 
run at the same time, not one after another. This means that the programs in a pipeline
can be interactive; the kernel looks after whatever scheduling and synchronization is needed
to make it all work. As you probably suspect by now, the shell arranges things when you
ask for a pipe; the individual programs are oblivious to the redirection." [@kernighan1984unix]

> "Even though the Unix system introduces a number of innovative programs and techniques, 
no single program or idea makes it work well. Instead, what makes it effective is an approach
to programming, a philosophy of using the computer. Although that philosophy can't be written
down in a single sentence, at its heart is the idea that the power of a system comes more from
the relationships among programs than from the programs themselves. Many Unix programs do 
quite trivial things in isolation, but, combined with other programs, become general and 
useful tools." [@kernighan1984unix]

> "What is 'Unix'? In the narrowest sense, it is a time-sharing operating system *kernel*:
a program that controls the resources of a computer and allocates them among its users.
It lets users run their programs; it controls the peripheral devices (discs, terminals, 
printers, and the like) connected to the machine; and it provides a file system that 
manages the long-term storage of information such as programs, data, and documents.
In a broader sense, 'Unix' is often taken to include not only the kernel, but also
essential programs like compiles, editors, command languages, programs for copying and
printing files, and so on. Still more broadly, 'Unix' may even include programs
develpoed by you or others to be run on your system, such as tools for document 
preparation, routines for statistical analysis, and graphics packages." [@kernighan1984unix]

> "A common observation is that more of the data scientistâ€™s time is occupied
with data cleaning, manipulation, and 'munging' than it is with actual
statistical modeling (Rahm and Do, 2000; Dasu and Johnson, 2003). Thus, the
development of tools for manipulating and transforming data is necessary for
efficient and effective data analysis. One important choice for a data scientist
working in R is how data should be structured, particularly the choice of
dividing observations across rows, columns, and multiple tables. The concept of
'tidy data,' introduced by Wickham (2014a), offers a set of guidelines for
organizing data in order to facilitate statistical analysis and visualization. ... This framework makes it easy for analysts to reshape, combine, group and otherwise manipulate data. Packages such as ggplot2, dplyr, and many built-in R modeling and plotting functions require the input to be in a tidy form, so keeping the data in this form allows multiple tools
to be used in sequence in a seamless analysis pipeline (Wickham, 2009; Wickham and Francois,
2014)."
[@robinson2014broom]

> "A smart visualization can transform biologists' understanding of their data" 
[@callaway2016visualizations]

> "Scientific figures are typically rendered as static images. But these are divorced
from the underlying data, which prevents readers from exploring them in more detail
by, for instance, zooming in om features of interest. For genomicists needing to 
cram millions of data points into dense visuals a few centimeters big, this can be
particularly problematic. The same is true for researchers working with computational 
algorithms. Scientists often post software on open-source repositories such 
as GitHub, but getting the code to run properly is easier said than done. Reviewers 
and other interested parties often require extra software and configuration to make 
the algorithms work. Some journals now bridge that gap by supporting interactive
figures and code. One of those is F1000Research..." [@perkel2018future]

> "Open-source options also exist for creating interactive images, including Boke, 
htmlwidgets, pygal and ipywidgets. Most are used programmatically, generally 
within either R or Python code, which is commonly used in the sciences. ... Two 
other products let researchers create interactive apps that make use of widgets such 
as drop-down menus and slider controls to blend data, graphics and code: Shiny, 
made by RStudio in Boston, Massachusetts, for R, and Plotly's Dash for Python. 
They work by transmitting the user's widget actions to a remote server, which runs
the underlying code and updates the package." [@perkel2018future]

> "Very little of what I've built over the years is monolithic---just a single chunk.
Most of the time, I build things in components, then attach those pieces together 
as I go. So yes, the component parts are pieces that have been made small in 
precise ways from larger chunks of material, but eventually they will be assembled
to create much larger and more complex objects than any of the raw source materials."
[@savage2020every]

> "After they've been built in, mechanical fasteners make everything after that 
easier. They allow for disassembly, reconfiguration, as well as replacement."
[@savage2020every]

> "That's the reason I prefer mechanical solutions. They can be undone. Whatever
I'm putting together can be pulled apart again without damaging the construction. 
... it takes more engineering, more fiddling, and definitely more time. But the 
trade-off is more options. And I want options. That's the space I like to exist in 
as a maker." [@savage2020every]

> "Similar to early many, beginner makers start with a rudimentary set of tools for
basic creative tasks: a hammer (of course), a set of screwdrivers, scissors, 
some pliers, maybe a crescent wrench, and some kind of cutting device. Almost
everyone who has strived to make things has some combination of this list. Then, 
as we get more experienced, we seek out better versions of the tools we already 
have as well as new tools that can facilitate the learning of new techniques---new
ways of cutting things apart, and new ways of putting them back together." [@savage2020every]

> "Once we start to expand past the basic complement of tools, what to add to our 
collections becomes a multifactor calculus based on reliability, cost, space, time, 
repairability, skill, and need. These choices are nontrivial, because the tools we use
are extensions of our hands and our minds. The best tools 'wear in' to fit you based
on how you use them, they get smooth where you grab them. They tell the story of their
utility with their patina of use. A toolbox of tools you know well and use lovingly is
a magnificent thing." [@savage2020every]

> "The reality is that tool choice is both less important and more important than you 
think it is. It is less important to the extent that tool usage is entirely 
subjective, which means there is no one right way to do things. But it is more
important, because the best tool for any job is the one you're most comfortable with, 
the one that you can make do what you want it to do, whose movements you fully 
understand." [@savage2020every]

> "You must concentration on fundamentals, at least what *you think* at the time 
are fundamentals, and also develop the ability to learn new fields of knowledge
when they arise so you will not be left behind..." [@hamming1997art]

> "How are you to recognize 'fundamentals'? One test is that they have lasted a 
long time. Another test is from the fundamentals all the rest of the field
can be derived by using the standard methods in the field." [@hamming1997art]


### Subsection 2

### Practice quiz

