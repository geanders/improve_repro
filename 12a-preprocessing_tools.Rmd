## Selecting software options for pre-processing {#module12a}

[Intro]

**Objectives.** After this module, the trainee will be able to:

- Describe software approaches for pre-processing data 
- Compare the advantages and disadvantages of Graphical User Interface--based
versus scripted approaches and of open-source versus proprietary approaches
to pre-processing

The previous module described some common themes and processes in preprocessing
biomedical data. While we've covered some key processes of preprocessing, we
haven't talked yet about the tools you can use to implement it. These are often
combined together into a pipeline (also called a workflow). These pipelines can
become fairly long and complex when you need to preprocess data that are
complex.

Most preprocessing pipelines will be run on the computer, with software tools. An
exception might be for very simple preprocessing tasks---one example is
generating the average cage weight for a group of mice based on the total cage
weight and the number of mice. However, even simple processes like this, which
can be done by hand, can also be done with a computer, and doing so can help
avoid errors and to provide a record of the calculation that was used for the
preprocessing.

You will have a choice about which type of software you use for preprocessing.
There are two key dimensions that separate these choices---first, whether the
software is point-and-click versus script-based, and, second, whether the
software is proprietary versus open-source. It is important to note
that, in some cases, it may make sense to develop a pipeline that chains
together a few different software programs to complete the required
preprocessing.

In this module, we'll talk about the advantages and disadvantages of these
different types of software. For reproducibility and rigor, there are many
advantages to using software that is script-based and open source for data
preprocessing, and so in later modules, we'll provide more information on how
you can use this type of software for preprocessing biomedical data. We also
recognize, however, that there are some cases where such software may not be a
viable option for some or all of the data preprocessing for a project.

### GUI versus code script

When you pick software for preprocessing, the first key dimension to consider
is whether the software is "point-and-click" or script-based. 
Let's start with a definition of each. 

Point-and-click software is more formally known as GUI software, where GUI stand
for "graphical user interface". These are programs where your hand is on the
mouse most of the time, and you use the mouse to select actions and options from
buttons and other widgets that are shown by the software on the screen. This
type of software is also sometimes called "widget-based", as it is built around
widgets like drop-down menus and slider bars [@perkel2018future].

A basic example of GUI-based software is your computer's calendar application
("application" is a common synonym for "software"). To navigate across dates on
your calendar, you use your mouse to click on arrows or dates.
The software includes some text entry---for example, if you add something to
your calendar, you can click on a textbox and enter a description of the
activity using your keyboard. However, the basic way that you navigate and use
the software is via your computer mouse. 

Script-based software uses a script, rather than clickable buttons and graphics,
as its main interface. A script, in this case, is a line-by-line set of
instructions describing what actions you want the software to perform. With
script-based software, you typically keep your keys on the keyboard more often
than on the mouse. Many script-based software programs will also allow you to
also send the lines of instructions one at a time in an area referred to as a
*console*, which will then return the result from each line after you run it.
Script-based software is also sometimes called software that is "used
programatically" [@perkel2018future]. Several script-based software programs are
commonly used with biomedical data including R, Python, and Unix bash scripts,
as well as some less common but emerging software programs like Julia.

When comparing point-and-click software to script-based software for
preprocessing, there are a few advantages to point-and-click software, but
many more to script-based software. In terms of code rigor and reproducibility, 
script-based software comes out well ahead, especially when used to its 
full advantage. 

Let's start, though, by acknowledging some appealing features of point-and-click
software. These features likely contribute to its wide popularity and to the
fact that the vase majority of software that you use in your day-to-day life
outside of research is probably point-and-click.

First, point-and-click software is often easier to learn to use, at least in 
terms of basic use. The visual icons help you navigate choices and actions in 
the software. Most GUIs are designed to take the underlying processes and 
make them easier for a new user to access and use. They do this through 
an interface that is visual, rather than language- and script-based. 
Further, many people are most familiar with point-and-click 
software, since so many everyday applications are of this type, and so its 
interface can feel more familiar to users. 
They also are easier for a new user to pick up because they typically 
provide a much smaller set of options than a full programming language does.

By contrast, coding languages take more investment of time and energy to
initially learn how to use. This is because a coding language is just that---a
language. It is built on a (often large) set of vocabulary that you must learn
to be proficient, as you must learn the names and options for a large set
of functions within the language. Further, it has rules and logic you must learn
in terms of options for how to structure and access data and how the inputs and
outputs of different functions can be chained together to build pipelines for
preprocessing and analysis.

Coding also requires you to be precise in this language. As Brian Kernighan
writes in his book *D is for Digital*: 

> "A computer is the ultimate sorcerer's apprentice, able to follow instructions
tirelessly and without error, but requiring painstaking accuracy in the 
specification of what to do." [@kernighan2011d]

However, while there is a higher investment required to learn script-based 
software versus point-and-click software, there is also a higher payoff from 
that effort. Script-based software creates a full framework for you to 
combine tools in interesting ways and to build new tools when you need them. 
With point-and-click software, there's always a layer between the user and 
the computer logic, and you are constrained to only use tools that were 
designed by the person who programmed the point-and-click software. By 
contrast, with script-based software, you have more direct access to the 
underlying computer logic, and with many popular script-based languages 
(R, Python), you have extraordinary power and flexibility in what you can 
ask the program to do.

As an analogy, think about traveling to a country where you don't yet speak the
language. You have a few choices in how you could communicate. You could
memorize a few key phrases that you think you'll need, or get a phrase book that
lists these key phrases. Another choice is to try to learn the language,
including learning the grammar of the language, and how thoughts are put
together into phrases. Learning the language, even at a basic level, will take
much more time. However, it will allow you much greater ability to express
yourself. If you only know set phrases, then you may know how to ask someone at
a bakery for loaf of bread, if the person who wrote the phrase book decided to
include that, but not how to ask at a hotel for an extra blanket, if that wasn't
included. By contrast, if you've learned the language, you have learned how to
form a question, and so you can extrapolate to express a great variety of
things.

Point-and-click software is often like using a phrase book for a foreign 
language---if the person who developed the tool didn't imagine something that 
you need, you're stuck. Scripted software is more like learning a language---you 
have to learn the rules (grammar) and vocabulary (names of functions and 
their parameters), but once you do, you can combine them to address a wide
variety of tasks, including things no one else has yet thought of. 

In the late 1990s, a famous computer scientist named Richard Hamming wrote a
book called, "The Art and Science of Engineering", in which he talks a lot about
the process of building things and the role that programming can play in this
process. He predicted at the time that by 2020, it will be the experts in a
particular field that do programming for that field, rather than experts in
computer programming trying to build tools for other fields [@hamming1997art].
He notes:

> "What is wanted in the long run, of course, is that the man with the problem 
does the actual writing of the code with no human interface, as we all to often 
have these days, between the person who knows the problem and the person who
knows the programming language. This date is unfortunately too far off to 
do much good immediately, but I would think by the year 2020 it would be
fairly universal practice for the expert in the field of application to do 
the actual program preparation rather than have experts in computers (and 
ignorant in the field of application) do the program preparation." [@hamming1997art]

The rise of open-source, scripted programs like Python and R is rapidly helping to 
achieve this vision---scientists in a variety of fields now write their own
small software programs and tools, building on the framework of larger
open-source languages. Training programs in many scientific fields recommend
or require at least one course in programming in these languages, often 
taught in conjunction with data analysis and data management. 

Another element that has helped make script-based software more accessible is
the development of programming languages that are easier to learn and use. Very
early programming languages required the programmer to understand a lot about
how the computer was built and organized, including thinking about where and how
data were stored in the computer's memory. As programming languages have
developed, such "low-level" languages have remained in use, as they often allow
for unmatched speed in processing. However, "higher-level" programming languages
have become more common, and while these might be somewhat slower in
computational processing power, they are much faster for humans to learn and to
create tools with, as they abstract away many of the details that make low-level
programming more difficult.

Because of the development of easier-to-learn high-level programming languages
like R and Python, it is possible for a scientist to become proficient in one of
these script-based programs in about a year. In our own experience, we have
found that often one semester of a dedicated course or serious self-study,
followed with several months of regularly applying the software to research
data, is enough for a scientist to become productive in using a script-based
software like R or Python for research. With another year or so of regular use,
scientists can often start making their own small software extensions to the
language. However, in a 2017 article on analyzing scRNA-seq data, the author
noted that "relatively few biologists are comfortable working in those
environments", referring to Unix and R [@perkel2017single], and noted that this
was a barrier to using many of the available tools for working with scRNA-seq
data at the time. 

It is true that this is a substantially larger investment in training than a
short course or workshop, which might be adequate for learning the basics of 
many point-and-click software programs. This can be a
critical barrier, especially for scientists who are advanced in their career and
may have minimal time for further training. Further, it's more of a barrier in
analyzing some types of biomedical data, due to the extreme size and complexity
of the data [@nekrutenko2012next].

However, it is much less of a time investment than it takes to become an expert
in a scientific field. It takes years of training to become an expert in
cellular biology or immunology, for example. Richard Hamming's vision was that
the experts can ask the best and most creative questions of the data, and that
it is best to remove the barrier of a different computer programmer, so that the
expert can directly create the program and leverage the full capabilities of the
computer. Higher-level programming languages now are accessible enough that this
vision is playing out across scientific fields.

Another advantage of script-based software---and one that is related to the idea
of experts in a scientific field directly programming---is that often the most
cutting edge algorithms and pipelines will be available first in scripted
languages, and only later be added into point-and-click software programs. This
means that you may have earlier access to new algorithms and approaches if you
are comfortable coding in a script-based language. 

In some cases, biologists aim to analyze data that represents the cutting edge
of equipment and measurement technology, or that is very specialized for a
particular field. In these cases, scripted, open-source packages will often be
the first place where algorithms working with the data are available. For
example, an article about scRNA-seq from 2017 noted that, at the time, there
were "very few, if any, 'plug-and-play' packages" for working with scRNA-seq
data, and of those available, they were "user-friendly but have the drawback
that they are to some extent a 'black box', with little transparency as to the
precise algorithmic details and parameters employed." [@haque2017practical]
Similarly, another article in the same year noted that at the time, "most
scRNA-seq tools exist as Unix programs or packages in the programming language
R", although "some ready-to-use pipelines have been developed"
[@perkel2017single].

Script-based approaches also encourage the user to learn how the underlying process
works. The approach encourages the user to think more like a car owner who gets
under the hood from time to time than like one who only drives the car. This
approach does take more time to learn and develop, but with the upside that the
user will often have a much deeper understanding of what is happenening in each 
step, as well as how to fix or adjust different steps to fix a pipeline or 
adapt one pipeline to meet another analysis need.

Another key advantage of script-based software is that, in writing the 
script, you are thoroughly documenting the steps you took to preprocess the 
data. When you create a code script, the script itself includes all
the steps and details of the process. In combination with information about the
version of all software used and the raw data input to the pipeline, it creates 
a fully reproducible record of the data preprocessing and analysis. 

This means both that you will be able to re-do all the steps
yourself in the future, if you need to, but that also that other researchers can
explore and replicate what you do. You may want to share your process with
others in your laboratory group, for example, so they can understand the choices
you made and steps you took in pre-processing the data. You may also want to
share the process with readers of the articles you publish, and this may in fact
be required by the journal. Further, the use of a code script encourages you to
document this code and this process, even more so when you move beyond a script
and include the code in a reproducible pre-processing protocol. Well-documented
code makes it much easier to write up the method section later in manuscripts
that leveraged the data collected in the experiment.

By contrast, while you could write down the steps that you took and the buttons
you pressed when using point-and-click software, it's very easy to forget to
record a step. When you use a code script, it will not run if you forget a step
or a detail of that step. Some GUI-based programs are taking steps to try to
ameliorate this, allowing a user to save or download a full record that records
the steps taken in a given pipeline or allow the user to develop a full,
recorded workflow (one example is FlowJo Envoyâ€™s workflow model for analyzing
data from flow cytometry). As a note, there are some movements towards
"integrative frameworks", which can help improve reproducibility for pipelines
that span different types of software (Galaxy, Gene Prof) [@nekrutenko2012next].

When you write a script to do a task with data, it is like writing a recipe that
can be applied again and again. By writing a script, you encode the process a
single time, so you can take the time to check and recheck to make sure that
you've encoded the process correctly. This helps in avoiding small errors when
you do the preprocessing---if you are punching numbers into a calculator over
and over, it's easy to mistype a number or forget a step every now and then,
while the code will ensure that the same process is run every time and that it
faithfully uses the numbers saved in the data for each step, rather than relying
on a person correctly entering each number in the calculation.

Scripts can be used across projects, as well, and so they can ensure consistency
in the calculation across projects. If different people do the calculation in
the lab for different projects or experiments, and they are doing the
calculations by hand, they might each do the calculation slightly differently,
even if it's only in small details like how they report rounded numbers. A
script will do the exact same thing every time it is applied. You can even share
your script with colleagues at other labs, if you want to ensure that your data
preprocessing is comparable for experiments conducted in different research
groups, and many scientific journals will allow supplemental material with 
code used for data preprocessing and analysis, or links within the manuscript
to a repository of this code posted online.

There are also gains in efficiency when you use a script. For small
pre-processing steps, these might seem small for each experiment, and certainly
when you first write the script, it will likely take longer to write and test
the script than it would to just do the calculation by hand (even more if
you're just starting to learn how to write code scripts). However, since the
script can be applied again and again, with very little extra work to apply it
to new data, you'll save yourself time in the future, and over a lot of
experiments and projects, this can add up. This makes it particularly useful to
write scripts for preprocessing tasks that you find yourself doing again and
again in the lab. 

This is often a gain that fully pays back the investment in learning the
software---it can make data preprocessing and analysis much more efficient over
the long term. Code scripts often are easier to automate than a workflow through
a point-and-click system. For example, if you need to process a number of files
that all follow the same format, you can often develop a script using one of
those files, check that script very carefully, and the apply it with minimal
modifications to each of the files in the full set. This allows you to spend
more time on the template script, making sure that it works as you expect, and
then apply it quickly, whereas working through multiple files with
point-and-click software may essentially boil down to a lot of time spent in
repetition. This kind of automation can also help in limiting errors from human
mistakes in by-hand processing [@gibb2014reproducibility]. 

### Open-source versus proprietary software

The other dimension to consider for software for preprocessing is whether it is
open-source or proprietary. Open-source software is software where you can
access, explore, and build on all the underlying code for the software. It also
most often is free. By contrast, the code that powers proprietary software is
typically kept private, so you can use the product but cannot explore the way
that it is built or extend it in the same way that you can open-source software.
In biomedical research, many script-based languages are open-source, while many
point-and-click programs are proprietary. However, this is not a hard and fast
rule, and there are examples of open-source point-and-click software (for
example, the Inkscape program for vector graphic design) as well as proprietary
script-based software (for example, Matlab and SAS). There are advantages and
disadvantages to both types of software, but in terms of rigor and
reproducibility, open-source software often has the advantage.

One key advantage of open-source software is that all code is open, so you can
dig down to figure out exactly how each step of the program works. Futher, in
many cases for open-source scientific software, the algorithms and their
principles have gone through peer review as part of the academic publication
process. With proprietary software, on the other hand, details of algorithms may
be considered protected intellectual property, and so it may be hard to find out
the details of how the underlying algorithms work [@nekrutenko2012next]. Also,
the algorithms may not have gone through peer-review, especially if they are
considered private intellectual property.

Transparency is a key element of reproducibility [@neff2021past]. As Gordon
Lithgow and coauthors note in a commentary on reproducibility, "Improved
reproducibility comes from pinning down methods." [@lithgow2017long] If the
algorithms of software can be investigated, then scientists who are using two
different programs (for example, one program in Python and one in R) can
determine if their choice of program is causing differences in their results. By
contrast, if two research groups use two different types of proprietary
software, the algorithms that underlie the processing are often kept secret and
so cannot be compared. In that case, if the two groups conduct the same
experiment and get different results, it's impossible to rule out whether the
difference was caused by the choice of software.

Gordon Lithgow, Monica Driscoll, and Patrick Phillips wrote a commentary for 
*Nature* describing their experiences in replicating research. They 
describe the advice they give students who are trying to do an
experiment that should work: 

> "If there is nothing wrong with the reagents and reproducibility is still an 
issue, then as I like to tell students, there are two options: (1) the physical 
constants of the universe and hence the laws of physics are in a state of flux 
in their round-bottomed flask, or (2) the researcher is doing something wrong 
and either doesn't know it or doesn't want to know it. Then I ask them which 
explanation they think I'm leaning towards." [@gibb2014reproducibility]

If you get different results from another group, it is critical to have a 
detailed description of the methods that each group used to figure out why 
the groups are getting different results. Open-source software provides this 
at the level of computational analysis, because the openness of the software
means anyone can explore the exact details of how each algorithm runs. 

Another advantage of open-source software is that older versions of the software
are often well-archived and easily available to reinstall and use if needed to
reproduce an analysis that was done using an earlier version of the software
than the current main version at the time of the replication.

Another advantage is that open-source software is often free. This makes it 
economical to test out, and it means that trainees from a lab will have no 
problem continuing to use the software as they move to new positions. 
The cost with open-source software, then, comes not with the price to buy 
the software, but with the investment that is required to learn it. 

When data are the output of complex laboratory equipment, there will often be
proprietary software that is available for this pre-processing. This software
may be created by the same company that made the equipment, or it may be created
and sold by other companies. This software might conduct some steps using
defaults, and others based on the user's specifications. These are often
provided through "GUIs" (graphical user interfaces), where the user does a
series of point-and-click steps to process the data. In some software, this
series of point-and-click steps is recorded as the user does them, so that these
steps can be "re-run" later or on a different dataset.

However, for many types of biological data, including output from equipment like
flow cytometers and mass spectrometers, open-source software has been developed
that can be used for this preprocessing. Often, the most cutting edge methods
for data preprocessing are first available through open-source software
packages, if the methods are developed by researchers rather than by the
companies, and often many of the algorithms that are made available through the
equipment manufacturer's proprietary software are encoded versions of an
algorithm first shared by researchers as open-source software.

One facet where proprietary software has an advantage is that it will often have
more comprehensive company-based user support than open-source software. The
companies that make and sell proprietary software will usually have a user
support team to answer questions and help develop pipelines and may also offer 
training programs or materials.

Some open-source software also has robust user support, although sometimes a bit
less organized under a common source. In some cases, this has developed as a
result of a large community of users who help each other. Message boards like
StackOverflow provides a forum for users to ask and respond to questions. Some
companies also exist that provide, as their business model, user support for
open-source software. While open-source software is usually free, these
companies make money by providing support for that software.

User support is sparser for some of smaller software packages that are developed
as extensions of open-source software. For example, many of the packages for
preprocessing types of biomedical data are built by small bioinformatics teams
or individuals at academic or research institutions. Often this software is
developed by a single person or very small team as one part of their job
profile, with limited resources for user support and for providing training.
These extensions build on larger, more supported open-source software (e.g., R
or Python), but the extension itself is built and maintained by a very small
team that may not have the capacity to respond quickly to user questions. Many
open-source software developers try to create helpful documentation in the form
of help files and package vignettes (tutorials on how to use the software they
created), but from a practical point of view it is difficult for small
open-source developers to provide the same level of user support that a large
proprietary software company can.

This is often the case with cutting-edge open-source software for biomedical
preprocessing. These just-developed software packages are less likely to be
comprehensively documented than longer-established software. Further, it can
take a while for the community of software users to develop once software is
available, and while this is a limitation of new software for both open source
and proprietary languages, it can represent more of a problem for open-source
software, where there is typically not a company-based helpline and so the
community of users often represents one of the main sources for help and
troubleshooting.

### Discussion questions


