## Selecting software options for pre-processing {#module12a}

[Intro]

**Objectives.** After this module, the trainee will be able to:

- Describe software approaches for pre-processing data 
- Compare the advantages and disadvantages of Graphical User Interface--based
versus scripted approaches and of open-source versus proprietary approaches
to pre-processing

The previous module described some common themes and processes in 
preprocessing biomedical data. These are often combined together into a pipeline
(also called a workflow). These pipelines can become fairly long and complex 
when you need to preprocess data that are complex. 

While we've covered some key processes of preprocessing, we haven't talked yet
about the tools you can use to implement it. Most preprocessing will be done on
the computer, with software tools. An exception might be for very simple
preprocessing tasks---one example is generating the average cage weight for a
group of mice based on the total cage weight and the number of mice. However,
even simple processes like this, which can be done by hand, can also be done
with a computer, and doing so can help avoid errors and to provide a record of
the calculation that was used for the preprocessing.

You will have a choice about which type of software you use for preprocessing.
There are two key dimensions that separate these choices---first, whether the
software is point-and-click versus script-based, and, second, whether the
software is proprietary versus open-source. It is important to note
that, in some cases, it may make sense to develop a pipeline that chains
together a few different software programs to complete the required
preprocessing.

In this module, we'll talk about the advantages and disadvantages of these
different types of software. For reproducibility and rigor, there are many
advantages to using software that is script-based and open source for data
preprocessing, and so in later modules, we'll provide more information on how
you can use this type of software for preprocessing biomedical data. We also
recognize, however, that there are some cases where such software may not be a
viable option for some or all of the data preprocessing for a project.

**GUI versus code script**

When you pick software for preprocessing, the first key dimension to consider
is whether the software is "point-and-click" or script-based. 
Let's start with a definition of each. 

Point-and-click software is more formally known as GUI software, where GUI stand
for "graphical user interface". These are programs where your hand is on the
mouse most of the time, and you use the mouse to select actions and options from
buttons and other widgets that are shown by the software on the screen. This
type of software is also sometimes called "widget-based", as it is built around
widgets like drop-down menus and slider bars [@perkel2018future].

A basic example of GUI-based software is your computer's calendar application
("application" is a common synonym for "software"). To navigate across dates on
your calendar, you use your mouse to click on arrows or dates on the calendar.
The software includes some text entry---for example, if you add something to
your calendar, you can click on a textbox and enter a description of the
activity using your keyboard. However, the basic way that you navigate and use
the software is via your computer mouse. 

Script-based software uses a script, rather than clickable buttons and graphics,
as its main interface. A script, in this case, is a line-by-line set of
instructions describing what actions you want the software to perform. With
script-based software, you typically keep your keys on the keyboard more often
than on the mouse. Many script-based software programs will also allow you to
also send the lines of instructions one at a time in an area referred to as a
"console", which will then return the result from each line after you run it.
Script-based software is also sometimes called software that is "used
programatically" [@perkel2018future]. Several script-based software programs are
commonly used with biomedical data including R, Python, and Unix bash scripts,
as well as some less common but emerging software programs like Julia.

When comparing point-and-click software to script-based software for
preprocessing, there are a few advantages to point-and-click software, but
many more to script-based software. In terms of code rigor and reproducibility, 
script-based software comes out well ahead, especially when used to its 
full advantage. 

While script-based software has many advantages when it comes to rigor and
reproducibility, there are some appealing features of point-and-click software.
These features likely contribute to its wide popularity and to the fact that the
vase majority of software that you use in your day-to-day life outside of
research is probably point-and-click.

First, point-and-click software is often easier to learn to use, at least in 
terms of basic use. The visual icons help you navigate choices and actions in 
the software. Most GUIs are designed to take the underlying processes and 
make them easier for a new user to access and use. They do this through 
an interface that is visual, rather than language- and script-based. 
Further, many people are most familiar with point-and-click 
software, since so many everyday applications are of this type, and so its 
interface can feel more familiar to users. 
They also are easier for a new user to pick up because they typically 
provide a much smaller set of options than a full programming language does.

By contrast, coding languages take more investment of time and energy to
initially learn how to use. This is
because a coding language is just that---a language. It is built on a set (often
large) of vocabulary that you must learn to be proficient in it, as you must
learn the names and options for a large set of functions within the language.
Further, it has rules and logic you must learn in terms of options for how to
structure and access data and how the inputs and outputs of different functions
can be chained together to build pipelines for preprocessing and analysis.

However, while there is a higher investment required to learn script-based 
software versus point-and-click software, there is a very high payoff from 
that effort. Script-based software creates a full framework for you to 
combine tools in interesting ways and to build new tools when you need them. 
With point-and-click software, there's always a layer between the user and 
the computer logic, and you are constrained to only use tools that were 
designed by the person who programmed the point-and-click software. By 
contrast, with script-based software, you have more direct access to the 
underlying computer logic, and with many popular script-based languages 
(R, Python), you have extraordinary power and flexibility in what you can 
ask the program to do.

As an analogy, think about traveling to a country where you don't yet speak the
language. You have a few choices in how you could communicate. You could
memorize a few key phrases that you think you'll need, or get a phrase book that
lists these key phrases. Another choice is to try to learn the language,
including learning the grammar of the language, and how thoughts are put
together into phrases. Learning the language, even at a basic level, will take
much more time. However, it will allow you much greater ability to express
yourself. If you only know set phrases, then you may know how to ask someone at
a bakery for loaf of bread, if the person who wrote the phrase book decided to
include that, but not how to ask at a hotel for an extra blanket, if that wasn't
included. By contrast, if you've learned the language, you have learned how to
form a question, and so you can extrapolate to express a great variety of
things.

Point-and-click software is often like using a phrase book for a foreign 
language---if the person who developed the tool didn't imagine something that 
you need, you're stuck. Scripted software is more like learning a language---you 
have to learn the rules (grammar) and vocabulary (names of functions and 
their parameters), but once you do, you can combine them to address a wide
variety of tasks, including things no one else has yet thought of. 

In the late 1990s, a famous computer scientist named Richard Hamming wrote 
a book called, "The Art and Science of Engineering", in which he talks 
a lot about the process of building things and the role that programming 
can play in this process. He predicts that by 2020, it will be the experts in a particular field that do programming for that field, rather than experts in 
computer programming trying to build tools for other fields [@hamming1997art].
He notes:

> "What is wanted in the long run, of course, is that the man with the problem 
does the actual writing of the code with no human interface, as we all to often 
have these days, between the person who knows the problem and the person who
knows the programming language. This date is unfortunately too far off to 
do much good immediately, but I would think by the year 2020 it would be
fairly universal practice for the expert in the field of application to do 
the actual program preparation rather than have experts in computers (and 
ignorant in the field of application) do the program preparation." [@hamming1997art]

The rise of open-source, scripted programs like Python and R has helped to 
achieve this vision---scientists in a variety of fields now write their own
small software programs and tools, building on the framework of larger
open-source languages. Training programs in many scientific fields recommend
or require at least one course in programming in these languages, often 
taught in conjunction with data analysis and data management. 

Another element that has helped make script-based software more accessible is 
the development of programming languages that are easier to learn and use. 
Very early programming languages required the programmer to understand a lot
about how the computer was built and organized, including thinking about where
and how data were stored in the computer's memory. As programming languages
have developed, such "low-level" languages have remained in use, as they 
often allow for unmatched speed in processing. However, more "higher-level"
programming languages have also developed, and while these might be 
somewhat slower in processing, they are much faster for humans to learn 
and create tools with, as they abstract away many of the details that make
low-level programming more difficult. 

Because of the development of easier-to-learn high-level programming languages
like R and Python, it is possible for a scientist to become proficient in using
one of these script-based programs in about a year. Often, we find that one
semester of a dedicated course or serious self-study, followed with several
months of regularly applying the software to research data, is enough for a
scientist to become productive in using a script-based software like R or Python
for research. With another year or so of regular use, scientists can often start
making their own small software extensions to the language. However, in a 2017
article on analyzing scRNA-seq data, the author noted that "relatively few
biologists are comfortable working in those environments", referring to Unix and
R [@perkel2017single], and noted that this was a barrier to using many of the
available tools for working with scRNA-seq data at the time. We would
argue that becoming comfortable with these tools requires an investment, but is
not an insurmountable barrier for biologists.

Scripted-based approaches encourage the user to learn how the underlying process
works. The approach encourages the user to think more like a car owner who gets
under the hood from time to time than like one who only drives the car. This
approach does take more time to learn and develop, but with the upside that the
user will often have a much deeper understanding of what is happenening in each 
step, as well as how to fix or adjust different steps to fix a pipeline or 
adapt one pipeline to meet another analysis need.

It is true that this is a substantially larger investment in training than a
short course or workshop, which might be adequate for learning the basics of 
many point-and-click software programs. There is a particularly high time
investment required to learn to use scripted software well. This can be a
critical barrier, especially for scientists who are advanced in their career and
may have minimal time for further training. Further, it's more of a barrier in
analyzing some types of biomedical data, due to the extreme size and complexity
of the data [@nekrutenko2012next].

However, it is much less of a time investment than it takes to become an expert
in a scientific field. It takes years of training to become an expert in
cellular biology or immunology, for example. Richard Hamming's vision was that
the experts can ask the best and most creative questions of the data, and that
it is best to remove the barrier of a different computer programmer, so that the
expert can directly create the program and leverage the full capabilities of the
computer. Higher-level programming languages now are accessible enough that this
vision is playing out across scientific fields.

> "That a language is easy for the computer expert does not mean it is
necessarily easy for the non-expert, and it is likely non-experts who will do
the bulk of the programming (coding, if you wish) in the near future."
[@hamming1997art]

> "What is wanted in the long run, of course, is that the man with the problem 
does the actual writing of the code with no human interface, as we all to often 
have these days, between the person who knows the problem and the person who
knows the programming language. This date is unfortunately too far off to 
do much good immediately, but I would think by the year 2020 it would be
fairly universal practice for the expert in the field of application to do 
the actual program preparation rather than have expers in computers (and 
ignorant in the field of application) do the program preparation." [@hamming1997art]

Another advantage of script-based software---and one that is related to the idea
of experts in a scientific field directly programming---is that often the most
cutting edge algorithms and pipelines will be available first in scripted
languages, and only later be added into point-and-click software programs. This
means that you may have earlier access to new algorithms and approaches if you
are comfortable coding in a script-based language. 

In some cases, biologists aim to analyze data that represents the cutting edge
of equipment and measurement technology, or that is very specialized for a
particular field. In these cases, scripted, open-source packages will often be
the first place where algorithms working with the data are available. For
example, an article about scRNA-seq from 2017 noted that, at the time, there
were "very few, if any, 'plug-and-play' packages" for working with scRNA-seq
data, and of those available, they were "user-friendly but have the drawback
that they are to some extent a 'black box', with little transparency as to the
precise algorithmic details and parameters employed." [@haque2017practical]
Similarly, another article in the same year noted that at the time, "most
scRNA-seq tools exist as Unix programs or packages in the programming language
R", although "some ready-to-use pipelines have been developed"
[@perkel2017single].

Another key advantage of script-based software is that, in writing the 
script, you are thoroughly documenting the steps you took to preprocess the 
data. When you create a code script, the script itself includes all
the steps and details of the process. In combination with information about the
version of all software used and the raw data input to the pipeline, it creates 
a fully reproducible record of the data preprocessing and analysis. 

By contrast, while you could write down the steps that you took and the buttons
you pressed when using point-and-click software, it's very easy to forget to
record a step. When you use a code script, it will not run if you forget a step
or a detail of that step. Some GUI-based programs are taking steps to try to
ameliorate this, allowing a user to save or download a full record that records
the steps taken in a given pipeline or allow the user to develop a full,
recorded workflow (one example is FlowJo Envoy’s workflow model for analyzing
data from flow cytometry). As a note, there are some movements towards
"integrative frameworks", which can help improve reproducibility for pipelines
that span different types of software (Galaxy, Gene Prof) [@nekrutenko2012next].

The final key advantage of scripted software is that---and this is often a gain
that fully pays back the investment in learning the software---it can make data
preprocessing and analysis much more efficient over the long term. Code scripts
often are easier to automate than a workflow through a point-and-click system. For
example, if you need to process a number of files that all follow the same
format, you can often develop a script using one of those files, check that
script very carefully, and the apply it with minimal modifications to each of
the files in the full set. This allows you to spend more time on the template
script, making sure that it works as you expect, and then apply it quickly,
whereas working through multiple files with point-and-click software may
essentially boil down to a lot of time spent in repetition. This 
kind of automation can also help in limiting errors from human mistakes
in by-hand processing [@gibb2014reproducibility].

The other dimension to consider for software for preprocessing is whether it is
open-source or proprietary. Open-source software is software where you can
access, explore, and build on all the underlying code for the software. It also
most often is free. By contrast, the code that powers proprietary software is
typically kept private, so you can use the product but cannot explore the way
that it is built or extend it in the same way that you can open-source software.
In biomedical research, many script-based languages are open-source, while many
point-and-click programs are proprietary. However, this is not a hard and fast
rule, and there are examples of open-source point-and-click software (for
example, the Inkscape program for vector graphic design) as well as proprietary
script-based software (for example, Matlab and SAS). There are advantages and
disadvantages to both types of software, but in terms of rigor and
reproducibility, open-source software often has the advantage.

Transparency is a key element of reproducibility [@neff2021past]. If the
algorithms of software can be investigated, then scientists who are using two
different programs (for example, one program in Python and one in R) can
determine if their choice of program is causing differences in their results. By
contrast, if two research groups use two different types of proprietary
software, the algorithms that underlie the processing are often kept secret and
so cannot be compared. In that case, if the two groups conduct the same
experiment and get different results, it's impossible to rule out whether the
difference was caused by the choice of software.


> "Improved reproducibility comes from pinning down methods." [@lithgow2017long]

> "As chemists, we have to be able to go to the literature, take a procedure, 
and carry out a similar or identical transformation with our own hands. Frustratingly,
this doesn't always happen, and the next-to-worst-case scenario possible is 
when it's one of your own reactions that can't be reproduced by a lab 
elsewhere. Unsurprisingly, one step worse than this is when you can't even 
reproduce one of your own reactions in your own lab!" [@gibb2014reproducibility]

> "If there is nothing wrong with the reagents and reproducibility is still an 
issue, then as I like to tell students, there are two options: (1) the physical 
constants of the universe and hence the laws of physics are in a state of flux 
in their round-bottomed flask, or (2) the researcher is doing something wrong 
and either doesn't know it or doesn't want to know it. Then I ask them which 
explanation they think I'm leaning towards." [@gibb2014reproducibility]

Another facet where proprietary software has an advantage is that it will often have
more comprehensive company-based user support than open-source software. The
companies that make and sell proprietary software will usually have a user
support team to answer questions and help develop pipelines and may also offer 
training programs or materials.

Some open-source software also has robust user support, although sometimes a bit
less organized under a common source. In some cases, this has developed as a
result of a large community of users who help each other. Message boards like
StackOverflow provides a forum for users to ask and respond to questions. Some
companies also exist that provide, as their business model, user support for
open-source software. While open-source software is usually free, these
companies make money by providing support for that software.

User support is sparser for some of smaller software packages that are developed
as extensions of open-source software. For example, many of the packages for
preprocessing types of biomedical data are built by small bioinformatics teams
or individuals at academic or research institutions. Often this software is
developed by a single person or very small team as one part of their job
profile, with limited resources for user support and for providing training.
These extensions build on larger, more supported open-source software (e.g., R
or Python), but the extension itself is built and maintained by a very small
team that may not have the capacity to respond quickly to user questions. Many
open-source software developers try to create helpful documentation in the form
of help files and package vignettes (tutorials on how to use the software they
created), but from a practical point of view it is difficult for small
open-source developers to provide the same level of user support that a large
proprietary software company can.

This is often the case with cutting-edge open-source software for biomedical
preprocessing. These just-developed software packages are less likely to be
comprehensively documented than longer-established software. Further, it can
take a while for the community of software users to develop once software is
available, and while this is a limitation of new software for both open source
and proprietary languages, it can represent more of a problem for open-source
software, where there is typically not a company-based helpline and so the
community of users often represents one of the main sources for help and
troubleshooting.

For other facets, open-source software has important advantages over proprietary
software. One key advantage is that all code is open in open-source software, so
you can dig down to figure out exactly how each step of the program works.
Futher, in many cases for open-source scientific software, the algorithms and
their principles have gone through peer review as part of the academic
publication process.

With proprietary software, on the other hand, details of algorithms may be
considered protected intellectual property, and so it may be hard to find out
the details of how the underlying algorithms work [@nekrutenko2012next]. Also,
the algorithms may not have gone through peer-review, especially if they are
considered private intellectual property.


Another advantage of open-source software is that older versions of the software
are often well-archived and easily available to reinstall and use if needed to
reproduce an analysis that was done using an earlier version of the software
than the current main version at the time of the replication.

As a final advantage, open-source software is often free. This makes it 
economical to test out, and it means that trainees from a lab will have no 
problem continuing to use the software as they move to new positions. 
The cost with open-source software, then, comes not with the price to buy 
the software, but with the investment that is required to learn it. 


### Discussion questions






-------------------------------------------------------------------------------

**Data pre-processing**

When we take measurements of experimental samples, we do so with the goal of
using the data we collect to gain scientific knowledge. The data are direct
measurement of something, but need to be interpreted to gain knowledge.
Sometimes direct measurements line up very closely with a research
question---for example if you are conducting a study that investigates the
mortality status of each test subject then whether or not each subject to dies
is a data point that is directly related to the research question you are aiming
to answer. In this case these data may go directly into a statistical analysis
model without extensive pre-processing. However, there are often cases where we
collect data that are not as immediately linked to the scientific question.
Instead, these data may require pre-processing before they can be used to test
meaningful scientific hypotheses. This is often the case for data extracted
using complex equipment. Equipment like mass spectrometers and flow cytometers
leverage physics, chemistry, and biology in clever ways to help us derive more
information from samples, but one tradeoff is that the data from such equipment
often require a bit of work to move into a format that is useful for answering
scientific questions.

In the research process, these pre-processing steps should be done before
the data are used for further analysis. There are the first step in working
with the data after they are collected by the equipment (or by laboratory 
personal, in the case of data from simpler process, like plating samples
and counting colony-forming units). After the data are appropriately 
pre-processed, you can use them for statistical tests---for example, to 
determine if metabolite profiles are different between experimental groups---and
also combine them with other data collected from the experiment---for example, 
to see whether certain metabolite levels are correlated with the bacterial
load in a sample.

**Approaches for pre-processing data.**

There are two main approaches for pre-processing experimental data in this 
way. First, when data are the output of complex laboratory equipment, there
will often be proprietary software that is available for this pre-processing.
This software may be created by the same company that made the equipment, or
it may be created and sold by other companies. The interface will typically 
be a graphical-user interface (GUI), where you will use pull-down menus and
point-and-click interfaces to work through the pre-processing steps. You 
often will be able to export a pre-processed version of the data in a 
common file format, like a delimited file or an Excel file, and that version
of the data can then be read into more general data analysis software, like
Excel or R.

In the simplest case, the point-and-click interface that is used for this
approach to pre-processing could be Excel or another version of spreadsheet
software. An Excel spreadsheet might be used with data recorded "by hand" in the
laboratory, with the researcher using embedded equations (or calculating and
entering new values by hand) for steps like calculating values based on the raw
data (for example, determining the time since the start of an experiment based
on the time stamp of each data collection timepoint).

> "There are currently [2017] very few, if any, 'plug-and-play' packages that 
allow researchers to quality control (QC), analyse and interpret scRNA-seq 
data, although companies that sell the wet-lab hardware and reagents for 
scRNA-seq are increasingly offering free software (for example, Loupe from 
10x Genomics, and Singular from Fluidigm). These are user-friendly but have the
drawback that they are to some extent a 'black box', with little transparency 
as to the precise algorithmic details and parameters employed." [@haque2017practical]

The second approach is to conduct the pre-processing directly within general
data analysis software like R or Python. These programs are both open-source,
and include extensions that were created and shared by users around the world.
Through these extensions, there are often powerful tools that you can use to
pre-process complex experimental data. In fact, the algorithms used in
proprietary software are sometimes extended from algorithms first shared through
R or Python. With this approach, you will read the data into the program (R, 
for example) directly from the file output from the equipment. You can 
record all the code that you use to read in and pre-process the data in a 
code script, allowing you to reproduce this pre-processing work. You can 
also go a step further, and incorporate your code into a pre-processing 
protocol, which combines nicely formatted text with executable code, and
which we'll describe in much more detail later in this module and in the 
following two modules.

There are advantages to taking the second approach---using scripted code in an
open-source program---rather than the first---using proprietary software with a
GUI interface. The use of codes scripts ensures that the steps of pre-processing
are reproducible. This means both that you will be able to re-do all the steps
yourself in the future, if you need to, but that also that other researchers can
explore and replicate what you do. You may want to share your process with
others in your laboratory group, for example, so they can understand the choices
you made and steps you took in pre-processing the data. You may also want to
share the process with readers of the articles you publish, and this may in fact
be required by the journal. Further, the use of a code script encourages you to
document this code and this process, even moreso when you move beyond a script
and include the code in a reproducible pre-processing protocol. Well-documented
code makes it much easier to write up the method section later in manuscripts
that leveraged the data collected in the experiment.

Also, when you use scripted code to pre-process biomedical data, you will find
that the same script can often be easily adapted and re-used in later projects
that use the same type of data. You may need to change small elements, like the
file names of files with data you want to use, or some details about the methods
used for certain pre-processing steps. However, often almost all of the
pre-processing steps will repeat over different experiments that you do. By 
extending to write a pre-processing protocol, you can further support the 
ease of adapting and re-using the pre-processing steps you take with one 
experiment when you run later experiments that are similar.

### Approaches to simple preprocessing tasks

There are several approaches for tackling this type of data preprocessing, to
get from the data that you initial observe (or that is measured by a piece of
laboratory equipment) to meaningful biological measurements that can be analyzed
and presented to inform explorations of a scientific hypothesis. While there are
a number of approaches that don't involve writing code scripts for this
preprocessing, there are some large advantages to scripting preprocessing any
time you are preprocessing experimental data prior to including it in figures or
further analysis. In this section, we'll describe some common non-scripted
approaches and discuss the advantages that would be brought by instead using a
code script. In the next module, we'll walk through an example of how scripts
for preprocessing can be created and applied in laboratory research.

In cases where the pre-processing is mathematically straightforward and the
dataset is relatively small, many researchers do the preprocessing by hand in a
laboratory notebook or through an equation or macro embedded in a spreadsheet.
For example, if you have plated samples at different dilutions and are trying to
calculate from these the CFUs in the original sample, this calculation is simple
enough that it could be done by hand. However, there are advantages to instead
writing a code script to do this simple preprocessing.

When you write a script to do a task with data, it is like writing a recipe that
can be applied again and again. By writing a script, you encode the process a
single time, so you can take the time to check and recheck to make sure that
you've encoded the process correctly. This helps in avoiding small errors when
you do the preprocessing---if you are punching numbers into a calculator over
and over, it's easy to mistype a number or forget a step every now and then,
while the code will ensure that the same process is run every time and that it
faithfully uses the numbers saved in the data for each step, rather than relying
on a person correctly entering each number in the calculation.

Scripts can be used across projects, as well, and so they can ensure consistency
in the calculation across projects. If different people do the calculation in
the lab for different projects or experiments, and they are doing the
calculations by hand, they might each do the calculation slightly differently,
even if it's only in small details like how they report rounded numbers. A
script will do the exact same thing every time it is applied. You can even share
your script with colleagues at other labs, if you want to ensure that your data
preprocessing is comparable for experiments conducted in different research
groups, and many scientific journals will allow supplemental material with 
code used for data preprocessing and analysis, or links within the manuscript
to a repository of this code posted online.

There are also gains in efficiency when you use a script. For small
pre-processing steps, these might seem small for each experiment, and certainly
when you first write the script, it will likely take longer to write and test
the script than it would to just do the calculation by hand (even more if
you're just starting to learn how to write code scripts). However, since the
script can be applied again and again, with very little extra work to apply it
to new data, you'll save yourself time in the future, and over a lot of
experiments and projects, this can add up. This makes it particularly useful to
write scripts for preprocessing tasks that you find yourself doing again and
again in the lab. 

### Approaches to more complex preprocessing tasks

Other preprocessing tasks can be much more complex, particularly those that need
to conduct a number of steps to extract biologically meaningful measurements
from the measurements made by a complex piece of laboratory equipment, as well
as steps to make sure these measurements can be meaningfully compared across
samples.

For these more complex tasks, the equipment manufacturer will often provide
software that can be used for the preprocessing. This software might conduct
some steps using defaults, and others based on the user's specifications. These
are often provided through "GUIs" (graphical user interfaces), where the user
does a series of point-and-click steps to process the data. In some software,
this series of point-and-click steps is recorded as the user does them, so that
these steps can be "re-run" later or on a different dataset. 

For many types of biological data, including output from equipment like flow
cytometers and mass spectrometers, open-source software has been developed
that can be used for this preprocessing. Often, the most cutting edge methods
for data preprocessing are first available through open-source software packages, 
if the methods are developed by researchers rather than by the companies, and
often many of the algorithms that are made available through the equipment
manufacturer's proprietary software are encoded versions of an algorithm 
first shared by researchers as open-source software. 

It can take a while to develop a code script for preprocessing the raw data from
a piece of complex equipment like a mass spectrometer. However, the process of
developing this script requires a thoughtful consideration of the steps of
preprocessing, and so this is often time well-spent. Again, this initial time
investment will pay off later, as the script can then be efficiently applied to
future data you collect from the equipment, saving you time in pointing and
clicking through the GUI software. Further, it's easier to teach someone else
how to conduct the preprocessing that you've done, and apply it to future 
experiments, because the script serves as a recipe. 

When you conduct data preprocessing in a script, this also gives you access to
all the other tools in the scripting language. For example, as you work through
preprocessing steps for a dataset, if you are doing it through an R script, you
can use any of the many visualization tools that are available through R. By
contrast, in GUI software, you are restricted to the visualization and other
tools included in that particular set of software, and those software developers
may not have thought of something that you'd like to do. Open-source scripting
languages like R, Python, and Julia include a huge variety of tools, and once
you have loaded your data in any of these platforms, you can use any of these
tools.

If you have developed a script for preprocessing your raw data, it also becomes
much easier to see how changes in choices in preprocessing might influence your
final results. It can be tricky to guess whether your final results are sensitive, 
for example, to what choice you make for a particular tranform for part of your
data, or in how you standardize data in one sample to make different samples 
easier to compare. If the preprocessing is in a script, then you can test making 
these changes and running all preprocessing and analysis scripts, to see if it 
makes a difference in the final conclusions. If it does, then it helps you 
identify parts of preprocessing that need to be deeply thought through for the 
type of data you're collecting, and you may want to explore the documentation on 
that particular step of preprocessing to determine what choice is best for your
data, rather than relying on defaults.
 

### Potential quotes

> "The benefit of working with a programming language is that you have the code in
a file. This means that you can easily reuse that code. If the code has
parameters it can even be applied to problems that follow a similar pattern."
[@janssens2014data]

> "Data exploration in spreadsheet software is typically conducted via menus and
dialog boxes, which leaves no record of the steps taken." [@murrell2009introduction]

> "One reason Unix developers have been cool toward GUI interfaces is that, in their
designers' haste to make them 'user-friendly' each one often becomes frustratingly 
opaque to anyone who has to solve user problems---or, indeed, interact with it anywhere
outside the narrow range predicted by the user-interface designer." [@raymond2003art]

> "Many operating systems touted as more 'modern' or 'user friendly' than Unix achieve their
surface glossiness by locking users and developers into one interface policy, and offer an
application-programming interface that for all its elaborateness is rather narrow and rigid. 
On such systems, tasks the designers have anticipated are very easy---but tasks they have
not anticipated are often impossible or at best extremely painful. Unix, on the other hand, has
flexibility in depth. The many ways Unix provides to glue together programs means that components
of its basic toolkit can be combined to produce useful effects that the designers of the individual
toolkit parts never anticipated." [@raymond2003art]

> "During the late 1950s and early 1960s, another step was taken towards getting the 
computer to do more for programmers, arguably the most important step in the history of 
programming. This was the development of 'high-level' programming languages that were
independent of any particular CPU architecture. High-level languages make it possible to 
express computations in terms that are closer to the way a person might express them." 
[@kernighan2011d]

> "In early times, most software was developed by companies and most source code was 
unavailable, a trade secret of whoever developed it." [@kernighan2011d]

> "I think that it's important for a well-informed person to know something about 
programming, perhaps only that it can be surprisingly difficult to get very simple
programs working properly. There is nothing like doing battle with a computer to teach 
this lesson, but also to give people a taste of the wonderful feeling of accomplishment
when a program does work for the first time. It may also be valuable to have enough 
programming experience that you are cautious when someone says that programming is easy, 
or that there are no errors in a program. If you have trouble making 10 lines of code 
work after a day of struggle, you might be legitimately skeptical of someone who claims
that a million-line program will be delivered on time and bug-free." [@kernighan2011d]

> "Computer code is the preferred approach to communicating our instructions to the 
computer. The approach allows us to be precise and expressive, it provides a complete
record of our actions, and it allows others to replicate our work." [@murrell2009introduction]

> "Many biologists are first exposed to the R language by following a cookbook-type
approach to conduct a statistical analysis like a t-test or an analysis of 
variance (ANOVA). ALthough R excels at these and more complicated statistical 
tests, R's real power is as a data programming lanugage you can use to explore and
understand data in an open-ended, highly interactive, iterative way. Learning R as a 
data programming language will give you the freedom to experiment and problem solve
during data analysis---exactly what we need as bioinformaticians." [@buffalo2015bioinformatics]

> "Quite often, users don’t appreciate the opportunities. Noncomputational
biologists don’t know when to complain about the status quo. With modest amounts
of computational consulting, long or impossible jobs can become much shorter or
richer." --- Barry Demchak in [@altschul2013anatomy]

> "Now there are a lot of strong, young, faculty members who label themselves
as computational analysts, yet very often want wet-lab space. They're not
content just working off data sets that come from other people. They want to be
involved in data generation and experimental design and mainstreaming
computation as a valid research tool. Just as the boundaries of biochemistry and
cell biology have kind of blurred, I think the same will be true of
computational biology. It’s going to be alongside biochemistry, or molecular
biology or microscopy as a core component." --- Richard Durbin in
[@altschul2013anatomy]

> "I would say that computation is now as important to biology as chemistry is.
Both are useful background knowledge. Data manipulation and use of information
are part of the technology of biology research now. Knowing how to program also
gives people some idea about what's going on inside data analysis. It helps them
appreciate what they can and can't expect from data analysis software." ---
Richard Durbin in [@altschul2013anatomy]

> "**Does every new biology PhD student need to learn how to program?** To some,
the answer might be “no” because that’s left to the experts, to the people
downstairs who sit in front of a computer. But a similar question would be: does
every graduate student in biology need to learn grammar? Clearly, yes. Do they
all need to learn to speak? Clearly, yes. We just don't leave it to the
literature experts. That’s because we need to communicate. Do students need to
tie their shoes? Yes. It has now come to the point where using a computer is as
essential as brushing your teeth. If you want some kind of a competitive edge,
you’re going to want to make as much use of that computer as you can. The
complexity of the task at hand will mean that canned solutions don’t exist. It
means that if you’re using a canned solution, you’re not at the edge of
research." --- Martin Krzywinski in [@altschul2013anatomy]

> "Although we are tackling many different types of data, questions, and 
statistical methods hands-on, we maintain a consistent computational approach 
by keeping all the computation under one roof: the R programming language and
statistical environment, enhanced by the biological data infrastructure and 
specialized method packages from the Bioconductor project." [@holmes2018modern]

> "The availablility of over 10,000 packages [in R] ensures that almost all 
statistical methods are available, including the most recent developments. 
Moreover, there are implementations of or interfaces to many methods from 
computer science, mathematics, machine learning, data management, visualization
and internet technologies. This puts thousands of person-years of work by 
experts at your fingertips." [@holmes2018modern]


