## Extra quotes 

Quotes from research that we haven't used yet, but might want to as we edits.

> "Many journals provide mechanisms to make reproducibility possible, including
*PLoS*, *Nature*, and *Science*. This entails ensuring access to the computer 
code and datasets used to produce the results of a study. In contrast, replication of
scientific findings involves research conducted by independent researchers using 
their own methods, data, and equipment that validate or confirm the findings of
earlier studies. Replication is not always possible, however, so reproducibility 
is a minimum and necessaray standard for confirming scientific findings." [@keller2017evolution]

> "Reproducibility goes well beyond validating statistical results and includes empirical, 
computational, ethical, and statistical analyses. For example, empirical reproducibility 
ensures that the same results are obtained from the data and code used in the original 
study, and statistical reproducibility focuses on statistical design and analysis to 
ensure replication of an experiment. There are also definitions of ethical reproducibility, 
such as documenting the methods used in biomedical research or in social and behavioral 
science research so others can reproduce algorithms used in analysis." [@keller2017evolution]

> "Many studies have been undertaken to understand the reproducibility of scientific findings
and have come to different conclusions about the findings. For example, one scientist argues
that half of all scientific discoveries are false (Ioannidis 2005), others find that a large
portion of the reproduced findings produce weaker evidence compared with the original findings
(Nosek et al. 2015), and others find that 4/5 of the results are true positives (Jager and Leek 2013).
... Despite this controversy, the premise underlying reproducibility is data quality in the form of 
good experimental design and execution, documentation, and making scientific inputs available
for reproducing the scientific work." [@keller2017evolution]

> "In the computer science, engineering, and business worlds, data quality management focuses 
largely on administrative data and is driven by the need to have accurate, reliable data for daily 
operations. The kinds of data traditionally discussed in this data quality literature are
fundamental to the functioning of an organization---if the data are bad, ifrms will lose money or
defective products will be manufactured. The advent of data quality in the engineering and 
business worlds traces back to the 1940s and 1950s with Edward Deming and Joseph Juran.
Japanese companies embraced these methods and transformed their business practices using them.
Deming's approach used statistical process control that focused on measuring inputs and 
processes and thus minimized product inspections after a product was build." [@keller2017evolution]

> [In business, ] "Data quality is further defined from the perspective of the ease of use of the
data with respect to the integrity, accuracy, interpretability, and value assessed by the data
user and other attributes that make the data valuable." [@keller2017evolution]

> "Many scientists spend a lot of time using Excel, and without batting an eye will
change the value in a cell and save the results. I strongly discourage modifying 
data this way. Instead, a better approach is to treat all data as *read-only* and
only allow programs to read data and create new, separate files of results. Why 
is treating data as read-only important in bioinformatics? First, modifying the 
data in place can easily lead to corrupted results. For example, suppose you 
wrote a script that directly modifies a file. Midway through processing a large
file, your script encounters an error and crashes. Because you've modified the
original file, you can't undo the changes and try again (unless you have a backup)!
Essentially, this file is corrupted and can no longer be used. Second, it's easy to 
lose track of how we've changed a file when we modify it in place. Unlike a workflow
where each step has an input file and an output file, a file modified in place
doesn't give us any indication of what we've done to it. Were we to lose track of how
we've changed a file and don't have a backup copy of the original data, our changes
are essentially irreproducible. Treating data as read-only may seem counterintuitive
may seem counterintuitive to scientists familiar with working extensively in Excel, 
but it's essential to robust research (and prevents catastrophe, and helps reproducibility).
The initial difficulty is well worth it; it also fosters reproducibility. Additionally, 
any step of the analysis can easily be redone, as the input data is unchanged by the 
program." [@buffalo2015bioinformatics]

> "'Plain text' data files are encoded in a format (typically UTF-8) that can be read by humans and computers alike. The great thing about plain text is their simplicity and their ease of use: any programming language can read a plain text file. The most common plain text format is .csv, comma-separated values, in which columns are separated by commas and rows are separated by line breaks." [@gillespie2016efficient]

> **Designed data** is "data that have traditionally been used in scientific
discovery. Designed data include statistically designed data collections, such
as surveys or experiments, and intentional observational collections. Examples
of intentional observational collections include data obtained from specially
designed instruments such as telescopes, DNA sequencers, or sensors on an ocean
buoy, and also data from systematically designed case studies such as health
registries Researchers have frequently devoted decades of systematic research to
understanding and characterizing the properties of designed data collections."
This contrasts with administrative data and opportunity data.
[@keller2017evolution]

> "The need to address data quality is a persistent one in the physical and
biological sciences, where scientists often seek to understand subtle effects
that leave minute traces in large volumes of data. ... For most scientists,
three factors motivate their work on data quality: first, the need to create a
strong foundation of data from which to draw their own conclusions; second, the
need to protect their data and conclusions from the criticisms of others; and
third, the need to understand the potential flaws in data collected by others.
The work of these scientists in data quality primarily concentrates on the
design and execution of experiments, including in laboratory, field, and
clinical settings. The key ingredients are measurement implementation,
laboratory and experimental controls, documentation, analysis, and curation of
data." [@keller2017evolution]

> "The concept of data quality management developed in the 1980s in step with
the technological ability to access stored data in a random fashion.
Specifically, as data management encoding process moved from the highly
controlled and defined linear process of transcribing information to tape, to a
system allowing for the random updating and transformation of data fields in a
record, the need for a higher level of control over what exactly can go into a
data field (including type, size, and values) became evident. Two key data
quality concepts came from these data management advances---ensuring data
integrity and cleansing the legacy data. Data integrity refers to the rules and
processes put in place to maintain and assure the accuracy and consistency of a
system that stores, processes, and retrieves data. Data cleaning refers to the
identificatio of incomplete, incorrect, inaccurate, or irrelevant parts of the
data and then replacing, modifying, or deleting this so-called dirty or coarse
data." [@keller2017evolution]

> "As the capability to store increasing amounts of data grew, so did the
business motivation to improve the quality of administrative data and thereby
improve decision making, reduce costs, and gain trust of customers."
[@keller2017evolution]

> "Determine whether there is a community-based metadata schema or standard (i.e., 
preferred sets of metadata elements) that can be adopted." [@michener2015ten]

> Might make more sense in tidy data section: "Although many standards have been
defined for data and model representations, they only ensure that data and
models that comply with these standards can be used by software that support
these standards; they do not ensure that multiple software tools can be used
seamlessly. When software tools are developed by independent research groups or
companies without an explicit agreement as to how they can be integrated, this
can cause problems when forming a workflow of multiple tools. This is because
the tools are likely to be inconsistent in their operating procedures and their
use of various non-standardized data formats. Thus, users often have to convert
data formats, to learn operating procedures for each tool, and sometimes even to
adjust operating environments. This impedes productivity, undermines the
flexibility of the workflow, and is prone to errors." [@ghosh2011software]

> "Ontologies define the relationships and hierarchies between different terms
and allow the unique, semantic annotation of data." [@ghosh2011software]

> "There are several international and national bodies, such as OMG, W3C, IEEE,
ANSI, and IETF... that formally approve standards or provide a framework for
standards development. Although some of the systems biology standards have been
certified (for example, SBML is officially adapted by IETF), in general in life
sciences this procedure is not particularly important---many of the most
successful standards such as GO have not undergone any official approval
procedure, but instead have become *de facto* standards. In fact, many of the
most successful standards in other domains are *de facto* standards."
[@brazma2006standards]

> "There are four steps involved in developing a complete and self-contained
standard: conceptual model design, model formalization, development of a data
exchange format, and implementation of the supporting tools."
[@brazma2006standards]

> "Two competing goals should be balanced when developing the conceptual model
of a domain: domain specificity and the need to find the common ground for all
related applications. Arguably, the most useful standards are those that consist
of the minimum number of most informative parameters. Keeping the list short
makes it simple and practicable, while selecting the most informative features
ensures accuracy and efficiency. The need for minimalism is reflected by the
titles for many such standards---Minimum Information About XYZ."

> "For a standard to be successful, laboratory information management systems
(LIMS), databases and data analysis, and modeling tools shoudl comply with it.
One way of fostering this is to develop easy-to-use 'middleware'---software
components that hide the technical complexities of the standard and facilitate
manipulation of the standard format in an easy way." [@brazma2006standards]

> "**Semantics**: The meaning of something; in computer science, it is usually
used in opposition to syntax (that is, format)." [@brazma2006standards]

> "Excel is by far the most common spreadsheet software, but many other
spreaadsheet programs exist, notably the Open Office Calc software, which is an
open source alternative and stores spreadsheets in an XML-based open standard
format called Open Document Format (ODF). This allows the data to be accessed by
a wider variety of software. However, even ODF is not ideal is a storage format
for aa research data set because spreadsheet formats contain not only the data
that is stored in the spreadsheet, but also information about how to display the
data, such as fonts and colors, borders and shading." [@murrell2009introduction]

> "Spreadsheet software is useful because it displays a data set in a nice
rectangular grid of cells. The arrangement of cells into distinct columns shares
the same benefits as fixed-width format text files: it makes it very easy for a
human to view and navigate within the data. ... because most spreadsheet
software provides facilities to import a data set from a wide variety of
formats, these benefits of the spreadsheet *software* can be enjoyed without
having to suffer the negatives of using a spreadsheet *format* for data storage.
For example, it is possible to store the data set in a CSV format and use
spreadsheet software to view or explore the data." [@murrell2009introduction]

> "Thinking about what sorts of questions will be asked of the data is a good
way to guide the design of data storage." [@murrell2009introduction]

First, you can still use spreadsheets, but reduce their use to recording data, 
leaving all data cleaning and analysis to be handled with other software. To make 
it easier to collaborate with statisticians and to interface with a program like 
R for data cleaning and analysis, it will be easiest if you set up your data
recording to include with other statistical programs like R or Python. These
steps are described in a later section, "...".

- Each sheet of the spreadsheet should contain data from a single
experiment.
- Never use whitespace to represent a meaningful separation in data within 
a spreadsheet. Never include multiple tables of data in the same sheet. 
- The first row of the spreadsheet should include a short column name for each
column with data. All column name information should be within a single row
(i.e., avoid subheadings). Avoid any special characters (e.g., "%") in column
names. Instead, use only letters, numbers, and underscores ("_"), and start with
a letter. It is especially helpful if you can avoid spaces in column names.
- Missing data should be represented consistently in cells. "NA" is one choice.
If you want to clarify why data is missing, it's much better to add a column
(e.g., "why_missing") where you can provide those details in text, rather than
combining within a single column numerical observation data with textual reasons
for missingness in cells with missing values.

Next, you could record data using a statistical language like R. There is an
excellent Integrated Development Environment for R called RStudio, and it
creates a much clearer interface with R compared to running R from a commond
line, particularly for new users. RStudio allows you to open delimited plain
text files, like csvs, using a grid-style interface. This grid-style interface
looks very similar to a spreadsheet, but lacks the ability to include formulas
or macros. Therefore, this format enforces a separation of the recording of raw
data from the cleaning and analysis of the data.

[R Project templates]

Data cleaning and analysis can then be shifted away from the files used to 
record the data and into reproducible scripts. These scripts can be clearly 
documented, either through comments in the code or through open source 
documentation tools like RMarkdown than interweave code and text in a way that
allows the creation of documents that are easier to read than commented code. 

This documentation should explain why each step is being done. In cases where 
it is not immediately evident from the code *how* the step is being done, this
should be documented as well. Any assumptions being used should be clarified in 
the documentation.

> "When we have only the letters, digits, special symbols, and punctuation marks
that appear on a standard (US) English keyboard, then we can use a single byte
to store each character. This is called an ASCII encoding (American Standard
Code for Information Exchange). ... UNICODE is an attempt to allow computers to
work with all of the characters in all of the languages of the world. Every
character has its own number, called a 'code point', often written in the form
U+xxxxxx, where every x is a hexidecimal digit. ... There are two main
'encodings' that are used to store a UNICODE code point in memory. UTF-16 always
uses two bytes per character of text and UTF-8 uses one or more bytes, depending
on which characters are stored. If the text is only ASCII, UTF-8 will only use
one byte per character. ... This encoding is another example of additional
information that may have to be provided by a human before the computer can read
data correctly from a plain text file, although many software packages will cope
with different encodings automatically." [@murrell2009introduction]

> "A PDF document is primarily a description of how to *display* information.
Any data values within a PDF document will be hopelessly entwined with
information about how the data values should be displayed."
[@murrell2009introduction]

> "Another major weakness of free-form text files is the lack of information
*within the file itself* about the structure of the file. For example, plain
text files do not contain information about which special character is being
used to separate fileds in a delimited file, or any information about the widths
of fields within a fixed-width format. This means that the computer cannot
automatically determine where different fields are within each row of a plain
text file, or even how many fields there are. A fixed-width format avoids this
problem, but enforcing a fixed length for fields can create other difficulties
if we do not know the maximum possible length for all variables. Also, if the
values for a variable can have very different lengths, a fixed-width format can
be inefficient because we store lots of empty space for short values. The
simplicity of plain text files make it easy for a computer to read a file as a
series of characters, but the computer cannot easily distinguish individual data
values from the series of characters. Even worse, the computer has no way of
tellins what sort of data is stored in each field. ... In practice, humans must
suppy additional information about a plain text file before a computer can
successfully determine where the different fields are within a plain text file
*and* what sort of data is stored in each field." [@murrell2009introduction]

> "In bioinformatics, the plain-text data we work with is often encoded in
*ASCII*. ASCII is a character encoding scheme that uses 7 bits to represent 128
different values, including letters (upper- and lowercase), numbers, and special
nonvisible characters. While ASCII only uses 7 bits, nowadays computers use an
8-bit *byte* (a unit representing 8 bits) to store ASCII characters. ... Because
plain-text data uses characters to encode information, our encoding scheme
matters. When working with a plain-text file, 98% of the time you won't have to
worry about the details of ASCII and how your file is encoded. However, the 2%
of the time when encoding data does matter---usually when an invisible non-ASCII
character has entered the data---it can lead to major headaches."
[@buffalo2015bioinformatics]

> "Programs [in Unix] retreive the data in a file by a system call (a subroutine
in the kernel) called `read`. Each time `read` is called, it returns the next
part of a file---the next line of text typed on the terminal, for example.
`read` also says how many bytes of the file were returned, so end of file is
assumed when a `read` says 'zero bytes are being returned'. If there were any
bytes left, `read` would have returned some of them." [@kernighan1984unix]

> "The format of a file is determined by the programs that use it; there is a
wide variety of file types, perhaps because there is a wide variety of programs.
But since file types are not determined by the file system, the kernel can't
tell you the type of a file: it doesn't know it. The `file` command makes an
educated guess ... To determine the types, `file` doesn't pay attention to the
names (although it could have), because naming conventions are just conventions,
and thus not perfectly reliable. For example, files suffixed '.c' are almost
always C source, but there is nothing to prevent you from creating a '.c' file
with arbitrary contents. Instead, `file` reads the first hundred bytes of a file
and looks for clues to that file type. ... In Unix systems there is just one
kind of file, and all that is required to access a file is its name. The lack of
file formats is an advantage overall---programmers don't need to worry about
file types, and all the standard programs will work on any file."
[@kernighan1984unix]

> "The Unix file system is organized so you can maintain your own personal files
without interfering with files belonging to other people, and keep people from
interfering with you too." [@kernighan1984unix]

> "The clinical patient health record is a longitudinal administrative record of
an individual's health information: all the data related to an individual's or a
population's health. The health record is a set of nonstandardized data that
spans multiple levels of aggregation, from a single measurement element (blood
pressure) to collections of diagnoses and related clinical observations. This
complexity is compounded by the high degree of human interaction involved in the
productio of clinical records, including self-reported data, medical diagnosis,
and other patient information." [@keller2017evolution]

> "Sharing data through repositories enhances both the quality and the value of
the data through standardized processes for curation, analysis, and quality
control. By allowing broad access to data, these repositories encourage and
support the use of previously collected data to test and extend previous
results. Data repositories are quite common in science fields such as astronomy,
genomics, and earth sciences. ... These repositories have accelerated discovery
by expanding the reach of these data to scientists who are not involved in the
initial data collection and experiments. Repositories address challenges that
affect data quality through governance, interoperability across systems, and
costs." [@keller2017evolution]

> One example of a repository is "the sharing of cDNA microarray data through
research consortia, which has led to a common set of standards and relatively
homogeneous data classes. There are many issues with the sharing of these data,
which requires the transformation of biologic to numeric data. These issues may
include loss of context, such as laboratory processes followed, and therefore
lack of information about the quality of the data when they are transformed. To
avoid this loss of information, the consortium ensures that documentation is
comprehensive so that other researchers can assess the quality of the data and
make comparisons with other studies using the same data. This documentation also
include information on when incorrect assignments of sequence identity are made
so that errors are not perpetuated in other studies." [@keller2017evolution]


> "**Data exchange format:** A file or message format that is formally defined
so that software can be built that 'knows' where to find various pieces of
information." [@brazma2006standards]


> "Everything in the Unix system is a file. That is less of an
oversimplification than you might think. When the first version of the system
was designed, before it even had a name, the discussions focused on the
structure of a file system that would be clean and easy to use. The file system
is central to the success and convenience of the Unix system. It is one of the
best examples of the 'keep it simple' philosophy, showing the power achieved by
careful implementation of a few well-chosen ideas." [@kernighan1984unix]

> "A file is a sequence of bytes. (A byte is aa small chunk of information,
typically 8 bits long. For our purposes, a byte is equivalent to a character.)
No structure is imposed on a file by the system, and no meaning is attaached to
its contents---the meaning of the bytes depends solely on the programs that
interpret the file. Furthermore, ... this is true not just of disc files but of
peripheral devices as well. Magnetic tapes, mail messages, characters typed on
the keyboard, line printer output, data flowing in pipes---each of these is just
a sequence of bytes as far as the systems and the programs in it are concerned."
[@kernighan1984unix]

> "The Comma-Separated Value (CSV) format is a special case of a plain text
format. Although not a formal standard, CSV files are very common and are a
quite reliable plain text delimited format that at least solves the problem of
where the fields are in each row of the file. The main rules for the CSV format
are: (1) **Comma-delimited**: Each field is separated by a comma (i.e., the
character , is the delimiter).; (2) **Double-quotes are special**: Fields
containing commas must be surrounded by double-quotes ... . (3) **Double-quote
escape sequence:** Fields containing double quotes must be surrounded by
double-quotes *and* each embedded double-quote must be represented using two
double quotes ... .; (4) **Header information**: There can be a single header
containing the names of the fields." [@murrell2009introduction]

> "A data file metaformat is a set of syntactic and lexical conventions that is
either formally standardized or sufficiently well established by practice that
there are standard service libraries to handle marshaling and unmarshaling it.
Unix has evolved or adopted metaformats suitable for a wide range of
applications [including delimiter-separated values and XML]. It is good practice
to use one of these (rather than an idiosyncratic custom format) whenever
possible. The benefits begin with the amount of custom paarsing and generation
code that you may be able to avoid writing by using a service library. But the
most important benefit is that developers and even many users will instantly
recognize these formats and feel comfortable with them, which reduces the
friction costs of learning new programs." [@raymond2003art]

> "There are three flavors you will encounter: tab-delimited, comma-separated, and
variable space-delimited. Of these three formats, tab-delimited is the most
commonly used in bioinformatics. File formats such as BED, GTF/GFF, SAM, tabular
BLAST output, and VCF are all examples of tab-delimited files. Columns of a
tab-delimited file are separated by a single tab character (which has the escape
code \t). A common convention (but not a standard) is to include metadata on the
first few lines of a tab-delimited file. These metadata lines begin with # to
differentiate them from the tabular dataa records. Because tab-delimated files
use a tab to delimit columns, tabs in data are not allowed. Comma-separated
values (CSV) is another common format. CSV is similar to tab-delimited, except
the delimiter is a comma character. While not a common in bioinformatics, it is
possible that the data stored in CSV format contain commas (which would
interfere with the ability to parse it). Some variants just don't allow this,
while others use quotes around entries that could contain commas. Unfortunately,
there's no standard CSV format that defines how to handle this and many other
issues with CSV---though some guidelines are given in RFC 4180. Lastly, there
are space-delimited formats. A few stubborn bioinformatics programs use a
variable number of spaces to separate columns. In general, tab-delimited formats
and CSV are better choices than space-delimited formats because it's quite
common to encounter data containing spaces." [@buffalo2015bioinformatics]

> "There are long-standing Unix traditions aabout how textual data formats ought
to look. Most of these derive from one or more of the standard Unix metaformats
... just described [e.g., DSV, XML]. It is wise to follow these conventions
unless you have strong and specific reasons to do otherwise. ... (1) *One record
per newline-terminated line, if possible.* This makes it easy to extract records
with text-stream tools. For data interchange with other operating systems, it's
wise to make your file-format parser indifferent to whether the line ending is
LF or CR-LF. It's also conventional to ignore trailing whitespace in such
formats; this protects against common editor bobbles. (2) *Less than 80
characters per line if possible.* This makes the format browseable in an
ordinary-sized terminal window. If many records must be longer than 80
characters, consider a stanza format... (3) *Use # as an introducer for
comments.* It's good to have a way to embed annotations and comments in data
files. It's best if they're actually part of the file structure, and so will be
preserved by tools that know its format. For comments that are not preserved
during parsing, # is the conventional start character. (4) *Support the
backslash convention.* The least surprising way to support nonprintable control
characters is by parsing C-like backslash escapes ... " [@raymond2003art]




> "You can take apart these formats and find out which decisions were made to
create them ... even old Microsoft Word, which in a long and painful political
bottle, finally settled down and 'opened' its format, countless hundres of pages
of documentation defining how words apper, how tables of contents are
registered, how all of the things that make up a Word document are to be
represented. The Microsoft Office File Formats specifications are of a most
disturbing, fascinating quality: one can read through them and think: *Yes, I
see this, I think I understand. But why?* ... Even Word is opened now, just
regular XML. Strange XML to be sure. All the codes once hidden are revealed."
[@ford2015on]


> "We wish to draw a distinction between data that is machine-actionable as a
result of specific investment in software supporting that data-type, for
example, bespoke parsers that understand life science wwPDB files or space
science Space Physics Archive Search and Extract (SPASE) files, and data that is
machine-actionable exclusively through the utilization of general-purpose, open
technologies. To reiterate the earlier point---ultimate machine actionability
occurs when a machine can make a useful decision regarding data that it has not
encountered before. This distinction is important when considering both (a) the
rapidly growing and evolving data environment, with new technologies and new,
more complex data-types continuously being developed, and (b) the growth of
general-purpose repositories, where the data-types encounted by an agent are
unpredictable. Creating bespoke parsers, in all computer languages, is not a
sustainable activity." [@wilkinson2016fair]

> "[One] way data can come from the Internet is through a web API, which stands
for *application programming interface*. The number of APIs that are being
offered by organizations is growing at an ever increasing rate... Web APIs are
not meant to be presented in a nice layout, such as websites. Instead, most web
APIs return data in a structured format, such as JSON or XML. Having data in a
structured format has the advantage that the data can be easily processed by
other tools." [@janssens2014data]


> "The *pileup format* [is] a plain-text format that summarizes reads' bases at
each chromosome position by stacking or 'piling up' aligned reads."
[@buffalo2015bioinformatics]


> "Data compression, the process of condensing data so that it takes up less
space (on disk drives, in memory, or across network transfers), is an
indespensible technology in modern bioinformatics. For example, sequences from a
recent Illumina HiSeq run when compressed with Gzip take up 21,408,674,240
bytes, which is a bit under 20 gigabytes. Uncompressed, this file is a whopping
63,203,414,514 bytes (around 58 gigabytes). This FASTQ file has 150 million
200bp reads, which is 10x coverage of the hexaploid wheat genome. The
compression ratio (uncompressed size/ compressed size) of this data is
approximately 2.95, which translates to a significant space saving of about 66%.
Your own bioinformatics projects will likely contain much more data, especially
as sequencing costs continue to drop and it's possible to sequence genomes to
higher depth, include more biological replicates or time points in expression
studies, or sequence more individuals in genotyping studies. For the most part,
data can remain compressed on the disk throughout processing and analysis. Most
well-writted bioinformatics tools can work natively with compressed data as
input, without requiring us to decompress it to disk first. Using pipes and
redirection, we can stream compressed data and write compressed files directly
to the disk. Additionally, common Unix tools like *cat*, *grep*, and *less* all
have variants that work with compressed data, and Python's *gzip* module allows
us to read and write compressed data from within Python. So while working with
large datasets in bioinformatics can be challenging, using the compression tools
in Unix and software libraries make our lives much easier."
[@buffalo2015bioinformatics]

> "Non-text files definitely have their place. For example, very laarge
databases usually need extra address information for rapid access; this has to
be binary for efficiency. But every file format that is not text must have its
own family of support programs to do things that the standard tools could
perform if the format were text. Text files may be a little less efficient in
maachine cycles, but this must be balanced against the cost of extra software to
maintain more specialized formats. If you design a file format, you should think
carefully before choosing a non-textual representation." [@kernighan1984unix]


> "*Out-of-memory approaches* [are] computational strategies built arouond
storing and working with data kept out of memory on the disk. Reading data from
a disk is much, much slower than working with data in memory... but in many
cases this is the approach we have to take when in-memory (e.g., loading the
entire dataset into R) or streaming approaches (e.g., using Unix pipes ...)
aren't appropriate." [@buffalo2015bioinformatics]

> "In general, it is possible to jump directly to a specific location within a
binary format file, whereas it is necessary to read a text-based format from the
beginning and one character at a time. This feature of accessing binary formats
is called **random access** and it is generlaly faster than the typically
**sequential access** of text files." [@murrell2009introduction]

> "We often need fast read-only access to data linked to a genomic location or
range. For the scale of data we encounter in genomics, retrieving this type of
data is not trivial for a few reasons. First the data might not fit entirely in
memory, requiring an approach where data is kept out of memory (in other words,
on a slow disk). Second, even powerful relational database systems can be
sluggish when querying out millions of entries that overlap a specific
region---an incrediably common operation in genomics. [BGZF and Tabix] are
specifically designed to get around these limitations, allowing fast
random-access of tab-delimited genome position data."
[@buffalo2015bioinformatics]

> "Samtools now supports (after version 1) a new, highly compressed file format
known as *CRAM*. Compressing alignments with CRAM can lead to a 10%--30%
filesize reduction compared to BAM (and quite remarkably, with no significant
increase in compression or decompression time compared to BAM). CRAM is a
*reference-based* compression scheme, meaning only the aligned sequence that's
different from the reference sequence is recorded. This greatly reduces file
size, as many sequence may align with minimal difference from the reference. As
a consequence of this reference-based approach, it is imperative that the
reference is available and does not change, as this would lead to a loss of data
kept in the CRAM format. Because the reference is so important, CRAM files
contain an MD5 checksum of the reference file to ensure it has not changed. CRAM
also has support for multiple different *lossy compression* methods. Lossy
compression entails some information about an alignment and the original read is
lost. For example, it's possible to bin base quality scores using a lower
resolution binning scheme to reduce the filesize." [@buffalo2015bioinformatics]




> "Very often we need efficient random access to subsequences of a FASTA file
(given regions). At first glance, writing a script to do this doesn't seem
difficult. We could, for example, write a script that iterates through FASTA
entries, extracting sequences that overlaps the range specified. However, this
is not an efficient method when extracting a few *random* subsequences. To see
why, consider accessing the sequence from position chromosome 8 (123,407,082 to
123,419,742) from the mouse genome. This approach would needlessly parse and
load chromosomes 1 through 7 into memory, even though we don't need to extract
subsequences from these chromosomes. Reading entire chromosomes from disk and
copying them into memory can be quite inefficient---we would have to load all
125 megabytes of chromosome 8 to extract 3.6kb! Extracting numerous random
subsequences from a FASTA file can be quite computationaally costly. A common
computational strategy that allows for easy and fast random access is *indexing*
the file. Indexed files are ubiquitous in bioinformatics."
[@buffalo2015bioinformatics]

> "We can avoid needlessly reading the entire file off of the disk by using an
index that points to where certian blocks are in the file. In the case of our
FASTA file, the index essentially stores the location of where each sequence
begins in the file (as well as other necessary information). When we look up a
range like chromosome 8 (123,407,082--123,410,744), *samtools faidx* uses the
information in the index to quickly calculate exactly where in the file those
bases are. Then, using an operation called a file *seek*, the program jumps to
this exact position (called the *offset*) in the file and starts reading the
sequence. Having precomputed file offsets combined with the ability to jump to
those exact positions is what makes accessing sections of an indexed file fast."
[@buffalo2015bioinformatics]


> "The data revolution within the biological and physical science world is
generating massive amounts of data from ... a wide range of ... projects, such
as those undertaken at the Large Hadron Collider and
genomics-proteomics-metabolomics research." [@keller2017evolution]


> "Community standards for data description and exchange are crucial. These
facilitate data reuse by making it easier to import, export, compare, combine,
and understand data. Standards also eliminate the need for the data creator to
develop unique descriptive practices. They open the door to development of
disciplinary repositories for specific classes of data and specialized software
management tools. GenBank, the US NIH genetic sequence database, and the US
National Virtual Observatory are good examples of what is possible here. In
2007, the US National Science Foundation, recognizing the importance of such
standards, established the Community Based Data Interoperability Networks
(INTEROP) funding programme for the development of tools, standards, and data
management best practices within specific disciplinary communities. ... Although
many classes of scientific data aren't ready, or aren't appropriate, for
standardization, well chosen investments in standardization show a consistently
high pay-off." [@lynch2008big]


> "For certain types of important digital objects, there are well-curated,
deeply-integrated, special-purpose repositories such as Genbank, Worldwide
Protein Data Bank, and UniProt... However, not all datasets or even data types
can be captured by, or submitted to, these repositories. Many important datasets
emerging from traditional, low-throughput bench science don't fit in the data
models of these special-purpose repositories, yet these datasets are no less
important with respect to integrative research, reproducibility, and reuse in
general. Apparently in response to this, we see the emergence of numerous
general-purpose data repositories [e.g., FigShare, Mendeley]. ... Such
repositories accept a wide range of data types in a wide range of formats,
generally do not attempt to integrate or harmonize the distributed data, and
place few restrictions (or requirements) on the descriptors of the data
deposition. The resulting data ecosystem, therefore, appears to be moving away
from centralization, is becoming more diverse, and less integrated, thereby
exacerbating the discovery and re-usability problem for both human and
computational stakeholders." [@wilkinson2016fair]

> "It would be unwise to bet that these formats [SAM/BAM files] won't change (or
even be replaced at some point)---the field of bioinformatics is notorious for
inventing new data formats (the same goes with computing in general) ... So
learning how to work with specific bioinformatics formats may seem like a lost
cause, skills such as following a format specification, manipulating binary
files, extracting information from bitflags, and working with application
programming interfaces (API) are essential skills when working with any format."
[@buffalo2015bioinformatics]

> From a working group on bioinformatics and data-intensive science: "Many simple
analyses are not automated because data formats are a moving target. ... The
community has been slow to share tools, partially because tools are not robust
against different input formats." [@barga2011bioinformatics]

> "Different centres generate data in different formats, and some analysis tools
require data to be in particular formats or require different types of data to
be linked together. Thus, time is wasted reformatting and reintegrating data
multiple times during a single analysis. For example, next-generation sequencing
companies do not deliver raw sequencing data in a format common to all
platforms, as there is no industry-wide standard beyond simple text files that
include the nucleotide sequence and the corresponding quality values. As a
result, carrying out sequencing analyses across different platforms requires
tools to be adapted to specific platforms. It is therefore crucial to develop
interoperable sets of analysis tools that can be run on different computational
platforms depending on which is best suited for a given application, and then
stitch those tools together to form analysis pipelines."
[@schadt2010computational]

> "Many important datasets emerging from traditional, low-throughput bench
science don't fit in the data models of ... special-purpose repositories [like
Genbank, Worldwide Protein Data Bank, and UniProt], yet these datasets are no
less important with respect to integrative research, reproducibility, and reuse
in general. Apparently in response to this, we see the emergence of numerous
general-purpose data repositories [e.g., FigShare, Mendeley]. ... Such
repositories accept a wide range of data types in a wide range of formats,
generally do not attempt to integrate or harmonize the distributed data, and
place few restrictions (or requirements) on the descriptors of the data
deposition. The resulting data ecosystem, therefore, appears to be moving away
from centralization, is becoming more diverse, and less integrated, thereby
exacerbating the discovery and re-usability problem for both human and
computational stakeholders." [@wilkinson2016fair]

> "Simplicity, but not oversimplification, is the key to success [in developing
standards]." [@brazma2006standards]


> "Minimum reporting guidelines, terminologies, and formats (hereafter referred to 
as reporting standards) are increasingly used in the structuring and curation of
datasets, enabling data sharing to varying degrees. However, the mountain of 
frameworks needed to support data sharing between communities inhibits the 
development of tools for data management, reuse and integration. ... The same
framework [on the other hand] enables researchers, bioinformaticians, and data
managers to operate within an open data commons." [@sansone2012toward]


> "'One of the core issues of Bioinformatics is dealing with a profusion of (often poorly
defined or ambiguous) file formats. Some *ad hoc* simple human readable formats have
over time attained the status of de facto standards.'-- Peter Cock et al. (2010)"
[@buffalo2015bioinformatics]

> "Developing and using a standard is often an investment that will not pay off
immediately, therefore there is a much better chance of success if the user community
decides that the respective standard is needed." [@brazma2006standards]

> "Although standardization is not a goal in itself, its importance is growing
in a high-throughput era. This is similar to what happened to manufacturing
during industrialization. The data from high-throughput technologies are being
generated at a rate that makes managing and using these data sets impossible on
a case-by-case basis. Although some of the data generated by the newest
technologies might have a low signal-to-noise ratio to make data re-usable, the
data quality is improving as the technology matures, and it is a waste of
resources not to share and re-use these expensive datasets. However, this is
only possible if the instrumentation that generates these data, laboratory-based
storage information management systems and databases, data analysis tools, and
systems modeling software can talk to each other easily. This is the purpose of
standardization." [@brazma2006standards]

> "A standard is successful only if it is used, and it is important to ensure
that supporting software tools are designed and implemented."
[@brazma2006standards]



> "In the late 2000s, there arose the 'NoSQL movement', coalescing around a
collective desire of many programmers to move beyond the strictures of the
relational model and unshackle themeselves from SQL. *Our data varied and
diverse,* they said, even if programmers weren't that varied and diverse, *and
we are tired of pretending that one technology will address the need for speed.*
Dozens of new databases appeared, each with different merits. There were
key-value databases, like Kyoto Cabinet, which optimized for speed of retrieval.
There were search-engine libraries, like Apache Lucene, which made it relatively
eaasy to search through enormous corpora of text---your own Google. There was
Mongo DB, which allowed for 'documents', big arbitrary blobs of dataa, to be
stored without nice rows and consistent structure. People debated, and continue
to debate, the value of each. ... There is as yet no absolute challenger to the
relationship model. When people think *database*, they still think *SQL*."
[@ford2015i]


> "By information or data communication standard we mean a convention on how to
encode data or information about a particular domain (such as gene function)
that enables unambiguous transfer and interpretation of this information or
data." [@brazma2006standards]

> "The proper acquisition and handling of data is crucially important for both
the generation and verification of hypotheses. The rapid development of
high-throughput experimental techniques is transforming life-science research
into 'big data' science, and although numerous data-management systems exist,
the heterogeneity of formats, identifiers, and data schema pose serious
challenges. In this context, data-management systems need standardized formats
for data exchange, globally unique identifiers for data mapping, and common
interfaces that allow the integration of disparate software tools in a
computational workflow." [@ghosh2011software]

> "Data quality [for health registries data] is driven by multiple dimensions
such as clinical data standardization, the existence of common definitions of
data fields, and the validity of self-reported patient conditions and outcomes.
Recognized issues include the definitions of data fields and their relational
structure, the training of personnel related to data collection data processing
issues (data cleaning), and curation." [@keller2017evolution]


If you have data in a structured, tabular format that doesn't follow these
rules, you don't need to consider it "dirty", though---just think of "tidy" as
the tagname for this particular structure of data (the name, in this case,
connects the data format with a set of tools in R called the "tidyverse").

> "Software systems are transparent when they don't have murky corners or hidden
depths. Transparency is a passive quality. A program is passive when it is possible
to form a simple mental model of its behavior that is actuaally predictive for all
or most cases, because you can see through the machinery to what is actually going 
on." [@raymond2003art]

> "Software systems are discoverable when they include features that are designed 
to help you build in your mind a correct mental model of what they do and how they
work. So, for example, good documentation helps discoverability to a programmer. Discoverability
is an active quality. To achieve it in your software, you cannot merely fail to be obscure, 
you have to go out of your way to be helpful." [@raymond2003art]

> "Elegant code does much with little. Elegant code is not only correct but visibly, 
*transparently* correct. It does not merely communicate an algorithm to a computer, 
but also conveys insight and assurance to the mind of a human that reads it. By seeking
elegance in our code, we build better code. Learning to write transparent code is a first, 
long step toward learning how to write elegant code---and taking care to make code 
discoverable helps us learn how to make it transparent. Elegant code is both transparent and
discoverable." [@raymond2003art]

> "To design for transparency and discoverability, you need to apply every tactic for
keeping your code simple, and also concentrate on the ways in which your code is a 
communication to other human beings. The first questions to ask, after 'Will this design
work?' are 'Will it be reaadable to other people? Is it elegant?' We hope it is clear ...
that these questions are not fluff and that elegance is not a luxury. These qualities
in the human reaction to software are essential for reducing its bugginess and
increasing its long-term maintainability." [@raymond2003art]

> "The Unix style of design applies the do-one-thing-well approach at the level of 
cooperating programs as well as cooperating routines within a program, 
emphasizing small programs connected by well-defined interprocess communication
or by shared files. Accordingly, the Unix operating system encourages us to break our
programs down into simple subprocesses, and to concentrate on the interfaces between
these subprocesses." [@raymond2003art]

> "The ability to combine programs [with piping] can be extremely useful. But the real
win here is not cute combinations; it's that because both pipes and *more(1)* exist, 
*other programs can be simpler*. Pipes mean that programs like *ls(1)* (and other
programs that write to standard out) don't have to grow their own pagers---and we're 
saved from a word of a thousand built-in pagers (each, naturally, with its own divergent
look and feel). Code bloat is avoided and global complexity reduced. As a bonus, if 
anyone needs to customize pager behavior, it can be done in *one* place, by changing
*one* program. Indeed, multiple pagers can exist, and will all be useful with every application
that writes to standard output." [@raymond2003art]

> "Unix was born in 1969 and has been in continuous production use ever since. That's several 
geological eras by computer industry standards. ... Unix's durability and adaptability have
been nothing short of astonishing. Other technologies have come and gone like mayflies. 
Machines have increased a thousand-fold in power, languages have mutated, industry practice
has gone through multiple revolutions---and Unix hangs in there, still producing, still paying 
the bills, and still commanding loyalty from many of the best and brightest software technologists
on the planet." [@raymond2003art]

> "One of the many consequences of the exponential power-versus-time curve in computing, and the
corresponding pace of software development, is that 50% of what one knows becomes obsolete over
every 18 months. Unix does not abolish this phenomenon, but does do a good job of containing it. 
There's a bedrock of unchanging basics---languages, system calls, and tool invocations---that one 
can actually keep for entire years, even decades. Elsewhere it is impossible to predict what will 
be stable; even entire operating systems cycle out of use. Under Unix, there is a fairly sharp 
distinction between transient knowledge and lasting knowledge, and one can know ahead of time
(with about 90% certainty) which category something is likely to fall in when one learns it. Thus
the loyalty Unix commands." [@raymond2003art]

> "Unix is famous for being designed around the philosophy of small, sharp tools, each
intended to do one thing well. This philosophy is enabled by using a common underlying
format---the line-oriented, plain text file. Databases used for system administration
(users and passwords, network configuration, and so on) are all kept as plain 
text files. ... When a system crashes, you may be faced with only a minimal 
environment to restore it (you may not be able to access graphics drivers, 
for instance). Situations such as this can really make you appreciate the simplicity of
plain text." [@hunt2000pragmatic]

> "Unix is the foundational computing environment in bioinformatics because its
design is the antithesis of [a] inflexible and fragile approach. The Unix shell
was designed to allow users to easily build complex programs by interfacing
smaller modular programs together. This approach is the Unix philosophy: 
'This is the Unix philosophy: Write programs that do one thing and do it well.
Write programs to work together. Write programs to handle text streams, because 
that is a universal interface.'--Doug McIlory". [@buffalo2015bioinformatics]

> "Passing the output of one program directly into the input of another 
program with pipes is a computationally efficient and simple way to interface
Unix programs. This is another reason why bioinformaticians (and software engineers
in general) like Unix. Pipes allow us to build larger, more complex tools from
modular parts. It doesn't matter what language a program is written in, either; pipes
will work between anything as long as both programs understand the data passed
between them. As the lowest common denominator between most programs, plain-text
streams are often used---a point that McIlroy makes in his quote about the 
Unix philosophy." [@buffalo2015bioinformatics]

If the data is the same regardless of whether it's "tidy" or not, then why all
the fuss about following the "tidy" principles when you're designing the format
you'll use to record your data? The magic here in this---if you follow these
principles, then your data can be immediately input into a collection of powerful
tools for visualizing and analyzing the data, without further cleaning steps. 
What's more, all those tools (the set of tools is calld the "tidyverse") will 
typically *output* your data in a "tidy" format, as well. 

These small tools can be combined together because they take the same input
(data in a "tidy" format) and they output in the same format (also data in a
"tidy" format). This is such a powerful idea that many of the best loved toys
work on the same principle. Think of interlocking plastic block sets, like Lego.
You can create almost anything with a large enough set of Legos, because they
can be combined in almost any kind of way. Why? Because they all follow a
standard size for the ... on top of each block, and they all "input" ... of that
same size on the bottom of the block. That means they can be joined together in
any order and combination, and as a result very complex structures can be
created. It also means that each piece can be small and easy to understand---if
you're building a Lego structure, even something very fancy, you'll probably use
lots of rectangular brinks that are two ... across and four ... long, and that's
easy enough to describe that you could probably get a young child to help you
find those pieces when you need them.

The "tidy" data format is an implementation of a structured data format popular
among statisticians and data scientists. By consistently using this data format,
researchers can combine simple, generalizable tools to perform complex tasks in
data processing, analysis, and visualization. 

> "Base R graphics came historically first: simple, procedural, conceptually 
motivated by drawing on a canvas. There are specialized functions for different
types of plots. These are easy to call---but when you want to combine them to 
build up more complex plots or exchange one for another, this quickly gets 
messy, or even impossible. The user plots ... directly onto a (conceptual)
canvas. She explictly needs to deal with decisions such as how much space to allocate
to margins, axis labels, titles, legends, subpanels; once something is 'plotted', 
it cannot be moved or erased. There is a more high-level approach: in the 
*grammar of graphics*, graphics are built up from modular logical pieces, so that
we can easily try different visualization types for our data in an intuitive and
easily deciphered way, just as we can switch in and out parts of a sentence in human
language. There is no concept of a canvas or a plotter; rather, the user gives
`ggplot2` a high-level description of the plot she wants, in the form of an 
R object, and the rendering engine takes a holistic view of the scene to lay out
the graphics and render them on the output device." [@holmes2018modern]

-----------------------------------------------------------------------------

**Older text**

It is usually very little work to record data in a structure
that follows the "tidy data" principles, especially if you are planning to record
the data in a two-dimensional, tabular format already, and following these
principles can bring some big advantages. We explain these rules and provide
examples of biomedical datasets that both comply and don't comply with these
principles, to help make it clearer how you could structure a "tidy-compliant"
structure for recording experimental data for your own research.

If the data is the same regardless of whether it's "tidy" or not, then why all
the fuss about following the "tidy" principles when you're designing the format
you'll use to record your data? The magic here ix this---if you follow these
principles, then your data can be immediately input into a collection of
powerful tools for visualizing and analyzing the data, without further cleaning
steps (as discussed in the previous module). What's more, all those tools (the
set of tools is calld the "tidyverse") will typically *output* your data in a
"tidy" format, as well.

Once you have tools that input and output data in the same way, it becomes very
easy to model each of the tools as "small, sharp tools"---each one does one
thing, and does it really well. That's because, if each tool needs the same
type of input and creates that same type of output, those tools can be chained
together to solve complex problems. The alternative is to create large software
tools, ones that do a lot to the input data before giving you some output.
"Big" tools are harder to understand, and more importantly, they make it hard
to adapt your own solutions, and to go beyond the analysis or visualization that
the original tool creators were thinking of when they created it. Think of it this
way---if you were writing an essay, how much more can you say when you can mix and
match words to create your own sentences versus if you were made to combine
pre-set sentences?

### Creating the rules for collecting data in the same time each time

It is likely that there are certain types of experiments that you conduct
regularly, and that they're often trying to answer the same type of
question and generate data of a consistent type and structure. This is
a perfect chance to lay down rules or a pattern for how members of
your research group will record that data.

These rules can include:

1. How many units of observation does the experiment typically have?
Say, for example, that you are measuring the influence of two drugs on
bactieral load in an animal at
three time points. There may be some measurements taken at the unit of the drug
(for example, measurements related to its chemical composition) and some
taken at the unit of animal and time point (for example, the concentration of drug in
an animal's blood at a certain time point). This will help you define how many
tables you should use to collect the data---one for each unit of observation.
2. Which measurements will be recorded for each observation? In tidy data, the measurements
taken for an observation are recorded in rows, so you then specify what
column names should be used for each measurement (e.g., "sample_time",
"animal_weight"). If data is being recorded using multiple tables (because there
are multiple units of observation), make sure that each table include
"ID" columns that can be used to link across the tables. For example, each
table might have a column with a unique ID for each drug being tests, or tables
with measurements on animals might each have a column that uniquely identifies
the animal in an observation.
3. What units will be used for recording each measurement? For timestamp-type
measurments, like the date and time that an experiment started and the time of
each sample measurement, what timezone will be used? (Even if it's always the
same one, this can come in useful every now and then if you need to figure out
something like whether that location's timezone followed Daylight Savings Time,
for an experiment that spans the switch between Standard and Daylight Savings).


[Figure: Three tables---measurements on a drug (chemistry), measurements on an animal (weight),
measurements on an animal at time points (drug concentration)]

You can then take this information and design a *template* for collecting that
type of data. A template is, in this case, a file that gives the "skeleton" of
the table or tables. You will create this template file and save it somewhere
easy for lab members to access, with a filename that makes it clear that this is
a template. For example, you may create a folder with all the templates for
tables for your experiment, and name a template in it for collecting something
like animal weights at the start of the experiment something like
"animal_wt_table_template.csv" or "animal_wt_table_template.xlsx".
Each time someone starts an experiment collecting that type of data, he or she
can copy that template file, move it to the directory with files for that
experiment and rename it. When you open that copy of the file, you can record
observations directly into it.

[Figure: Example template file]

```
####################################################################################
####################################################################################
#
# Column names and meanings
#
# animal_id:        A unique identifier for each animal.
# animal_wt_g:      The weight of the animal, recorded in grams.
# date_wt_measured: The date that the animal's weight was measured, recorded as
#                   "month day, year", e.g., "Jan 1, 2019"
# cage_id:          A unique identifier for the case in which the animal was housed
#
# Other table templates for this experiment type:
# drug_conc_by_time.csv: A template for recording drug concentrations in the animals
#                        by time point
#
animal_id, animal_wt_g, date_wt_measured, cage_id
   "A101",        50.2,   "Jan 1, 2019",      "B"
```

Adding in one row of sample values, to be deleted each time the template is
copied and used, can be a very helpful addition. This will help the user remember
the formats that are expected for each column (for example, the format the
date should be recorded in), as well as small details like which columns should
include quotation marks.

These template tables can be created as flat files, like comma-separated value
files. However, if this is too big of a jump, they can also be created as
spreadsheet files. Many of the downsides of spreadsheet files are linked to
the use of embedded macros, integration of raw and processed / calculated data,
and other factors, rather than related to their use as a method to record data.
However, do note that plain text files like flat files can be opened in RStudio
in a spreadsheet-like view in RStudio. Data can be recorded directly here, in
a format that will feel comfortable for spreadsheet users, but without all the
bells and whistles that we're aiming to avoid in spreadsheet programs like Excel.

[Figure---Opening a csv file with a spreadsheet like view]

There are some advantages to shifting to record data in flat files like CSVs,
rather than Excel files, and using the spreadsheet-style view in RStudio to work
with those files if you find it easier than working with the files in a text
editor (which can get tough, since the values in a column don't always visually
line up, and you have to remember to put in the right number of columns). By
recording the data in a plain text file, you can later move to tracking changes
that are made to the data using the version control tool *git*. This is a
powerful tool that can show who made changes to a file and when, with exact
details on the changes made and room for comments on why the change was made.
However, *git* does not provide useful views of changes made to binary files
(like Excel), only those made in plain text files. Further, plain text files are
guaranteed to not try to "outsmart" you---for example, they will not try to
convert something that looks like a date into a date. Instead, they will leave
things exactly as you typed them in. Finally, later in this book we will build
up to creating templates that do even more---for example, templates for reports
you need to write and presentations you need to give, as well as templates for
the whole structure of a project. Plain text files fit very nicely into this
developing framework, while files in complex binary formats like xlxs don't fit
as naturally.

Google Sheets is another tool that might come in useful. [More about using this
with R.]

This idea of creating template files for data recording isn't
revolutionary---many laboratory groups have developed spreadsheet template files
that they share and copy to use across similar experiments that they conduct.
The difference here is in creating a table for recording data *that follows the
tidy data principles*, or at least comes close to them (any steps away from
characteristics like embedded macros and use of color to record information will
be helpful).

The next chapter will walk through two examples of changing from non-tidy table
templates to ones that record data in a way that follows the tidy data
principles.



### Subsection 1

> "Or maybe your goal is that your data is *usable* in a wide range of
applications? If so, consider adopting standard formats and metadata
standards early on. At the very least, keep track of versions of data
and code, with associated dates." [@goodman2014ten]

> "Standards for data include, for example, data formats, data exchange
protocols, and meta-data controlled vocabularies." [@barga2011bioinformatics]

> "Software systems are transparent when they don't have murky corners or hidden
depths. Transparency is a passive quality. A program is passive when it is possible
to form a simple mental model of its behavior that is actuaally predictive for all
or most cases, because you can see through the machinery to what is actually going
on." [@raymond2003art]

> "Software systems are discoverable when they include features that are designed
to help you build in your mind a correct mental model of what they do and how they
work. So, for example, good documentation helps discoverability to a programmer. Discoverability
is an active quality. To achieve it in your software, you cannot merely fail to be obscure,
you have to go out of your way to be helpful." [@raymond2003art]

> "Elegant code does much with little. Elegant code is not only correct but visibly,
*transparently* correct. It does not merely communicate an algorithm to a computer,
but also conveys insight and assurance to the mind of a human that reads it. By seeking
elegance in our code, we build better code. Learning to write transparent code is a first,
long step toward learning how to write elegant code---and taking care to make code
discoverable helps us learn how to make it transparent. Elegant code is both transparent and
discoverable." [@raymond2003art]

> "To design for transparency and discoverability, you need to apply every tactic for
keeping your code simple, and also concentrate on the ways in which your code is a
communication to other human beings. The first questions to ask, after 'Will this design
work?' are 'Will it be reaadable to other people? Is it elegant?' We hope it is clear ...
that these questions are not fluff and that elegance is not a luxury. These qualities
in the human reaction to software are essential for reducing its bugginess and
increasing its long-term maintainability." [@raymond2003art]

> "Software is maintainable to the extent that people who are not its author can
successfully understand and modify it. Maintainability demands more than code that
works; it demands code that follows the Rule of Clarity and communicates successfully
to human beings as well as the computer." [@raymond2003art]

> "An equivalent to the laboratory notebook that is standard good practice in
labwork, we advocate the use of a computational diary written in the R markdown
format. ... Together with a version control system, R markdown helps with
tracking changes." [@holmes2018modern]

> "R.A. Fisher, one of the fathers of experimental design, is quoted as
saying 'To consult the statistician after an experiment is finished is
often merely to ask him to conduct a post mortem examination. He can
perhaps say what the expierment died of.' So it is important to design an
experiment with the analysis already in mind. Do not delay thinking about
how to analyze the data until after they have been acquired. ...
Dailies: start with the analysis as soon as you have acquired some data.
Don't wait until everything is collected, as then it's too late to
troubleshoot. ... Start writing the paper while you're analyzing the data.
Only once you're writing and trying to present your results and
conclusions will you realize what you should have done properly to support
them." [@holmes2018modern]

> "In the same way a file director will view daily takes to correct potential
lighting or shooting issues before they affect too much footage, it is a
good idea not to wait until all runs of an experiment have been finished
before looking at the data. Intermediate data analyses and visualizations
will track unexpected sources of variation and enable you to adjust the
protocol. Much is known about the sequential design of experiments, but
even in a more pragmatic setting it is important to be aware of your sources
of variation as they occur and adjust for them." [@holmes2018modern]

> "Analysis projects often begin with a simple script, perhaps to try out a
few initial ideas and explore the quality of the pilot data. Then more ideas are
added, more data come in, other datasets are integrated, more people become
involved. Eventually the paper need to be written, the figures need to be done
'properly' and the analysis needs to be saved for the scientific record and
to document its integrity." [@holmes2018modern]

> "**Use literate programming tools.** Examples are Rmarkdown and Jupyter. This
makes code more readable (for yourself and for others) than burying
explanations and usage instructions in comments in the source code or in
separate README files. In addition, you can directly embed figures and tables
in these documents. Such documents are good starting points for the supplementary
material of your paper. Moreover, they're great for reporting analyses to your
collaborators." [@holmes2018modern]

### Don't Repeat Yourself!

One of the core tenets of programming is the philosophy of "Don't Repeat
Yourself" (a.k.a., the "DRY Principle").[Source of "Don't Repeat
Yourself"---*The Pragmatic Programmer*] With programming, you can invest a
little bit of time to code your computer to do things that take a lot of your
time otherwise. In this way, you can automate repetitive tasks.

> "The DRY principle, for Don't Repeat Yourself, is one of the colloquial tenets
of programming. That is, you should name things once, do things once, create a
function once, and let the computer repeat itself." [ford2015code]

> "Code, in other words, is really good at making things *scale*. Computers
may require utterly precise instructions, but if you get the instructions
right, the machine will tirelessly do what you command over and over and
over again, for users around the world. ... Solve a problem once, and you've
solved it for everyone." [Coders, p. 20]

> "Since they have, at their beck and call, machines that can repeat
instructions with robotic perfection, coders take a dim view of doing
things repetitively themselves. They have a dislike of inefficiency
that is almost aesthetic---they recoil from it as if from a disgusting
smell. Any opportunity they have to automate a process, to do something
more efficiently, they will." [Coders, p. 20]

> "Programmers are obsessed with efficiency. ... Removing the friction
from a system is an aesthetic joy; [programmers'] eyes blaze when they
talk about making something run faster, or how they eliminated some
bothersome human effort from a process." [Coders, p. 122]

> "Computers, in many ways, inspire dreams of efficiency greater than
any tool that came before. That's because they're remarkably good at
automating repetitive tasks. Write a script once, set it running, and
the computer will tirelessly execute it until it dies or the power
runs out. What's more, computers are strong in precisely the ways that
humans are weak. Give us a repetitive task, and our mind tends to
wander, so we gradually perform it more and more irregularly. Ask us
to do something at a precise time or interval, and we space out and
forget to do it. ... In contrast, computers are clock driven and superb
at doing the same thing at the same time, day in and day out."
[Coders, p. 124]

> "Larry Wall, the famous coder and linguist who created the Perl
programming language, deeply intuited this coderly aversion to
repetition. In his book on Perl, he and coauthors wrote that one of the
key virtues of a programmer is 'laziness'. It's not that you're too lazy
for coding. It's that you're too lazy to do routine things, so it
inspires you to automate them." [Coders p. 126]

In scientific research, there are a lot of these repetitive tasks, and as tools for
automation continue to develop, there are many opportunities to "automate away" busywork.

> "Science often involves repetition of computational tasks such as processing
large number of data files in the same way or regenerating figures each time
new data are added to an existing analysis. Computers were invented to do these
kinds of repetitive tasks but, even today, many scientists type the same
commands in over and over again or click the same buttons repeatedly." [wilson2014best]

> "Whenever possible, rely on the execution of programs instead of manual procedures
to modify data. Such manual procedures are not only inefficient and error-prone,
they are also difficult to reproduce." [sandve2013ten]

> "Other manual operations like the use of copy and paste between documents should
also be avoided. If manual operations cannot be avoided, you should as a minimum
note down which data fiels were modified or moved, and for what purpose." [sandve2013ten]

Statisticians have been doing this for a while for data cleaning analysis tasks.
For example, if you need to read in an Excel file into a statistical programming
language like R, you could write a few lines of code to do that anew each time
you get a new file. However, say you get Excel files over and over that follow
the same format--for example, files with the same number of columns, the same
names for those columns, and the same type of data. You can write a *script*---a
recorded file with a few lines of code, in this case---that reads in the file.
You can apply this script to each new file.

This saves you a little bit of time. It also ensures that you do the exact same thing
with every file you get. It also means that you can reproduce what you do now to a file
in the future. Say, for example, that you are working on a project and you read in a file
and conduct an analysis. Your laboratory group sends the paper out for review. Months
later, you get back comments from the reviewers, and they are wondering what would
happen if you had analyzed the data a bit differently---say, used a different
statistical test. If you use a script to read in the data file, then when you re-run
it to address the reviewers' comments, you can be sure that you are getting your
data into the statistical program in the exact same way you did months ago, and so
you're not unintentionally introducing differences in your results because you
are doing some small things differently in processing the file.

This idea can extend across the full data analysis you do on a project. You are only
saving a little bit of time and effort, maybe, by automating the step where you read
the data from a spreadsheet into the statistical program. And it takes some time to
write that script the first time, so it can be tempting to do it fresh each time you
need to do it. However, you can also write scripts that will automate cleaning your data.
Maybe you want to identify data points with very high (maybe suspect) values for a certain
measurement, or remove observations with missing data. You can also write scripts that
will automating processing your data---doing things like calculating the time since the
start of an experiment based on the recorded sampling time for an observation. Each of
these steps might be small, but the time saved really adds up since you typically
need to perform many of these steps each time you run a new experiment.

There are many cases in life where you'll need to make the choice between spending
some time upfront to make something more efficient, versus doing it more quickly the first
time but then having to do it "from scratch" again each following time. For example,
say that you're teaching a class, and you need to take attendance for each class period.
You could write down the names of each student at the first class and save that, and
then the next class write down the name of each student who shows up that day on a separate
sheet of paper, and so on for each class meeting. Conversely, you could take some extra
time before the first class and create a table or spreadsheet file with every student's
name and the date of each class, and then use that to mark attendance. The first method
will be quicker the first day, but more time consuming each following time. The second
method requires a small initial investment, but with time-saving returns in the following
class meetings.

For people who use scripts and computer programs to automate their data-related tasks,
it quickly becomes very confusing how anyone who doesn't could argue that they don't because
they don't have time to learn how to. The amount of time you end up saving based on your
initial investment is just so high if you're working with data, that it would have to take
a huge time investment to not be worth it. Plus---the thrill of running something that you've
automated! It's a very similar feeling to the feeling you get when a student or postdoc that
you've spent a lot of time training has gotten to the point where you can just ask them
to run something, and they do, and it means you don't have to.

Here are some of the problems that are solved by automating your small tasks:

1. **It gets done the same way every single time.** Even simple tasks can be done with
numerous small modifications. You will probably remember some of those choices and
settings and modifications the next time you need to do the same thing, but probably
not all of them, and so those choices will not be exact from one time to the next.
If the computer is doing it based on a clear set of instructions, it will be.

2. **It gets done more quickly.** Or if not more quickly (some large data might take
some time to process), at least the spent time is the computers time, not yours. You
can leave the computer to run the script while you get on with other things that
a computer can't do.

3. **Anyone who does it can do it the same way.** Just as you might not do something
exactly the same way from one time to the next, one person in a laboratory group is
likely to do things at least slightly different than other members of the group.
Even with very detailed instructions, few instructions written for humans can be
so detailed and precise to ensure that something is done exactly the same way by
everyone who follows them. If everyone is given the same computer script to run,
however, and they all instruct the computer to run that script, the task will be
done in exactly the same way.

4. **It is easier teach new people how to do the task.** Often, with a script to
automate a task, you just need to teach someone new to the laboratory group how
to get the computer to run a script in a certain language. When you need them to
run a new script, the process will be the same. The script encapsulates all the
task-specific details, and so the user doesn't need to understand all of them to
get something to run. What's more, once you want to teach a new lab member how
everything it working, so they can understand the full process, the script provides
the exact recipe. You can teach them how to read and understand scripts in that
language, and then the scripts you've created to automate tasks serve as a recipe
book for everything going on in terms of data analysis for the lab.

5. **You can create tools to share with others.** If you've written a script that's
very useful, with a bit more work you can create it into a tool that you can share
with other research groups and perhaps publish a paper about. Papers about R
software extensions (also called *packages*) and data analysis workflows and
pipelines are becoming more and more common in biological contexts.

6. **It's more likely to be done correctly.** Boring, repetative tasks are easy
to mess up. We get so bored with them, that we shift our brains into a less-attentive
gear when we're working on them. This can lead to small, stupid mistakes, ones
at the level of typos but that, with data cleaning and analysis, can have much
more serious ramifications.


> "We view workflows as a paradigm to: 1) expose non-experts to well-understood
end-to-end data analysis processes that have proven successful in challenging
domains and represent the state-of-the-art, and 2) allow non-experts to easily
experiment with different combinations of data analysis processes, represented
as workflows of computations that they can easily reconfigure and that the
underlying system can easily manage and execute." [hauder2011making]

> "While reuse [of workflows] by other expert scientists saves them time and
effort, reuse by non-experts is an enabling matter as in practice they would
not be able to carry out the analytical tasks without the help of
workflows." [hauder2011making]

> "We observed that often steps that could be easily automated were performed manually
in an error-prone fashion." [vidger2008supporting]

Biological research is quickly moving where a field where projects often required
only simple and straightforward data analysis once the experimental data was
collected---with the raw data often published directly in a table in the manuscript---to
a field with very complex and lengthy data analysis pipelines between the experiment
and the final manuscript. To ensure rigor and clarity in the final research results,
as well as to allow others to reproduce the results exactly, the researcher must
document all details of the computational data analysis, and this is often
missing from papers. RMarkdown documents (and their analogues) can provide all these
details unambiguously---with RMarkdown documents, you can even run a command to
pull out all the code used within the document, if you'd like to submit that
code script as a stand-alone document as a supplement to a manuscript.

> "More recently, scientists who are not themselves computational experts are
conducting data analysis with a wide range of modular software tools and packages.
Users may often combine these tools in unusual or nove ways. In biology,
scientists are now routinely able to acquire and explore data sets far beyond
the scope of manual analysis, including billions of DNA bases, millions of genotypes,
and hundreds of thousands of RNA measurements. ... While propelling enormous
progress, this increasing and sometimes 'indirect' use of computation poses
new challenges for scientific publication and replication. Large datasets are
often analyzed many times, with modifications to the methods and parameters, and
sometimes even updates of the data, until the final results are produced. The
resulting publication often gives only scant attendtion to the computations details.
Some papers have suggested these papers are 'merely the advertisement of
scholarship whereas computer programs, input data, parameter values, etc., embody
the scholarship itself.' However, the actual code or software 'mashup' that
gave rise to the final analysis may be lost or unrecoverable." [mesirov2010accessible]

> "Bioinformatic analyses invariably involve shepherding files through a series
of transformations, called a pipeline or workflow. Typically, these transformations
are done by third-part executable command line software written for Unix-compatible
operating systems. The advent of next-generation sequencing (NGS), in which millions
of short DNA sequences are used as the source input for interpreting a range of
biological phenomena, has intensified the need for robust pipelines. NGS analyses
tend to involve steps such as sequence alignment and genomic annotation that are
both time-intensive and parameter-heavy." [leipzig2017review]

> "**Rule 7: Always Store Raw Data behind Plots.** From the time a figure is first
generated to it being part of a published article, it is often modified several
times. In some cases, such modifications are merely visual adjustments to
improve readability, or to ensure visual consistency between figures. If raw data
behind figures are stored in a systematic manner, so as to allow raw data for
a given figure to be easily retrieved, one can simply modify the plotting
procedure, instead of having to redo the whole analysis. An additional
advantage of this is that if one really wants to read fine values in a figure,
one can consult the raw numbers. ... When plotting is performed using a
command-based system like R, it is convenient to also store the code
used to make the plot. One can then apply slight modifications to these
commands, instead of having to specify the plot from scratch." [sandve2013ten]

### Don't repeat your report-writing!

Until a few years ago, statisticians and data analysts frequently automated the
data cleaning, processing, and analysis tasks. But that still left the paper
and report writing to be done by hand. This process is often repetitive. You would
do your analysis and create some tables or figures. You would save these from
your statistical program and then paste them into your report or paper draft.
If you decided that you needed to change your analysis a bit, or if you got a new
set of data to analyze in a similar way, you had to go back to the statistical
program, run things again there, save the tables and figure files again, and paste
them in the report or paper again to replace the outdated version. If there were
numbers from the analysis in the text of the paper, then you had to go back through
the text and update all of those with the newer numbers, too.

Do you still write your papers and reports like this? I can tell you that there is
now a *much* better way. Computer scientists and other programmers started thinking
quite a while ago about how to create documents that combine computer code and
text for humans, and to do it in a way where the computer code isn't just a static
copy of what someone once told the computer to do, but instead a living, working,
*executable* set of instructions that the computer can run anytime you ask it to.

These ideas first perculated with Donald Knuth, who many consider to be the greatest
computer programmer of all time [Bill Gates, for example, has told anyone who reads Dr. Knuth's
magnum opus, *The Art of Computer Programming*, to come see him right away about a job].
As Dr. Knuth was writing a book on computer programming, he became frustrated with the
quality of the typesetting used in the final book. In a field that requires a lot of mathematical
and other symbols incorporated into the text, it takes a bit more to make an attractive
book than with simpler text. Dr. Knuth therefore took some time to create a programming
for typesetting. (You may have heard of it---if you ever notice that a journal's
Instructions to Authors allow authors to submit articles in "LaTeX" or "TeX", that's
using a system built off of Donald Knuth's typesetting program.)

And *then*, once he had that typesetting program, he started thinking about how
programmers document their code. When one person does a very small code project,
and that one person is the only person who will ever go back to try to modify or
understand the code, that person might be able to get away with poor
documentation in the code. However, interesting code projects can become
enormous, with many collaborators, and it becomes impossible to understand and
improve the code if it doesn't include documentation explaining, in human terms,
what the code is doing at each step, as well as some overall documentation
explaining how different pieces of the code coordinate to get something big
done.

Traditionally, code was documented by including small comments within the code. These comments
are located near the code that they explain, and the order of the information in the
code files are therefore dominated by the order of the instructions to the computer,
not the order that you might explain what's going on to a human. To "read" the code and
the documentation, you end up hopscotching through the code, following the code inside
one function when it calls another function, for example, to where the code for that
second function is defined and then back to the first, and so on. You often follow paths as
you get deeper and deeper into helper functions and the helper functions for those functions,
that you feel like you're searching through a set of Russian dolls and then coming back up
to start on a new set of Russian dolls later down the line.

Donald Knuth realized that, with a good typesetting program that could itself be programmed,
you could write your code so that the documentation for humans took precedence, and could
be presented in a very clear and attractive final document, rather than hard-to-read
computer code with some plain-text comments sprinkled in. Computers don't care what order
the code is recorded in---as long as you give them some instructions on how to decipher
code in a certain format or order, they can figure out how to use it fine. But human brains
are a bit more finicky, and we need clear communication, laid out in a logical and helpful
order. Donald Knuth created a paradigm of *literate programming* that interleaved
executable code inside explanations written for humans; by making the code executable, it
meant that the document was a living guide. When someone changed the program, they did
it by changing the documentation---documentation wasn't left as the final, often neglected,
step to refine once the "real code" was written (and the "real work" done).

> "Programs must be written for people to read, and only incidentally for machines
to execute. A great program is a letter from current you to future you or
the the person who inherits your code. A generous humanistic document." [ford2015what]

Well, this was a fantastic idea. It hasn't been universely leveraged, but the projects that
do leverage it are much stronger for it. But that's not where the story ends. If you are
someone who does a little bit of coding (maybe small scripts to analyze and visualize your
data, for example) and a lot of "documenting" of the results, and if you're not planning
on doing a lot of large coding projects or creating software tools, it's not immediately
how you'd use these literate programming ideas.

Well, there are many people who do a little bit of programming in service to a larger
research project. While they are not creating software that needs classical software
documentation, they do want to document the results that they get when they run their
scripts, and they want to create reports and journal articles to share what they've
found. Several people took the ideas behind literate programming---as it's used to
document large software projects---and leveraged it to create tools to automate
writing in data-related fields.

[F. Leisch?] was the first to do this with the R programming language, with a
tool called "Sweave" ("S"-"weave", as R builds off of another programming
language called "S" and Leisch's program would "weave" together S / R code and
writing). This used Donald Knuth's typesetting program. It allowed you to write
a document for humans (like a report or journal article) and to intersperse bits
of code in the paper. You'd put each code piece in the spot in the paper where
the text described what was going on or where you wanted the results that it
generated for example, if you had a section in the Methods where you talked
about removing observations that were outliers, you would add in the code that
took out those outliers right there in the paper. And if you had a placed in the
Results that talked about how your data were different between two experimental
groups, you would add the code that generated the plot to show that right there
in the paper.

To tell the computer how to tell between code and writing, you would add a little
weird combination of text each time that you wanted to "switch" into code and then
another one each time you wanted to switch back into writing for humans. (These
combinations were so weird because that guaranteed that it was a combination you
would probably never want to type otherwise, so you wouldn't have a lot of
cases of the computer getting confused between whether the combo meant to switch
to code or whether it was just something that came up in the regular writing.)
You'd send the document, code and writing and all, through R once you had it
written up. R would ignore everything that wasn't code. When it got to the code
pieces, it would run them, and if the code created output (like a figure or table),
it would "write" that into the document at that point in the text. Then you'd run
the document through Donald Knuth's typesetting program (or an extension of it),
and the whole document would get typeset into an attractive final product (often
a pdf, although you had some choices on the type of output).

This meant that you got very attractive final documents. It also meant that your
data analysis code was well documented---it was "documented" by the very article
or report you wrote based on it, because the code was embedded right there in the
final product! It also meant that you could save a lot of time if you needed to
go back and change some of your code later (or input a different or modified dataset).
You just had to change that small piece of code or data input, and then essentially
press a button to put everything together again, and the computer would re-write the
whole report for you, with every figure and table updated. It even let you write
small bits of computer code directly into the written text, in case you need to
write something like "this study included 52 subjects", where the "52" came from
you counting up the number of rows in one of your datasets---if you later added three
more subjects and re-ran the analysis with the updated dataset, the report would
automatically change to read "this study included 55 subjects".

Leisch's system is still out there, but another has been adopted much more
widely, building on it. Yihui Xie started work on a program that tweaked and
improved Leisch's Sweave program, creating something called "knitr"
("knit"-"R"---are you noticing a pattern in the names?). Xie's knitr program,
along with its extensions, is now widely used for data analysis projects. What's
more, it's grown to allow for larger or more diverse writing projects---this
book, for example, is written using an extension called "bookdown", and
extensions also exist for create blogs that include executable R code
("blogdown") and websites with documentation for R packages ("packagedown").

So now, let's put these two pieces together. We know that programmers love to
automate small tasks, and we know that there are tools that can be used to
"program" tasks that involve writing and reporting. So what does this mean if
you frequently need to write reports that follow a similar pattern and start
from similar types of data? If you are thinking like a code, it means that
you can move towards automating the writing of those reports.

One of us was once talking to someone who works in a data analysis-heavy field,
and she was talking about how much time she spends copying the figures that her
team creates, based on a similar analysis of new data that's regularly
generated, into PowerPoint presentations. So, for this weeks report, she's
creating a presentation that shows the same analysis she showed last week, just
with newer data. Cutting and pasting is an enormous waste of time---there are
tools to automate this.

### Automating reports

First---think through the types of written reports or presentations you've
created in the past year or two. Are there any that follow a similar pattern?
Any that input the same types of data, but from different experiments, and then
report the same types of statistics or plots for them? Are there Excel
spreadsheets your lab uses that generate specific tables or plots that you often
cut and paste for reports or presentations? Look through your computer file
folders or email attachments if you need to---many of these might be small
regular reports that are so regular that they don't pop right to mind. If you
are creating documents that match any of these conditions, you probably have
something ripe for converting to a reusable, automatable template.

> "Think like Henry Ford; he saw that building cars was a repeatable process and
came up with the moving assembly line method, revolutionizing production. You
may not be building a physical product, but chances are you are producing something.
... Look for the steps that are nearly identical each time, so you can build your
own assembly line." [rose2018dont]

... [Creating a framework for the report]

> "Odds are, if you're doing any kind of programming, especially Web programming,
you've adopted a framework. Whereas an SDK is an expression of a corporate
philosophy, a framework is more like a product pitch. Want to save time? Tired
of writing old code? Curious about the next new thing? You use a graphics
framework to build graphical applications, a Web framework to build Web
applications, a network framework to build network servers. There are
hundreds of frameworks out there; just about every language has one.
A popular Web framework is Django, which is used for coding in Python.
Instagram was bootstrapped on it. When you sit down for the first time
with Django, you run the command 'startproject', and it makes a directory
with some files and configuration inside. This is your project directory. Now
you have access to libraries and services that add to and enhance the standard
library." [ford2015what]

One key advantage of creating a report template is that it optimizes the time of
statistical collaborators. It is reasonable for a scientists with a couple of
courses worth of statistical training to design and choose the statistical tools
for simple and straightforward data analysis. However, especially as the
biological data collected in experiments expands in complexity and size, a
statistician can recommend techniques and approaches to draw more knowledge
from the data and to appropriately handle non-standard features of the data.
There is substantial work involved in the design of any data analysis pipeline
that goes beyond the very basics. It waste time and resources to recreate this
with each new project, time that---in the case of statistical collaborators---could
probably be better spent in extending data analysis goals beyond the simplest
possible analysis to explore new hypotheses or to add exploratory analysis that
could inform the design of future experiments.

> "Workflows effectively capture valuable expertise, as they represent how an
expert has designed computational steps and combined them into an
end-to-end process." [hauder2011making]

When collaborative work between scientists and statisticians can move towards
developing repeatable data analysis scripts and report templates, you will start
to think more about common patterns and common questions that you ask across
many experiments in your research program, rather than focusing on the immediate
needs for a specific project. You can start to think of the data analysis tools
that are general purpose for your research lab, develop those into clean,
well-running scripts or functions, and then start thinking about more
sophisticated questions you want to ask of your data. The statisticians you
collaborate will be able to see patterns across your work and help to develop
global, and perhaps novel, methods to apply within your research program, rather
than piecemeal small solutions to small problems.

> "Although foundational knowledge is taught in major universities and colleges,
advanced data analytics can only be acquired through hands-on practical training.
Only exposure to real-world datasets allows students to learn the importance of
preparing and cleansing the data, designing appropriate features, and
formulating the data mining task so that the data reveals phenomena of interest.
However, the effort required to implement such complex multi-step data analysis
systems and experiment with the tradeoffs of different algorithms and feature
choices is daunting. For most practical domains, it can take weeks to months for
a student to setup the basic infrastructure, and only those who have access to
experts to point them to the right high-level design choices will endeavor on
this type of learning. As a result, acquiring practical data analytics skills
is out of reach for many students and professionals, posing severe limitations
to our ability as a society to take advantage of our vast digital data resources."
[hauder2011making]

> "In practice designing an appropriate end-to-end process to prepare and analyze the
data plays a much more influential role than using a novel classifier or
statistical model." [hauder2011making]

It is neither quick nor simple to design the data analysis plan and framework
for a research experiment. It is not simply naming a statistical test or two.
Instead, the data analyst must start by making sure they understand the data,
how it was measured, how to decipher the format in which it's stored, what
questions the project is hoping to answer, where there might be problems in the
data (and what they would look like), and so on. If a data analyst is helping
with a lot of projects using similar types of data to answer similar questions,
then he or she should, in theory, need less time for these "framework" types of
questions and understanding. However, if data isn't shared in the same format
each time, it will still take overhead to figure out that this is indeed the
same type of data and that code from a previous project can be adapted or
repurposed.

Let's think about one area where you likely repeat very similar steps
frequently---writing up short reports or slide presentations to share your
to-date research results with your research group or colleagues. These
probably often follow a similar structure. For example, they may start with
a section describing the experimental conditions, and then have a slide
showing a table with the raw data (or a simple summary of it, if there's a lot
of data), and then have a figure showing something like the difference in
experimental measurements between to experimental groups.

[Figure: Three simple slides for a research update---experimental conditions,
table of raw data, boxplots with differences between groups.]

> "The cornerstone of using DRY in your work life is the humble template.
Whenever you create something, whether it's an email, a business document,
or an infographic, think if there's something there you could save for
future use. The time spend creating a template will save you exponentially
more time down the road." [rose2018dont]

You could start very simply in turning this into a template. You could start by
creating a PowerPoint document called "lab_report_template.pptx". It could
include three slides, with the titles on each slide of "Experimental
conditions", "Raw data", and "Bacterial burden by group", and maybe with some
template set to provide general formatting that you like (font, background
color, etc.). That's it. When you need to write a new report, you copy this file,
rename the copy, and open it up. Now instead of needing to start from a blank
PowerPoint file, you've shaved off those first few steps of setting up the
pieces of the file you always use.

[Figure: Simplest possible template]

This very simple template won't save you much time---maybe just a minute or so for
each report. However, once you can identify other elements that you commonly use
in that type of report, you can add more and more of these "common elements" to the
template, so that you spend less time repeating yourself with each report. For
example, say that you always report the raw data using the same number of columns
and the same names for those columns. You could add a table to that slide in your
template, with the columns set with appropriate column names. You can always add or
delete rows in the table if you need to in your reports, but now each time you
create a new report, you save yourself the time it takes to create the table
structure and add the column names. Plus, now you've guaranteed that the first
table will use the exact same column names every time you give a report! You'll never
have to worry about someone wondering if you are using a different model animal
because you have a column named "Animal ID" in one report, while your last report
had "Mouse ID", for example. And because you're making a tool that you'll use many
times, it becomes worthwhile to take some time double-checking the clean-up, so
you're more likely to avoid things like typos in the slide titles or in columns names
of tables.

[Figure: Template with a table skeleton added.]

You can do the same thing for written reports or paper manuscripts. For example,
most of the papers you like may have the classic scientific paper sections: "Introduction",
"Data and Methods", "Results", and "Discussion". And then, you probably typically include
a couple of pages at the beginning for the title page and abstract, and then a section
at the end with references and figure captions. Again, you could create a file called
"article_template.docx" with section headings for each of the sections and with space for
the title page, abstract, and references. Presumably, you are always an author on papers you're
writing, so go ahead and add your name, contact information, and affiliation in the right
place on the title page (I bet you have to take the time to do that every time you start
a paper---and if you're like me, you have to look up the fax number for your building
every time you do). You probably need to mention funding sources on the title page for
every paper, too. Do you need to look those grant numbers up every time? Nope! Just
put all your current ones in the title page of your template, and then you can just
delete those that don't apply when you start a new paper.

[Figure: Simple article template]

Again, you can build on this simple template. Look through the "Data and Methods"
section of several of your recent papers. Are there certain elements that you commonly
report there? For example, is there a mouse model you use in most of your experiments,
that you need to describe? Put it in the template. Again, you can always delete or
modify this information if it doesn't apply to a specific paper. But for any information
that you find yourself copying and pasting from one paper draft to another, add it to
your template. It is so much more delightful to start work on a paper by *deleting* the
details that don't apply than by staring down a blank sheet of paper.

[Quote---Taking away everything that isn't the statue.]

> "Most docs you work on will have some sort of repeatable process. For example,
when I sit down to write a blog post, I go through the same repeatable steps when
setting up my file: Title, Subtitle, Focus Keywords, Links to relevant articles /
inspiration, Outline of subheds, Intro / hook, etc. ... Even though it is a
well-worn process, I can save time by creating a writing template with these
sections already pre-set. Not only does this save time, but it also saves mental
energy and helps push me into 'Writing' mode instead of 'Set-up' or 'Research' mode."
[mackay2019dry]

This template idea is so basic, and yet far fewer people use it than would seem
to make sense. Maybe it's because it does require some forward thinking, about
the elements of presentations, reports, and papers that are common across your
body of work, not just the details that are pertinent to a specific project. It
also does require some time investment, but not much more that adding all these
element to a single paper or presentation takes. If you can see the appeal of
having a template for the communication output that you create from your
research, and if you try it an like it, then you are well on your way to having
a programmers mindset. The joy of programming is exactly this kind of joy---a
little thinking and time at the start and you have these little tools that do
some of your work for you over and over again. In fact, a Python programmer has
even written a book whose title captures this intrinsic *esprit*: "[Automating
the Boring Stuff?].

But wait. There's more. Do you always do the same calculations or statistical
tests with the data you're getting in? Or at least often enough that it would
save time to have a template? There is a way to add this into the template that
you create for your presentation, report, or paper.

> "Your templates are living documents. If you notice that you're making the same
change over and over, that means it's time to update the template itself."
[rose2018dont]

Researchers create and use Excel templates for this purpose. The template
may have macros embedded in it to make calculations or create basic graphs.
However, spreadsheets---whether created from templates or not---share
the limitations discussed in an earlier chapter. What's more, they can't
easily be worked into a template that creates a final document to
communicate results, whether that's a slide presentation or a
a written document. Finally, they are in a binary format that can't
clearly be tracked with version control like git.

[R Project templates? Can you create them? Clearly something like that is
going on when you start a new package...]

### Scripts and automated reports as simple pipelines

Scientific workflows or pipelines have become very popular in many biological
research areas. These are meant to meet many of the DRY goals---create a
recipe that can be repeated at different times and by different research groups,
clearly record each step of an analysis, and automate steps or processes that
are repeated across different research projects so they can be completed
more efficiently.

There are very sophisticated tools now available for creating biological data
analysis pipelines and workflows,[leipzig2017review] including tools like Galaxy
and Taverna. Simple code scripts and tools that build on them (like makefiles,
RMarkdown documents, and Jupyter Notebooks), however, can be thought of as the
simpler (and arguably much more customizable) little sibling of these more
sophisticated tools.

> "Scripts, written in Unix shell or other scripting languages such as Perl, can
be seen as the most basic form of pipeline framework." [leipzig2017review]

> "Naive methods such as shell scripts or batch files can be used to describe
scientific workflows." [mishima2011agile]

Flexibility can be incorporated into scripts, and the tools that build directly off
them, through including *variables*, which can be set in different configurations
each time the script is run [leipzig2017review].

More complex pipeline systems do have some advantages (although generalizable tools
that can be applied to scripts are quickly catching up on most of these). For
example, many complex data analysis or processing steps may use open-source
software that is under continuing development. If the creators of that software
modify it between the time that you submit your first version of an article and
the time that you need to submit revisions, and you have updated the version
of the package on your computer, the code may no longer run the same way. The same
thing can happen if someone else tries to run your code---if they are trying to
run it with a more recent version of some of the open-source software used in the
code, they may run into problems.

This problem of changes in *dependencies* of the code (software programs, packages,
or extensions that the code loads as runs as part of its process) is an important
challenge to reproducibility in many areas of science. Pipeline software can improve
on simpler scripts by helping limit dependency problems [by ...]. However,
R extensions are rapidly being developed that also address this issue. For example,
the `packrat` package ...., while [packrat update Nichole was talking about].

> "Dependencies refer to upstream files (or tasks) that downstream transformation
steps require as input. When a dependency is updated, associated downstream files
should be updated as well." [leipzig2017review]

The tools that we've discussed for reproducable and automatable report writing---like
Rmarkdown and Jupyter Notebooks---build off of a tool for coordinating and
conducting a process involving multiple scripts and input files, or a "build tool".
Among computer programmers, perhaps the most popular build tool is called "make".
This tool allows coders to write a "Makefile" that details the order that scripts
should be run in a big process, and what other scripts and inputs they require.
With these files, you can re-run a whole project, and do it in the right order,
and the only steps that will be re-run are those where something will change based
on whatever change you just made to the code or input data.

> "To avoid errors and inefficiencies from repeating commands manually, we recommend
that scientists use a build tool to automate workflows, e.g., specify the ways in
which intermediate data files and final results depend on each other, and on the programs
that create them, so that a single command will regenerate anything that needs to
be regenerated." [wilson2014best]

For example, say that you have a large project that starts by inputing data, cleans
or processes it using a step that takes a lot of time to run, analyzes the simpler
processed data, and then creates some plots and tables based on this analysis. With
a makefile, if you want to change the color of the labels on a plot, you can change that
code and re-run the Makefile, and the computer will re-make the plots, but not re-run
the time-intensive early data processing steps. However, if you update the raw data
for the project and re-run the Makefile, the computer will (correctly) run everything
from the very beginning, since the updated data needs to be reprocessed, all the way
through to creating the final plots and tables.

> "A file containing commands for an interactive system is often called a script, though
there is really no difference between this and a program. When these scripts are
repeatedly used in the same way, or in combination, a workflow management tool can
be used. The most widely used tool for this task is probably Make, although many
alternatives are now available. All of these allow people to express the dependencies
between files, i.e., to say that if A or B has changed, then C needs to be updated
using a specific set of commands. These tools have been successfully adopted for
scientific workflows as well." [wilson2014best]

> "This experience motivated the creation of a way to encapsulate all aspects of
our in silico analyses in a manner that would facilitate independent replication
by another scientist. Computer and computational scientists refer to this goal as
'reproducible research', a coinage attributed to the geophysicist Jon Claerbout in 1990,
who imposed the standard of makefiles for construction of all the filgures and computational
results in papers published by the Stanford Exploration Project. Since that time, other
approaches have been proposed, including the ability to insert active scripts
within a text document and the use of a markup language that can produce
all of the text, figures, code, algorithms, and settings used for the computational
research. Although these approaches may accomplish the goal, they are not practical
for many nonprogramming experimental scientists using other groups' or commercial
software tools today." [mesirov2010accessible]

> "All science campaigns of sufficient complexity consist of numerous interconnected computational
tasks. A workflow in this context is the composition of several such computing
tasks." [deelman2018future]

> "Scientific applications can be very complex as software artifacts. They may contain
a diverse amalgam of legacy codes, compute-intensive parallel codes, data conversion routines, and remote
data extraction and preparation. These individual codes are often stitched
together using scripted languages that specify the data and software to be
executed, and orchestrate the allocation of computing resources and the
movement of data across locations. To manage a particular set of codes, a number
of interdependent scripts may be used." [gil2008data]

[Disadvantages of more complex pipeline tools over starting from scripts]

> "Unlike command line-based pipeline frameworks ... workbenches allow
end-users, typically scientists, to design analyses by linking preconfigured
modular tools together, typically using a drag-and-drop graphical interface.
Because they require exacting specifications of inputs and outputs,
workbenches are intrinsically a subset of configuration-based pipelines."
[leipzip2017review]

> "Magnificent! Wonderful! So, what's the downside? Well, frameworks lock you
into a way of thinking. You can look at a website and, with a trained eye, go,
'Oh, that's a Ruby on Rails site.' Frameworks have an obvious influence on
the kind of work developers can do. Some people feel that frameworks make things
too easy and that they become a crutch. It's pretty easy to code yourself into
a hole, to find yourself trying to force the framework to do something it
doesn't want to do. Django, for example, isn't the right tool for building a giant
chat application, nor would you want to try competing with Google Docs using
a Django backend. You pay a price in speed and control for all that convenience.
The problem is really in knowing how much speed, control, and convenience you need."
[ford2015what]

> "Workbenches and class-based frameworks can be considered heavyweight. There
are costs in terms of flexibility and ease of development associated with making
a pipeline accessible or fast. Integrating new tools into workbenches clearly
increases their audience but, ironically, the developers who are most capable of
developing plug-ins for workbenches are the least likely to use them."
[leipzip2017review]

> "Business workflow management systems emerged in the 1990's and are well accepted
in the business community. Scientific workflows differ from business workflows in that
rather than coordinating activities between individuals and systems, scientific
workflows coordinate data processing activities." [vigder2008supporting]

> "The concept of workflows has traditionally been used in the areas of process
modelling and coordination in industries. Now the concept is being applied to
the computational process including the scientific domain." [mishima2011agile]

> "Although bioinformatics-specific pipelinessuch as bcbio-nextgen and Omics Pipe
offer high performance automated analysis, they are not frameworks in the sense they
are not easily extensible to integrate new user-defined tools." [leipzig2017review]

Writing a script-based pipeline does require that you or someone in your laboratory
group develops some expertise in writing code in a "scripting language" like R or
Python. However, the barriers to entry for these languages continues to come down, and
with tools that leverage the ideas of templating and literate programming, it is
becoming easier and easier for new R or Python users to learn to use them quickly.
For example, one of us teaches a three-credit R Programming class, designed for researchers
who have never coded. The students in the class are regularly creating code projects
by the end of the class that integrate literate programming tools to weave together
code and text and saving these documents within code project directories that include
raw data, processed data, and scripts with code definitions for commonly used pieces of
code (saved as functions). These are all the skills you'd need to craft an R project
template for your research group that can serve as a starting point for each future
experiment or project.

> "Without an easy-to-use graphical editor, developing workflows requires some programming
knowledge." [vigder2008supporting]

> "Scripting languages are programming languages and as a result are inaccessible to
any scientists without computing background. Given that a major aspect of scientific
research is the assembly of scientific processes, the fact that scientists cannot
assemble or modify the applications themselves results in a significant bottleneck."
[gil2008data]

> "As anyone who's ever shared a networked folder---or organized a physical
filing cabinent---knows, without a good shared filing system your office will
implode." [ford2015code]

> "You can tell how well code is organized from across the room. Or by squinting or
zooming out. The shape of code from 20 feet away is incrediably informative. Clean
code is idiomatic, as brief as possible, obvious even if it's not heavily
documented. Colloquial and friendly." [ford2015code]

> "[Wesley Clark] wanted to make the world's first 'personal computer', one that
could fit in a single office or laboratory room. No more waiting in line; one
scientist would have it all to himself (or, more rarely, herself). Clark wanted
specifically to target biologists, since he knew they often needed to crunch
data in the middle of an experiment. At that time, if they were using a huge IBM
machine, they'd need to stop and wait their turn. If they had a personal
computer in their own lab? They could do calculations on the fly, rejiggering
their experiment as they went. It would even have its own keyboard and screen,
so you could program more quickly: no clumsy punch cards or printouts. It would
be a symbiosis of human and machine intelligence. Or, as Wilkes put it, you'd
have 'conversational access' to the LINC: You type some code, you see the result
quickly. Clark knew he and his team could design the hardware. But he needed
Wilkes to help create the computers' operating system that would let the user
control the hardware in real time. And it would have to be simple enough that
biologists could pick it up with a day or two of training." [Coders, p. 32]

> "When they had a rough first prototype [of the LINC] working, Clark tested
it on a real-life problem of biological research. He and his colleague
Charles Molnar dragged a LINC out to the lab of neurologist Arnold Starr, who
had been trying and failing to record the neuroelectric signals cats produce
in their brains when they heard a sound. Starr had put an electrode implant
into a cat's cortex, but he couldn't distinguish the precise neuroelectirc
signal he was looking for. In a few hours, Molnar wrote a program for the
LINC that would play a clicking noise out of a speaker, record precisely
when the electrode fired, and map on the LINC's screen the average response
of the cat to noises. It worked: As data scrolled across the screen, the
scientists 'danced a jig right around the equipment'." [Coders, p. 33]


If you have built a pipeline as an R or Python script, but there is an open source software
tool that you need to use that is written in another language, you can right a "wrapper"
function that calls that software from within the R or Python process. And chances are good,
if that software is a popular tool, that someone else has already written one, so you can
just leverage that code or tool. Open-source scripting languages and R and Python "play
well with others", and can communiciate and run just about anything that you could run at
a command line.

> "Our approach to dealing with software integration is to wrap applications with Python
wrappers." [vigder2008supporting]

The templating process can eventually extend to making small tools as software functions
and extensions. For example, if you regularly create a certain type of graph to show
your results, you could write a small function in R that encapsulates the common code
for creating that. One research group I know of wanted to make sure their figures all
had a similar style (font, color for points, etc.), but didn't like the default values,
and so wrote a small function that applied their style choices to every plot they made.
Once your research group has a collection of these small functions, you can in turn
encapsulate them in a R package (which is really just a collection of R functions, plus
maybe some data and documentation). This package doesn't have to be shared outside your
research group---you can just share it internally, but then everyone can load and use
it in their computational work. With the rise of larger datasets in many fields, and the
accompanying need to do more and more work on the computer to clean, manage, and
analyze the data, more scientists are getting into this mindset that they are not just
the "end users" of software tools, but they can dig in and become artisans of small tools
themselves, building on the larger structure and heavier lifting made available by the
base software package.

> "End-user software engineering refers to research dedicated to improving the
capability of end-users who need to perform programming or engineering tasks. For many,
if not all, of these end-users, the creation and maintenance of software is a
secondary activity performed only in service of their real work. This scenario
applies to many fields include science. However, there is little research specifically
focused on scientists as end-user software engineers." [vigder2008supporting]

John Chambers (one of the creators of R's precursor S, and heavily involved in R
deveopment) defines programming as "a language and environment to turn ideas into new
tools." [Programming with Data, p. 2]

> "Sometimes, it seems that the software we use just sort of sprang into
existance, like grass growing on the lawn. But it didn't. It was created by
someone who wrote out---in code---a long, painstaking set of instructions
telling the computer precisely what to do, step-by-step, to get a job done.
There's a sort of priestly class mystery cultivated around the word *algorithm*,
but all they consist of are instructions: Do this, then do this, then do this.
News Feed [in Facebook] is now an extraordinarily *complicated* algorithm
involving some trained machine learning; but it's ultimately still just a list
of rules." [Coders, p. 10]

> "One of your nonnegotiable rules should be that every person in the lab must
keep a clear and detailed laboratory notebook. The business of the lab is
results and the communication of those results, and the lab notebook is the
all-important documentation of each person's research. There are dozens of
reasons to keep a clear and detailed lab notebook and only one---laziness---for
not. Whether the work is on an esoteric branch of clam biology or is heading
toward a potentially lucrative patent, it makes sense to keep data clear and retrievable for both present and future lab members." [@leips2010helm]

> "Paper lab notebooks are most commonly seen, valued for their versatility,
low expense, and ease of use. The paper notebook type may be determined
by the department or institution. Especially at latge companies, there may
also be a policy that dictates format, daily signatures by supervisors, and
lock-up at night. If there is no requirement, everyone in your lab should
use the same kind of bound lab notebook, as determined by you. It should have
numbered pages, gridlines, and a tough enough binding that it does not fall
apart after a few months of rigorous use on the bench." [@leips2010helm]

> "Electronic lab notebooks (ELNs) may be used to enter, store, and analyze
data. Coupled with a sturdy notebook computer, they can be used at the bench
for notes and alterations to the protocol. Lab members and collaborators can
share data and drawings. Reagents can be organized. Shareware versions are
available as well as many stand-alone programs that can be tweaked for your
needs by the company. A lab that generates high-throughput, automated, or
visual data; collaborates with other labs; or has a high personnel turnover
should consider using an ELN (Phillips 2006)." [@leips2010helm]

> "Laboratory Information Management Systems (LIMSs) are programs that are
coupled to a database as well as to lab equipment and facilitate the entry
and storage of laboratory data. These systems are expensive and are designed
for more large-scale testing and production than for basic research labs. They
can be very useful tools: Some can manage protocols, schedule maintenance of
lab instruments, receive and process data from multiple instruments, track
reagents and samples, print sample labels, do statistical analyses, and be
customized to your needs. But although LIMSs can handle a great deal of
data analysis, they cannot yet substitute for a lab notebook." [@leips2010helm]

> "You should demand a level of care with lab notebooks. Everything in it should
be understandable not only to the owner, but to you." [@leips2010helm]

> "Whether you check notebooks, or have a lab member present to you with
raw data, or stop at everyone's lab bench a few times a week, you must
have a feeling for the quality and results of each person's raw data. It is
very easy to make assumptions on the basis of the polished data you
see at a research meeting, but many a lab member has gone astray with over-
or misinterpreted data. By keeping an eye on the raw data, you can be
ready to comment on the number of repetitions, alternative experiments,
or the implications of a minor result." [@leips2010helm]

> "In general, data reuse is most possible when: 1) data; 2) metadata (information
describing the data); and 3) information about the process of generating those data,
such as code, are all provided." [@goodman2014ten]

> "So far we have used filenames without ever saying what a legal name is, so it's time for a couple
of rules. First, filenames are limited to 14 characters. Second, although you can use almost any
character in a filename, common sense says you should stick to ones that are visible, and that you
should avoid characters that might be used with other meanings. ... To avoid pitfalls, you would
do well to use only letters, numbers, the period and the underscore until you're familiar with the
situation [i.e., characters with pitfalls]. (The period and the underscore are conventionally used
to divide filenames into chunks...) Finally, don't forget that case distinctions matter---junk, Junk,
and JUNK are three different names." [@kernighan1984unix]

> "The [Unix] system distinguishes your file called 'junk' from anyone else's of the same name. The
distinction is made by grouping files into *directories*, rather in the way that books are placed om
shelves in a library, so files in different directories can have the same name without any conflict.
Generally, each user haas a personal or *home directory*, sometimes called login directory, that
contains only the files that belong to him or her. When you log in, you are 'in' your home directory.
You may change the directory you are working in---often called your working or *current directory*---but
your home directory is always the same. Unless you take special action, when you create a new file it is
made in your current directory. Since this is initially your home directory, the file is unrelated
to a file of the same name that might exist in someone else's directory. A directory can contain
other directories as well as ordinary files ... The natural way to picture this organization is as a
tree of directories and files. It is possible to move around within this tree, and to find any file in the system
by starting at the root of the tree and moving along the proper branches. Conversely, you can start where
you are and move toward the root." [@kernighan1984unix]

> "The name '/usr/you/junk' is called the *pathname* of the file. 'Pathname' has an intuitive meaning:
it represents the full name of the path from the root through the tree of directories to a particular
file. It is a universal rule in the Unix system that wherever you can use an ordinary filename, you can
use a pathname." [@kernighan1984unix]

> "If you work regularly with Mary on information in her directory, you can say 'I want to work on Mary's
files instead of my own.' This is done by changing your current directory with the `cd` command...
Now when you use a filename (without the /'s) as an argument to `cat` or `pr`, it refers to the file
in Mary's directory. Changing directories doesn't affect any permissions associated with a file---if you
couldn't access a file from your own directory, changing to another directory won't alter that fact." [@kernighan1984unix]

> "It is usually convenient to arrange your own files so that all the files related to one thing are in a
directory separate from other projects. For example, if you want to write a book, you might want to
keep all the text in a directory called 'book'." [@kernighan1984unix]

> "Suppose you're typing a large document like a book. Logically this divides into many small pieces,
like chapters and perhaps sections. Physically it should be divided too, because it is cumbersome
to edit large files. Thus you should type the document as a number of files. You might have separate
files for each chapter, called 'ch1', 'ch2', etc. ... With a systematic naming convention, you can tell at
a glance where a particular file fits into the whole. What if you want to print the whole book? You could
say `$ pr ch1.1 ch1.2 ch 1.3 ...`, but you would soon get bored typing filenames and start to make mistakes.
This is where filename shorthand comes in. If you say `$ pr ch*` the shell takes the `*` to mean 'any
string of characters,' so ch* is a pattern that matches all filenames in the current directory that
begin with ch. The shell creates the list, in alphabetical order, and passes the list to `pr`. The
`pr` command never sees the `*`; the pattern match that the shell does in the current directory
generates aa list of strings that are passed to `pr`." [@kernighan1984unix]

> "The current directory is an attribute of a process, not a person or a program. ... The notion of a
current directory is certainly a notational convenience, because it can save a lot of typing, but
its real purpose is organizational. Related files belong together in the same directory. '/usr' is
often the top directory of a user file system... '/usr/you' is your login directory, your current
directory when you first log in. ... Whenever you embark on a new project, or whenever you have
a set of related files ... you could create a new directory with `mkdir` and put the files there." [@kernighan1984unix]

> "Despite their fundamental properties inside the kernel, directories sit in the file system as
ordinary files. They can be read as ordinary files. But they can't be created or written as
ordinary files---to preserve its sanity and the users' files, the kernel reserves to itself all
control over the contents of directories." [@kernighan1984unix]

> "A file has several components: a name, contents, and administrative information such as
permissions and modifications times. The administrative information is stored in the inode
(over the years, the hyphen fell out of 'i-node'), along with essential system data such as
how long it is, where on the disc the contents of the file are stored, and so on. ...
It is important to understand inodes, not only to appreciate the options on `ls`, but because
in a strong sense the inodes *are* the files. All the directory hierarchy does is provide
convenient names for files. The system's name for a file is its *i-number*: the number of the
inode holding the file's information. ... It is the i-number that is stored in the first two bytes
of a directory, before the name. ...
The first two bytes in each directory entry are the only connection between the name of a file and its
contents. A filename in a directory is therefore called a *link*, because it links a name in the
directory hierarchy to the inode, and hence to the data. The same i-number can appear in more than
one directory. The `rm` command does not actually remove the inodes; it removes directory entries
or links. Only when the last link to a file disappears does the system remove the inode, and hence
the file itself. If the i-number in a directory entry is zero, it means that the link has been
removed, but not necessarily the contents of the file---there may still be a link somewhere else." [@kernighan1984unix]

### Subsection 2

> "The file system is the part of the operating system that makes physical storage media
like disks, CDs and DVDs, removable memory devices, and other gadgets look like hierarchies
of files and folders. The file system is a great example of the distinction between
logical organization and physical implementation; file systems organize and store
information on many differet kinds of devices, but the operating system presents the
same interface for all of them." [@kernighan2011d]

> " A *folder* contains the names of other folders and files; examining a folder will
reveal more folders and files. (Unix systems traditionally use the word *directory*
instead of folder.) The folders provide the organizational structure, while the files
hold the actual contents of documents, pictures, music, spreadsheets, web pages, and
so on. All the information that you computer holds is stored in the file system and
is accessible through it if you poke around. This includes not only your data, but the
executable forms of programs (a browser, for example), libraries, device drivers, and the
files that make up the operating system itself. ... The file system manages all this
information, making it accessible for reading and writing by applications and the rest of
the operating system. It coordinates accesses so they are performed efficiently and
don't interfere with each other, it keeps track of where data is physically located,
and it ensures that the pieces are kept separate so that parts of your email don't
mysteriously wind up in your spreadsheets or tax returns." [@kernighan2011d]

> "File system services are available through system calls at the lowest level,
usually supplemented by libraries to make common operations easy to program."
[@kernighan2011d]

> "The file system is a wonderful example of how a wide variety of physical
systems can be made to present a uniform logical appearance, a hierarchy of folders
and files." [@kernighan2011d]

> "A folder is a file that contains information about where folders and files are
located. Because information about file contents and organization must be perfectly
accurate and consistent, the file system reserves to itself the right to manage and
maintain the contents of folders. Users and application programs can only change the
folder contents implicitly, by making requests of the file system." [@kernighan2011d]

> "In fact, folders *are* files; there's no difference in how they are stored except
that the file system is totally responsible for folder contents, and application
programs have no direct way to change them. But otherwise, it's just blocks on the disk,
all managed by the same mechanisms." [@kernighan2011d]

> "A folder entry for this [example] file would contain its name, its size of 2,500 bytes,
the date and time it was created or changed, and other miscellaneous facts about it
(permissions, type, etc., depending on the operating system). All of that information
is visible through a program like Explorer or Finder. The folder entry also contains
information about where the file is stored on disk---which of the 100 million blocks
[on the example computer's hard disk] contain its bytes. There are different ways to
manage that location information. The folder entry could contain a list of block numbers;
it could refer to a block that itself contains a list of block numbers; or it could
contain the number of the first block, which in turn gives the second block, and so
on. ... Blocks need not be physically adjacent on disk, and in fact they typically
won't be, at least for large files. A megabyte file will occupy a thousand blocks, and
those are likely to be scattered to some degree. The folders and the block lists are
themselves stored in blocks..." [@kernighan2011d]

> "When a program wants to access an existing file, the file system has to search for
the file starting at the root of the file system hierarchy, looking for each component
of the file path name in the corresponding folder. That is, if the file is
`/Users/bwk/book/book.txt` on a Mac, the file system will search the root of the file
system for `Users`, then search within that folder for `bwk`, then within that folder
for `book`, then within that for `book.txt`. ... This is a divide-and-conquer strategy,
since each component of the path narrows the search to files and folders that lie within
that folder; all others are eliminated. Thus multiple files can have the same name for
some component; the only requirement is that the full path name be unique. In practice,
programs and the operating system keep track of the folder that is currenlty in use
so searches need not start from the root each time, and the system is likely to
cache frequently-used folders to speed up operations." [@kernighan2011d]

> "When quitting R, the option is given to save the 'workspace image'. The workspace
consists of all values that have been created during a session---all of the data values
that have been stored in RAM. The workspace is saved as a file called `.Rdata` and then
R starts up, it checks for such a file in the current working directory and loads it
automatically. This provides a simple way of retaining the results of calculations from
one R session to the next. However, saving the entire R workspace is not the recommended
approach. It is better to save the original data set and R code and re-create results by
running the code again." [@murrell2009introduction]



> "Project directory organization isn't just about being tidy, but is essential to the
way by which tasks are automated across large numbers of files" [@buffalo2015bioinformatics]


> "Naming files and directories on a computer matters more than you may think. In
transitioning from a graphical user interface (GUI) based operating system to
the Unix command line, many folks bring the bad habit of using spaces in
file and directory names. This isn't appropriate in a Unix-based environment, because
spaces are used to separate arguments in commands. ... Although Unix doesn't require
file extensions, including extensions in file names helps indicate the type of each
file. For example, a file named *osativa-genes.fasta* makes it clear that this is
a file of sequences in FASTA format. In contrast, a file named *osativa-genes* could
be a file of gene models, notes on where these *Oryza sativa* genes came from, or
sequence data. When in doubt, explicit is always better than implicit when it comes to
filenames, documentation, and writing code." [@buffalo2015bioinformatics]

> "Scripts and analyses often need to refer to other files (such as data) in your
project hierarchy. This may require referring to parent directories in you directory's
hierarcy ... In these cases, it's important to always use *relative paths* ... rather
than *absolute paths* ... As long as your internal project directory structure remains the
same, these relative paths will always work. In contrast, absolute paths rely on you particular
user account and directory structures details *above* the project directory level
(not good). Using absolute paths leaves your work less portable between collaborators and
decreases reproducibility." [@buffalo2015bioinformatics]

> "*Document the origin of all data in your project directory.* You need to keep track of
where data was downloaded from, who gave it to you, and any other relevant information.
'Data' doesn't just refer to your project's experimental data---it's any data that
programs use to create output. This includes files your collaborators send you from their
separate analyses, gene annotation tracks, reference genomes, and so on. It's critical
to record this important data about you're data, or *metadata*. For example, if you downloaded
a set of genic regions, record the website's URL. This seems like an obvious recommendation,
but ocuntless times I've encountered an analysis step that couldn't be easily reproduced
because someone forgot to record the data's source." [@buffalo2015bioinformatics]

> "*Record data version information.* Many databases have explicit release numbers,
version numbers, or names (e.g., TAIR10 version of genome annotation for *Arabidopsis
thaliana*, or Wormbase release WS231 for *Caenorhabditis elegans*). It's important to
record all version information in your documentation, including minor version numbers."
[@buffalo2015bioinformatics]

> "*Describe how you downloaded the data.* For example, did you use MySQL to download a
set of genes? Or the USCS Genome Browser? THese details can be useful in tracking down
issues like when data is different between collaborators." [@buffalo2015bioinformatics]

> "Bioinformatics projects involve many subprojects and subanalyses. For example, the
quality of raw experimental data should be assessed and poor quality regions removed
before running it through bioinformatics tools like aligners or assemblers. ... Even
before you get to actually analyzing the sequences, your project directory can get
cluttered with intermediate files. Creating directories to logically separate subprojects
(e.g., sequencing data quality improvement, aligning, analyzing alignment results, etc.)
can simplify complex projects and help keep files organized. It also helps reduce the
risk of accidentally clobbering a file with a buggy script, as subdirectories help
isolate mishaps. Breaking a project down into subprojects and keeping these in separate
subdirectories also makes documenting your work easier; each README pertains to the
directory it resides in. Ultimately, you'll arrive at your own project organization
system that works for you; the take-home point is: leverage directories to help stay
organized." [@buffalo2015bioinformatics]


> "Because lots of daily bioinformatics work involves file processing, programmatically
accessing files makes our job easier and eliminates mistakes from mistyping a filename
or forgetting a sample. However, our ability to programmatically access files with
wildcards (or other methods in R or Python) is only possible when our filenames are
consistent. While wildcards are powerful, they're useless if files are inconsistently
named. ... Unfortunately, inconsistent naming is widespread across biology, and is
the source of bioinformaticians everywhere. Collectively, bioinformaticians have
probably wasted thousands of hours fighting others' poor naming schemes of files,
genes, and in code." [@buffalo2015bioinformatics]

> "Another useful trick is to use leading zeros ... when naming files. This is useful
because lexicographically sorting files (as `ls` does) leads to correct ordering. ...
Using leading zeros isn't just useful when naming filenames; this is also the best
way to name genes, transcripts, and so on. Projects like Ensembl use this naming
scheme in naming their genes (e.g., ENSG00000164256)." [@buffalo2015bioinformatics]


> "In order to read or write a file, the first thing we need to be able to do is specify
*which* file we want to work with. Any function that works with a file requires a
precise description of the name of the file and the location of the file. A filename
is just a character value..., but identifying the location of a file can involve a
**path**, which describes a location on a persistent storage medium, such as a hard drive."
[@murrell2009introduction]

> "A regular expression consists of a mixture of **literal** characters, which have their
normal meaning, and **metacharacters**, which have a special meaning. The combination
describes a **pattern** that can be used to find matches amongst text values." [@murrell2009introduction]

> "A regular expression may be as simple as a literal word, such as `cat`, but regular
expressions can also be quite complex and express sophisticated ideas, such as
`[a-z]{3,4}[0-9]{3}`, which describes a pattern consisting of either three or four
lowercase letters followed by any three digits." [@murrell2009introduction]

> "... it's important to mind R's working directory. Scripts should *not* use
`setwd()` to set their working directory, as this is not portable to other
systems (which won't have the same directory structure). For the same reason,
use *relative* paths ... when loading in data, and *not* absolute pathers...
Also, it's a good idea to indicate (either in comments or a README file)
which directory the user should set as their working directory." [@buffalo2015bioinformatics]

> "**Centralize the location of the raw data files and automate the derivation of
intermediate data.** Store the input data on a centralized file server that is
profesionally backed up. Mark the files as read-only. Have a clear and linear
workflow for computing the derived data (e.g., normalized, summarized, transformed,
etc.) from the raw files, and store these in a separate directory. Anticipate that
this workflow will need to be run several times, and version it. Use the
`BiocFileCache` package to mirror these files on your personal computer.
[footnote: A more basic alternative is the rsync utility. A popular solution offered
by some organizations is based on ownCloud. Commercial options are Dropbox,
Google Drive and the like]." [@holmes2018modern]


> "Using an RCS [revision control system] has changed how I work. ... a day's
work is no longer a featureless slog toward the summit, but a sequence of small
steps. What one feature could I add? What one problem could I fix? Once a step is
made and you are sure your code base is in a safe and clean state, commit a revision,
and if your next step turns out disastrously, you can fall back to the revision you
just committed instead of starting from the beginning." [@klemens201421st]

> With version control, "Our filesystem now has a time dimension. We can query the
RCS's repository of file information to see what a file looked like last week and
how it changed from then to now. Even without the other powers, I have found that
this alone makes me a more confident writer." [@klemens201421st]

> "The most rudimentary means of revision control is via `diff` and `patch`, which
are POSIX-standard and therefore most certainly on your system." [@klemens201421st]

> "Git is a C program like any other, and is based on a small set of objects.
The key object is the commit object, which is akin to a unified diff file.
Given a previous commit object and some changes from that baseline, a new commit
object encapsulates the information. It gets some support from the *index*, which
is a list of the changes registered since the last commit object, the primary
use of which will be in generating the next commit object. The commit objects
link together to form a tree much like any other tree. Each commit object will
have (at least) one parent commit object. Stepping up and down the tree is akin to
using `patch` and `patch -R` to step among versions." [@klemens201421st]

> "Having a backup system organized enough that you can delete code with confidence
and recover as needed will already make you a better writer." [@klemens201421st]

> "GitHub issues are a great way to keep track of bugs, tasks, feature requests,
and enhancements. While classical issue trackers are primarily intended to be
used as bug trackers, in contrast, GitHub issue trackers follow a different
philosophy: each tracker has its own section in every repository and can be used
to trace bugs, new ideas, and enhancements by using a powerful tagging system.
The main objective of issues in GitHub is promoting collaboration and providing
context using cross-references. Raising an issue does not require lengthy forms
to be completed. It only requires a title and, preferably, at least a short description.
Issues have very clear formatting and provide space for anyone with a GitHub account
to provide feedback. ... Additional elements of issues are (i) color-coded labels
that help to categorize and filter issues, (ii) milestones, and (iii) one assignee
responsible for working on the issue." [@perez2016ten]

> "As another illustration of issues and their generic and wide application, we
and others used GitHub issues to discuss and comment on changes in manuscripts
and address reviewers' comments." [@perez2016ten]

> "A good approach is to store at least three copies in at least two
geographically distributed locations (e.g., original location such as a desktop
computer, an external hard drive, and one or more remote sites) and to adopt a
regular schedule for duplicating the data (i.e., backup)." [@michener2015ten]

> "One study surveyed neuroscience researchers at a UK institute. "The backup 'rule
of three' states that for a file to be sufficiently backed up it should be kept
in three separate locations using two different types of media with one offsite
backup. A lack of an adequate backup solution could mean permanently lost data,
effort and time. In this research, more than 82% of the respondents seemed to be
unaware of suitable backup procedures to protect their data. Some respondents
kept a single backup of work on external hard disks. Others used the
Universities local networked servers as their means of backup."
[@altarawneh2017pilot]

> "Departmental or institutional servers provide an area to store large files
such as graphics files as well as e-mail and documents. Such systems will
usually have frequent routine backups of all data, often onto optical
disks. They might also encrypt the data, which makes it less able to be
hacked. This is the most dependable form of long-term storage." [@leips2010helm]

> "It's very important to keep a project notebook containing detailed information
about the chronology of your computational work, steps you've taken, information
about why you've made decisions, and of course all pertinent information to
reproduce your work. Some scientists do this in a handwritten notebook, others in
Microsoft Word documents. As with README files, bioinformaticians usually like keeping
project notebooks in simple plain-text because these can be read, searched, and
edited from the command line and across network connections to servers. Plain text
is also a future-proof format: plain-text files written in the 1960s are still
readable today, whereas files from word processors only 10 years old can be
difficult or impossible to open and edit. Additionally, plain text project notebooks can
also be put under version control ... While plain-text is easy to write in your
text editor, it can be inconvenient for collaborators unfamiliar with the command
line to read. A lightweight markup language called *Markdown* is a plain-text format
that is easy to read and painlessly incorporated into typed notes, and can also be
rendered to HTML or PDF." [@buffalo2015bioinformatics]



> "Markdown is just plain-text, which means that it's portable and programs to edit
and read it will exist. Anyone who's written notes or papers in old versions of
word processors is likely familiar with the hassle of trying to share or update
out-of-date proprietary formats. For these reasons, Markdown makes for a simple
and elegant notebook format." [@buffalo2015bioinformatics]

> "Information, whether data or computer code, should be organized in such a way that
there is only one copy of each important unit of information." [@murrell2009introduction]

> "A typical encounter with Bioconductor (Box 1) starts with a specific scientific need, for example, differential analysis of gene expression
from an RNA-seq experiment. The user identifies the appropriate
documented workflow, and because the workflow contains functioning code, the user runs a simple command to install the required
packages and replicate the analysis locally. From there, she proceeds
to adapt the workflow to her particular problem. To this end, additional documentation is available in the form of package vignettes
and manual pages." [@huber2015orchestrating]

> "**Case study: high-throughput sequencing data analysis.** Analysis of
large-scale RNA or DNA sequencing data often begins with aligning reads to a
reference genome, which is followed by interpretation of the alignment patterns.
Alignment is handled by a variety of tools, whose output typically is delivered
as a BAM file. The Bioconductor packages Rsamtools and GenomicAlignments provide
a flexible interface for importing and manipulating the data in a BAM file, for
instance for quality assessment, visualization, event detection and
summarization. The regions of interest in such analyses are genes, transcripts,
enhancers or many other types of sequence intervals that can be identified by
their genomic coordinates. Bioconductor supports representation and analysis of
genomic intervals with a 'Ranges' infrastructure that encompasses data
structures, algorithms and utilities including arithmetic functions, set
operations and summarization (Fig. 1). It consists of several packages
including IRanges, GenomicRanges, GenomicAlignments, GenomicFeatures,
VariantAnnotation and rtracklayer. The packages are frequently updated for
functionality, performance and usability. The Ranges infrastructure was designed
to provide tools that are convenient for end users analyzing data while
retaining flexibility to serve as a foundation for the development of more
complex and specialized software. We have formalized the data structures to the
point that they enable interoperability, but we have also made them adaptable to
specific use cases by allowing additional, less formalized userdefined data
components such as application-defined annotation. Workflows can differ vastly
depending on the specific goals of the investigation, but a common pattern is
reduction of the data to a defined set of ranges in terms of quantitative and
qualitative summaries of the alignments at each of the sites. Examples include
detecting coverage peaks or concentrations in chromatin
immunoprecipitation–sequencing, counting the number of cDNA fragments that match
each transcript or exon (RNA-seq) and calling DNA sequence variants (DNA-seq).
Such summaries can be stored in an instance of the class GenomicRanges."
[@huber2015orchestrating]

> "Visualization is essential to genomic data analysis. We distinguish among
three main scenarios, each having different requirements. The first is rapid
interactive data exploration in 'discovery mode.' The second is the recording,
reporting and discussion of initial results among research collaborators, often
done via web pages with interlinked plots and tool-tips providing interactive
functionality. Scripts are often provided alongside to document what was done.
The third is graphics for scientific publications and presentations that show
essential messages in intuitive and attractive forms. The R environment offers
powerful support for all these flavors of visualization—using either the various
R graphics devices or HTML5-based visualization interfaces that offer more
interactivity---and Bioconductor fully exploits these facilities. Visualization
in practice often requires that users perform computations on the data, for
instance, data transformation and filtering, summarization and dimension
reduction, or fitting of a statistical model. The needed expressivity is not
always easy to achieve in a point-and-click interface but is readily realized in
a high-level programming language. Moreover, many visualizations, such as heat
maps or principal component analysis plots, are linked to mathematical and
statistical models---for which access to a scientific computing library is
needed." [@huber2015orchestrating]

> " It can be surprisingly difficult to retrace the computational steps
performed in a genomics research project. One of the goals of Bioconductor is to
help scientists report their analyses in a way that allows exact recreation by a
third party of all computations that transform the input data into the results,
including figures, tables and numbers. The project’s contributions comprise an
emphasis on literate programming vignettes, the BiocStyle and ReportingTools
packages, the assembly of experiment data and annotation packages, and the
archiving and availability of all previously released packages. ... Full remote
reproducibility remains a challenging problem, in particular for computations
that require large computing resources or access data through infrastructure
that is potentially transient or has restricted access (e.g., the cloud).
Nevertheless, many examples of fully reproducible research reports have been
produced with Bioconductor." [@huber2015orchestrating]

> "Using Bioconductor requires a willingness to modify and eventually compose
scripts in a high-level computer language, to make informed choices between
different algorithms and software packages, and to learn enough R to do the
unavoidable data wrangling and troubleshooting. Alternative and complementary
tools exist; in particular, users may be ready to trade some loss of
flexibility, automation or functionality for simpler interaction with the
software, such as by running single-purpose tools or using a point-and-click
interface. Workflow and data management systems such as Galaxy and Illumina
BaseSpace provide a way to assemble and deploy easy-touse analysis pipelines
from components from different languages and frameworks. The IPython notebook
provides an attractive interactive workbook environment. Although its origins
are with the Python programming language, it now supports many languages,
including R. In practice, many users will find a combination of platforms most
productive for them." [@huber2015orchestrating]

> "A lab manual is perhaps the best way to inform new lab members of the ins
and outs of the lab and to keep all members updated on protocols and
regulations. What could be included in a lab manual? Anything you do not
want to explain over and over, anything that will make the lab more
functional and that can make life easier for yourself and lab members." [@leips2010helm]

> "Most PIs wish the labs were more organized, but it is not a huge priority,
that is, until the first student leaves and no one can find a particular
cell line in the freezer boxes. Resolutions are made, the crisis passes,
and all goes on as before until the next person leaves. Although it is
probably inevitable that there will be some confusion when a long-time
lab member moves on, an organized lab will not be as affected as an unorganized
one." [@leips2010helm]

> "LaTeX gives you output documents that look great and have consistent
cross-references and citations. Much of your output document is created
automatically and much is done behind the scenes. This gives you extra time to
think about the ideas you want to present and how to communicate those ideas in
an effective way. " [@van2012latex]

> "LaTeX provides state-of-the-art typesetting" [@van2012latex]

> "Many conferences and publishers accept LaTeX. In addition they provide
classes and packages that guarantee documents conforming to the required
formatting guidelines." [@van2012latex]

> "LaTeX automatically numbers your chapters, sections, figures, and so on."
[@van2012latex]

> "LaTeX has excellent bibliography support. It supports consistent citations
and an automatically generated bibliography with a consistent look and
feel. The style of citations and the organisation of the bibliography
is configurable." [@van2012latex]

> "LaTeX is *very* stable, free, and available on many platforms."
[@van2012latex]

> "LaTeX was written by Leslie Lamport as an extension of Donald Knuth's
TeX program. It consists of a Turing-complete procedural markup language
and a typesetting processor. The combination of the two lets you control
both the visual presentation *as well as* the content of your documents."
[@van2012latex]

> "Roughly speaking LaTeX is built on top of TeX. This adds extra functionality
to TeX and makes writing your documents much easier." [@van2012latex]

> "To create a perfect output file and have consistent cross-references
and citations, latex also writes information to and reads information from
*auxiliary* files. Auxiliary files contain information about page numbers of
chapters, sections, tables, figures, and so on. Some auxiliary files are
generated by latex itself (e.g., aux files). Others are generated by
external programs such as bibtex, which is a program that generates information
for the bibliography. When an auxiliary file changes then LaTeX may be out
of sync. You should rerun latex when this happens." [@van2012latex]

> "LaTeX is a markup language and document preparation system. It forces you
to focus on the content and *not* on the presentation. In a LaTeX program
you write the content of your document, you use commands to provide markup
and automate tasks, and you import libraries." [@van2012latex]

> "The main purpose of [LaTeX] commands is to provide markup. For example,
to specify the author of the document you write `\author{<author name>}`.
The real strength of LaTeX is that it also is a Turing-complete programming
language, which lets you define your own commands. These commands let you do
real programming and give you ultimate control over the content and the
final visual presentation. You can reuse your commands by putting them in
a library." [@van2012latex]

> "The paragraph is one of the most important basic building blocks of your
document. The paragraph formation rules depend on how latex treats spaces,
empty lines, and comments. Roughly, the rules are as follows. In its default
model, latex treats a sequence of one or more spaces as a single space. The
end of the line is the same as a space. However: An empty line acts
as an end-of-paragraph specifier..." [@van2012latex]

**Including executable code in other languages.**

In your RMarkdown documents, you include executable code in special sections
("chunks") that are separated from the regular text using a special combination
of characters, as described earlier in this module and in the previous module.
By default, in Rmarkdown files the code in these chunks are executed using the R
programming language. However, you can also include executable code in a number
of other programming languages. For example, you could set some code chunks to
run Python, others to run Julia, and still others (e.g., bash) to run a shell script.

This can be very helpful if you have steps in you Python that use code in
different languages. For example, there may be a module in Python that
works well for an early step in your data preprocessing, and then later
steps that are easier with general R functions. This presents no problem in
creating an RMarkdown data pre-processing protocol, as you can include
different steps using different languages.

The program that is used to run the code in a specific chunk is called the
"engine" for that chunk [ref---R Markdown def guide]. You can change the engine
by changing the combination of characters you use to demarcate the start of
executable code. When you are including a chunk of R code, you mark it off
starting with the character combination ```` ```{r} ````. You change this to
give the engine you would like to use---for example, you would include a chunk
of Python code using ```` ```{python} ```` [ref---R Markdown def guide]. When
your RMarkdown document is rendered, your computer will use the specified
software to run each code chunk. Of course, to run that piece of code, your
computer must have that type of software installed and available. For example,
if you include a chunk of code that you'd like to run with a Python engine, you
must have Python on your computer.

While you can use many different software programs as the engine for each code
chunk, there are a few limitations with some programs. For many open-source
software programs, the results from running a chunk of code with that engine
will be available for later code chunks that also use that engine to use as an
input [ref---R Markdown def guide]. This is not the case, however, for most of
the available engines. For example, if you use the SAS software program as the
engine for one of your code chunks, the output from running that code will not
be available to input to later code in the document.

**Caching code results.**

Some code can take a while to run, particularly if it is processing very large
datasets. By default, RMarkdown will re-run all code in the document every time
you render it. This is usually the best set-up, since it allows you to confirm
that the code is all executing as desired each time the code is rendered.
However, if you have steps that take a long time, this can make it so the
RMarkdown document takes a long time to render each time you render it.

To help with this problem, RMarkdown has a system that allows you to *cache*
results from some or all code chunks in the document. This is a really nice
system---it will check the inputs to that part of the code each time the
document is run. If those inputs have changed, it will take the time to re-run
that piece of code, to use the updated inputs. However, if the inputs have not
changed since the last time the document was rendered, then the last results for
that chunk of code will be pulled from memory and used, without re-running the
code in that chunk. This saves time most of the times that you render the
document, while taking the time to re-run the code when necessary, because the
inputs have changed and so the outputs may be different.

There are some downsides to caching. For example, caching can increase the
storage space it takes to save Rmarkdown work, as intermediate results are
saved. However, if some of your code is very time-intensive to run, it may make
sense to look into caching options with Rmarkdown. For more on caching with
Rmarkdown documents, see [this
section](https://bookdown.org/yihui/rmarkdown-cookbook/cache.html) of the *R
Markdown Cookbook* [ref].

**Outputting to other formats.**

You can use RMarkdown to create documents other than traditional reports.
Scientists might find the outputs of presentations and posters particularly
useful.

RMarkdown has allowed a pdf slide output for a long time. This output leverages
the "beamer" format from LaTeX. You can create a series of presentation slides
in RMarkdown, using Markdown to specify formatting, and then the document will
be rendered to pdf slides. These slides can be shown using pdf viewer software,
like Adobe Acrobat, set either to full screen or to the presentation option.
More recently, capability has been added to RMarkdown that allows you to create
PowerPoint slides. Again, you will start from an RMarkdown document, using
Markdown syntax to do things like divide content into separate slides.
Regardless of the output format you choose (pdf slides or PowerPoint), the code
to generate figures and tables in the presentation can be included directly in
the RMarkdown file, so it is re-run with the latest data each time you render
the presentation.

It is also possible to use RMarkdown to create scientific posters, although this
is a bit less common and there are fewer tutorial resources with instructions on
doing this. To find out more about creating scientific posters with Rmarkdown,
you can start by looking at the documentation for some R packages that have been
created for this process. Two include the `posterdown` package [ref], with
documentation available at
https://reposhub.com/python/miscellaneous/brentthorne-posterdown.html, and the
`pagedown` package [ref], with documentation available at
https://github.com/rstudio/pagedown. There are also some blog posts available
where researchers describe how they created a poster with Rmarkdown; one
thorough one is "How to make a poster in R" by Wei Yang Tham, available at
https://wytham.rbind.io/post/making-a-poster-in-r/.

This idea of customizing Rmarkdown documents has evolved in another useful way
through the idea of Rmarkdown templates. These are templates that are
customized---often very highly customized---while allowing you to write the
content using Rmarkdown. One area where these templates can be very useful to
scientists is with article templates that are customized for specific
scientific journals. A number of scientific journals have created LaTeX
templates that can be used when writing drafts to submit to the journal.
These templates produce a draft that is nicely formatted, following all
the journal's guidelines for submission, and in some cases formatted as the
final article would be for the journal. These templates have existed for
a long time, particularly for journals in fields in which LaTeX is commonly
used for document formatting, including physics and statistics. However,
the templates traditionally required you to use LaTeX, which is a complex
markup language with a high threshold for learning to use it.

Now, many of these article templates have been wrapped within an Rmarkdown
template, allowing you to leverage them while writing all the content in
Rmarkdown syntax, and allowing you to include executable code directly in the
draft. An example of the first page of an article created in Rmarkdown using
one of these article templates is shown in Figure \@ref(fig:rticleexample).

These Rmarkdown templates are typically available through R packages, which you
can install on your computer in the same way you would install any R package
(i.e., with the `install.packages` function). Many journal article templates are
available through the `rticles` package [ref], including the template used to
create the manuscript shown in Figure \@ref(fig:rticleexample). You can find
more information about the `rticles` package on its GitHub page, at
https://github.com/rstudio/rticles. There is also a section in the book *R
Markdown: A Definitive Guide* [ref] on writing manuscripts for scientific
journals using Rmarkdown, available online at
https://bookdown.org/yihui/rmarkdown/rticles-templates.html.

As a similar idea, you can created parameterized RMarkdown documents. These are
a simple way to create a kind of template for reports in your laboratory.
You can create these is a similar way to regular RMarkdown documents, but
they include an area where you can change some inputs each time you render
the document. There is a section on parameterized reports in *R
Markdown: A Definitive Guide* [ref].

You can also use RMarkdown to create much larger outputs, compared to simpler reports
and protocols. RMarkdown can now be used to create very large and dynamic
documents, including online books (which can also be rendered to pdf versions
suitable for printing), dashboard-style websites, and blogs. Once members of
your research group are familiar with the basics of RMarkdown, you may want to
explore using it to create these more complex outputs. The book format divides
content into chapters and special sections like appendices and references. It
includes a table of contents based on weblinks, so readers can easily navigate
the content. It uses a book format as its base that allows readers to do things
like change the font size and search the book text for keywords. The book
containing these modules is one example of using bookdown. If you would like to
explore using bookdown to create online books based on Rmarkdown files, there
are a number of resources available. There is an online book available with
extensive instructions on using this package, available at
https://bookdown.org/yihui/bookdown/. There is also a helpful website with more
details on this package, https://bookdown.org/. The website include a gallery of
example books created with bookdown https://bookdown.org/home/archive/, which
you can use to explore the types of books that can be created.

You can also use Rmarkdown documents to create webpages, with pages included for
blogs. This format allows you to create a very attractive website that includes
a blog section, where you can write and regularly post new blogs, keeping the
site dynamic. It is a nice entry point to developing and maintaining a website
for people who are learning to code in R but otherwise haven't done much coding,
as you can do all the steps within RStudio. There are templates for these blogs
that are appropriate for creating personal or research group websites for
academics. These websites can be created to highlight the research and people in
your research lab. You can encourage students and postdocs to create personal
sites, to raise the profile of their research. In the past, we have even used
one as a central, unifying spot for a group study, with students contributing
blog posts as their graded assignment
(https://kind-neumann-789611.netlify.app/). To learn how to create websites with
blogs, you can check the book *blogdown: Creating Websites with R Markdown*
[ref], which is available both in print and free online at
https://bookdown.org/yihui/blogdown/. This process takes a bit of work to
initially get the website set up, but then allows for easy and straightforward
maintenance.

Finally, a simpler way to make basic web content with RMarkdown is through their
*flexdashboard* format. This format creates a smaller website that is focused on
sharing data results---you can see a gallery of examples at
https://rmarkdown.rstudio.com/flexdashboard/examples.html. This format is
excellent for creating a webpage that allows users to view complex, and
potentially interactive, results from data you've collected. It can be
particularly helpful for groups that need to quickly communicate regularly
updated data to viewers. During the COVID-19 pandemic, for example, many public
health departments maintained dashboard-style websites to share evolving data on
COVID-19 in the community. Using RMarkdown in this case has the key advantage of
allowing you to easily update the dashboard webpage as you get new or updated
data, since it is easy to re-run any data processing, analysis, and
visualization code in the document. To learn how to use RMarkdown to create
dashboard websites, you can check out RStudio's flexdashboard site at
https://rmarkdown.rstudio.com/flexdashboard/index.html. There is also guidance
available in one of the chapters of *R Markdown: The Definitive Guide* [ref]:
https://bookdown.org/yihui/rmarkdown/dashboards.html.

**More complex formatting.**

As mentioned earlier, Markdown is a fairly simple markup language. Occasionally,
this simplicity means that you might not be able to create fancier formatting
that you might desire. There is a method that allows you to work around this
constraint in RMarkdown.

In Rmarkdown documents, when you need more complex formatting, you can shift
into a more complex markup language for part of the document. Markup languages
like LaTeX and HTML are much more expressive than Markdown, with many more
formatting choices possible. For example, there is functionality within
LaTeX and HTML to create much more complex tables than in Markdown. However,
there is a downside---when you include formatting specified in these
more complex markup languages, you will limit the output formats that you can
render the document to. For example, if you include LaTeX formatting within
an RMarkdown document, you must output the document to PDF, while if you
include HTML, you must output to an HTML file. Conversely, if you stick with
the simpler formatting available through the Markdown syntax, you can easily
switch the output format for your document among several choices.

The *R Markdown Cookbook* [ref] includes chapters on how to customize Rmarkdown
output through LaTeX
(https://bookdown.org/yihui/rmarkdown-cookbook/latex-output.html) and HTML
(https://bookdown.org/yihui/rmarkdown-cookbook/html-output.html). These
customizations can include creating custom formats for the entire document (for
example, you can customize the appearance of a whole HTML document by
customizing the CSS style file for the document). They can also include
smaller-level customizations, like changing the citation style that is used in
conjunction with a BibTeX file by adding to the preamble for LaTeX output.

One area of customization that is particularly useful and simple to implement is
with customized tables. The Markdown syntax can create very simple tables, but
does not allow the creation of more complex tables. There is an R package called
`kableExtra` [ref] that allows you to create very attractive and complex tables
in RMarkdown documents.

This package leverages more of the power of underlying markup languages, rather
than the simpler Markdown language. If you remember, Markdown is pretty easy to
learn because it has a somewhat limited set of special characters and special
markings that you can use to specify formatting in your output document. This
basic set of functionality is often all you need, but for complex table
formatting, you will need more. There is much more available in the deeper
markup languages that you can use specifically to render pdf documents (software
derived from TeX) and the one that you can use specifically to render HTML (the
HTML markup language). As a result, you will need to create RMarkdown files that
are customized to a single output format (pdf or HTML) to take advantage of this
package.

You can install this package the same as any other R package from CRAN, using
`install.packages`. You will need to use then need to use `library("kableExtra)`
within your RMarkdown document before you use functions from the package. The
`kableExtra` package is extensively documented through two vignettes that come
with package, one if the output will be in pdf
(https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_pdf.pdf)
and one if it will be in HTML
(https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html).
There is also information on using `kableExtra` available through *R Markdown
Cookbook* [ref]: https://bookdown.org/yihui/rmarkdown-cookbook/kableextra.html.

> "WordPerfect was always the best word processor. Because it allowed for insight into
its very structure. You could hit a certain key combination and suddenly the screen
would split and you'd reveal the codes, the bolds and italics, and so forth,
that would define your text when it was printed. It was beloved of legal secretaries
and journalists alike. Because when you work with words, at the practical, everyday
level, the ability to look under the hood is essential. Words are not simple. And
WordPerfect acknowledged that. Microsoft Word did not. Microsoft kept insisting that
what you saw on your screen was the way things *were*, and if your fonts just kept
sort of randonmly changing, well, you must have wanted it that way. Then along came
HTML, and what I remember most was that sense of being back inside the file. Sure,
HTML was a typographic nightmare, a bunch of unjustified Times New Roman in 12 pt on
screens with chiclet-size pixels, but under the hood you could see all the pieces.
Just like WordPerfect. That transparency was a wonderful thing, and it renewed
computing for me." [@ford2015on]

> "TeX was created by Donald E. Knuth, a professor at Stanford University who has
achieved international renown as a mathematician and computer scientist.
Knuth also has an aesthetic sense uncommon in his field, and his work output is
truly phenomenal. TeX is a happy byproduct of Knuth's mammoth enterprise,
*The Art of Computer Programming*. This series of reference books, designed
to cover the whole gamut of programming concepts and techniques, is a
*sine qua non* for all computer scientists." [@seroul2012beginner]

> "Roughly speaking, text processors fall into two categories:
(1) WYSIWYG systems: what you see is what you get. You see on the screen at all
times what the printed document will look like, and what you type has immediate
effect on the appearance of the document. (2)  markup systems, where you type your text
interspersed with formatting instructions, but don't see their effect right away. You must run a program to examine the
resulting image, whether on paper or on the screen. In computer science jargon,
markup systems must compile the source file you type.  WYSIWYG systems have the obvious
advantage of immediate feedback, but they
are not very precise: what is acceptable at a resolution of 300 dots per inch, for an
ephemeral publication such as a newsletter or flier, is no longer so for a book that
will be phototypeset at high resolution. The human eye is extraordinarily sensitive:
you can be bothered by the appearance of a text without being able to pinpoint why,
just as you can tell when someone plays the wrong note in an orchestra, without
being able to identify the CUlprit. One quickly leams in typesetting that the beauty,
legibility and comfortable reading of a text depend on minute details: each element
must be placed exactly right, within thousandths of an inch. For this type of work,
the advantage of immediate feedback vanishes: fine details of spacing, alignment,
and so on are much too small to be discernible at the screen's relatively low
resolution, and even if it such were not the case, it would still be a monumental chore
to find the right place for everything by hand. For this reason it is not surprising that in the world of professional typesetting
markup systems are preferred. They automate the task of finding the right place
for each character with great precision. Naturally, this approach is less attractive for
beginners, since one can't see the results as one types, and must develop a feeling
for what the system will do. But nowadays, you can have the best of both worlds
by using a markup system with a WYSIWYG *front end*; we'll talk about such front
ends for TEX later on. TEX was developed in the late seventies and early eighties,
before WYSIWYG systems were widespread. But were it to be redesigned now, it would
still be a markup
language. To give you an idea of the precision with which TEX operates: the internal
unit it uses for its calculations is about a hundred times smaller than the
wavelength of visible light! (That's right, a hundred times.) In other words, any
round-off error introduced in the calculations is invisible to the naked eye."
[@seroul2012beginner]

> "You should be sure to understand the difference between a text editor and a text
processor. A text processor is a text editor together with formatting software that
allows you to switch fonts, do double columns, indent, and so on. A text editor
puts your text in a file on disk, and displays a portion of it on the screen. It doesn't
format your text at all. We insist on the difference because those accustomed to WYSIWYG systems are
often not aware of it: they only know text processors. Where can you find a text
editor? Just about everywhere. Every text processor includes a text editor which
you can use. But if you use your text processor as a text editor, be sure to save your
file using a 'save ASCII' or 'save text only' option, so that the text processor's own
formatting commands are stripped off. If you give TEX a file created without this
precaution, you'll get garbage, because TEX cannot digest your text processor's
commands." [@seroul2012beginner]

> "TeX enabled authors to encode their precise intent into their manuscripts:
This block of text is a computer program, while this word is a keyword in that
program. The language it used, called TeX markup, formalized the slow,
error-prone communication that is normally carried out with the printer over
repeated galley proofs." [@apte2019lingua]

> "The idea of writing markup inside text wasn’t especially novel; it has been
used from 1970’s runoff (the UNIX family of printer-preparation utilities) to
today’s HTML tags. TeX was new in that it captured key concepts necessary for
realistic typesetting and formalized them." [@apte2019lingua]

> "With these higher-level commands, the free TeX engine, and the LaTeX book,
the use of TeX exploded. The macro file has since evolved and changed names, but
authors still typically run the program called latex or its variants. Hence,
most people who write TeX manuscripts know the program as LaTeX and the commands
they use as LaTeX commands." [@apte2019lingua]

> "The effect of LaTeX on scientific and technical publishing has been profound.
Precise typesetting is critical, particularly for conveying concepts using
chemical and mathematical formulas, algorithms, and similar constructs. The
sheer volume of papers, journals, books, and other publications generated in the
modern world is far beyond the throughput possible via manual typesetting. And
TeX enables automation without losing precision. Thanks to LaTeX, book authors
can generate camera-ready copy on their own. Most academic and journal
publishers accept article manuscripts written in LaTeX, and there’s even an open
archive maintained by Cornell University where authors of papers in physics,
chemistry, and other disciplines can directly submit their LaTeX manuscripts for
open viewing. Over 10,000 manuscripts are submitted to this archive every month
from all over the world." [@apte2019lingua]

> "For many users, a practical difficulty with typesetting using TeX is
preparing the manuscripts. When TeX was first developed, technical authors were
accustomed to using plain-text editors like WordStar, vi, or Emacs with a
computer keyboard. The idea of marking up their text with commands and running
the manuscript through a typesetting engine felt natural to them. Today’s
typesetters, particularly desktop publishers, have a different mental model.
They expect to see the output in graphical form and then to visually make edits
with a mouse and keyboard, as they would in any WYSIWYG program. They might not
be too picky about the quality of the output, but they appreciate design
capabilities, such as the ability to flow text around curved outlines. Many
print products are now produced with tools like Microsoft Word for this very
reason. TeX authors cannot do the same work as easily." [@apte2019lingua]

> "Poor documentation can lead to irreproducibility and serious errors. There's
a vast amount of lurking complexity in bioinformatics work: complex workflows,
multiple files, countless program parameters, and different software versions.
The best way to prevent this complexity from causing problems is to document
everything extensively. Documentation also makes your life easier when you need to
go back and rerun an analysis, write detailed methods about your steps for a
paper, or find the origin of some data in a directory." [@buffalo2015bioinformatics]


> "Scatterplots are useful for visualizing treatment-response comparisons ...,
assocations between variables ..., or paired data (e.g., a disease biomarker in
several patients before and after treatment)." [@holmes2018modern]

> "Sometimes we want to show the relationshiips between more than two variables.
Obvious choices for including additional dimensions are plot symbol shapes and
colors. ... Another way to show additional dimensions of the data is to show
multiple plots that result from repeatedly subsetting (or 'slicing') the data
based on one (or more) of the variables, so that we can visualize each part
separately. This is called faceting and it enables us to visualize data in up to
four or five dimensions. So we can, for instance, investigate whether the
observed patterns among the other variables are the same or different across the
range of the faceting variable." [@holmes2018modern]

> "You can add an enormous amount of information and expressivity by making your
plots interactive. ... The package `ggvis` is an attempt to extend the good
features of `ggplot2` into the realm of interactive graphics. In contrast to
`ggplot2`, which produces graphics into R's traditional graphics devices (PDF,
PNG, etc.), `ggvis` builds upon a JavaScript infrastructure called *Vega*, and
its plots are intended to be viewed in an HTML browser." [@holmes2018modern]

> "Heatmaps are a powerful way of visualizing large, matrix-like datasets and
providing a quick overview of the patterns that might be in the data. There
are a number of heatmap drawing functions in R; one that is convenient and
produces good-looking output is the function `pheatmap` from the eponymous
package." [@holmes2018modern]

> "Plots in which most points are huddled in one area, with much of the available
spaces sparesly populated, are difficult to read. If the histogram of the
marginal distribution of a variable has a sharp peak and then long tails to
one or both sides, transforming the data can be helpful. ... The plots in
this chapter that involve microarray data use the logarithmic transformation
[footnote: 'We used it implicitly, since the data in the `ExpressionSet` object
`x` already came log-transformed']---not only in scatterplots... for the x- and
y-coordinates but also ... for the color scale that represents the
expression fold change. The logarithm transformation is attractive because it has
a definite meaning---a move up or down by the same amount on a log-transformed
scale corresponds to the same multiplicative change on the original scale:
log(ax) = log a + log x. Sometimes, however, the logarithm is not good enough,
for instance when the data include zero or negative values, or when even on the
logarithmic scale the data distribution is highly uneven." [@holmes2018modern]

> "To visualize genomic data, in addition to the general principles we have
discussed in this chapter, there are some specific considerations. The data are
usually associated with genomic coordinates. In fact, genomic coordinates offer
a great organizing principle for the integration of genomic data. ... The main
challenge of genomic data visualization is the size of the genomes. We need
visualization at multiple scales, from whole genome down to the
nucleotide level. It should be easy to zoom in and out, and we may need different
visualization strategies for the different size scales. It can be convenient to
visualize biological molecules (genomes, genes, transcripts, proteins) in a linear
manner, although their embedding in the physical world can matter (a great deal)."
[@holmes2018modern]

> "Visualizing the data, either 'raw' or along the various steps of processing,
summarization, and inference, is one of the most important activities in applied
statistics and, indeed, in science. It sometimes gets short shrift in textbooks
since there is not much deductive theory. However, there are many good (and bad)
practices, and once you pay attention to it, you will quickly see whether a
certain graphic is effective in conveying its message, or what
choices you could make to create powerful and aesthetically attractive data
visualizations." [@holmes2018modern]

### Applied exercise

> "Most scholarly works have citations and a bibliography or reference section. ...
The purpose of the bibliography is to provide details of the works that are
cited in the text. We shall refer to cited works as *references*. ...
The bibliography entries are listed as <citation label> <bibliography
content>. The <citation label> of a reference is also used when the work is
cited in the text. The <bibliography content> lists the relevant information
about the work. ... Even within a single work there may be different styles of
citations. *Parenthetical citations* are usually formed by putting one or
several citation labels inside square brackets or parentheses. However,
there are also other forms of citations that are derived from information in
the citation label. ... The `\bibliographystyle` command tells LaTeX which
style to use for the bibliography [e.g., labels as numbers, labels as
names and years]. The bibliography style called <style>, is defined in the
file <style>.bst. ... for citations you use the `\cite` command. The
argument of the `\cite` command is the logical label of the work you cite."
[@van2012latex]

> "Since bibliographies are important and since it's easy to get them wrong,
some of the work related to the creation of the bibliography has been
automated. This is done by BibTeX. The BibTeX tool requires an external human-readable bibliography database. The database may be viewed as a collection
of records. The record defines the title of the work, the author(s) of the work, the year of publication, and so on. The record also defines the logical
label of the work. This is the label you use when you `\cite` the work.
The advantage of using BibTeX is that you provide the information of the
entries in the bibliography and that BibTeX generates the text for the
bibliography. This guarantees consistency and ease of mind. Furthermore,
the BibTeX database is reusable and you may find BibTeX descriptions of
many scholarly works on the web. ... Generating the bibliography with
BibTex is a multi-stage process, which requires an external program called
bibtex. The bibtex program is to BibTeX what the latex program is to LaTeX."
[@van2012latex]

> "You specify the name of the BibTeX database and the location of the
bibliography with the `\bibliography` command. The argument of the command
is the basename of the BibTeX database. So if you use `\bibliography{<db>}`,
then your database is <db>.bib." [@van2012latex]

> "There are several problems with the basic LaTeX citation mechanism.
The `biblatex` package overcomes many of these. It provides a more flexible
citation mechanism and lets you configure your own citation style."
[@van2012latex]

> "The `biblatex` package distinguishes between *parenthetical* and
*textual* citations." [@van2012latex]

> "This chapter is an introduction to presenting pictures that are stored in
external files. Historically, this was an important mechanism for importing
pictures. ... The `figure` environment is used to present pictures, diagrams,
and graphs. The environment creates a *floating* environment. *Floating*
environments don't allow pagebreaks and they may 'float' to a convenient
location in the output document. This mechanism gives LaTeX more freedom to
choose better page breaks for the remaining text. ... However, it should be
noted that you can also force the typesetting of a floating environment at the
'current' position in the output file. ... LaTeX gives some control over the
placement of floating figures, of floating tables, and other floats. For figures
the placement is controlled with an optional argument of the `figure`
environment. The same mechanism is used for the `table` environment... The
optional argument, which controls the placement, may contain any combination of
the letters `t` [top of the page], `b` [bottom of a page], `p` pseparate page
with no text], `h` [approximately at the current position], and `H` [at the
current position] ... The default value for the optional argument is `tbp`.
LaTeX parses the letters in the optional argument from left to right and puts
the figure at the position corresponding to the first letter for which it thinks
the position is 'reasonable'. Good positions are the top of the page, the bottom
of the page, or a page with floats only, because these positions do not disturb
the running text too much." [@van2012latex]

> "We have already seen how to present information in `tabular` environments.
The `table` environment creates a *floating* table. As with the `figure`
environment, this puts the body of the environment in a numbered table,
which may be put in a different place in the document than where it's
actually defined. The table placement is controlled with an optional argument.
This optional argument works as with the optional argument of the
`figure` environment." [@van2012latex]

> "LaTeX's basic support for mathematics is limited, which is why the
American Mathematical Society (AMS) provides a package called `amsmath`,
which redefines some existing commands and environments and provides
additional commands and environments for mathematical
typesetting." [@van2012latex]

> "LaTeX has three basic modes that determine how it typesets its input.
These modes are: **text mode** In this mode the output does not have mathematical
content and is typeset as text. ... **ordinary math mode** In this mode the
output has mathematical content and is typeset in the running text. Ordinary
math mode is more commonly referred to as inline math mode. **display math
mode** In this mode the output has mathematical content and is typeset in a
display." [@van2012latex]

> "The `$` operator switches from text mode to ordinary math mode and back...
The mathematical expressions in the output are typeset in the running text."
[@van2012latex]

> "The `equation` environment is for typesetting a *single* numbered
displayed equation. It is one of the most commonly used environments for
typesetting display math material." [@van2012latex]

LaTeX math typesetting includes [@van2012latex]:

- subscripts and superscripts
- Greek letters
- text in formulae
- aligned sets of equations
- delimiters (parentheses, brackets)
- fractions
- sums, products, integration, differentiation
- square roots
- relational symbols (less than or equal to, greater than or equal to symbols)
- arrows
- matrices
- accents, hats, dots, overlines

> "This chapter introduces the `beamer` class, which is widely used for
computer presentations. Some people call such presentations *powerpoint
presentations*. The beamer class is seamlessly integrated with the `tikz`
package and lets you present *incremental* presentations, which are
presentations that incrementally add text and graphics to a page of the
presentation." [@van2012latex]

> "In May 1977, Donald Knuth of Stanford University started work on the
text-processing system that is now know as 'TeX and METAFONT. ... In the early
1990s, Donald Knuth officially announced that TeX would not undergo any further
development in the interest of stability. Perhaps unsurprisingly, the 1990s saw
a flowering of experimental projects that extended TeX in various directions;
many of these are coming to fruition in the early 21st century..."
[@mittelbach2004latex]


> The development of TeX from its birth as one of Don's 'personal productivity
tools' (created simply to ensure the rapid completion and typographic quality
of his then-current work on *The Art of Computer Programming*) was largely
influenced and nourished by the American Mathematical Society on behalf of
UL research mathematicians." [@mittelbach2004latex]

> "The most obviously important files in any LaTeX-based documentation project
are the *input source files*. Typically, there will be a master file that uses
other subsidiary files... These files most often have the extension `.tex`...
they are commonly known as 'plain text files' since they can be prepared with a basic text editor. Often, external graphical images are included in the typeset document utilizing the `graphics` interface... LaTeX also needs several files
containing structure and layout definitions: *class* files with the extension
`.cls`; *option* files with the extension `.clo`; *package* files with the
extension `.sty`... Many of these are provided by the base system set-up, but
others may be supplied by individual users." [@mittelbach2004latex]

> "One of the ideas behind LaTeX is the separation between layout and structure
(as far as possible), which allows the user to concentrate on content rather
than having to worry about layout issues." [@mittelbach2004latex]

> "Commands placed between `\documentclass` and `\begin{document}` are in
the so-called *document preamble*. All style parameters must be defined in this
preamble, either in package or class files or directly in the document *before*
the `\begin{document}` command, which sets the values for some of the global
parameters." [@mittelbach2004latex]

> "Basic LaTeX offers excellent mathematical typesetting capabilities
for straightforward documents. However, when complex displayed equations or
more advanced mathematical constructs are heavily used, something more is
needed. Although it is possible to define new commands or environments to
ease the burden of typing in formulas, this is not the best solution.
The American Mathematical Society (AMS) provides a major package, `amsmath`,
which makes the preparation of mathematical documents much less time-consuming
and more consistent." [@mittelbach2004latex]

> "Citations are cross-references to bibliographical information outside
the current document, such as to publications containing further information
on a subject and source information about used quotations. ... There are
numerous ways to compile bibliographies and reference lists. They can be
prepared manually, if necessary, but usually they are automatically
generated from a database containing bibliographical infromation..."
[@mittelbach2004latex]

> "There are four common methods of referring to sources: the 'short-title',
'author-date', 'author-number', and 'number-only' systems." [@mittelbach2004latex]

> "The standard LaTeX environment for generating a list of references or a
bibliographyu is called `thebibliography`. In its default implementation it
automatically generates an appropriate heading and implements a vertical
list structure in which every publication is represented as a separate item."
[@mittelbach2004latex]

> "Inside a document, publications are cited by referring to the *cite-key*
argument of the `\bibitem` commands. For this purpose LaTeX offers the `\cite`
command, which takes such a key as its argument. It can, in fact, take a
comma-separated list of such keys and offers an optional argument to specify
additional information such as page or chapter numbers." [@mittelbach2004latex]

> "The BibTeX program gathers all citation keys used in a document, looks them
up in a bibliographical database, and generates a complete `thebibliography`
environment that can be loaded by LaTeX in a subsequent run." [@mittelbach2004latex]

> "... the basic structure of a BibTeX entry consists of three parts: 1.
A publication *entry type* (e.g., 'book', 'article', 'inproceedings',
'phdthesis'). 2. A *user-chosen keyword* identifying the publication. If you
want to reference the entry in your document, then the argument *cite-key*
of the `\cite` command should be identical (also in case) to this keyword.
3. A *series of fields* consisting of a field identifier with its data between
quotes or curly braces (e.g., 'author', 'journal', and 'title')." [@mittelbach2004latex]

> "BibTeX entries are read by BibTeX in the bibliography database (the `.bib`
file), and the formatting of the entries is controlled by an associated
bibliography style (the `.bst` file), which contains a set of instructions
written in a stack-based language." [@mittelbach2004latex]

> "The BibTeX program was designed by Oren Patashnik to provide a flexible
solution to the problem of automatically generating bibliography lists conforming to different layout styles. It automatically detects the citation requests in
a LaTeX document (by scanning its .aux file or files), selects the needed
bibliographical information from one or more specified databases, and formats
it according to a specified layout style. Its output is a file containing
the bibliography listing as LaTeX code that will be automatically loaded and
used by LaTeX on the following run." [@mittelbach2004latex]

> "A BibTeX database is a plain text (ASCII) file that contains bibliographical
entries internally structured as keyword/value pairs. ... Each entry in a
BibTeX database consists of three main parts: a *type* specifier, followed
by a *key*, and finally the *data* for the entry itself. The *type* describes
the general nature of the entry (e.g., whether it is an article, book, or
some other publication). The *key* is used in the interface to LaTeX; it
is the string that you have to place in the argument of a `\cite` command when
referencing that particular entry. The *data* part consists of a series of
*field entries* depending on the *type*), which can have one of two forms...
The comma is the field separator. Spaces surrounding the equals sign or the
comma are ignored. Inside the text part of a field (enclosed in a pair of
double quotes or a pair of braces) you can have any string of characters,
but braces must be matched. The quotes or braces can be omitted for text
consisting entirely of numbers (like the year field...). ... BibTeX *ignores*
the case of the letters for the entry type, key, and field names. You must,
however, be careful with the key. LaTeX honors the case of the keys specifically
as the argument for a `\cite` command, so the key for a given bibliographic
entry must match the one specified in the LaTeX file." [@mittelbach2004latex]

> "Various organizations and individuals have developed style files for
BibTeX that correspond to the house style of particular journals or editing
houses." [@mittelbach2004latex]

> "The document format 'R Markdown' was first introduced in the `knitr`
package in early 2012. The idea was to embed code chunks (of R or other
languages) in Markdown documents. In fact, `knitr` supported several
authoring languages from the beginning in addition to Markdown, including
LaTeX, HTML, AsciiDoc, reStructuredText, and Textile. Looking back over
the five years, it seems to be fair to say that Markdown has become the
most popular document format, which is what we expected. The simplicity
of Markdown clearly stands out among these document formats." [@xie2018r]

> "However, the original version of Markdown invented by John Gruber was
often found overly simple and not suitable to write highly technical
documents. For example, there was no syntax for tables, footnotes,
math expressions, or citations. Fortunately, John MacFarlane created a
wonderful package named Pandoc to convert Markdown documents (and many
other types of documents) to a large variety of output formats. More
importantly, the Markdown syntax was significantly enriched.
Now we can write more types of elements with Markdown while still enjoying
its simplicity." [@xie2018r]

> "In a nutshell, R Markdown stands on the shoulders of `knitr` and Pandoc.
The former executes the computer code embedded in Markdown, and converts
R Markdown to Markdown. The latter renders Markdown to the output format
you want (such as PDF, HTML, Word, and so on.)." [@xie2018r]

> "R Markdown may not be the right format for you if you find these elements
not enough for your writing: paragraphs, (section) headers, block quotations,
code blocks, (numbered and unnumbered) lists, horizontal rules, tables, inline
formatting (emphasis, strikeout, superscripts, subscripts, verbatim, and
small caps text), LaTeX math expressions, equations, links, images, footnotes,
citations, theorems, proofs, and examples. We believe this list of elements
suffice for most technical and non-technical documents." [@xie2018r]

> "Please do not underestimate the customizability of R Markdown because of the
simplicity of its syntax. In particular, Pandoc templates can be surprisingly
powerful, as long as you understand the underlying technologies such as LaTeX and
CSS, and are willing to invest time in the appearance of your output
documents (reports, books, presentations, and / or websites)." [@xie2018r]

> "R Markdown documents are often portable in the sense that they can be
compiled to multiple types of output formats. Again, this is mainly due to the
simplified syntax of the authoring language, Markdown. The simpler the elements
in your document are, the more likely that the document can be converted to
different formats. Similarly, if you heavily tailor R Markdown to a specific
output format (e.g., LaTeX), you are likely to lose the portability, because not
all features in one format work in another format." [@xie2018r]

> "Last but not least, your computing results will be more likely to be
reproducibly if you use R Markdown (or other `knitr`-based source documents),
compared to the manual cut-and-paste approach. This is because the results
are dynamically generated from computer source code. If anything goes wrong
or needs to be updated, you can simply fix or update the source code, compile
the document again, and the results will automatically be updated. You can
enjoy reproducibility and convenience at the same time." [@xie2018r]

> "There is a fundamental assumption underneath R Markdown that users should
be aware of: we assum it suffices that only a limited number of features are
supported in Markdown. By 'features', we mean the types of elements you can
create with native Markdown. The limitation is a great feature, not a bug."
[@xie2018r]

> "R Markdown provides an authoring framework for data science. You can use a
single R Markdown file to both: save and execute code, and; generate high
quality reports that can be shared with an audience. R Markdown was designed
for easier reproducibility, since both the computer code and narratives
are in the same document, and results are automatically
generated from the source code." [@xie2018r]

> "There are three basic components of an R Markdown document: the
metadata, text, and code. The metadata is written between the pair of
three dashes `---`. The syntax for the metadata is YAML (YAML Ain't Markup
Language), so sometimes it's also called the YAML metadata or the YAML
frontmatter. Before it bites you hard, we want to warn you in advance
that indentation matters in YAML, so do not forget to indent the sub-fields
of a top field properly. ... The body of a document follows the metadata.
The syntax for text (also known as prose or narratives) is Markdown...
There are two types of computer code... a code chunk starts with three
backticks ... and ends with three backticks... An inline R code expressions
starts with ``` `r ``` and ends with a backtick." [@xie2018r]

> "It is fine for humans to err (in computing), as long as the source code
is readily available." [@xie2018r]

> "The idea [of R Markdown] should be simple enough: interweave narratives
with code in a document, knit the document to dynamically generate results
from the code, and you will get a report. This idea was not generated by
R Markdown, but came from an early programming paradigm called 'Literate
Programming' (Knuth, 1984)." [@xie2018r]

> "The text in an R Markdown document is written with the Markdown syntax.
Precisely speaking, it is Pandoc's Mardown. There are many flavors of
Markdown invented by different people, and Pandoc's flavor is the
most comprehensive one to our knowledge." [@xie2018r]

Markdown syntax allows: inline formatting (e.g., italics, subscript,
superscript), hyperlinks, including images from external files, footnotes,
bibliographical referencing, section headers, ... [@xie2018r]

> "There are multiple ways to insert citations [in Markdown], and we
recommend that you use BibTeX databases, because they work better when
the output format is LaTeX / PDF. ... The key idea is that when you have
a BibTeX database (a plain-text file with the conventional filename extension
`.bib`) that contains entries like ... you may add a field named
`bibliography` to the YAML metadata, and set its value to the path of the
BibTeX file. Then in Markdown, you may use `@R-base` ... to reference the
BibTeX entry. Pandoc will automatically generate a list of references in the
end of the document." [@xie2018r]

> "In general, you'd better leave at least one empty line between adjacent
but different elements, e.g., a header and a paragraph. This is to avoid
ambiguity to the Markdown renderer." [@xie2018r]

> "Inline LaTeX equations can be written in a pair of dollar signs using the
LaTeX syntax..." [@xie2018r]

> "There are a lot of things you can do in a code chunk: you can produce
text output, tables, or graphics. You have fine control over all these output
via chunk options, which can be provided inside the curly braces. ... There are
a large number of chunk options in `knitr` document at https://yihui.name/knitr/options. We list a subset of them below [eval, echo, results, collapse, warning, message, error, include, cache, fig.width, fig.height, out.width, out.height, fig.align, dev, fig.cap]." [@xie2018r]

> "Chunk options in `knitr` can be surprisingly powerful. For example,
you can create animations from a series of plots in a code chunk." [@xie2018r]

> "If caching is enabled the same code chunk will not be evaluated the next
time the document is compiled (if the code chunk was not modified), which can
save you time. However, I want to honestly remind you of the two hard problems
in computer science (via Phil Karlton): naming things, and cache invalidation.
Caching can be handy but also tricky sometimes." [@xie2018r]

> "PDF documents are generated thorugh the LaTeX files generated from R
Markdown. A highly surprising fact to LaTeX beginners is that figures float
by default: even if you generate a plot in a code chunk on the first page,
the whole figure environment may float to the next page. This is just how
LaTeX works by default. It has a tendency to float figures to the top or
bottom of pages. Although it can be annoying and distracting, we recommend
that you refrain from playing the 'Whac-A-Mole' game in the beginning of your
writing, i.e., desparately trying to position figures 'correctly' while
they seem to be always dodging you. You may wish to fine-tune the positions
once the content is complete using the `fig.pos` chunk option." [@xie2018r]

> "Formatting tables can be a very complicated task, especially when certain
cells span more than one column or row. It is even more complicated when you
have to consider different output formats. For example, it is difficult to make
a complex table work for both PDF and HTML output." [@xie2018r]

> "A less well-known fact about R Markdown is that many other languages are also
supported, such as Python, Julia, C++, and SQL. The support comes from the
`knitr` package, which has provided a large number of *language engines*.
Language engines are essentially functions registered in the object
`knitr::knit_engine`. ... To use a different language engine, you can change the
language name in the chunk headerf rom `r` to the engine name... For engines
that rely on external interpreters such as `python`, `perl`, and `ruby`, the
default interpreters are obtained from `Sys.which()`, i.e., using the
interpreter found via the environment variable `PATH` of the system. If you want
to use an alternative interpreter, you may specify its path in the chunk option
`engine.path`. ... Most engines will execute each code chunk in a separate new
session (via a `system()` call in R), which means objects created in memory in a
previous code chunk will not be directly available to latter code chunks. ...
Currently, the only expections are `r`, `python`, and `julia`. Only these
engines execute code in the same session throughout the document. To clarify,
all `r` code chunks are executed in the same R session, all `python` code chunks
are executed in the same Python session, and so on, by *the R session and the
Python session are independent.*" [@xie2018r]

> "You can also write Shell scripts in R Markdown, if your system can run them
(the executable `bash` or `sh` should exist). Usually this is not a problem for
Linux or macOS users. It is not impossible for Windows users to run Shell scripts, but you will have to install additional software (such as Cygwin or the Linux Subsystem)." [@xie2018r]

> "R Markdown documents can also generate interactive content. THere are two
types of interactive R Markdown documents: you can use the HTML Widgets
framework, or the Shiny framework (or both). ... The HTML Widgets framework is
implemented in the R package `htmlwidgets`, interfacing JavaScript libraries
that create interactive applications, such as interactive graphics and tables.
... The `shiny` package builds interactive web apps powered by R. ... You may
use Shiny to run any R code that you like in response to user actions. Since web
browsers cannot execute R code, Shiny interactions occur on the server side and
rely on a live R session. By comparison, HTML widgets do not require a live R
session to support them, because the interactivity comes from the client side
(via JavaScript in the web browser)." [@xie2018r]

> "By default, MathJax scripts are included in HTML documents for rendering LaTeX and MathML equations." [@xie2018r]

> "Within R Markdown documents that generate PDF output, you can use raw LaTeX, and even define LaTeX macros. See Pandoc's documentation on the raw_tex extension for details." [@xie2018r]

> "By default, citations are processed through `pandoc-citeproc`, which works for all output formats. For PDF output, sometimes it is better to use LaTeX packages to process citations, such as `natbib` or `biblatex`. To use one of these packages, just set the option `citation_package` [in the YAML]." [@xie2018r]

> "Dashboards are particularly common in business-style reports. THey can
be used to highlight brief and key summaries of a report. THe layout of
a dashboard is often grid-based, with components arranged in boxes of various
sizes." [@xie2018r]

> "Most R Markdown applications are single documents. That is you, have a
single R Markdown source document, and it generates a single output file.
However, it is also possible to work with multiple Rmd documents in a
project, and organize them in a meaningful way (e.g., pages can
reference each other). Currently there are two major ways to build
multiple Rmd documents: blogdown for building websites, and bookdown for
authoring books." [@xie2018r]

> "With blogdown, you can write a blog post or a general page in an Rmd
document, or a plain Markdown document. These source documents will be
built into a static webset, which is essentially a folder containing static
HTML files and associated assets (such as images and CSS files). You
can publish this folder to any web server as a website. Because it is
only a single folder, it can be easy to maintain. ... Because the website
is generated from R Markdown, the content is more likely to be reproducible,
and also easier to maintain (no cut-and-paste results). Using Markdown
means your content could be more protable in the sense that you may convert
your pages to PDF or other formats in the future, and you are not tied to the
default HTML format." [@xie2018r]

> "Academic journals often have strict guidelines on the formatting for
submitted articles. As of today, few journals directly support R
Markdown submissions, but many support the LaTeX format. While you can
convert R Markdown to LaTeX, different journals have different
typesetting requirements and LaTeX styles, and it may be slow and
frustrating for all authors who want to use R Markdown to figure out
the technical details about how to properly convert a paper based on
R Markdown to a LaTeX document that meets the journal requirements. The
rticles package is designed to simplify the creation of documents that
conform to submission standards. A suite of custom R Markdown templates
for popular journals is provided... Understanding of LaTeX is recommended,
but not essential, to use this package. R Markdown templates may sometimes
inevitably contain LaTeX code, but usually we can use the simpler Markdown
and knit syntax to produce elements like figures, tables, and
math equations..." [@xie2018r]

The `rticles` package includes templates for the Elsevier and Springer
family of journals.

> "A century ago society had no way to combat TB, save for limiting its spread
by sequestering affected individuals in sanatoriums. Back then TB, often 
called 'consumption', was widespread even in places that today have a relatively
low incidence of the scourge, such as North America and Europe. Scientists
began to gain on the disease in 1921, when a vaccine made by French immunologists
Albert Calmette and Camille Guerin, both at the Pasteur Institute in Paris, 
first entered into public use. (Initially believed to protect against both 
adult and childhood forms of the disease, the BCG vaccine, as it is known, was
later shown through an extensive series of tests to confer consistent protection
against only severe childhood forms.)" [@barry2009new]

> "Preventing TB infection in the first place is, of course, better than treating 
people after they have become sick. To that end, efforts to create a vaccine that
confers better protection against the disease than does the BCG vaccine are under
way. Some developers are trying to improve the existing vaccine; others are 
attempting to make entirely new ones. But for the moment, the work is mostly 
doomed to trial and error because we do not understand why the current vaccine does 
not work nor how to predict what will work without testing candidates in humans."
[@barry2009new]

> "In other diseases for which vaccines are available, surviving an initial infection
provides immunity to future infection. In TB, however, initial infection does not 
offer any such protection. A vaccine that is based simply on an attenuated version of 
TB therefore will not work." [@barry2009new]

> "About a third of the global population harbors a latent TB infection until 
something---such as stress or another illness---reactivates the bugs, leading both 
the bacteria and the body's own immune response to attack lung tissue, setting 
transmission to other individuals in motion." [@lehrman2013diabolical]

> "Once insdie the body, the TB germ actually does not do very much. It is the 
body's own attempts to rid itself of the infection that causes the most damage. 
For example, the white blood cells of the immune system create the cavities in the
lungs where TB gets walled off." [@lehrman2013diabolical]

One important distinction is between data structures and data types. A 
*data type* refers to the characteristic of the data. Is it a date, for
example, or a number, or a character string? R can do different types of
things with different types of data---for example, R can add together 
two pieces of data that are numbers, while for two pieces of data that are
dates, it can tell which is the later date. For character strings, R can 
look for patterns in the string (for example, does it include any capital 
letters? Does is start with "b"?). All pieces of data are, at the deepest
level, stored as a string of 0s and 1s. By assigning a data type like 
"character string" or "numeric" to each piece of data, R can make more 
sense of each piece of data in terms of what operations are reasonable to 
perform on the data, helping to translate those 0s and 1s into something
more meaningful. 

In R, your data can be stored as different *types* of data: whole numbers can be
stored as an *integer* data type, continuous [?] numbers through a few types of
*floating* data types, character strings as a *character* data type, and logical
data (which can only take the two values of "TRUE" and "FALSE") as a *logical*
data type. More complex data types can be built using these---for example,
there's a special data type for storing dates that's based on a combination of
an [integer?] data type, with added information counting the number of days [?]
from a set starting date (called the [Unix epoch?]), January 1, 1970. (This
set-up for storing dates allows them to be printed to look like dates, rather
than numbers, but at the same time allows them to be manipulated through
operations like finding out which date comes earliest in a set, determining the
number of days between two dates, and so on.) R uses these different data types
for several reasons. First, by using different data types, R can improve its 
efficiency [?] in storing data. Each piece of data must---as you go deep in the
heart of how the computer works---as a series of binary digits (0s and 1s). 
Some types of data can be stored using fewer of these *bits* (*bi*nary dig*its*).
Each measurement of logical data, for example, can be stored in a single bit, 
since it only can take one of two values (0 or 1, for FALSE and TRUE, respectively).
For character strings, these can be divided into each character in the string 
for storage (for example, "cat" can be stored as "c", "a", "t"). There is a set
of characters called the ASCII character set that includes the lowercase and 
uppercase of the letters and punctuation sets that you see on a standard 
US keyboard [?], and if the character strings only use these characters, they 
can be stored in [x] bits per character. For numeric data types, integers can 
typically be stores in [x] bits per number, while continuous [?] numbers, 
stored in single or double floating point notation [?], are stored in [x] 
and [x] bits respectively. When R stores data in specific types, it can be
more memory efficient by packing the types of data that can be stored in less
space (like logical data) into very compact structures.

One of the "building block"
data structures in R is the vector. This data structure is one dimensional and
can only contain data that have the same data type---you can think of this as a
bead string of values, each of the same type. For example, you could have a
vector that gives a series of names of study sites (each a character string), or
a vector that gives the dates of time points in a study (each a date data type),
or a vector that gives the weights of mice in a study (each a numeric data
type). You cannot, however, have a vector that includes some study site names
and then some dates and then some weights, since these should be in different
data types. Further, you can't arrange the data in any structure except a
straight, one-dimensional series if you are using a vector. The dataframe
structure provides a bit more flexibility---you can expand into two dimensions, 
rather than one, and you can have different data types in different columns of
the dataframe (although each column must itself have a single data type). 


To be able to understand some key differences in the Bioconductor approach and the tidyverse approach, you first need to understand how programming uses **data structures** to store data, and that there can be numerous different data structures available within a programming language to handle different types of data. 

When you process data using a programming language, there will be different
structures that you can use to store data as you work with it. You can think of
these data structures as containers where you keep your data in the programming
environment while you work with it, and different structures organize the data
in different ways.

If you've read the earlier modules, you've already seen one example of a data
structure. In other modules, we've discussed the "tidyverse" approach to
processing data in R---this approach emphasizes the *dataframe* as a way to
store data while you're working with it (in other words, a data structure). In
fact, the use of the dataframe as data structure for data storage is one of the
defining features of the "tidyverse" approach. We mentioned in earlier modules
that the tidyverse approach is based on using a common interface, so that you
can mix and match small functions in different ways---the common interface is
the dataframe. The tidyverse approach is built on the use of a common structure
for storing data, the dataframe---almost all functions take data in this
structure and almost all return data in this structure.

Figure \@ref(fig:dfdatastructure) shows an annotated example of a dataframe,
highlighting some of the key elements of its structure. A dataframe stores data
in a two-dimensional structure, combining rows and columns. Each column is
constrained to have data of the same type---in other words, all values in a
column could be numeric (e.g., 1, 4, 10), or all could be character strings
(e.g., "Mouse 1", "Mouse 3"), but the same column cannot combine some values
that are numeric and some that are character strings. Across the dataframe, all
columns must have the same length (i.e., if you printed out the full dataframe,
it would look like a rectangle). All the column values should be lined up, so
that as you are reading across a row, the values in the column cells are from
the same observation or unit.

```{r dfdatastructure, echo = FALSE, out.width = "\\textwidth", fig.cap = "An example of the dataframe data structure. This data structure is the most frequently used data structure within the tidyverse approach, and its use is in fact a defining element of the approach.", fig.fullwidth = TRUE}
knitr::include_graphics("figures/dataframe.png")
```



We just covered two of the most common data structures in R: the vector and 
the dataframe. You will come across a number of other structures as you work 
in R. 

One other very common data structure is the list. This is a very flexible 
data structure, and it allows enormous flexibility in collecting other types
of data structures (including other lists) into a single R object. 

In addition to dataframes, there are a number of other simple, general purpose
data structures that are often used to store data in R, and that you're likely
to come across as you work in R. These include **vectors**, which are used to
store one-dimensional strings of data of a single type (e.g., all numeric, or
all character strings; as a note, you can think of each column in a dataframe as
a vector), **matrices**, which are also used to store data of a single type, but
with a two-dimensional structure, and **arrays**, which, like matrices and
vectors, store data of a single type, but in three dimensions. Another common
general purpose data structure in R is the *list*, which allows you to combine
data stored in any type of structure to create a single R object, giving
enormous flexibility (but minimal set structure from one object to another).
This data structure is the building block for some of the more complex specific
data structures, which we'll cover next. 
The list structure in R has enormous
flexibility in terms of storing lots of data in lots of possible places. This
data can have different types and even different substructures. Some data
structures in R are very constrained in what type of data they can store and
what structure they use to store it. 

There are a number of simple,
general purpose data structures that are often used to store data in R. These
include **vectors**, which are used to store one-dimensional strings of data of
a single type (e.g., all numeric, or all character strings), **matrices**, which
are also used to store data of a single type, but with a two-dimensional
structure, and **dataframes**, which are used to store multiple vectors of the
same length, and so allow for storing measurements of different data types for
multiple observations.

[Figure: examples of these three structures]

Other data structures are more customized for specific types of data. For 
example, there are a number of specialized data structures that are commonly 
used in Bioconductor packages to store specific types of genomic [?] data. 
These structures have been specially designed to meaningfully arrange the 
types of data that are commonly generated in certain types of experiments [?]. 
While the common types of data structures the we mentioned before (vectors, 
dataframes, and lists) are all provided as part of base R, many of these more
specialized data structures are defined in R extensions (packages that you 
install once you've installed base R), and so you cannot access those data
structures until you have installed additional packages. 

In a later module, we will go into more depth about some of these specialized
data structures in R, and how in certain cases they are powerful tools for 
complex analysis or for working with complex data.  

**Link between functions and object classes**

In a language like R, you will find that data structures can be closely tied
to data structures and data types. In many cases, a function is designed to 
only work with data that is of a certain type or that is stored in a certain 
data structure. 

For example, the package named `lubridate` provides helpful functions for 
working with data that represent dates. Most of the functions in this package
will only work on a vector (the data structure) that contains pieces of 
data that are dates (the data type). There are functions in this package 
that can do things like extract the year, month, or day of month from a date, 
but only if you input a vector of data in with the date data type. 

Many other functions require that you input a dataframe. For example, there 
are functions in the `dplyr` package that allow you to extract a subset of 
a dataframe---for example, to extract a smaller dataframe with only certain 
rows or certain columns from the original. These functions require that you 
input data in a dataframe structure. 

For more complex or customized data structures, like the data structures
within the Bioconductor project, there will often be a whole suite of 
customized data structures, and functions associated with those data structures, 
that are used during a pipeline of analyzing data from a certain type of 
experiment. For example, there are data structures, and functions associated
with those structures, that are customized to work with flow cytometry data, 
and other collections of data structures and functions for working with 
metabolomics data, and so on. 

As a note, there are a few functions that are "generics"---they have been coded
in such a way that they will work with a variety of data structures. For 
these functions, they will often output different things depending on the type
of data structure that is input when the function is called. For example, 
the `summary` function is a generic function---you can input objects with a
variety of data structures and the function will work, providing some type
of summary of the object in each case. If you input an object with a vector
data structure, the `summary` function will provide a summary of the contents
of that vector---if the data type is numeric, it will give values like the 
median, the range, and the number of missing values, while if the data type
is logical (TRUE or FALSE for each data piece), it will give a count of the 
number of TRUE and FALSE values in the vector. If you input an object instead
that is a dataframe, the `summary` function will output a summary of each of
the columns in the dataframe. Although there are such generic functions that
work with different data structures, the general rule in R is that a 
function is typically designed to work with a certain data structure (in 
fact, the generic functions are coded to have different functions that work 
with each type of data structure, and so a structure-specific function is 
called "under the hood" once one of the generic functions is called). 

Regardless of the tools that you use to read data from your files into R, there
is one other tools for data input that is very powerful and useful to learn.
Once you've figured out the code to read in data from a single file, R's
programmable structure allows you to expand that code to read in data from many
different files of the same type and structure and combine that data into a
single large object in your R session. This process can often streamline your
data workflow substantially and so is well worth the time to learn.

Doing this combines three steps: 

1. Developing the code to read in and process data from a single file of
that type and structure and encapsulate that code into a function
2. Create an object that lists all the files you'd like to read in (for 
example, all the files in a certain subdirectory)
3. Use functions from the `purrr` package to apply the input function you 
created to all those files and then reformat the input into a structure
that's easy to work with (e.g., a single large dataframe)


------------------------------------------------------------------------

### Example set of projects

As a motivating example, we'll use an example based on a set of real immunology
experiments. This example highlights how a research laboratory will often
conduct a similar type of experiment many times, so it lets us demonstrate how
the design of the project's files within a project directory can be reused
across similar experiments. It will allow us to show you how you can move from
designing a file directory for a single experiment to designing one that can be
used repeatedly, and then how you can take advantage of consistency in the
directory structure across projects to make tools and templates that can be
reused.

> "To say that scientists erred in assuming that the first-line drugs from the 
1950s would be sufficient to combat TB is a profound understatement." [@barry2009new]

> "The current treatment course, which was developed in the 1960s, is a
demanding regimen consisting of four first-line drugs created in the 1950s and
1960s: isoniazid, ethambutol, pyrazinamide, and rifampin. Patients who follow
the regimen as directed take an average of 130 doses of the drugs, ideally under
direct observation by a health care worker. This combination is extremely
effective against active, drug-susceptible TB as long as patients are compliant
and complete the entire six- to nine-month course. Drug-resistant strains
develop when patients do not complete the full protocol, whether because they
start to feel better or because their drug supply is interrupted for some
reason. Inconsistent use of antibiotics gives the bacteria time to evolve into a
drug-resistant form. Once a drug-resistant strain has developed in one person,
that individual can spread the resistant version to others. ... According to the
World Health Organization [as of 2009], nearly 5 percent of the roughly eight
million new TB cases that occur each year involve strains of Mtb that are
resistant to the two most commonly used drugs in the current first-line regimen:
isoniazip and rifampin. Most cases of this so-called multidrug-resistant TB
(MDR-TB) are treatable, but they require therapy for up to two years with
second-line anti-TB drugs that produce severe side effects. Moreover, MDR-TB
treatment can cost up to 1,400 times more than regular treatment. ... Worst of
all, over the past few years health surveys have revealed an even more ominous
threat, that of extensively drug-resistant TB (XDR-TB). This type ... is
resistant to virtually all the highly-effective drugs used in second-line
therapy." [@barry2009new]

The examples for this and the next few modules are based on a collection of
studies that were conducted with similar designs and similar goals---all aimed
to test candidate treatments for tuberculosis. Most studies in this set tested
one or more treatments as well as one or more controls. The controls could
include negative controls, like saline solution, or positive controls, like a
drug already in use to treat the disease, isoniazid. A few of the studies tested
only controls, to help in developing baseline expectations for things like the
bacterial load in different mouse strains used in studies in the set. The
set of studies tested some treatments that were monotherapies (only one drug
given to the animal) as well as some that were combinations of two or three
different drugs. For many of the drugs that were tested, they were tested at
different doses and, in some cases, different methods of delivery or different
mouse models.

Each of the treatments were given to several mice that had been infected with
*Mycobacterium tuberculosis*. During the treatment, the mice were weighed
regularly. This weight measurement helps to determine if a particular treatment
is well-tolerated by the animals---if not, it may show through the treated mice
losing weight during treatment. After a period of
time, the mice were sacrificed and one lobe from their lungs was used to
determine each mouse's bacterial load, through plating the material from the
lobe and counting the colony forming units (CFUs). One aim of the data analysis
is to compare the bacterial load of mice under various treatments to the
bacterial load of mice in the control group.

The full set of studies included 19 different studies. These were conducted at
different times, but the data for all of the studies can be collected using a
common format. In this module, as well as the following two, we'll be exploring
how you can use RStudio's Project functionality to organize data from one or
more studies. We'll particularly focus on how, by using a common format for data
collection, you can create tools that can be used repeatedly for different
experiments to ensure that methods are the same across all studies of a similar
type, as well as to improve the reproducibility of the studies. 

Let's walk
through the types of data that were collected for each study.
First, there was some metadata recorded for each study. Figure
\@ref(fig:metadata) gives an example. This includes information about the strain
of mouse that was used in the study, treatment details (including the method of
giving the drug or drugs, how often they were given each week, and for how many
weeks), how much bacteria the animals were exposed to (measured both in terms of
the inoculum they were given and their bacterial load one day after they were
given that inoculum, which was based on sacrificing one animal the day after
challenging all the animals with the bacteria), and, if the study included a
novel drug as part of the tested treatment, the batch number of that drug.

```{r metadata1, fig.fullwidth = TRUE, echo = FALSE, out.width = "\\textwidth", fig.cap = "Example of recording metadata for a study in the set of example studies for this module."}
knitr::include_graphics("figures/project_metadata.png")
```

Next, the researchers recorded some information about each treatment group
within the experiment. This typically included at least one negative control. In
some cases, there was also a positive control, in which the animals were treated
with a drug that's in standard use against tuberculosis already (e.g.,
isoniazid). Most studies would also test one or more treatments, which could
include monotherapies or combined therapies. Figure \@ref(fig:treatmentdetails)
shows an example of the data that were recorded on each treatment in the study.
These data include the names and doses of up to three drugs in each treatment,
as well as a column where the researcher can provide detailed specifications of
the treatment.

```{r treatmentdetails1, fig.fullwidth = TRUE, echo = FALSE, out.width = "\\textwidth", fig.cap = "Example of recording treatment details for a study in the set of example studies for this module."}
knitr::include_graphics("figures/project_treatment_details.png")
```

Once the animals were challenged with the bacteria, treatment began, and two
main types of data were measured and recorded. First, the mice were weighed once
a week. For convenience, the mice were not weighed
individually. Instead, mice with the same treatment were kept in a single cage,
and the entire cage was weighed, the weight of the cage itself factored out, and
the average weight of mice for that treatment determined by dividing the weight
of all mice in the cage by the number of mice in the cage. These weights were converted to a measure of the percent change in
weight since the start of treatment. If the animals' weights decrease during the
treatment, it is a marker that the treatment is not well-tolerated by the
animals. Figure \@ref(fig:mouseweight) shows an example of how these data were
recorded. All animals within a treatment group were kept in the same cage, and
this cage was measured once a week. By dividing the weight of all animals in the
cage by the number of animals, the researchers could estimate the average weight
of animals in that treatment group, which is recorded as shown in Figure
\@ref(fig:mouseweights).

```{r mouseweight1, fig.fullwidth = TRUE, echo = FALSE, out.width = "\\textwidth", fig.cap = "Example of recording weekly weights of mice in each treatment group for the example set of studies."}
knitr::include_graphics("figures/project_mouse_weights.png")
```

Finally, after the treatment period, the mice were sacrificed and a portion of
each mouse's lung was used to estimate the bacterial load in that mouse. Figure
\@ref(fig:bacterialload) shows an example of how the data on the bacterial load
in each mouse was recorded.

```{r bacterialload1, fig.fullwidth = TRUE, echo = FALSE, out.width = "\\textwidth", fig.cap = "Example of recording the bacterial load in the lungs of each mouse at the end of treatment for the example set of studies."}
knitr::include_graphics("figures/project_bacterial_load.png")
```

As you can see, these data were all recorded using templates that were designed
for the tidy collection of laboratory data (see modules 2.4 and 2.5). These
spreadsheets were used only to record the data, and then processing, analysis,
and visualization were done in a separate file. Specifically, for this set of
studies a preliminary report was designed, with an example shown in Figure [x].
This report uses the first page to provide a nicely format version of the
metadata for the study, including a table with overall details and a table with
details for each specific treatment that was tested. The second page provides a
graph that shows the percent weight change for mice in each treatment group
compared to the weight of that group at the start of treatment. The third page
provides a graph that shows the bacterial loads in each mouse, grouped by
treatment, as well as the results of running a statistical test to test, for
each treatment group, the hypothesis that the mean of a transformed version of
the measure of bacterial load (log-10) for the group was the same as for the
untreated control group.

```{r prelimreport1, fig.fullwidth = TRUE, echo = FALSE, out.width = "\\textwidth", fig.cap = "Example of the preliminary report generated for each study in the set of example studies for this module. The first page includes metadata on the study, as well as details on each treatment that was tested. The second page shows how mouse weights in each treatment group changed over the course of treatment, to help identify if a treatment was well-tolerated. The third page graphs the bacterial load in each mouse, grouped by treatment, and gives the result of a statistical analysis to test which treatment groups had outcomes that were significantly different from the untreated control group."}
knitr::include_graphics("figures/project_prelim_report.png")
```

Let's take a closer look at a few of these elements. For example, Figure
\@ref(fig:studytable) shows the tables from the first page of the report shown
in Figure \@ref(fig:prelimreport). If you look back to the data collection for
this study (e.g., Figures \@ref(fig:metadata) and \@ref(fig:treatmentdetails)),
you can see that all of the information in these tables was pulled from data
recorded at the start of the study.

```{r studytable1, fig.fullwidth = TRUE, echo = FALSE, out.width = "\\textwidth", fig.cap = "Example of one element of the preliminary report generated for each study in the set of example studies for this module. The first page provides tables with metadata about the study and details about each treatment that was tested."}
knitr::include_graphics("figures/project_study_info_table.png")
```

Figure \@ref(fig:mouseweightsplot) shows the second page of the report. This
figure has taken the mouse weights---which were recorded in one of the data
collection templates for the project (Figure \@ref(fig:mouseweights))---and used
them to generate a plot of how average mouse weight in each treatment group
changed over the course of the treatment.

```{r mouseweightsplot1, fig.fullwidth = TRUE, echo = FALSE, out.width = "\\textwidth", fig.cap = "Example of one element of the preliminary report generated for each study in the set of example studies for this module. The second page provides a plot of how the weights of mice in each treatment changed over the course of treatment."}
knitr::include_graphics("figures/project_mouse_weights_graph.png")
```

Figure \@ref(fig:bactcompare) shows the last page of the report. This page
starts with a figure that shows the bacterial load in the lungs of each mouse in
the study at the end of the treatment period. In this figure, the measurement
for each mouse is shown with a point, and these points are grouped by the
treatment group of the mouse. Boxplots are added to show the distribution across
the mice in each group. The color is used to show whether the treatment was a
negative control, a positive control, a monotherapy, or a combined therapy. The
second part of the page gives a table with the results from running a
statistical analysis to compare the bacterial load for mice in each treatment
group to the bacterial load in the mice in the untreated control group. Color is
added to the table to highlight treatments that had a large difference in
bacterial load from the untreated control, as well as treatments for which the
difference from the untreated control was estimated to be statistically
significant. All the data for these results, including the labels for the plot,
are from the data collected in the data collection templates shown earlier.

```{r bactcompare1, fig.fullwidth = TRUE, echo = FALSE, out.width = "\\textwidth", fig.cap = "Example of one element of the preliminary report generated for each study in the set of example studies for this module. The third page provides results on how bacterial load in the lungs compares among treatments at the end of the treatment period."}
knitr::include_graphics("figures/project_bact_compare_plot.png")
```



### Designing the template for the example set of projects

In the example project that we just described, you can see that we ended up 
with a lot of different files. We used separate files for collecting data and
for creating a preliminary report on that data. For collecting the data, we could
either use separate sheets in one spreadsheet file, or entirely separate files, 
or some combination. The end result is that we have several project files, 
in comparison to if we had used a single sheet of a spreadsheet file to
both record and work with the data. 

Figure [x] gives an example of a project directory organization that might make
sense for the example set of studies described at the beginning of this module. ... 
In the report, we'll design the script for the report (the RMarkdown file) so
that it can leverage the order in how we've arranged files in the file system,
since this is enforced by the project directory template and so is the same
across different projects. This will let us repeat and reuse code scripts across
all the projects that use this template. This strategy is used often in handling
complex bioinformatics data [@buffalo2015bioinformatics], but it can also be
leveraged to improve the reproducibility and reliability when only using less
complex data recorded in the laboratory, as with the data shown in the example
for this module.

### Creating and using the project directory template


Figure \@ref(fig:replacingplaceholdermetadata) gives an example of this process. One
of the files that is included in the example template directory shown earlier is
a spreadsheet to record metadata on the experiment. This spreadsheet file has two
sheets, one that records overall metadata on the study (for example, the weeks of 
treatment given and the strain of mouse used) and one that records details on each 
of the treatments that was tested. In the file in the template directory, these
spreadsheet pages include placeholder data. These are formatted in red, so that 
they visually can be identified as placeholders. By including these placeholder data, 
the researcher can see an example of the format that you expect to be used in recording
data in this file. Once the project template is copied, the researcher will replace
these data with the real data, and then change the font color to black to indicate that
the placeholder data have been replaced (Figure \@ref(fig:replacingplaceholdermetadata)).

```{r replacingplaceholdermetadata1, fig.fullwidth = TRUE, echo = FALSE, out.width = "\\textwidth", fig.cap = "The template includes a file with experiment metadata, with a sheet for recording the overall details of the experiment. A user can open this file and replace the placeholder values (in red) with real values for the experiment. By changing the text color to black, the user can have a visual confirmation that the placeholder data have been replaced with real study data."}
knitr::include_graphics("figures/project_replace_placeholder_metadata.png")
```

Another sheet of this spreadsheet allows the researcher to record the details of 
each of the treatments that were tested in the experiment. Again, placeholder data are
included in the template in a red font to help show the researcher how to record the
data, and these are meant to be replaced with real data from the specific experiment
(Figure \@ref(fig:replacingplaceholdertreatment2)). A similar format is used in the 
template file to record data from the experiment, including the weights of each animal
over each week of treatment and the final bacterial load in each animal at the end
of treatment. Again, there are placeholder values in the template file, which the 
researcher will replace with real data after copying the project template for a new
experiment.

```{r replacingplaceholdertreatment3, fig.fullwidth = TRUE, echo = FALSE, out.width = "\\textwidth", fig.cap = "The template includes a file with experiment metadata, with a sheet for recording the details of each treatment. A user can open this file and replace the placeholder values (in red) with real values for the treatments in the experiment. By changing the text color to black, the user can have a visual confirmation that the placeholder data have been replaced with real study data."}
knitr::include_graphics("figures/project_replacing_placeholder_treatment_data.png")
```

The project template also includes a file that provides a template to create a report
based on the data from the experiment. This file is created using the RMarkdown format, 
which combines text with executable code. You can create this template so that it 
inputs the experimental data from the file formats created for the data recording
files in the project template. By doing this, the researcher should be able to "knit" 
this report for a new experiment, and it should recreate the report based on the 
data recorded for that experiment (Figure \@ref(fig:makingareport)). By knitting
this template report, you can create a nicely formatted version of the report for
the experimental data (Figure \@ref(fig:examplereport1)).

```{r makingareport1, fig.fullwidth = TRUE, echo = FALSE, out.width = "\\textwidth", fig.cap = "Example of how a user can create a report from the template. The template includes an example report, which is written using RMarkdown. The user can open this template report file and use the 'Knit' button in RStudio to render the file. As long as the experimental data are recorded using the data template files, the code for this report can process the data to generate a report from the data. The user can also make changes and additions to the template report."}
knitr::include_graphics("figures/project_opening_and_running_report.png")
```

```{r examplereport2, fig.fullwidth = TRUE, echo = FALSE, out.width = "\\textwidth", fig.cap = "Example of the output from 'knitting' a report from the project template"}
knitr::include_graphics("figures/project_example_report_study001.png")
```


---------------------------------------------------------------------------


As a reminder,
this example set of studies covers a group of studies to explore novel
treatments for tuberculosis. Each study investigates how mice that are
challenged with tuberculosis respond to different treatments, both in terms of
how well they handle the treatment (assessed by checking if their weight
decreases notably while on treatment) and also how well the treatment manages to
limit the growth of tuberculosis in the mouse's lungs.

We will walk through the process of creating a project directory template that could
be used to manage and analyze data from any of the specific studies in this set of 
studies. We'll cover two ways that you could do this. The first is simpler---it involves
creating a basic file directory with the desired template files and file directory 
structure and then copying this file directory every time you want to start a new
project for a study in this set of studies. The second way is a bit more complex and 
time-consuming to set up, but has the benefit of providing a very nice interface for 
members of your laboratory group to use when they start a new project. This second way
is to create a full R Project template that can be accessed from RStudio anytime you 
create a new R Project. This type of template may not be worth the extra set-up time
for project types that your research group only uses rarely, but for types of projects
that your group conducts time and time again, it can be a powerful way to enforce a common
project directory structure, and this in turn will allow your group to create reusable
tools that work in coordination with this project structure. 

### Creating and using a basic template

Let's start by looking at the more basic way to create a project template. This involves 
no fancy tools---in fact, it's so straightforward that at first it might seem to simple 
to be useful. For this basic approach, you will create an example file directory that 
includes template files and that captures you desired project directory structure, and
then members of your group will copy and rename that template every time they start a 
new project of that type. 

Figure \@ref(fig:basicprojecttemplateuse) gives a basic walk-through of the
simple steps you'll use to start a new project directory once you've created
this type of template. First, you will find the project directory template in
your computer's file system, copy it to where you'd like to save the files for
the new project, and rename the directory to your new project's name. At this
point, you can use RStudio to make this directory an RStudio Project. Next,
you'll open the data collection template files and replace the placeholder
example data in the template (shown in red font) with the real data from your
study. The placeholder data can help you remember the format you should use to
record the real data. Finally, once you've recorded the data for the study or 
experiment, you can open the example report template file. If you've designed
this report template well, it should run with the new data you've recorded to 
create a report for the experiment. At this stage, you can add to the report 
or customize it for the new project by changing the Rmarkdown file and re-rendering
it to update the report. 

```{r basicprojecttemplateuse3, fig.cap = "Steps in using a basic project directory template that you have created for a type of study or experiment.", fig.fullwidth = TRUE, out.width = "\\textwidth"}
knitr::include_graphics("figures/project_template_basic_use.png")
```

There are a few steps you'll need to take to create this type of basic project directory 
template: 

1. List the data you typically collect or files you create for that type of study or experiment
2. Create template files for any data collection that is typical for that type of study 
or experiment. Use example or placeholder data to create examples of those files.
3. Create a directory structure that divides the types of files into subdirectories of similar 
types.
4. Create one or more templates of report files that access and report on the data in the
project template

In modules [x], we showed how you can create tidy data collection templates to use to 
collect data, and how these can be paired with reproducible reporting tools to separate
the steps of data collection and reporting (modules [x] go into much more depth on these
reproducible reporting tools). Once you have decided on the types of data that you will 
usually collect for the type of study that this template is for, you can use that process
to create tidy data collection templates for each type of data.

In addition to the data that you record in the laboratory by hand, the type of study may 
also typically have data that's generated and recorded by laboratory equipment. For example, 
the type of study may often include data collected from flow cytometry, to measure certain 
cell populations in samples, or from mass spectometry, to measure levels of certain molecules. 
For these data, the recording format will typically be determined by the equipment, and 
so you won't need to create data collection templates for the data. However, you should store 
these data files in your project directory as well, where they are easy to access and integrate
with other data as you analyze the data for the study. 

The recorded data files and the files that come directly from equipment can all be considered
raw data files. In addition, you may typically create some files with pre-processed data. 
For example, if you have sequencing data [?], you may initially get large [what type] files
from the [what type] equipment. You may use a program like [what] to pre-process these files
to [do what]. In addition to saving the raw [what type] data files, you'll also want to 
save the processed data files in your project directory, since these are the files that you'll
analyze and integrate with other data from the project. 

Once you have determined the types of files that you'll normally include in your
project, you can decide how to organize them into subdirectories in a project
file directory. As you do this, it will be helpful to have example or template
files for each file type. For data that you will record yourself, these can be
the templates that you developed to collect the data in a tidy format (modules
[x]), while for data from equipment, these can just be one or more example files
from the equipment that you have collected for a past project. Having these example files
will help you to develop a template project report that can input the type of data that
you typically collect for this type of project. 

For the example set of studies for this module, there are a few types of data
that we plan to typically collect for each study. First, we will be recording
metadata for each experiment. This will include a study ID, as well as details
like the mouse strain that we used in that experiment, the route used to
administer the treatment, the treatments per week and total weeks of treatment,
the inoculum used for the challenge, and so on. Second, we'll be recording some
details about each experimental group that was tested. This includes the drug or
drugs that were tested, doses of each, and some exact details about the
treatment regimen for that group. Both of these types of data can be recorded at
the beginning of the study. Two other types of data will also be recorded, both
of them during the study rather than at the start. The first is weights of the
mice each week. These weights will be recorded for each treatment group each
week of treatment, to help see if there are drugs that are poorly tolerated by
the mice (which can show up through weight decreases in mice in that group). The
second is the bacterial load in the lungs of each mouse at the end of the
treatment period.

To create the project directory template for these studies, then, we'll create
data collection templates for each of these types of data. We'll create a
separate spreadsheet for each type of data, but we can group them into files if
we'd like. In our example, we created two files to store this type of data, one
for the metadata that are recorded at the start of the experiment (overall
experiment details and the details of each tested treatment) and one for the
data that are collected over the course of the experiment (mouse weights and
bacterial loads). Within each file, we've used separate sheets to record the 
different types of data. This allows us to keep similar types of data together
in the same file, while having a tidy collection format for each specific type
of data. [Figure] 

```{r projectdatacollection1, fig.cap = "Data collection templates for the example project directory template. These templates were created in two files, one for metadata, which is saved in the main directory of the project, and one for data collected in the laboratory during the experiment, which is saved in the 'data' subdirectory. Each file is saved as a spreadsheet file, with two sheets in each file to store different types of data.", fig.fullwidth = TRUE, out.width = "\\textwidth"}
knitr::include_graphics("figures/project_data_collection.png")
```

All of these data collection files are designed using the principles of tidy data
collection (modules [x]). This will ensure that it will be easy to read these
recorded data in a programming language like R for analysis and visualization. 
Specifically, when we designed these template files, we thought a lot about 
things like using a two-dimensional structure (one row of header names and then
values within each of the columns), using column names that would be easy for
a programming language to parse (e.g., no special characters or spaces in the
column names), and so on. 

Now that we have these data collection templates for this type of study, we can 
decide how to organize the project directory into subdirectories with the different
types of data. In this case, we'll use a simple structure. We'll save the metadata
file in the top level of the project directory, since it provides metadata on the 
project as a whole. For the data that are collected during the experiment, we'll 
move that data collection file into a subdirectory called "data", with its own 
subdirectory for "recorded_data" (indicating that we recorded it by hand in the 
laboratory). If you had other types of data, you could create other subdirectories
for each type. For example, you could have a "flow_data" subdirectory for data
collected through flow cytometry. 

If you had raw data that required pre-processing, you could create
subdirectories both for the raw data and for the processed data that result from
pre-processing steps. For example, if you had data from [RNA sequencing?], you might
have [initial files] and [processed files], as well as a [bash?] script that you used
to generate the processed files from the initial raw data. You could create a directory 
called "raw_data" to use to store both the initial raw data and the script used to 
process that data, then a "rna_seq_data" subdirectory in the "data" subdirectory to 
store the smaller pre-processed files. 

```{r projecttemplatecomplex2, fig.cap = "Example of a more complex project directory structure that could be created, with directories added to store data collected through flow cytometry and single cell RNA sequencing.", fig.fullwidth = FALSE, out.width = "\\textwidth"}
knitr::include_graphics("figures/project_template_morecomplex.png")
```

With the previous steps, you will have determined the types of files you normally 
have for this type of study, as well as structured the project directory to organize
these files. The next step is to create a template report. You can create this using
tools for reproducible reports---in R, a key tool for this is RMarkdown. Here, we'll 
cover using this tool for creating a report briefly, but there are many more details 
in modules [x]. 

When it comes to project directories, it turns out that you can use the
directory structure in your favor when you create script-based reports, like
RMarkdown reports. There are functions in R, for example, that will allow you to
print all the files in a specified subdirectory. Say that you have several flow
cytometry files in a subdirectory of the "data" subdirectory called "flow_data".
You could use this function in R to create a list of all the files in that
subdirectory, and then you can run other functions to do the same operations on
all of those files.

When you create a project directory template, we recommend that you create a
subdirectory named something like "reports" to use to store any Rmarkdown report
files for the project. This organization will make it clear where you've stored your
reports in the project directory. You'll be able to use file and directory pathnames
to access all the data in the project, so it will be easy to use the study's data
in the report even if they're in separate subdirectories. There's only one tool 
you'll need to do this---you'll need to learn how to use relative pathnames 
within R code to access files in a different part of your project directory. 

A pathname gives the directions to a file that is stored somewhere, for example
on your computer or a server. There are two ways to state a pathname---you can 
state it as either an absolute pathname or a relative pathname. An absolute 
pathname gives the directions to the file from the root directory of the 
computer where it's stored. In other words, it gives the directions starting 
from the very earliest point in the file directory for that computer. A relative
pathname, on the other hand, gives the directions to the file from something 
called the current working directory---that is, the directory that your current
program is currently operating from. 

You have likely already used relative pathnames extensively. If you read in a file
to a statistical program like R or Python, if that file is in the current working 
directory, you only have to give its file name to point the program to the file. 
You may not have realized it, but in this case, you were using the simplest type of 
relative pathname---since the file is already in your working directory, you 
don't need to give directions that move through different directories to get from
your current working directory to the file. 

If you have a file in a subdirectory of the current working directory, you can
use a relative pathname to access it by giving the direction through the
subdirectories to get to the file. For example, if you want to read in a file
named "bacterial_counts.xlsx" that is in a subdirectory of the current working
directory called "data", you could point to that file using the relative
pathname "data/bacterial_counts.xlsx". Instead, if you want to point to a file
with the same name that's in a subdirectory called "reported_data" of the "data"
subdirectory, you'd use the relative pathname
"data/reported_data/bacterial_counts.xlsx". 

You can use relative pathnames to navigate, too, to files that aren't in a 
subdirectory for the current file. To do this, you can use [abbreviations?]
for filenames. One of the most useful is ".", which stands for the parent 
(i.e., one above) the current working directory. For example, if you have 
a project directory, and you've put an RMarkdown file in the "reports" 
subdirectory, but within the code in that file you want to read in data from 
the "data" subdirectory of the same project, you could do so with a relative 
pathname like "./data/bacterial_counts.xlsx". This pathname says to move up 
one directory from the current working directory (in other words, to the 
directory that the current work directory is a subdirectory of) and then
look for a different subdirectory of that parent directory called "data", 
then read a file from that subdirectory. 

This strategy is necessary when you're using RMarkdown for your reports
and have created a project directory template with different subdirectories
for your data versus your reports. In an RMarkdown document, the code will 
run using the directory where the RMarkdown file is stored as the working
directory [correct even with Projects?]. Therefore, if you organize your 
project directory in the type of structure we've recommended, then you'll
need to use this type of relative pathname to access data elsewhere in 
the project directory when you write code for the report. 

We can take a look at how this works in the project directory template we
created for the example set of studies. In module [x] we gave details on 
what is included in the template report for this project directory template. 
Briefly, it is a file that will generate a couple of tables with metadata
on the experiment (overall experiment details as well as details on each 
of the treatments), then a graph showing mouse weights over time, then a 
graph showing the bacterial load at the end of the study in mice grouped 
by treatment, and finally a table giving the results of comparing the 
bacterial load in each treatment group to the bacterial load in the control
group.

We created an Rmarkdown file that does this analysis and visualization and
included it in the project template directory. This means that the report file
will be copied and available each time someone copies the project template
directory at the start of a new project. We wrote the code in a way that will
input data that are stored in the data collection files that also come with the
project directory template. Since we named those files in the directory
template, we can refer to them with the same name in the code for the report. We
wrote the code in the report in a way that it will still run if there are more
or fewer observations in any of the data collection files, so the report
template has some flexibility in terms of how each study in the set of studies
might vary. For example, in the example set of studies, some of the experiments
were run using only a control group of mice, while others were run to test as
many as [x] different treatment groups. The report template can accommodate
these differences across studies in the set of studies.

...

In many cases, you may have a more complex design for your project directory. For
example, if you were collecting flow cytometry data for the project as well, then 
you would want a subdirectory in the project that is specifically designed to 
store files from the flow cytometry component of the experiment. This subdirectory
would likely include several files, rather than just one. Further, you would not
know ahead of time what the name of these files would be (as you do with the data
collection template files that are included in the template directory). However, 
you can still easily write code for a template report file that will work with 
multiple files of a similar type, even if you don't know what the names will be, 
as long as you know what the name of their subdirectory will be. There are functions
in R like `list.files` that can be used to list all the file names for the files 
in a given directory. You can use this function to create a vector of all the file
names. For example, you could run: 

```{r eval = FALSE}
flow_filepaths <- list.files("../data/flow_data", full.names = TRUE)
```

to get an object (`flow_filepaths`) that lists each of the filepaths for the files
stored in the "flow_data" subdirectory within the "data" subdirectory of the 
project. You could then "map" a function or group of functions across these
files to read them in, process them, and join them into a single dataframe in R. 
By using this process, you can write template code in the report for the project
that should work in most cases for the data that you collect for a given type of 
study. 

The report template is included in the project directory template, so it will be
copied and available for you to use anytime you start a new project using that 
template. However, you are not obligated to keep the report identical to the template. 
Instead, the template report serves as a starting point, and you can add to it or
adapt it as you work on a study. 

For example, in our example template report, we've included the results of
applying a statistical test that compares each treatment group to the control
group. Instead, for a specific study of this type, you may want to control
treatments against each other. For example, some of the studies in this example
set of studies include a positive control group, where the mice are treated with
a drug that is already in common use for the disease. In some cases, the
researcher may want to control the bacterial load in groups treated with a novel
drug to groups treated with the positive control. You could easy add to the
report template in that case, adding statistical tests where you compare
different treatment groups to each other.

### Creating an R Project template

In the previous section, we covered a very basic way to make a project directory 
template: you make an example directory of project files, and then anytime you 
start a project for that type of study, you copy, rename, and use that example 
directory as a template. 

There is a second way to make project templates for your research group. This
method requires a lot more work at the beginning. However, it makes it very easy
for anyone in your group to use the template once it's been created. This method
is to create an R Studio Project template. To be clear, with the basic method
covered in the last chapter, you can create a project directory based on you
basic template and then convert it to an RStudio Project within RStudio.
However, with this second method, you will gain the option to create a new
project based on your template from within RStudio, and many of the details of
creating the template are encapsulated within the process, so you're more likely
to be able to enforce a common project directory structure across projects. 

This method does require a good bit of familiarity with R programming, as it
requires you to create a new R package. In this section, we'll go over the
process, using the example set of studies for this module as a motivating
example. ...

...

There are several steps that you'll take to create the RStudio Project
template: 

1. Decide on project directory structure
2. Create templates for recording data
3. Create a template for a report for the project
4. Create an R package to implement the Project template
5. Move the templates for data recording and the report into the "inst" 
directory of the Project package 
6. Write code in the package to create project structure and copy in templates
7. Customize the Project Wizard for the Project in the package code
8. Build the Project template package and share it with your research group

Several of these are the same as the steps to create a basic project directory
template. Specifically, steps 1--3 are the same steps that you would take to 
create a basic project directory template. 

The novel steps for this process come from step 4 and later. Rather than
creating a file directory that your research group will copy and rename, we'll
put all the elements of the project template within an R package that you can
build and then share with your research group. They will be able to install this
package, and then whenever they start a project, they will be able to initialize
it as this special type of project from within RStudio, as described in the
previous module.

Under this method, the template is put together and shared as an R package. An R
package is a collection of R code and data that extend the functionality of base
R. If you have coded with R, you've likely used these types of packages to get
access to useful functions. For example, the "tidyverse" (module [x]) is a suite
of popular R packages with functions for working with data. Anyone can create
and share an R package, either broadly and publicly by submitting it to a
repository like Bioconductor (in which case it will need to follow some
additional constraints and pass some checks) or privately as a compressed file
that anyone you share it with can download to their computer and install.

The new steps in this process start with step 4, which is to create an R package
to implement the Project template. You can create the framework for an R package
very easily in R study. Open RStudio, go to "File", and select "New Project", as
you would if you were creating any type of RStudio Project. Choose "New Directory", 
but then instead of choosing the generic "New Project", choose "R Package". This 
will create an open a new Project directory for you package, one that includes
templates for many of the files that you need to start a new R package. (As a note, 
this is using a special R project template, just like you'll be creating!)

[More on the initial package directory]

...

For most R packages, you create them to use to share new R functions. 
Therefore, the package directory is focused on new R scripts. However, 
the R package structure is very flexible, and it can be effectively used
to share other things as well. For example, R packages can be created
to share datasets, with very few or no new functions included. Similarly, 
the R package that we'll make with your template will allow you to share
a directory structure and some "starter" files to go into that structure, 
rather than R code. 

Since we're using the R package structure for this alternative use, it may
at first seem a bit confusing why we need to put certain files in certain
places or write code that moves files around. Briefly, R packages are 
created and shared essentially as file directories, but these file 
directories must follow a very specific structure, with specific names
for different subdirectories and files within the structure. We'll need
to follow these rules as we organize the template files within our R package
for our Project template. We will explain as we move through this section 
where you should save everything in your R package directory, but this is 
just to clarify that there is a reason that the structure might seem a 
bit convoluted---we're leveraging a structure normally meant for a bit of 
a different use. 

You can make as many template files as you'd like to include in your Project
template. When you make the R project with your template, you'll store these all
in a special directory that you should call "inst". In an R package, the "inst"
directory can act as a bit of a catch-all, where you can store files that you
would like to be included when someone installs your package, but that don't 
naturally fit in another part of the package directory. 

Within this "inst" subdirectory, you can create as many subdirectories as you'd
like, to help you organize your template files. The only rule is that there are
a handful of names that you should avoid for these subdirectory names. This is
because, when the package is installed, all the subdirectories in this "inst"
subdirectory will be moved up to the main package directory, and so you should
avoid directory names that have a specific meaning in an R package. Namely, you
should avoid naming any subdirectories in the "inst" subdirectory any of the
following: "data", "R", "src", "man", "demo", "tests", "exec", "po", "tools",
"vignettes" [?], "build" [?], "share" [?], "licenses" [?].

In the template for the example set of studies for this module, for example, we
created three subdirectories in the "inst" subdirectory to use to store
templates for this type of Project: "data_collection_templates", where we stored
an Excel file with the template for collecting data during each experiment
(i.e., the mice weights and final bacterial loads), "metadata_templates", which
includes an Excel file with the template for collecting metadata on the
experiment on a whole and on each of the tested treatments, and
"report_templates", where we stored an RMarkdown file with the template report
for the project. For any templates that include code, keep in mind that you'll
need to be careful in setting the pathnames to access any data in the project
directory. [More on this?]

Once you've added any templates to the "inst" directory, the next step is to 
write code in the template package. For this type of package, which aims to 
set up a new Project structure, you'll only include R code for one purpose---the
R code will run when someone first opens a new Project of this type, and it 
will operate to set up the initial project structure. R can be used to do 
many different things---while you may have mostly used it before to read in and
analyze data, it can also be used to do things like make new directories on 
you computer, or to copy files from one place in your computer's file directory 
to another. We'll be using these types of commands in this template R package, 
as it will allow us to move the template files that you created into the 
right place in a user's new project directory when they chose to use your 
template to make a new project. 

There are four key R functions that we'll be using to do these tasks. The first
is the `dir.create` function. This can be used to create new directories on
the user's computer. You can include the name that you'd like to use as the 
new subdirectory. We'll use this function to create the subdirectories in the 
new project, and so this function will let us create a structure within the
user's subdirectory.

The second key R function that we'll use is `file.copy`. This function allows
us to copy a file from one place in the user's computer to another spot. When 
the user installs our package, all of the template files will be included with
that installation. With `file.copy`, we'll be able to copy these template files
into the right spot on each user's computer each time they open a new Project
with this template. 

The third key R function also helps with this process of copying the template
files. In order to copy the files, we need to be able to find where they're 
stored on the user's computer. Since they've been installed with the package, 
the template files will be located on the user's computer based on where that
user stores their installed R packages. This location can differ by user, 
but fortunately there's a function called `system.file` that lets us figure out
the file path for any file that's stored in an R package that the user has
installed, on that user's computer. This function will therefore allow us to 
get the original filepath for each of the template files, so we can copy them
into the new Project. 

Finally, the fourth key R function is `file.path`. This function can create a
file path, and it's helpful because it can allow you to write code that will
generate the full path to a file or subdirectory on the user's computer, based
on the relationship of that file or subdirectory to the working directory of the
project they've created. We'll use this function, for example, to create the
file path for the new subdirectories that should go in the Project when the user
opens it. Even though each user could be storing their project in a different
part of their computer's file directory, this function can determine the path we
should use for each subcomponent in the project.

[How to create directories and move in the template files]

Using these functions, we'll create the subdirectory structure of the project
and then move any template files into the right place in that structure. This 
will happen in an R function---in many cases, this might be the only R function
that you'll include in the package that defines the Project template. You
can name this R function anything you'd like (later, we'll show how you can 
connect this function so that it's automatically run when someone makes their
new Project with the template). You should save the code for the function in the
R subdirectory of your package directory. 

In the example for this module, we've named the function `create_project` and 
stored the code for it in a file called "create_project.R" in the R 
subdirectory of our package. This function should take at least two inputs: 
`path`, which will be the filepath to the project's directory, and `...`
(which we'll call "dots"), which allows other arguments to pass into the 
function. 

Within the function, we've included code that will create all the subdirectories
for this type of project, as well as code that moves the template files into the
right place in those subdirectories. First, ...

[How to customize the Project Wizard for the project]

As you work on the package, you can build it and test it on your own computer. 
If you are working in RStudio, you can use a "Build" Pane that is available
when you're working on a R Project that is a package. In this pane, there
is a "Install and Reload" button [doublecheck]---if you click this button, 
it will build the package and install it on your computer in the same 
place that other R packages are installed (for example, packages that you 
install from CRAN). 

Once you've built and installed your package, give it a try. You can go 
up to the "File" menu in RStudio and select "New Project". Select 
"New Directory", and this will take you to a menu with the different types
of R Projects you can create. You should now see your Project template
as one of the options. Click on this, and you can test out if the template's 
working like you hoped it would. 

[How a different user can download and install a package that isn't on CRAN]

If you would like to create one of these RStudio Project templates for studies
in your research group, there are more details on the process at ... In addition, 
since this method requires building an R package, you may find resources on creating
R packages to be useful. One excellent book on the topic, which is available for
free online, is [R Packages] by Hadley Wickham and Jenny Bryan.


In this module, we will explain why the tidyverse approach is currently not
always ideal throughout all steps of pre-processing, analysis, and visualization
of the types of data that you may collect through a biomedical research
experiment. We will explain what data structures are, and present some of the
types of data structures commonly used in packages in the Bioconductor project.
These more complex data structures largely leverage a system in R called the S4
object-oriented system, which translates some ideas from object-oriented
programming to use to handle large and complex data in R.

In this module, we will cover several of the most popular data structures (each
available as an S4 object class) that are used to work with data within
Bioconductor packages. In a later module, we will explain how you can build a
pipelin that combines Bioconductor and tidyverse approaches, in which early
steps in data pre-processing use the Bioconductor approach to handle large and
complex initial data, and later steps shift to use a tidyverse approach, once it
is appropriate to store data in simpler structures like dataframes.

In this
and following modules, we will therefore explain the advantages and disadvantages of
complex versus simpler data storage formats in R. We will also explain how these
advantages and disadvantages weigh out differently in different stages of a data
preprocessing and analysis workflow. Finally, we will describe how you can
leverage both to your advantage, and in particular the tools and approaches that
you can use to shift from a Bioconductor-style approach---with heavy use of
complex data storage formats---early in your preprocessing pipeline to a
tidyverse approach---centered on storing data in a simple, tidy dataframe
object---at later stages, when the data are more suitable to this simpler
storage format, which allows you to leverage the powerful and widely-taught
tidyverse approach in later steps of analysis and visualization.

In these modules, we will focus on explaining these ideas within the R
programming language. This language is a very popular one for both biomedical
data sets and also for more general tasks in data management and analysis.
However, these principles also apply to other programming languages,
particularly those that can be used in an interactive format, including Python
and Julia.

Data in R can be stored in a variety of other formats, too. When you are working 
with biological data---in particular, complex or large data output from laboratory
equipment---there can be advantages to using data structures besides dataframes. 
In this section, we'll discuss some of the complex characteristics of biomedical 
data that recommend the use of data structures in R beyond the dataframe. We'll 
also discuss how the use of these other data structures can complicate the use of
"tidyverse" functions and principles that you might learn in beginning R programming
courses and books. In later modules, we'll discuss how to connect your work in R 
to clean and analyze data by performing earlier pre-processing steps using more 
complex data structures and then transferring when possible to dataframes for 
storing data, to allow you to take advantage of the power and ease of the 
"tidyverse" approach as early as possible in your pipeline. 

As you learn R, you will almost certainly learn how to create and work with
the more general data structures, including how to explore the data stored in
each of them. By contrast, you may never learn many of the more complex data
storage formats, especially if you are not using packages from Bioconductor.
However, there are a number of good reasons why R packages---especially those
shared through Bioconductor---define and use more complex data formats. In this
and following modules, we will explain the advantages and disadvantages of
complex versus simpler data storage formats in R. We will also explain how these
advantages and disadvantages weigh out differently in different stages of a data
preprocessing and analysis workflow. Finally, we will describe how you can
leverage both to your advantage, and in particular the tools and approaches that
you can use to shift from a Bioconductor-style approach---with heavy use of
complex data storage formats---early in your preprocessing pipeline to a
tidyverse approach---centered on storing data in a simple, tidy dataframe
object---at later stages, when the data are more suitable to this simpler
storage format, which allows you to leverage the powerful and widely-taught
tidyverse approach in later steps of analysis and visualization.

In these modules, we will focus on explaining these ideas within the R
programming language. This language is a very popular one for both biomedical
data sets and also for more general tasks in data management and analysis.
However, these principles also apply to other programming languages,
particularly those that can be used in an interactive format, including Python
and Julia.


An open source programming environment is like Andy Warhol's studio---the
door is always open, and people are free to walk in off the street and 
chip in to help create new things [@judkins2016art]. This means, though, 
that there need to be some general design principles to help coordinate
all these contributions.

The R programming language offers a wide variety of structures that can be
used to store data as you work with it, including steps of preprocessing 
and analysis of the data. Some of these structures are defined through the
base R language that you first install, while other structures are specially 
defined through the extension R packages you add as you continue to work 
with R. These packages are specific to the tasks you aim to do, and if they
define their own data storage structures, those structures are typically 
customized to that task. 

For example, there are packages---including the `xcms` package, for
example---that allow you to load and preprocess data from LC-MS experiments.
These packages include functionality to load data from a specialized format
output by mass spectometry equipment, as well as identify and align peaks within
the data that might indicate, for example, metabolite features for a
metabolomics analysis. The `xcms` package defines its own structures that are
used to store data during this preprocessing, and also draws on specialized data
structures defined in other R extension packages, including the `OnDiskMSnExp`
data object class that is defined by the `MSnbase` package. 

Complex data structures like these can be very precise in defining what types of
data they contain and where each component of the data goes. Later in this and
other modules, we will provide more details about the advantages and
disadvantages of these types of specialized data storage formats, especially in
the context of improving transparency, rigor, and reproducibility across the
steps of preprocessing experimental biomedical data.

Dataframes are very clearly and simply organized. However, they can be
too restrictive in some cases. Sometimes, you might have data that are taken at
different levels of observation---for example, you might have some measurements
that are specific to a specific sample, but then other measurements or data that
are common to the experiment as a whole (metadata). 

Also, the simple dataframe structure doesn't have the capacity to store data
taken at these different levels within the same structure (at least, not without
a lot of repetition). Further, the dataframe won't work for massive datasets.
Sometimes, you will get massive amounts of data from equipment like
spectrometers or cytometers, and these datasets can be so big that they can't be
easily read into R. One strategy with massive data is to read in only the bits 
you need as you need them, rather than reading in all the data and then 
using R commands to work with the full dataset. We'll look later in this module
about how more complex data structures can facilitate this approach when 
working with massive data in R.

The dataframe structure has the advantage of being simple to understand and use.
By using the dataframe as a common structure, the tidyverse approach is able to
create a powerful environment for working with data, because the use of a common
structure allows you to program using small, simple functions that can be
combined in different ways to solve complex tasks.

However, the dataframe lacks flexibility in storing data that does not naturally 
follow a two-dimensional structure, and it can struggle to handle massive datasets.
Therefore, in some cases, it is appropriate to adopt approaches that store data
in more complex data structures.

There are two main features of biomedical data---in particular, data collected
from laboratory equipment like flow cytometers and mass spectrometers---that
make it useful to use more complex data structures in R in the earlier stages of
preprocessing the data. First, the data are often very large, in some cases so
large that it is difficult to read them into R. Second, the data might combine
various elements, each with their own natural structures, that you'd like to
keep together as you move through the steps of preprocessing the data.

The code for different implementations of a method (in other words, different
ways it will run with new object classes) can come in different R packages.
This allows a developer to add his or her own applications of methods, suited
for object classes he or she creates. 

A class defines the structure for a way of storing data. When you create
an object that follows this structure, it's an instance of that class.
The `new` function is used to create new instances of a class.  

When a generic function determines what code to run based on the class of the 
object, it's called method dispatch.

**Examples of complex biomedical data suited for complex data structures**

In the last module, we covered the tidyverse approach to working with 
data in R. This approach hinges on using a common data structure, the 
dataframe. With many tasks in data management and analysis, this data 
structure is sufficient, and so can be leveraged with the tidyverse 
approach. However, there are some cases where data are not well suited
for a dataframe structure. In these cases, you may need to use some of 
R's more complex data structures, at least for part of your data
preprocessing and analysis pipeline.

What characteristics might make data less suited for a dataframe structure
in R? There are a variety of characteristics that could cause this. First, 
if data is extremely large, it may exceed a size that is reasonable for a
dataframe structure. Some of the more complex data structures in R allow
much of the data to stay "on disc", rather than be loaded into RAM, 
when the data are structured to provide access from R. 

Other datasets 
may not fit well within the two-dimensional, non-ragged structure that 
is characteristic of the dataframe structure. For example, some biomedical
data may have data that records characteristics at several levels of the 
data. It may have records on the levels of gene expression within each 
sample, separate information about each gene that was measured, and 
another separate set of information that characterizes each of the samples. 
While it is critical to keep "like" measurements aligned with data like this---in 
other words, to ensure that you can connect the data that characterizes
a gene with the data that provides measures of the level of expression of 
that gene in each sample---these data do not naturally have a two-dimensional 
structure and so do not fit naturally into a dataframe structure. 

**Building on list data structures**

R allows for very complex and specialized data structures, suitable to be
customized for very specific needs with large or complex data. These 
structures tend to build on the generic list data structure. 

Another common
general purpose data structure in R is the *list*, which allows you to combine
data stored in any type of structure to create a single R object, giving
enormous flexibility (but minimal set structure from one object to another).
This data structure is the building block for some of the more complex specific
data structures, which we'll cover next.

The R programming language offers a wide variety of structures that can be
used to store data as you work with it, including steps of preprocessing 
and analysis of the data. Some of these structures are defined through the
base R language that you first install, while other structures are specially 
defined through the extension R packages you add as you continue to work 
with R. These packages are specific to the tasks you aim to do, and if they
define their own data storage structures, those structures are typically 
customized to that task. 

Many of these more specific data structures are built on the more generic idea
of the **list** data structure, which provides a very flexible way to combine
other data structures to create a single R object. In R, you can think of a 
list data structure as having various slots where it can store data, and 
each of these slots can store data stored in another structure. For example, 
one slot of a list might store a dataframe of data, while another might store 
a vector or a dataframe for a different level of observation. A slot could 
even store another list. As an example, if we want to store the type of 
data shown in Figure \@ref(fig:complexdatastructure), we could use a list 
data structure with one slot that stores the metadata for the experiment, 
another that stores a dataframe or matrix with the assay data, another with a 
dataframe or matrix that stores the phenotype data, and another with 
a dataframe or matrix that stores the gene-level data. Each of these slots in
the list will get a name, and we can use that name to access each of these
pieces of data later. The list, in this case, allows us to store all these
different types of data from an experiment in the same structure in R, so 
we can make sure we keep the data together in a single structure, even though
it's too complex to fit in a simple dataframe.

Many R data structures are built on a general structure called a "list". This
data structure is a useful basic general data structure, because it is 
extraordinarily flexible. The list data structure is flexible in two 
important ways: it allows you to include data of different *types* in the 
same data structure, and it allows you to include data with different 
dimensions---and data stored hierarchically, including various other data
structures---within the list structure. We'll cover each of these points a bit 
more below and describe why they're helpful in making the list a very good 
general purpose data structure.

The list data structure is much more flexible than the vector or dataframe. It essentially allows you to 
create different "slots", and you can store any type of data in each of these
slots. In each slot you can store any of the other types of data structures in 
R---for example, vectors, dataframes, or other lists. You can even store unusual
things like R *environments* [?] or *pointers* that give the directions to where
data is stored on the computer without reading the data into R (and so saving 
room in the RAM memory, which is used when data is "ready to go" in R, but which 
has much more limited space than the mass [?] storage on your computer). 

Since you can put a list into the slot of a list, it allows you to create deep,
layered structures of data. For example, you could have one slot in a list where
you store the metadata for your experiment, and this slot might itself be a list
where you store one dataframe with some information about the settings of the
laboratory equipement you used to collect the data, and another dataframe that
provides information about the experimental design variables (e.g., which animal
received which treatment). Another slot in the larger list then might have
experimental measurements, and these might either be in a dataframe or, if the
data are very large, might be represented through pointers to where the data is
stored in memory, rather than having the data included directly in the data
structure.

Given all these advantages of the list data structure, then, why not use it all 
the time? While it is a very helpful building block, it turns out that its flexibility
can have some disadvantages in some cases. This flexibility means that you can's
always assume that certain bits of data are in a certain spot in each instance
of a list in R. Conversely, if you have data stored in a less flexible structure, 
you can often rely on certain parts of the data always being in a certain part of
the data structure. In a "tidy" dataframe, for example, you can always assume
that each row represents the measurements for one observation at the unit of 
observation for that dataframe, and that each column gives values across all 
observations for one particular value that was measured for all the observations.
For example, if you are conducting an experiment with mice, where a certain number
of mice were sacrificed at certain time points, with their weight and the bacteria
load in their lungs measured when the mouse was sacrificed, then you could store
the data in a dataframe, with a row for each mouse, and columns giving the 
experimental characteristics for each mouse (e.g., treatment status, time point 
when the mouse was sacrificed), the mouse's weight, and the mouse's bacteria 
load when sacrificed. You could store all of this information in a list, as
well, but the defined, two-dimensional structure of the dataframe makes it much 
more clearly defined where all the data goes in the dataframe structure, while
you could order the data in many ways within a list.

There is a big advantage to having stricter standards for what parts of data go 
where when it comes to writing functions that can be used across a lot of data. 
You can think of this in terms of how cars are set up versus how kitchens are 
set up. Cars are very standardized in the "interface" that you get when you sit
down to drive them. The gas and brakes are typically floor pedals, with the gas 
to the right of the brake. The steering is almost always provided through a wheel 
centered in front of the driver's torso. The mechanism for shifting gears (e.g., 
forward, reverse) is typically to the right of the steering wheel, while 
mecahnisms for features like lights and windshield wipers, are typically to the 
left of the steering wheel. Because this interface is so standardized, you can 
get into a car you've never driven before and typically figure out how to 
drive it very quickly. You don't need a lot of time exploring where everything 
is or a lot of directions from someone familiar with the car to figure out where
things are. Think of the last time that you drove a rental car---within five minutes, 
at most, you were probably able to orient yourself to figure out where everything
you needed was. This is like a dataframe in R---you can pretty quickly figure out
where everything you might need is stored in the data structure, and people can 
write functions to use with these dataframes that work well generally across lots
of people's data because they can assume that certain pieces of data are in 
certain places. 

By contrast, think about walking into someone else's kitchen and orienting yourself
to use that. Kitchen designs do tend to have some general features---most will have
a few common large elements, like a stove somewhere, a refrigerator somewhere, 
a pantry somewhere, and storage for pots, pans, and utensils somewhere. However, 
there is a lot of flexibility in where each of these are in the kitchen design, 
and further flexibility in how things are organized within each of these structures. 
If you cook in someone else's kitchen, it is easy to find yourself disoriented in the
middle of cooking a recipe, where a utensil that you can grab almost without 
thinking in your own kitchen requires you to stop and search many places in 
someone else's kitchen. This is like a list in R---there are so many places that
you can store data in a list, and so much flexibility, that you often find yourself
having to dig around to find a certain element in a list data structure that someone
else has created, and you often can't assume that certain pieces are in certain 
places if you are writing your own functions, so it becomes hard to write 
functions that are "general purpose" for generic list structures in R. 

There is a way that list structures can be used in R in a way that retains some
of their flexibility while also leveraging some of the benefits of
standardization. This is R's system for creating *objects*. These object
structures are built on the list data structure, but each object is constrained
to have certain elements of data in certain structures of the data. These
structures cannot be used as easily as dataframes in a "tidyverse" approach,
since the tidyverse tools are built based on the assumption that data is stored
in a tidy dataframe. However, they are used in many of the Bioconductor
approaches that allow powerful tools for the earlier stages in preprocessing
biological data. The types of standards that are imposed in the more specialized
objects include which slots the list can have, the names they have, what order
they're in (e.g., in a certain object, the metadata about the experiment might
always be stored in the first slot of the list), and what structures and/or data
types the data in each slot should have.



**S3 and S4 objects in R**

Many complex data structures in R are defined as S3 or S4 objects. 

### Advantages of complex data structures

**Bundling and aligning**

One characteristic that can make data complex is if it includes measurements
that are taken at several different levels of observation. For example, a single
data set may have some observations or measurements that are taken at the level
of the sample (e.g., the age, sex, and treatment of each study subject from
which the samples are taken). It may have other measurements that are taken at
the level of the gene (e.g., [values that you would have per gene]), and then
measurements that are the result of a specific assay [?] (e.g., the expression
level of each of the measured genes within each sample).

These different types of data could be stored in different dataframes, with 
a common identifier to help link them. They could even be stored in separate
dataframes with the positions used to link them (for example, if each 
column represents a gene in one dataframe and each row represents a gene
in another, then you assume that the second column in the first dataframe
aligns with the second row in the second dataframe), although this method
can be prone to errors, especially one you begin subsetting the original 
dataframes in the course of data preprocessing and analysis. 

More complex data structures can help in both bundling all these pieces of
data into a single object, while at the same time enforcing alignment across
the pieces of data, so you can line up the measurements for a specific gene 
or sample across data taken at different levels. 

The data from genomic and other high-throughput experiments
often are too complex and/or large to make dataframes practical as data 
structures, at least until data can be simplified through pre-processing. 
In this section, we'll look at an approach to pre-processing these data, leveraging
more complex data structures when needed. Once data have been pre-processed, 
they are often simplified to the point where they can be stored in a dataframe, 
and so it is possible to create workflows that move into a tidyverse approach
once data can reasonably be stored in dataframes. This creates a powerful pipeline, 
using more complex tools when necessary and then moving into the more 
straightforward tidyverse approach when possible. In the next module, we'll
discuss how you can adopt this type of combined approach.

Most laboratory equipment can output a raw data file that you can then read into R. 
For many types of laboratory equipment, these raw data files follow a strict format. 
The file formats will often have different pieces of data stored in specific spots. 
For example, the equipment might record not only the measurements taken for the 
sample, but also information about the setting that were applied to the equipment
while the measurements were taken, the date of the measurements, and other metadata
that may be useful to access when preprocessing the data. Each piece of data may 
have different "dimensions". For example, the measurements might provide one 
measurement per metabolite feature or per marker. Some metadata might also be
provided with these dimensions (e.g., metadata about the markers for flow 
cytometry data), but other metadata might be provided a single time per sample
or even per experiment---for example, the settings on the equipment when the 
sample or samples were run. 

When it comes to data structures, dataframes and other two-dimensional data storage
structures (you can visualize these as similar to the format of data in a spreadsheet, 
with rows and columns) work well to store data where all data conform to a common 
dimension. For example, a dataframe would work well to store the measurements 
for each marker in each sample in a flow cytometry experiment. In this case, 
each column could store the values for a specific marker and each row could 
provide measurements for a sample. In this way, you could read the measurements
for one marker across all samples by reading down a column, or read the measurements
across all markers for one sample by reading across a row. 

When you have data that doesn't conform to these common dimensions [unit of
measurement?] however, a dataframe may work poorly to store the data. For
example, if you have measurements taken at the level of the equipment settings
for the whole experiment, these don't naturally fit into the dataframe format.
In the "tidyverse" approach, one approach to handling data with different units
of measurement is to store data for each unit of measurement in a different
dataframe and to include identifiers that can be used to link data across the
dataframes. More common, however, in R extensions for preprocessing biomedical
data is to use more complex data structures that can store data with different
units of measurement in different slots within the data structure, and use these
in conjunction with specific functions that are built to work with that specific
data structure, and so know where to find each element within the data
structure.

Let's start by looking at how some data might have a structure that is too
complex to fit into a two-dimensional dataframe. Figure
\@ref(fig:complexdatastructure) shows an example of different components of data
that may need to be stored in a data structure that is more complex than a
dataframe. Data from a genomic experiment may include data from several levels,
including metadata that describes the entire experiment, as well as assay data,
phenotype data, and gene-level data. More complex data structures in R can be
used to store all these pieces of data inside a single data structure.


```{r complexdatastructure, echo = FALSE, out.width = "\\textwidth", fig.cap = "An example of different components of data that may need to be stored in a data structure that is more complex than a dataframe. Data from a genomic experiment may include data from several levels, including metadata that describes the entire experiment, as well as assay data, phenotype data, and gene-level data. More complex data structures in R can be used to store all these pieces of data inside a single data structure."}
knitr::include_graphics("figures/complex_data_structure.png")
```

**Handling large data**

Two approaches for this are on disc and through R environments. 


There's a second reason why dataframe structures don't always work for data from
biological experiments, which has to do with the size of data (and so how much
memory it requires). A computer has several ways that it can store data. The
primary storage is closely connected with the computer's processing unit, where 
calculations are made, and so data stored in this primary storage can be 
processed by code very quickly. R uses this approach, and so when you load data
in R to be stored in one of its traditional data structures, that data is 
moved into part of the computer's primary storage (its random access memory, or 
RAM). 

Data can also be stored in other devices on a computer, including hard drives and 
solid state drives that are built into the computer (the computer's secondary 
storage devices) or even onto storage devices that can be removed from the 
computer, like USB drives or external hard drives (the computer's tertiary storage).
The size of available storage in these devices tends to be much, much larger than
the storage size of the computer's RAM. However, it takes longer to access data
in these secondary storage devices because they aren't directly connected to 
the processor, and instead require the data to move into RAM before it can 
be accessed by the processor, which is the only part of the computer that can 
do things to analyze, modify, or otherwise process the data.

Your data will often be saved on a file in the computer's secondary memory 
(e.g., in a file stored on the computer's solid state drive) before you read it
into R, then moved into memory (RAM, part of the primary storage) when you ask
R to load it into a data structure that you can access with code commands in R.
However, the storage size available in RAM will always be much, much smaller than
the storage size in secondary storage devices like solid state drives, and so 
with larger data, problems can arise if you try to read data into RAM that is 
too large for that primary storage device to accommodate. 

The traditional dataframe structure in R is therefore built after
reading the data in memory, into RAM. However, many biological experiments now create
data that is much too large to read into memory for R in a reasonable way
[@lawrence2014scalable; @hicks2021mbkmeans]. More complex data structures can
allow more sophisticated ways to handle massive data, and so they are often
necessary when working with massive biological datasets, particularly early in
pre-processing, before the data can be summarized in an efficient way. For
example, a more complex data structure could allow much of the data to be left
on disk, and only read into memory [RAM?] on demand, as specific portions of the
data are needed [@gatto2013msnbase; @hicks2021mbkmeans]. This approach can be
used to iterate across subsets of the data, only reading parts of the data into
memory at a time [@lawrence2014scalable]. Such structures can be designed to
work in a way that, if you are the user, you won't notice the difference in
where the data is kept (on disk versus in memory)---this means you won't have to
worry about these memory management issues, but instead can just gain from
everything going smoothly, even as datasets get very large [@gatto2013msnbase].

[Data size, on-disk backends for files, like HDF5 and netCDF---used for flow
cytometry file format?]

[Potential future direction---developments of tidyverse based front ends for 
data stored in databases or on-disk file formats---`sergeant` package is one
example, also running tidyverse commands on data in database, `matter` package?, 
`disk.frame` package?]


**Allow software development across large, diverse teams**

Robert Gentleman, one of the creators of Bioconductor [?], highlights
this as a key reason for using these more complex data structures. 
R is an open-source language, and it allows contributions from 
software developers worldwide. The Bioconductor project leverages the
base R software to build tools for working with complex genomic [?]
data, also allowing for the development of new extensions from 
developers worldwide. The software extensions are developed, therefore, 
not by a unified team that is all employed at the same place, but 
instead by a collection of people who are making software as part of 
their research program, or perhaps as part of an industry job, and who
write that software with the intent of having it work smoothly with 
other software packages that are available through Bioconductor. 

If you have ever been in a role where you organized volunteers to 
perform a large task, you can probably imagine how challenging it is to 
design a project in a way that allows for this type of software 
development to work. The use of specialized data structures is one
element that helps in this coordination for Bioconductor. It enforces a 
standard that all the programmers can work toward---they can work under
the assumption that the output from other functions in the Bioconductor
ecosystem will be structured in a certain way, and so they can work 
under the assumption that certain elements of the data will be in certain
spots in the data structure of the object.

[Example of the party game where you each add a sentence, only seeing part of
the story. Exquisite corpse. Surrealist game you could do with either writing or
drawing (later people adapted to other things, like music composition). With
drawing, you are typically drawing a person or other being, with the defined
parts of a head, torso, and legs. One person does a part (the head area), then
folds so the next person can only see enough to connect with what they did, then
the next person draws the next part (the torso area), again folds so the next
person can only see enough to connect, and so on. This results in writing or a
drawing that is connected across the different parts, but can be completely
independent within each part (and unrelated to other parts). Collaborative, 
combines different parts made by different people.]

The downside of a general list data structure, however, comes in when it comes
to developing software for data stored in that structure. The general list 
structure is very flexible, which is why it can store such different types of 
data, but this flexibility means that there's no guarantee about where in 
the data structure specific elements might be stored. By comparison, in a 
dataframe each row can be assumed to capture an observation, and each column will 
capture measurements or characteristics of that observation. Someone can 
therefore develop software to work with data stored in that structure, relying
on finding those type of data in those locations in the data structure. When
more complex data are stored in a general list object, the different components
could be store in different ways by different users. For example, for the 
type of complex data shown in Figure \@ref(fig:complexdatastructure), one person
might store the metadata in the first slot of a list and the phenotype data
in the second, while another person might store the data in a list with the 
metadata in the last slot and the phenotype data in the first. 

The way around this problem is to create data structures that build off the 
general list structure, but impose some rules that constrain the structure, so 
that the same types of data are always stored in the same spot. By doing this, 
software developers can develop code to work with data stored in that structure,
with the guarantee that they can always find certain elements of the data in 
certain spots in the data structure. Bioconductor makes heavy use of these 
types of specialized data structures, typically called "classes" in Bioconductor
tutorials and user manuals. This is because, in R, they are created using 
one of R's object-oriented approaches, most often one called S4.

Complex data structures like these can be very precise in defining what types of
data they contain and where each component of the data goes. For example, 
they may have a "phenoData" slot that only will store a specialized
dataframe with phenotype data describing each sample in the experiment [?], 
and another slot named "featureData" that will only store a specialized 
dataframe with data about each feature (e.g., gene) investigated in the experiment.
With this structure, a software developer can develop a program that inputs
data in this structure, always knowing where to find the feature data or the
phenotype data.

[Validation of data as it's entered in an S4 class]

R programmers get a lot of advantages from using these classes because they can
write functions under the assumption that certain pieces of the data will always
be in the same spot for that type of object. There is still flexibility in the
object, in that it can store lots of different types of data, in a variety of
different structures. While this "object oriented" approach in R data structures
does provide great advantages for programmers, and allow them to create powerful
tools for you to use in R, it does make it a little trickier in some cases for
you to explore your data by hand as you work through preprocessing. This is
because there typically are a variety of these object classes that your data
will pass through as you go through different stages of preprocessing, because
different structures are suited to different stages of analysis. Functions often
can only be used for a single class of objects, and so you have to keep track of
which functions pair up with which classes of data. Further, it can be a bit
tricky---at least in comparison to when you have data in a dataframe---to
explore your data by hand, because you have to navigate through different slots
in the object. By contrast, a dataframe always has the same two-dimensional,
rectangular structure, and so it's very easy to navigate and explore data in
this structure, and there are a large number of functions that are built to be
used with dataframes, providing enormous flexibility in what you can do with
data stored in this structure. 

### Biomedical data preprocessing with complex data structures

As we noted briefly earlier, the Bioconductor project is one area of 
R programming where complex data structures, rather than a tidyverse
approach, tends to dominate (although there does seem to be an evolution 
toward more tidyverse techniques in Bioconductor, as we'll discuss 
some in later modules). Bioconductor is critical to learn if you are
working with genomic [?] data, as many of the key tools and algorithms
for this type of data are shared through that project. This means that, 
for many biomedical researchers who are now generating complex, 
high-throughput data, it is worth learning how to use complex data
structures in R.

**Powerful algorithms**

One of the advantages of these complex data structures for biomedical 
data preprocessing is that they can be leveraged to develop very powerful 
algorithms for working with complex biomedical data. 

[Examples]

**Examples of types of complex pre-processing algorithms**

In some cases, more complex algorithms may be used to normalize 
data across different samples [?]---for example, the normalization
algorithm may leverage the assumption that the vast majority of 
values (e.g., expression levels of genes) are the same in all samples, 
with only a few varying between the samples. Starting from this 
assumption, an algorithm can be developed to normalize across the 
samples, to help in identifying genes with important differences in 
expression across the samples. 

[Other examples]

**Challenges**

It can be harder to explore your data along the way. With a tidyverse
approach, since data are always in dataframes, you can quickly learn 
some tools that let you extract parts of the data, summarize the data, 
and visualize it. This allows for exploratory data analysis at each 
stage as you develop your preprocessing and analysis pipeline. 
Conversely, if you have a pipeline where the data is continuously 
being moved from one data structure to another, you have to learn how 
to use and explore each type of data structure to become as facile with
exploring your data along the way. 

This can also make it harder to experiment, since often a function will 
be tied to a specific data structure, and so instead of learning a small
set of general use functions that can be combined together in different
ways (the tidyverse approach), you must learn more functions, since 
functions tend to be tied to a data structure (outside of generic functions), 
and the data structure tends to change frequently across the pipeline. 

To be clear, a pipeline in R that includes these complex data structures
will typically still be modular, in the sense that you can adapt and 
separate specific parts of the pipeline. However, they tend to be much 
less flexible than pipelines developed with a tidyverse approach. The 
data structure changes often, with certain functions outputing a data
structure that is needed for the next step, then the function of the 
next step outputting the data in a different structure, and so on. This
changing data structure means that the functions for each step often are
constrained to always be put in the same order. By comparison, the small 
tools that make up tidyverse functions can often be combined in many different
orders, letting you build a much larger variety of pipelines with them. 
Also, many of the functions that work with complex data types will do 
many things within one function, so they can be harder to learn and 
understand, and they are often much more customized to a specific action, 
which means that you have to learn more functions (since each does one
specific thing). 

### Disadvantages of complex data structures

**Harder to see all the data**

A first limitation of storing data in a complex structure is that it can 
make it trickier for you to explore the data as you work with it. When data
are stored in a simple structure, like a dataframe or a vector, there are
simple tools (like the `head` function) that allow you to see the first 
values or the first few rows. Further, the simpler structure of the data
(e.g., a one-dimensional vector or a two-dimensional dataframe) makes it
easy to generate this kind of snapshot of the data. 

When data are stored in more complex objects, by contrast, they often have
a more complex, hierarchical structure. For example, a list-based object
could incorporate elements of a dataframe in one part of the object, a 
vector in another, and another list in still another. While there are tools
to check out the contents of a list-based object (for example, the `str` 
function), the results can be hard to interpret usefully. If the object
includes lots of elements, with lots of levels of hierarchy in some of the
elements, then when you run this type of exploratory function, you can 
get a long print-out of complex output. 

To explore this type of complex output, you often have to spend a lot of 
time to tackle it from different directions. For example, the `str` function
allows you to limit output to just a certain number of levels of nesting, so 
you might start by just figuring out the top-level elements of the object. 
You could then dig deeper into certain elements of the object to take a look
at the data stored in each element. 

Therefore, while it is possible to explore the data stored in a more complex
data structure, it is often much more onerous to do so than it is to explore
data stored in a simpler data structure, like a vector or a dataframe. 
Similarly, it is more difficult to perform simple operations to explore the
data, like getting a summary of the range and quartiles of a set of numeric 
values, or a count of the number of values in a set that are recorded as
missing. We'll cover this limitation more in the next subsections, as it
relates to the difficulty of accessing data if they are stored in more 
complex data structures, as well as the difficulty in working with those 
data using general purpose tools like functions from packages in the tidyverse.

[Any tools to help with this for Bioconductor objects? Functions to view 
bits of the elements stored inside an object?]

**Harder to access all the data**

Just as it's harder to view data that are stored in a complex, rather than 
a simpler, data structure, it can also be harder to access or extract pieces 
of the data when they are stored in a more complex structure. 

[more on this]

**Harder to make use of general purpose tools**

**Higher entry barrier to learn to use well**

In previous sections, we described how the R programming language allows for 
object-oriented programming, and how customized objects are often used in 
preprocessing for biological data. This is a helpful approach for preprocessing, 
because it can handle complexities in biological data at its early stages of 
preprocessing, when R must handle complex input formats from equipment like
flow cytometers or mass spectrometers, and data sizes that are often very large.

However, once you have preprocessed your data, it is often possible to work with it
in a smaller, more consistent object type. This will give you a lot of flexibility 
and power. While object-oriented approaches can handle complex data, it can be a 
little hard to write and work with code that is built on an object oriented approach.
Working with this type of code requires you to keep track of what object type your
data is in at each stage of a code pipeline, as well as which functions can work with 
that type of object. 

Further, this type of coding, in practice at least, can be a bit inflexible.
Often, specific functions only work with a single or few types of functions. In
theory, object-oriented programming allows for *methods* that work in customized
ways with different types of objects to apply customized code to that type of
object for similar, common-sense results. For example, there are often `summary`
and `plot` methods for most types of objects, and these apply code that is
customized to that object type and output, respectively, summarized information
about the data in the object and a plot of the data in the object. However, when
you want to do more with the object that summarize it or create its default
plot, you often end up needing to move to more customized functions that work
only with a single or few object types. When you get to this point, you find that
you have to remember which functions work with which object type, and you have to 
use different functions at different stages of your code pipeline, as your code 
changes from one object class to another. 

Further, many of these functions input one object type and output a different
one. This evolution of object types for storing data can be difficult to navigate
and keep track of. Different object types store data in different ways, and so 
this evolution of data object types for storage can make it tricky to figure out 
how to extract and explore data along the pipeline. It makes it hard to write your
own code to explore and visualize the data along the way, as well, and so users
are often restricted to the visualization and analysis functions pre-made and 
shared in packages when working with data in complex object types, especially 
until the user becomes very comfortable with coding in R. 

Overall, what does this all mean? Object-oriented approaches offer real advantages
early in the process of pre-processing biological data, especially complex and
large data output from complex laboratory equipment. However, once this pre-processing
is completed, there is a big advantage in moving the data into a simple format
and then continuing coding, data analysis, and visualization using tools that 
work with this simple format. This is the approach taken by a suite of R packages
called the "tidyverse", as well as extensions that build off the approach that
this suite of tools embraces. This "tidyverse" approach is described in the 
next section.

In the previous parts of this module, we've highlighted some of the ways that
complex data structures are useful (and even necessary) for parts of the data
pre-processing you may do in R. However, they also have some downsides. In a later
module, we'll talk about how you can use a combined workflow that uses the 
Bioconductor approach (with more complex data structures) when necessary, but then
shifts into a tidyverse approach (based on keeping data in a dataframe
structure) as soon as possible in the workflow. Here, we'll describe some of the 
limitations of complex data structures to help explain why it's worthwhile to 
develop workflows with this combined approach.

The first limitation of using complex data structures is that it requires you to 
learn each of the data structures and where they keep different elements of the 
data. Each specialized data structure ("class" in Bioconductor) has defined 
rules for each of its data storage slots, and you must become familiar with 
these class-specific rules to be able to explore and extract data stored in that
structure. 

For example, the `ExpressionSet` data structure (defined in the `Biobase`
package in Bioconductor) is used to hold information from high-throughput
assays, like ... . It includes different slots for data from the assay,
phenotype data (e.g., ...), and feature data (e.g., ...), as well as slots that
can be used to store things like metadata about the experiment. Each of these
slots has its own name, and you would need to know these names to extract and
explore each of these elements of the data from the structure. If you are doing
an analysis of these type of data, the data might move from this `ExpressionSet`
data structure into other data structures, for example a `DGEList` data
structure (defined in the `edgeR` package in Bioconductor) after you have ...,.
and then a `DGEExact` data structure (also defined in the `edgeR` package in
Bioconductor) after you have performed [differential expression analysis?] ... .
To explore the data after each of these steps, you would need to know the rules,
including the names of each slot, of each of these separate, specialized data
structures.

The second limitation of using complex data structures is that it requires you 
to know which functions work with which data structures, and to only use the 
appropriate functions with the appropriate data structures. In essence, this 
requires you to develop an understanding of how the data moves from one
data structure to another throughout the workflow so that you can apply appropriate
functions at each step. 

This also means that you often have to learn a larger set of functions, as
different functions are needed to do similar things to data in different data
structures. For example, if you have a function that can normalize data in one
data structure, it may not work for data stored in a different data structure.
By contrast, when data are stored in a common structure (like a dataframe), the
same functions can always be used, across any step in the pipeline. 

There are some approaches that programmers have taken to get around this limitation,
using what are called "methods". Methods are functions that run differently 
depending on what type of data structure they are given. One example in R is 
the `summary` function, which outputs a summary of the object you input. This 
function first checks the class of the object (i.e., the data structure) and 
then has different sets of code that it uses depending on what the class is. In 
this way, a user can call `summary` on data stored in many different data 
structures and get back something that is appropriate for that data structure. 
This helps in limiting the number of function names you, as a user, must remember.
A number of methods exist in R, including `print`, `summary`, and `plot`, that 
apply across many different classes of objects (and so across many of the 
Bioconductor data structures). However, many of these methods are geared toward
providing "final" output---a final summary or plot that you might read and 
interpret directly, but not output that serves as a "step" in a longer workflow, 
so not output that will become input to another function.

Therefore, even the approach of methods can't get around another way that 
specialized data structures and their use of specialized functions constrain you
when pre-processing and analyzing data. When data are stored in specialized data
structures, with different structures used for each step of the process, it is
often the case that instead of being able to flexibly combine different
functions in different orders to use small steps to build up to complex
processes, you instead are often constrained to use functions in a predefined
order, as they shift your data from one structure to another. Although the
Bioconductor functions are powerful in pre-processing and analyzing these large,
complex data, they also constrain you somewhat to follow the steps and analysis
imagined by the original package creator, rather than providing flexibility to
create your own series of steps, as the tidyverse approach does.

 ------------------------------------------------------------------------------------

<!-- > "Input/output (I/O) is the technical term for reading and writing data: the process of getting information into a particular computer system (in this case R) and then exporting it to the ‘outside world’ again (in this case as a file format that other software can read). Data I/O will be needed on projects where data comes from, or goes to, external sources. However, the majority of R resources and documentation start with the optimistic assumption that your data has already been loaded, ignoring the fact that importing datasets into R, and exporting them to the world outside the R ecosystem, can be a time-consuming and frustrating process. Tricky, slow or ultimately unsuccessful data I/O can cripple efficiency right at the outset of a project. Conversely, reading and writing your data efficiently will make your R projects more likely to succeed in the outside world." [@gillespie2016efficient] -->

<!-- > "There are circumstances when datasets become too large to read directly into -->
<!-- R. Reading in a 4 GB text file using the functions tested above, for example, -->
<!-- consumes all available RAM on a 16 GB machine. To overcome this limitation, -->
<!-- external stream processing tools can be used to preprocess large text files. The -->
<!-- following command, using the Linux command line ‘shell’ (or Windows based Linux -->
<!-- shell emulator Cygwin) command split, for example, will break a large multi GB -->
<!-- file into many chunks, each of which is more manageable for R: `split -b100m -->
<!-- bigfile.csv` The result is a series of files, set to 100 MB each with the -->
<!-- `-b100m` argument in the above code. By default these will be called xaa, xab -->
<!-- and can be read in one chunk at a time (e.g. using read.csv(), fread() or -->
<!-- read_csv(), described in the previous section) without crashing most modern -->
<!-- computers. Splitting a large file into individual chunks may allow it to be read -->
<!-- into R. This is not an efficient way to import large datasets, however, because -->
<!-- it results in a non-random sample of the data this way. A more efficient, robust -->
<!-- and scalable way to work with large datasets is via databases, covered in -->
<!-- Section 6.6 in the next chapter." [@gillespie2016efficient] -->





<!-- > "R comes in two versions: 32 -bit and 64 -bit. Your operating system also -->
<!-- comes in two versions, 32 -bit and 64 -bit. Ideally you want 64 -bit versions of -->
<!-- both R and the operating system. Using a 32 -bit version of either has severe -->
<!-- limitations on the amount of RAM R can access. So when we suggest that you -->
<!-- should just buy more RAM, this assumes that you are using a 64 -bit operating -->
<!-- system, with a 64 -bit version of R. [Note: If you are using an OS version from -->
<!-- the last five years, it is unlikely to be 32 -bit OS.] A 32 -bit machine can -->
<!-- access at most only 4 GB of RAM. Although some CPUs offer solutions to this -->
<!-- limitation, if you are running a 32 -bit operating system, then R is limited to -->
<!-- around 3 GB RAM. If you are running a 64 -bit operating system, but only a 32 -->
<!-- -bit version of R, then you have access to slightly more memory (but not much). -->
<!-- Modern systems should run a 64 -bit operating system, with a 64 -bit version of -->
<!-- R. Your memory limit is now measured as 8 terabytes for Windows machines and 128 -->
<!-- TB for Unix-based OSs." [@gillespie2016efficient] -->

<!-- > "The OnDiskMSnExp class extends MSnExp and inherits all of its functionality -->
<!-- but is aimed to use as little memory as possible based on a balance between -->
<!-- memory demand and performance. Most of the spectrum-specific data, like -->
<!-- retention time, polarity, total ion current are stored within the object’s -->
<!-- featureData slot. The actual M/Z and intensity values from the individual -->
<!-- spectra are, in contrast to MSnExp objects, not kept in memory (in the assayData -->
<!-- slot), but are fetched from the original files on-demand. Because mzML files are -->
<!-- indexed, using the mzR package to read the relevant spectrum data is fast and -->
<!-- only moderately slower than for in-memory MSnExp." [@gatto2013msnbase] -->

<!-- > "[For OnDiskMSnExp:] To keep track of data manipulation steps that are applied -->
<!-- to spectrum data (such as performed by methods removePeaks or clean) a lazy -->
<!-- execution framework was implemented. Methods that manipulate or subset a -->
<!-- spectrum’s M/Z or intensity values can not be applied directly to a OnDiskMSnExp -->
<!-- object, since the relevant data is not kept in memory. Thus, any call to a -->
<!-- processing method that changes or subset M/Z or intensity values are added as -->
<!-- ProcessingStep items to the object’s spectraProcessingQueue. When the spectrum -->
<!-- data is then queried from an OnDiskMSnExp, the spectra are read in from the file -->
<!-- and all these processing steps are applied on-the-fly to the spectrum data -->
<!-- before being returned to the user. The operations involving extracting or -->
<!-- manipulating spectrum data are applied on a per-file basis, which enables -->
<!-- parallel processing. Thus, all corresponding method implementations for -->
<!-- OnDiskMSnExp objects have an argument BPPARAM and users can set a -->
<!-- PARALLEL_THRESH option flag 2 that enables to define how and when parallel -->
<!-- processing should be performed (using the BiocParallel package). Note that all -->
<!-- data manipulations that are not applied to M/Z or intensity values of a spectrum -->
<!-- (e.g. sub-setting by retention time etc) are very fast as they operate directly -->
<!-- to the object’s featureData slot." [@gatto2013msnbase] -->

<!-- > "The distinction between MSnExp and OnDiskMSnExp is often not explicitly -->
<!-- stated as it should not matter, from a user’s perspective, which data structure -->
<!-- they are working with, as both behave in equivalent ways. Often, they are -->
<!-- referred to as in-memory and on-disk MSnExp implementations." -->
<!-- [@gatto2013msnbase] -->

<!-- > "Big data is encountered in genomics for two reasons: the size of the genome -->
<!-- and the heterogeneity of populations. Complex organisms, such as plants and -->
<!-- animals, have genomes on the order of billions of base pairs (the human genome -->
<!-- consists of over three billion base pairs). The diversity of populations, -->
<!-- whether of organisms, tissues or cells, means we need to sample deeply to detect -->
<!-- low frequency events. To interrogate long and/or numerous genomic sequences, -->
<!-- many measurements are necessary. For example, a typical whole genome sequencing -->
<!-- experiment will consist of over one billion reads of 75–100 bp each. The reads -->
<!-- are aligned across billions of positions, most of which have been annotated in -->
<!-- some way. This experiment may be repeated for thousands of samples. Such a data -->
<!-- set does not fit within the memory of a current commodity computer, and is not -->
<!-- processed in a timely and interactive manner. To successfully wrangle a large -->
<!-- data set, we need to intimately understand its structure and carefully consider -->
<!-- the questions posed of it." [@lawrence2014scalable] -->

<!-- > "To compare data across samples, we often summarize experimental annotations -->
<!-- over a set of reference features to yield a feature-by-sample matrix. For -->
<!-- example, we might count read alignments overlapping a common set of genes across -->
<!-- a number of samples. Larger matrices often arise in genetics, where thousands of -->
<!-- samples are compared over millions of SNPs, positions that are known to vary -->
<!-- within a population. In every case, the summaries are tied to a genomic range." -->
<!-- [@lawrence2014scalable] -->



<!-- > "There are general strategies for handling large genomic data that are well -->
<!-- suited to R programs. Sometimes the analyst is only interested in one aspect of -->
<!-- the data, such as that overlapping a single gene. In such cases, restricting the -->
<!-- data to that subset is a valid and effective means of data reduction. However, -->
<!-- once our interests extend beyond a single region or the region becomes too -->
<!-- large, resource constraints dictate that we cannot load the entire data set into -->
<!-- memory at once, and we need to iterate over the data to reduce them to a set of -->
<!-- interpretable summaries. Iteration lends itself to parallelism, that is, -->
<!-- computing on multiple parts of the same problem simultaneously. Thus, in -->
<!-- addition to meeting memory constraints, iteration lets us leverage additional -->
<!-- processing resources to reduce overall computation time. Investing in additional -->
<!-- hardware is often more economical than investment in software optimization. This -->
<!-- is particularly relevant in scientific computing, where we are faced with a -->
<!-- diverse, rapidly evolving set of unsolved problems, each requiring specialized -->
<!-- software. The costs of investment in general purpose hardware are amortized over -->
<!-- each problem, rather than paid each time for software optimization. This also -->
<!-- relates to maintainability: optimization typically comes at a cost of increased -->
<!-- code complexity. Many types of summary and filter operations are cheap to -->
<!-- implement in parallel because the data partitions can be processed -->
<!-- independently. We call this type of operation embarrassingly parallel. For -->
<!-- example, the counting of reads overlapping a gene does not depend on the -->
<!-- counting for a different gene." [@lawrence2014scalable] -->


<!-- > "A special mode of restriction is to randomly generate a selection of records. -->
<!-- Down-sampling can address many questions, especially during quality assessment -->
<!-- and data exploration. For example, short reads are initially summarized in FASTQ -->
<!-- files containing a plain text representation of base calls and corresponding -->
<!-- quality scores. Basic statistics of quality assessment such as the nucleotide -->
<!-- count as a function of sequencing cycle or overall GC content are very well -->
<!-- characterized by random samples of a million reads, which might be 1% of the -->
<!-- data. This sample fits easily in memory. Computations on this size of data are -->
<!-- very nimble, enabling interactive exploration on commodity computers. An -->
<!-- essential requirement is that the data represent a random sample. The ShortRead -->
<!-- package is designed for the QA and exploratory analysis of the output from -->
<!-- high-througput sequencing instruments. It defines the FastqSampler object, which -->
<!-- draws random samples from FASTQ files." [@lawrence2014scalable] -->

<!-- > "An example of a situation where random sampling does not work is when -->
<!-- prototyping a statistical method that depends on a significant amount of data to -->
<!-- achieve reasonable power. Variant calling is a specific example: restricting the -->
<!-- number of reads would lead to less coverage, less power and less meaningful -->
<!-- results. Instead, we need to restrict the analysis to a particular region and -->
<!-- include all of the reads falling within it. To optimize range-based queries, we -->
<!-- often sort and index our data structures by genomic coordinates. We should -->
<!-- consider indexing an investment because an index is generally expensive to -->
<!-- generate but cheap to query. The justification is that we will issue a -->
<!-- sufficient number of queries to outweigh the initial generation cost. Three -->
<!-- primary file formats follow this pattern: BAM, Tabix and BigWig [7, 10]. Each -->
<!-- format is best suited for a particular type of data. The BAM format is specially -->
<!-- designed for sequence alignments and stores the complex alignment structure, as -->
<!-- well as the aligned sequence. Tabix is meant for indexing general rangebased -->
<!-- annotations stored in tabular text files, such as BED and GFF. Finally, BigWig -->
<!-- is optimized for storing genome-length vectors, such as the coverage from a -->
<!-- sequencing experiment. BAM and Tabix compress the primary data with block-wise -->
<!-- gzip compression and save the index as a separate file. BigWig files are -->
<!-- similarly compressed but are self-contained. The Rsamtools package is an -->
<!-- interface between R and the samtools library, which implements access to BAM, -->
<!-- Tabix and other binary file formats. Rsamtools enables restriction of BAM -->
<!-- queries through the ScanBamParam object. This object can be used as an argument -->
<!-- to all BAM input functions, and enables restriction to particular fields of the -->
<!-- BAM file, to specific genomic regions of interest and to properties of the -->
<!-- aligned reads (e.g., restricting input to paired-end alignments that form proper -->
<!-- pairs)." [@lawrence2014scalable] -->

<!-- > "One common scenario in high-throughput sequencing is the calculation of -->
<!-- statistics such as coverage (the number of short sequence reads overlapping each -->
<!-- nucleotide in the genome). The data required for this calculation usually come -->
<!-- from very large BAM files containing alignment coordinates (including the -->
<!-- alignment “cigar”), sequences and quality scores for tens of millions of short -->
<!-- reads. Only the smallest element of these data, the alignment coordinates, is -->
<!-- required for calculation of coverage. By restricting input to alignment -->
<!-- coordinates, we transform the computational task from one of complicated memory -->
<!-- management of large data to simple vectorized operations on in-memory objects." -->
<!-- [@lawrence2014scalable] -->

<!-- > "Some vectors, in particular, the coverage, have long stretches of repeated -->
<!-- values, often zeroes. An efficient compression scheme for such cases is -->
<!-- run-length encoding. Each run of repeated values is reduced to two values: the -->
<!-- length of the run and the repeated value. This scheme saves space and also -->
<!-- reduces computation time by reducing computation size. For example, the vector -->
<!-- 0, 0, 0, 1, 1, 5, 5, 5 would have run-values 0, 1, 5 and run-lengths 3, 2, 3. -->
<!-- The data have been reduced from a size of 8 to a size of 6 (3 values plus 3 -->
<!-- lengths). The IRanges Rle class is a run-length encoded vector that supports the -->
<!-- full R vector API on top of the compressed representation. Operations on an Rle -->
<!-- gain efficiency by taking advantage of the compression. For example, the sum -->
<!-- method computes a sum of the run values, using the run lengths as weights. Thus, -->
<!-- the time complexity is on the order of the number of runs, rather than the -->
<!-- length of the vector." [@lawrence2014scalable] -->

<!-- > "The Biostrings package [12] provides XStringViews for views on top of DNA, -->
<!-- RNA and amino acid sequences. XString is a reference, rather than a value as is -->
<!-- typical in R, so we can create multiple XStringViews objects without copying the -->
<!-- underlying data. This is an application of the *fly-weight* design pattern: -->
<!-- multiple objects decorate the same primary data structure, which is stored only -->
<!-- once in memory." [@lawrence2014scalable] -->

<!-- > "Iterative summarization of data may be modeled as three separate steps: -->
<!-- split, apply and combine [15]. The split step is typically the only one that -->
<!-- depends on the size of the input data. The apply step operates on data of -->
<!-- restricted size, and it should reduce the data to a scale that facilitates -->
<!-- combination. Thus, the most challenging step is the first: splitting the data -->
<!-- into chunks small enough to meet resource constraints. Two modes of splitting -->
<!-- are particularly applicable to genomic data: sequential chunking and genomic -->
<!-- partitioning. Sequential chunking is a popular and general technique that simply -->
<!-- loads records in fixedcount chunks, according to the order in which they are -->
<!-- stored. Genomic partitioning iterates over a disjoint set of ranges that cover -->
<!-- the genome. Typical partitioning schemes include one range per chromosome and -->
<!-- sub-chromosomal ranges of some uniform size. Efficient range-based iteration, -->
<!-- whether over a partitioning or list of interesting regions, depends on data -->
<!-- structures, file formats and algorithms that are optimized for range-based -->
<!-- queries." [@lawrence2014scalable] -->

<!-- > "As an alternative to streaming over chunks, we can iterate over a -->
<!-- partitioning of the genome or other domain. Genomic partitioning can be -->
<!-- preferable to streaming when we are only interested in certain regions. The -->
<!-- tileGenome function is a convenience for generating a set of ranges that -->
<!-- partition a genome. ... A caveat with partitioning is that since many query -->
<!-- algorithms return ranges with any overlap of the query, care must be taken to -->
<!-- intersect the results with each partition, so that reads are not double counted, -->
<!-- for example." [@lawrence2014scalable] -->



<!-- > "The most widely used partitional clustering algorithm is k-means [6–8]. The -->
<!-- algorithm partitions N cells into k clusters each represented by a centroid, or -->
<!-- mean profile, for the cells in the kth cluster. This algorithm is commonly used -->
<!-- not only on its own, but also as a component of ensemble clustering [9, 10]. -->
<!-- While k-means is easy to implement, it assumes that the user has enough -->
<!-- computational resources (specifically RAM) to store the data and all -->
<!-- intermediate calculations into memory. However, file sizes generated from -->
<!-- scRNA-seq experiments can be on the order of tens to hundreds of gigabytes. For -->
<!-- large enough data, k-means can be slow or completely fail if a user lacks -->
<!-- sufficient computational resources. Ensemble clustering approaches that depend -->
<!-- on the use of k-means [9, 10] run it multiple times (e.g., with different -->
<!-- parameter values or on a different data subset) limiting the usability of these -->
<!-- packages for large scRNA-seq datasets [11]. We note that our goal here is not to -->
<!-- debate the relative merits of k-means as a clustering algorithm—k-means is a -->
<!-- well-established method, which has been thoroughly investigated [12]—but to -->
<!-- provide users with the ability to use the popular k-means algorithm on large -->
<!-- single-cell datasets. To address the problems of using k-means with large data, -->
<!-- two solutions are (1) parallelization and (2) subsampling. Parallelization -->
<!-- approaches typically leverage some combination of (i) MapReduce [13] concepts to -->
<!-- handle a large volume of data over a distributed computing environment [14, 15], -->
<!-- (ii) k-dimensional (k-d) trees to either optimize for the nearest centroid [16] -->
<!-- or to partition datasets into subsets, representative of the larger dataset -->
<!-- [17], and (iii) leverage multi-core processors [18]. While these approaches do -->
<!-- improve the speed of k-means, they can be limited to the number of reducers for -->
<!-- each centroid and can often require extensive computational resources. In -->
<!-- contrast, subsampling approaches, such as the mini-batch k-means algorithm [19] -->
<!-- work on small, random subsamples of data (“mini batches”) that can fit into -->
<!-- memory on standard computers. We would emphasize, however, that while mini-batch -->
<!-- k-means only operates on small subsamples of the data at any one time, the -->
<!-- algorithm still minimizes the same global objective function evaluated over all -->
<!-- samples as in traditional implementations of k-means. Current implementations of -->
<!-- the mini-batch k-means algorithm [19] are available in standard programming -->
<!-- languages such as in the scikit-learn machine learning Python library [20] or in -->
<!-- the ClusterR R package [21]. However, these implementations either implicitly or -->
<!-- explicitly require all the data to be read into memory, and therefore do not -->
<!-- leverage the potential of the algorithm to provide a low memory footprint. To -->
<!-- address the described problems, we implemented the mini-batch k-means clustering -->
<!-- algorithm in the open-source mbkmeans R package [22], providing fast, scalable, -->
<!-- and memory-efficient clustering of scRNA-seq data in the Bioconductor framework -->
<!-- [5, 23]. Like existing implementations, our package can be applied to in-memory -->
<!-- data input for smaller datasets, but also to on-disk data, such as from the HDF5 -->
<!-- file format [24], which is widely used for distributing single-cell sequencing -->
<!-- data. For on-disk input, mbkmeans leverages the subsampling structure of the -->
<!-- algorithm to read into memory only the current 'mini batch' of data at any given -->
<!-- point, thereby greatly reducing the required memory (RAM) needed. ... Our -->
<!-- contribution is two-fold: we implement a mini-batch k-means algorithm for -->
<!-- on-disk data, and we benchmark the performance of a non-trivial algorithm for -->
<!-- HDF5 against its in-memory counterpart." [@hicks2021mbkmeans] -->

<!-- > "The mbkmeans software package implements the mini-batch k-means clustering -->
<!-- algorithm described above and works with matrix-like objects as input. -->
<!-- Specifically, the package works with standard R data formats that store the data -->
<!-- in memory, such as the standard matrix class in base R and sparse and dense -->
<!-- matrix classes from the Matrix R package [29], and with file-backed matrices, -->
<!-- e.g., by using the HDF5 file format [24]. In addition, the package provides -->
<!-- methods to interface with standard Bioconductor data containers such as the -->
<!-- SummarizedExperiment [30] and SingleCellExperiment [31] classes. We implemented -->
<!-- the computationally most intensive steps of our algorithm in C++, leveraging the -->
<!-- Rcpp [32] and beachmat [33] packages. Furthermore, we make use of Bioconductor’s -->
<!-- DelayedArray [34] framework, and in particular the HDF5Array [35] package to -->
<!-- interface with HDF5 files. The mbkmeans package was built in a modular format -->
<!-- that would allow it to easily operate on alternative on-disk data -->
<!-- representations in the future. To initialize the k centroids, the mbkmeans -->
<!-- package uses the k-means++ initialization algorithm [36] with a random subset of -->
<!-- b observations (the batch size), by default. Finally, to predict final cluster -->
<!-- labels, we use block processing through the DelayedArray [34] package to avoid -->
<!-- working with all the data at once." [@hicks2021mbkmeans] -->

<!-- > "A major challenge in the analysis of scRNA-seq data is the scalability of analysis methods as datasets increase in size over time. This is particularly problematic as experiments now frequently produce millions of cells [50–53], possibly across multiple batches, making it challenging to even load the data into memory and perform downstream analyses including quality control, batch correction and dimensionality reduction. Providing analysis methods, such as unsupervised clustering, that do not require data to be loaded into memory is an imperative step for scalable analyses. While large-scale scRNA-seq data are now routinely stored in on-disk data formats (e.g. HDF5 files), the methods to process and analyze these data are lagging." [@hicks2021mbkmeans] -->

<!-- > "Unlike other existing implementations of mini-batch k-means, our algorithm harnesses the structure of the mini-batch k-means algorithm to only read in the data needed for each batch, controlling memory usage for large datasets. This makes our implementation truly scalable and applicable to both standard in-memory matrix objects, including sparse matrix representations, and on-disk data representations that do not require all the data to be loaded into memory at any one time, such as HDF5 matrices." [@hicks2021mbkmeans] -->


<!-- > "Another way of reducing memory use is to store your data in a database and -->
<!-- only extract portions of the data into R as needed. While this takes some time -->
<!-- to set up, it can become quite a natural way to work." [@burns2011r] -->

<!-- > "Are tomorrow’s bigger computers going to solve the problem? For some people, -->
<!-- yes—their data will stay the same size and computers will get big enough to -->
<!-- hold it comfortably. For other people it will only get worse—more powerful -->
<!-- computers means extraordinarily larger datasets. If you are likely to be in this -->
<!-- latter group, you might want to get used to working with databases now." [@burns2011r] -->

<!-- > "Traditionally, the assay data are stored in-memory as an ordinary array object3. Storing the data in-memory becomes a real pain with the ever-growing size of ’omics datasets. It is now not uncommon to collect 10,000--100,000,000 measurements on 100--1,000,000 samples, which would occupy 10--1,000 gigabytes (Gb) if stored in-memory as ordinary R arrays. -->
<!-- The DelayedArray framework offers a solution to this problem. Wrapping an array-like object (typically an on-disk object) in a DelayedArray object allows one to perform common array operations on it without loading the object in memory. In order to reduce memory usage and optimize performance, operations on the object are either delayed or executed using a block processing mechanism." http://biocworkshops2019.bioconductor.org.s3-website-us-east-1.amazonaws.com/page/DelayedArrayWorkshop__Effectively_using_the_DelayedArray_framework_for_users/ -->

<!-- > "The DelayedArray framework enables the analysis of datasets that are too large to be stored or processed in-memory. This has become particularly relevant with the advent of large single-cell RNA-sequencing (scRNA-seq) studies containing tens of thousands to millions of cells." http://biocworkshops2019.bioconductor.org.s3-website-us-east-1.amazonaws.com/page/DelayedArrayWorkshop__Effectively_using_the_DelayedArray_framework_for_users/ -->

<!-- > "The data contained in an HDF5Matrix is actually stored on disk in a Hierarchical Data Format (HDF5) file. Consequently, the tenx_counts object takes up very little space in memory." http://biocworkshops2019.bioconductor.org.s3-website-us-east-1.amazonaws.com/page/DelayedArrayWorkshop__Effectively_using_the_DelayedArray_framework_for_users/ -->

<!-- > "The subsetting operation has been registered in what is termed a ‘delayed operation’. Registering a delayed operation does not modify the underlying data. Instead, the operation is recorded and only performed when the DelayedArray object is ‘realized’. Realization of a DelayedArray triggers the execution of the delayed operations carried by the object and returns the result as an ordinary array. -->
<!-- This allows us to chain together multiple operations and only perform them as required. ... To *realize* a DelayedArray object is to trigger execution of the delayed operations carried by the object and return the result as an ordinary array. ... A large DelayedArray object is preferably realized on disk, which is most commonly an HDF5 file." http://biocworkshops2019.bioconductor.org.s3-website-us-east-1.amazonaws.com/page/DelayedArrayWorkshop__Effectively_using_the_DelayedArray_framework_for_users/ -->

<!-- > "Hopefully you can now begin to see the general pattern, a strategy which the DelayedArray package calls 'block-processing': 1. Load a 'block' of the data into memory. -->
<!-- 2. Compute a summary statistic. -->
<!-- 3. Combine the block-level statistics in an appropriate way to get the final result." -->
<!-- http://biocworkshops2019.bioconductor.org.s3-website-us-east-1.amazonaws.com/page/DelayedArrayWorkshop__Effectively_using_the_DelayedArray_framework_for_users/ -->


<!-- > "Let’s move onto something a little more computationally challenging. Proper normalization is essential for all analyses of gene expression data. We apply the deconvolution method of Lun, Bach, and Marioni (2016) to compute size factors for all cells. -->
<!-- For highly heterogeneous datasets, like this sample of PBMCs, it is advisable to perform a rough clustering of the cells to better satisfy the assumptions of this normalization method. Namely, we want to avoid normalizing together cells with a large number of differentially expressed genes between them. -->
<!-- We will use the scran::quickCluster() function to perform a clustering based on the principal component scores generated from the log-expression matrix. This principal component analysis (PCA) in turn uses an approximate singular value decomposition (SVD) with the augmented implicitly restarted Lanczos bidiagonalization algorithm (irlba). If that all sounds rather complicated, then don’t worry: that’s the point of this example! We are able to apply these cutting edge techniques to our HDF5-backed data." http://biocworkshops2019.bioconductor.org.s3-website-us-east-1.amazonaws.com/page/DelayedArrayWorkshop__Effectively_using_the_DelayedArray_framework_for_users/ -->

<!-- > "A HDF5-backed SummarizedExperiment, like the 10x PBMC dataset we analysed in Real world encounter with DelayedArray analysing scRNA-seq data, is a light-weight shell (the SummarizedExperiment) around a large disk-backed data matrix (the HDF5Matrix)." http://biocworkshops2019.bioconductor.org.s3-website-us-east-1.amazonaws.com/page/DelayedArrayWorkshop__Effectively_using_the_DelayedArray_framework_for_users/ -->

<!-- > "A SummarizedExperiment derivative can have one or more of its assays that point to datasets (one per assay) in an HDF5 file. ... These objects have 2 parts: one part is in memory, and one part is on disk. The 1st part is sometimes called the object shell and is generally thin (i.e. it has a small memory footprint). The 2nd part is the data and is typically big. The object shell and data are linked together via some kind of pointer stored in the shell (e.g. an SQLite connection, or a path to a file, etc.). Note that this is a one way link in the sense that the object shell 'knows' where to find the on-disk data but the on-disk data knows nothing about the object shell (and is completely agnostic about what kind of object shell could be pointing to it). Furthermore, at any given time on a given system, there could be more than one object shell pointing to the same on-disk data. These object shells could exist in the same R session or in sessions in other languages (e.g. Python). These various sessions could be run by the same or by different users." http://biocworkshops2019.bioconductor.org.s3-website-us-east-1.amazonaws.com/page/DelayedArrayWorkshop__Effectively_using_the_DelayedArray_framework_for_users/ -->

<!-- > "For example, a normalized scRNA-seq dataset carries around two matrices: the raw counts and the normalized expression values. You might have enough RAM to load one of these a time but not both at once. With a HDF5-backed SingleCellExperiment you can easily just load into memory the matrix you actually need at a given step in the analysis." http://biocworkshops2019.bioconductor.org.s3-website-us-east-1.amazonaws.com/page/DelayedArrayWorkshop__Effectively_using_the_DelayedArray_framework_for_users/ -->

<!-- > "Don’t use a DelayedArray if you can avoid it! ... If you can load your data into memory and still compute on it then you’re always going to have a better time doing it that way. Analyses will be faster, simpler, and you will have more options available to you. But when this isn’t an option then the DelayedArray framework is a powerful set of packages to help you get your work done. I find it pretty remarkable that a first-class single-cell analysis workflow can so seamlessly support the use of in-memory and disk-backed data." http://biocworkshops2019.bioconductor.org.s3-website-us-east-1.amazonaws.com/page/DelayedArrayWorkshop__Effectively_using_the_DelayedArray_framework_for_users/ -->


There is a big advantage to having stricter standards for what parts of data go
where when it comes to writing functions that can be used across a lot of data.
You can think of this in terms of how cars are set up versus how kitchens are
set up. Cars are very standardized in the "interface" that you get when you sit
down to drive them. The gas and brakes are typically floor pedals, with the gas
to the right of the brake. The steering is almost always provided through a
wheel centered in front of the driver's torso. The mechanism for shifting gears
(e.g., forward, reverse) is typically to the right of the steering wheel, while
mecahnisms for features like lights and windshield wipers, are typically to the
left of the steering wheel. Because this interface is so standardized, you can
get into a car you've never driven before and typically figure out how to drive
it very quickly. You don't need a lot of time exploring where everything is or a
lot of directions from someone familiar with the car to figure out where things
are. Think of the last time that you drove a rental car---within five minutes,
at most, you were probably able to orient yourself to figure out where
everything you needed was. This is like a dataframe in R---you can pretty
quickly figure out where everything you might need is stored in the data
structure, and people can write functions to use with these dataframes that work
well generally across lots of people's data because they can assume that certain
pieces of data are in certain places.

By contrast, think about walking into someone else's kitchen and orienting
yourself to use that. Kitchen designs do tend to have some general
features---most will have a few common large elements, like a stove somewhere, a
refrigerator somewhere, a pantry somewhere, and storage for pots, pans, and
utensils somewhere. However, there is a lot of flexibility in where each of
these are in the kitchen design, and further flexibility in how things are
organized within each of these structures. If you cook in someone else's
kitchen, it is easy to find yourself disoriented in the middle of cooking a
recipe, where a utensil that you can grab almost without thinking in your own
kitchen requires you to stop and search many places in someone else's kitchen.
This is like a list in R---there are so many places that you can store data in a
list, and so much flexibility, that you often find yourself having to dig around
to find a certain element in a list data structure that someone else has
created, and you often can't assume that certain pieces are in certain places if
you are writing your own functions, so it becomes hard to write functions that
are "general purpose" for generic list structures in R.

Robert Gentleman, one of the creators of Bioconductor [?], highlights
this as a key reason for using these more complex data structures. 
R is an open-source language, and it allows contributions from 
software developers worldwide. The Bioconductor project leverages the
base R software to build tools for working with complex genomic [?]
data, also allowing for the development of new extensions from 
developers worldwide. The software extensions are developed, therefore, 
not by a unified team that is all employed at the same place, but 
instead by a collection of people who are making software as part of 
their research program, or perhaps as part of an industry job, and who
write that software with the intent of having it work smoothly with 
other software packages that are available through Bioconductor. 

If you have ever been in a role where you organized volunteers to 
perform a large task, you can probably imagine how challenging it is to 
design a project in a way that allows for this type of software 
development to work. The use of specialized data structures is one
element that helps in this coordination for Bioconductor. It enforces a 
standard that all the programmers can work toward---they can work under
the assumption that the output from other functions in the Bioconductor
ecosystem will be structured in a certain way, and so they can work 
under the assumption that certain elements of the data will be in certain
spots in the data structure of the object.

[Example of the party game where you each add a sentence, only seeing part of
the story. Exquisite corpse. Surrealist game you could do with either writing or
drawing (later people adapted to other things, like music composition). With
drawing, you are typically drawing a person or other being, with the defined
parts of a head, torso, and legs. One person does a part (the head area), then
folds so the next person can only see enough to connect with what they did, then
the next person draws the next part (the torso area), again folds so the next
person can only see enough to connect, and so on. This results in writing or a
drawing that is connected across the different parts, but can be completely
independent within each part (and unrelated to other parts). Collaborative, 
combines different parts made by different people.]

---------------------------------------------------------------------------

You can think of *data structures* as containers that hold your data in R, 
holding it in a way that lets you access and work with the data. 


Data structures, on the other hand, allow you to keep together, as well as
refer to, data that you have loaded into R. A single object can contain 
pieces of data with different data types, and the object's data structure
defines how all the pieces of data in the object are organized and how you 
can access and work with the data in the structure. For example, one of the
simplest data structures in R is the vector---this data structure requires
that all the data stored in it have the same data type (e.g., all be 
numeric), and it holds together a one-dimensional string of pieces of data
of that type. For example, a vector of the numbers one to five would be 
the string of those numbers---1, 2, 3, 4, 5---while a 
vector of the letters a to e would be the string of data with the "character"
type---"a", "b", "c", "d", "e". 

A second key data structure in R, the 
dataframe, provides a structure that stores one or more of these vectors, 
and so it allows you to store data with different types in the same object. 
For example, you could create an object with a dataframe structure that 
contains one column with a vector of the numbers one to five and another 
with the letters a through e. This structure would look something like this:

```{r echo = FALSE}
letters_numbers <- data.frame(numbers = 1:5, letters = c("a", "b", "c", "d", "e"))
letters_numbers
```

While the dataframe structure can combine vectors with data in different types
(in the example, the `numbers` column has a numeric data type and the `letters`
column has a character data type), it does have a rule for combining different
vectors. They all must be the same length. This means that the dataframe 
structure is always rectangular, with each column having the same number of 
rows. In the example above, both the `numbers` and `letters` columns are 
vectors with five values, so the dataframe ends up having two columns and 
five rows. 

The two-dimensional structure of a dataframe keeps the values
measured for each observation lined up with each other, and lets you keep them
aligned as you work with the data. You could also store data for each value as
separate objects, in one-dimensional vectors, which you can visualize as strings
of values of the same data type, like the dates that each observation was made,
or the weight of each study subject. However, when the data is in separate
vectors, it is easy to make coding mistakes, and coding is often less efficient.
If you want to remove one observation, for example, because you find it is a
duplicate, you would need to carefully make sure you remove it correctly from
each vector. When data are stored in a dataframe, you can remove the row for
that observation with one command, and you can be sure that you've removed the
value you meant to from each of the measured values. 

**"Tidy" data and data structures**

In this module, we aim to introduce you to something called the "tidyverse" 
approach to programming in R. This is a powerful approach, and we will 
detail it more extensively in the next section. Here, we want to introduce 
a key element of the approach---it is based on storing data in a single
data structure throughout your pipeline of code. Specifically, it focuses
on tools and techniques that work when you use a dataframe structure to 
store data as you work with the data, with a set of functions that both 
input and output objects that use the dataframe structure (as well as 
functions that input and output vectors, which are used to perform 
operations on the data in the columns within the dataframe---if you recall, 
each column in a dataframe has a vector data structure). 

The tidyverse approach requires one step beyond the data structure, and that
is how the data are arranged within that structure. You can store data in 
a dataframe structure as long as you can put it in two dimensions (rows and 
columns), with the same type of data in each column. With the same set of 
data, there are often many different arrangements you could make that satisfy 
these constraints. The "tidy" part of the "tidy data" structure---which is 
at the heart of the tidyverse approach---are a set of standards describing
exactly how you arrange the data within the dataframe structure. 

The R object class---dataframe, and more specifically, tibble---of the standard
format for data for a tidyverse approach is just the first part of the standard 
data format for the tidyverse approach. The second part of the standard format is 
how you organize your data in this format. To easily work with tidyverse functions, 
you'll want to make sure that your data is stored within that dataframe following
"tidy" data principals. These are fully described in an earlier module in this
book [which module]. If you use this data format to initially collect your
data, as described in an earlier module, you will find it very easy to read the
data into R and work within the tidyverse approach. When working with larger and
more complex data collected from laboratory equipment, you may find you need to 
do some preprocessing of the data using an object-oriented approach before you 
can move the data into this tidy format, but at that point, you can continue with
analysis and visualization of your data using a tidyverse approach. 

**The tibble data structure**

As a final note, there is a specialized data structure that is often 
associated with the tidyverse approach. It is called the "tibble", and it is 
a specialized version of the dataframe. What do we mean by it being a 
"specialized" version of a more common data structure? This means that there 
are a few functions that will give different results if the data are stored
in a tibble structure rather than the more generic dataframe structure. 
However, if a function does not have a specialized method for the tibble 
structure (and most functions don't), then the function will treat the object
as a regular data frame. Some of the few functions that have specialized
methods for tibbles include the `print` function, which is also the default
function that is run if you just type an object's name at the console and 
press "Enter". The `print` function, when called with an object in the tibble
structure, will only print out the first few rows (whereas, with a generic 
dataframe, it will print out all rows, sometimes resulting in a very long
print-out). Similarly, if there are many columns, it will only print out 
a certain number and just provide summaries for the rest (again, with a 
generic dataframe, everything would be printed). It also provides some nice
summaries of the dataframe as a whole, as well as of the type of data in each 
column. Overall, the `print` method for a tibble structure provides a clearer
and typically more useful overview of the data in the object than does the
`print` method for a generic dataframe structure. All this being said, as 
you first begin to work in the tidyverse approach, you often may not notice
whether your data is stored in a more generic dataframe structure or the 
more specialized tibble version, and it often won't matter much which of the
two structures is being used, since the tibble structure in most cases 
(i.e., for most functions) is just treated as the more generic dataframe
structure. 

Sometimes, you'll see that data in a tidyverse approach are stored in a special
type of dataframe called a "tibble"---this isn't very different from a
dataframe, and in fact is a special type of dataframe. It's only differences in
practice are that it has a slightly different `print` method. The `print` method
is the method that's run, by default, when you just type the R object's name 
at the console. A tibble prints more nicely than a basic dataframe. By default, 
it will only print the first few lines. By contrast, a dataframe will, by default, 
print everything---if you have a lot of data, this can create an overwhelming
amount of output when you just want to check out what the data looks like. The
printout of a tibble will also include some interesting annotations to help you 
see what's in the data, including the dimensions of the full dataframe and the 
data type of each column in the data. 

Sometimes, you'll see that data in a tidyverse approach are stored in a special
type of dataframe called a "tibble"---this isn't very different from a
dataframe, and in fact is a special type of dataframe. It's only differences in
practice are that it has a slightly different `print` method. The `print` method
is the method that's run, by default, when you just type the R object's name 
at the console. A tibble prints more nicely than a basic dataframe. By default, 
it will only print the first few lines. By contrast, a dataframe will, by default, 
print everything---if you have a lot of data, this can create an overwhelming
amount of output when you just want to check out what the data looks like. The
printout of a tibble will also include some interesting annotations to help you 
see what's in the data, including the dimensions of the full dataframe and the 
data type of each column in the data. 

> "In parallel with this chronological directory structure, I find it useful to maintain a chronologically organized lab notebook. This is a document that resides in the root of the results directory and that records your progress in detail. Entries in the notebook should be dated, and they should be relatively verbose, with links or embedded images or tables displaying the results of the experiments that you performed. In addition to describing precisely what you did, the notebook should record your observations, conclusions, and ideas for future work. Particularly when an experiment turns out badly, it is tempting simply to link the final plot or table of results and start a new experiment. Before doing that, it is important to document how you know the experiment failed, since the interpretation of your results may not be obvious to someone else reading your lab notebook." [@noble2009quick]

---------------------------------------------------------------------------------


It can be helpful to walk through some examples some of these preprocessing
steps, in the context of specific types of biomedical experiments. In this
section, we'll give an overview of the types of preprocessing steps you might 
need in two example experiments. In later modules, we'll go more in depth
into how this could be done in coded scripts using open-source software. 

**Simpler example: Measuring bacterial growth rates**

The first example is when you have collected data to measure bacterial 
growth rates. For example, you may be interested in how different the 
growth rate of *Mycobacterium tuberculosis* is in an environment with 
ample oxygen compared to low-oxygen environments, as this may have 
important implications for how fast the bacteria grow in certain parts
of the human body. You can collect this data by ..., saving data on 
... in a spreadsheet or other file. Once you have collected the 
data, you would then need to take a set of steps to preprocess it
and then analyze it, to allow you to answer your original research 
question (how much the growth rate differs in the low-oxygen 
environment). 

[Module 2.5]

Preprocessing and analysis steps: 

- Input data from spreadsheet in which it was recorded in the laboratory
(preprocessing)
- For each sampling time, use the date and time of the sampling to calculate
the time since the start of the experiment (preprocessing)
- Identify the period of exponential growth (preprocessing)
- Calculate growth rate for each sample during this period of exponential 
growth (analysis)

**More complex example: Characterizing lung cell populations**

The second example is when you have collected single-cell RNA-seq
data from a sample with the aim of characterizing the cell populations
in the animal's lungs.

One example of a pipeline of pre-processing and analysis might include
[@luecken2019current]:

- Read in raw sequencing data 
- Generate count matrices from the raw sequencing data [what count matrices are]
- Quality control to identify and remove low quality cells from further 
analysis, as well as to remove some transcripts (e.g., those that appear in 
only a few cells)
- Normalization to allow meaningful comparisons across cells, accounting for differences across cells in capture efficiency, amplification (reverse transcription?), sequencing, etc. This may involve processes that assume the 
number of mRNA molecules in each cell is the same, or alternatively that 
half or fewer of the genes are differentially expressed across cells.
- Batch collection
- Dimension reduction, to focus only on highly variable genes and exclude
from further analysis genes that have a similar expression across cells
(e.g., "housekeeping" genes)
- Clustering and cluster annotation, to identify different cell populations
- Differential expression analysis, to determine which genes have different
levels of expression in different cell populations [?]
- Comparing cell populations across samples from different conditions (e.g., 
treated versus control)




### Example dataset

In this module and several of the following modules, we'll use two example 
datasets that are based on real data collected from an immunology experiment. 
These data will help us to describe and motivate the steps of pre-processing
data using code scripts, as well as explaining the use of tidyverse tools
and the discussion of complex versus tidy data formats in R. 

The first dataset was introduced in module [x]. ...

The second dataset have not yet been introduced. They come from an experiment to
test a novel vaccine for COVID-19, and we'll be focusing on data collected
during this experiment that measured single-cell transcriptomics---in other
words, it characterizes the levels of messenger RNA with each of thousands of
cells collected from animals in different experimental groups. In this section,
we'll give you more details to help you understand this example dataset, as well
as instructions on how to download the data on your own computer, if you would
like to follow along with examples.

The results of this experiment have been published [ref], and so you can read
the full details in the published paper, but we'll provide an overview here.
This experiment tested a potential vaccine called SolaVAX. There are numerous
ways to make vaccines; this one uses an attenuated (in other words, weakened)
version of the full virus, and it is novel in that it attenuates the virus using
a light-activated ... (hence the "Sola" in "SolaVAX"). 

This experiment tested how well this vaccine worked, not only by itself, but
also when it was given in conjunction with something called an adjuvant. In the
context of vaccines, an adjuvant is a substance that is meant to shape the
body's immune response as it "learns" from a vaccine. For example, an adjuvant
can be something as simple as a substance that triggers a larger immune response
than the vaccine by itself, to ensure that the immune system responds at a
sufficient scale to the core components of the vaccine---that is, the components
of the vaccine that the immune system needs to recognize in the future to mount
a fast defense against that pathogen. This experiment tested two adjuvants
in conjunction with the SolaVAX vaccine, both of which the researchers were 
hoping might help in switching which type of T helper immune cells would 
drive the later response to COVID after vaccination. Specifically, they 
hypothesized that the adjuvants would bring about a later response that was
driven more by a type of T helper cell called Th2 rather than one called 
Th1. [Why this would be good.]

To run the experiment, the researchers used Golden Hamsters as a model 
animal. [Why?] They created four experimental groups: one control group, 
one group vaccinated with only SolaVAX, one vaccinated with SolaVAX plus
an adjuvant called [x], and one vaccinated with SolaVAX plus an adjuvant
called [y]. There were eight hamsters in each of these groups, and these
were further divided into two groups of four, so that the vaccine could 
be tested using two routes of administration: [the two routes]. 

The hamsters were vaccinated, and then after a period of time, they were
challenged with COVID. This allowed the researchers to see how successful 
each vaccination type was, in terms of how well the animals could limit
the replication of COVID in their bodies, and also to explore how the 
animals' immune systems responded as they tried to limit COVID replication
after exposure. Therefore, this experiment could help not only in seeing
which vaccine strategies were successful, but also to explore how and why 
they did or didn't work, at the level of the immune response. 

[x] days after the animals were exposed to COVID, they were sacrificed, 
and the researchers took samples from several areas to use to measure
levels of COVID as well as the immune response to the challenge. In tissue
samples from the lungs of the animals, they measured things like the 
[viral numbers?] to see the extent of COVID replication in that animal, 
and [histopathology], to see the extent of damage that the infection had
done to the animal's lungs. If the vaccine were successful, it would have
spurred a fast and powerful immune response, which you'd see through 
lower [viral numbers] and less damage to the animal's lungs.
They also used part of the lung sample to measure immune cell populations.
These measures can help to determine things like whether the immune 
response was driven more by the innate immune response (which you'd 
expect in an unvaccinated animal) or the adaptive immune response 
(which you hope to see in a vaccinated animal, as a sign that the vaccine
helped in allowing for the immediate immune response to be much more 
specific than that achieved by the innate immune system). 

Out of the many types of data that were collected for this experiment, we'll
focus on one type in our examples in this and following modules---data that were
collected that measured the single-cell transcriptomics of samples collected
from the lungs of each animal. While every cell in a body has the same genome,
the cells differ in which of those genes they express at a given time. The
expression of different genes in the genome can be measured based on the number
of messenger RNAs that are in the cell from each gene. This is what single cell
RNA sequencing aims to measure---the number of mRNAs from each of a large number
of genes, measuring the number in each cell in the sample separately. Because
different types of cells express different patterns and levels of genes, these
data can be used to help sort the cells into different types. For example, the
data can be used to help identify cells that are from the innate immune system
versus those that are hallmarks of an adaptive immune response. The data can
also help in categorizing cells within very specific cell categories---for
example, identifying Th1 versus Th2 cells out of the group of helper T cells,
which can help to address the hypothesis that these researchers had about how
the adjuvants might work. Finally, the data can help to identify how certain
types of cells work differently under different experimental conditions. For
example, do adaptive immune cells tend to express different genes (or different
levels of a gene) when the vaccination included an adjuvant versus when only the
vaccine was given?

These single cell RNA sequencing data were shared by the researchers through 
one of NIH's [?] databases for biological data. To get a copy of these 
data, go to the study's page on the Gene Expression Omnibus (GEO) database: 
https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE165190 (you can also 
search for the study on the database by its accession number, GSE165190). 
This page includes data from single cell
RNA sequencing for twelve samples from this study, three from each of the 
experimental conditions. You can download data for all the samples by clicking
the link on the page to download the file named "GSE165190_RAW.tar". This is 
a compressed file, so once you download it, you will need to uncompress it, 
which will give you a folder with a number of data files. On a Mac, you can 
decompress the file by double-clicking on the file named "GSE165190_RAW.tar"
in the Finder program. On a Windows computer, [how to do it. From R?]

Among the data files, there are three files for each sample. One includes
"barcodes" in the file name, one includes "features" in the file name, and one
includes "matrix" in the file name. For each sample, the "barcodes" file gives
..., the "features" file gives ..., and the "matrix" file gives ... . 

[More about working with these files?]

Collectively, these data provide a count that is related to the number of 
messanger RNA particles [?] from each of approximately [x] genes within each
cell in the sample. These counts can be used to group the cells into 
groups of similar cell types, identify those cell types, and explore how 
the gene expression within a cell type varies across experimental conditions. 

These data can therefore be used to answer interesting scientific questions.
However, before they can, it is important to do some pre-processing on the raw
data. This preprocessing serves several purposes. First, there are patterns
in the data that can be introduced as a result of the data collection process. 
These need to be corrected or otherwise accounted for to move the data into
a format where it can be meaningfully compared, for example across different
cells in a sample or across different samples. Second, some of the pre-processing
will help identify and resolve any issues related to quality control. For 
example, while the process of collecting these data will normally result in 
isolating single cells, in some cases the values for a cell might be for a 
poor-quality cell (for example, a dying cell) or might be a case where two 
cells were captured together [true for this platform?]. Pre-processing can 
help to identify and exclude these low-quality data points, so that the 
analysis can focus on the higher-quality data collected for the sample. 
Finally, some of the pre-processing will help us to prepare the data to be
used in analysis algorithms. For example, since the data is high dimensional
(that is, measurements are included for many genes [?]), we will often include
a step of dimension reduction in our analysis, to help pull out key patterns
in the data. Many of the dimension reduction algorithms will need to data to 
be scaled, so that the mean value of each measurement in a type of measurement
has an average of zero and a standard deviation [? variance?] of one. 

> "As a proxy for studying the proteome, many researchers have turned to 
protein-encoding, mRNA molecules (collectively termed the 'transcriptome'), 
whose expression correlates well with cellular traits and changes in 
cellular state. Transcriptomics was initially conducted on ensembles of 
millions of cells, firstly with hybridization-based microarrays, and later
with next-generation sequencing (NGS) techniques referred to as RNA-seq. 
RNA-seq on pooled cells has yielded a vast amount of information that 
continues to fuel discovery and innovation in biomedicine. ... 
Nevertheless, the averaging that occurs in pooling large numbers of 
cells does not allow detailed assessment of the fundamental biological 
unit---the cell---or the individual nuclei that package the genome.
Since the first scRNA-seq study was published in 2009, there has been 
increasing interest in conducting such studies. Perhaps one of the most
compelling reasons for doing so is that scRNA-seq can describe RNA molecules
in individual cells with high resolution and on a genomic scale." 
[@haque2017practical]

> "scRNA-seq permits comparison of the transcriptomes of individual cells.
Therefore, a major use of scRNA-seq has been to assess transcriptional
similarities and differences within a population of cells, with early reports
revealing unappreciated levels of heterogeneity... Thus, heterogeneity analysis
remains a core reason for embarking on scRNA-seq studies. Similarly, assessments
of transcriptional differences between individual cells have been used to
identiy rare cell populations that would otherwise go undetected in analyses of
pooled cells, for example malignant tumour cells within a seemingly homogeneous
group. ... In addition to resolving cellular heterogeneity, scRNA-seq can also
provide important informaiton about fundamental characteristics of gene
expression... Importantly, studying gene co-expression patterns at the
single-cell level might allow identification of co-regulated gene modules and
even inference of gene-regulatory networks that underlie functional
heterogeneity and cell-type specification." [@haque2017practical]

> "Single-cell RNA sequencing (scRNA-seq) is a recent and powerful technology 
developed as an alternative to previously existing bulk RNA sequencing methods. 
Bulk sequencing methods analyzed the average genetic content for individual 
genes across a large population of input cells within a sample (e.g., a tissue), 
potentially obscuring transcriptional features and other differences among 
individual cells. Conversely, scRNA-seq is able to discern such heterogeneous properties
within a sample and has great potential to reveal novel subpopulations and 
cell types." [@lytal2020normalization]

> "Single-cell RNA sequencing is widely used for high-resolution gene expression 
studies investigating the behavior of individual cells." [@mccarthy2017scater]

> "While scRNA-seq data can provide substantial biological insights, the complexity
and noise of the data is also much greater than that of conventional bulk RNA-seq.
Thus, rigorous analysis of scRNA-seq data requires careful quality control to remove
low-quality cells and genes, as well as normalization to adjust for biases and 
batch effects in the expression data. Failure to carry out these procedures 
correctly is likely to compromise the validity of all downstream analyses."
[@mccarthy2017scater]

> "Conventional 'bulk' methods of RNA sequencing (RNA-seq) process hundreds of
thousands of cells at a time and average out the differences. But no two cells
are exactly alike, and scRNA-seq can reveal the subtle changes that make each 
one unique. It can even reveal entirely new cell types." [@perkel2017single]

> "It's much more difficult to manipulate individual cells than large populations, 
and because each cell yields only a tiny amount of RNA, there's no room for 
error. Another problem is analyzing the enormous amounts of data that result---not
least because the tools can be unintuitive." [@perkel2017single]

### Scripting preprocessing tasks

Code scripts can be developed for any open-source scripting languages, including
Python, R, and Julia. These can be embedded in or called from literate programming
documents, like RMarkdown and Julia, which are described in other modules. The 
word "script" is a good one here---it really is as if you are providing the script
for a play. In an interactive mode, you can send requests to run in the programming
language step by step using a console, while in a script you provide the whole list
of all of your "lines" in that conversation, and the programming language will run 
them all in order without you needing to interact from the console. 

For preprocessing the data, the script will have a few predictible parts. First,
you'll need to read the data in. There are different functions that can be used
to read in data from different file formats. For example, data that is stored in
an Excel spreadsheet can be loaded into R using functions in a package called
`readxl`. Data that is stored in a plain-text delimited format (like a csv file)
can be loaded into R using functions in the `readr` package.

When preprocessing data from complex equipment, you can determine how to read the
data into R by investigating the file type that is output by the equipment. 
Fortunately, many types of scientific equipment follow standardized file formats. 
This means that open-source developers can develop a single package that can 
load data from equipment from multiple manufacturers. For example, flow cytometry 
data is often stored in [file format]. Other biological datasets use file 
formats that are appropriate for very large datasets and that allow R to work 
with parts of the data at a time, without loading the full data in. [netCDF?]
In these cases, the first step in a script might not be to load in all the data, 
but rather to provide R with a connection to the larger datafile, so it can 
pull in data as it needs it. 

Once the data is loaded or linked in the script, the script can proceed through
steps required to preprocess this data. These steps will often depend on the type
of data, especially the methods and equipment used to collect it. For example, for
mass spectrometry data, these steps will include ... . For flow cytometry data, 
these steps would include ... . 

The functions for doing these steps will often come from extensions that
different researchers have made for R. Base R is a simpler collection of data
processing and statistics tools, but the open-source framework of R has allowed
users to make and share their own extensions. In R, these are often referred to
as "packages". Many of these are shared through the Comprehensive R Archive
Network (CRAN), and packages on CRAN can be directly installed using the
`install.packages` function in R, along with the package's names. While CRAN
is the common spot for sharing general-purpose packages, there is a specialized
repository that is used for many genomics and other biology-related R packages
called Bioconductor. These packages can also be easily installed through a call 
in R, but in this case it requires an installation function from the `BiocManager`
package. Many of the functions that are useful for preprocessing biological 
data from laboratory experiments are available through Bioconductor.

Table [x] includes some of the primary R packages on Bioconductor that can be
used in preprocessing different types of biological data. There are often
multiple choices, developed by different research groups, but this list provides
a starting point of several of the standard choices that you may want to
consider as you start developing code.

Much of the initial preprocessing might use very specific functions that are
tailored to the format that the data takes once it is loaded. Later in the
script, there will often be a transfer to using more general-purpose tools in
that coding language. For example, once data is stored in a "dataframe" format
in R, it can be processed using a powerful set of general purpose tools
collected in a suite of packages called the "tidyverse". This set of packages
includes functions for filtering to specific subsets of the data, merging
separate datasets, adding new measurements for each observation that are
functions of the initial measurements, summarizing, and visualizing. The
tidyverse suite of R tools is very popular in general R use and is widely
taught, including through numerous free online resources. By moving from
specific tools to these more general tools as soon as possible in the script, a
researcher can focus his or her time in learning these general purpose tools
well, as these can be widely applied across many types of data.

By the end of the script, data will be in a format that has extracted
biologically relevant measurements. Ideally, this data will be in a general
purpose format, like a dataframe, to make it easier to work with using general
purpose tools in the scripting language when the data is used in further data
analysis or to create figures for reports, papers, and presentations. Often, you 
will want to save a version of this preprocessed version of the data in your 
project files, and so the last step of the script might be to write out the 
cleaned data in a file that can be loaded in later scripts for analysis and 
visualization. This is especially useful if these data preprocessing steps are 
time consuming, as is often the case for the large raw datasets output by 
laboratory equipment like flow cytometers and mass spectrometers.

Figure [x] gives an example of a data preprocessing script, highlighting these
different common areas that often show up in these scripts.

[Some data may be incorporated into the preprocessing by downloading it from 
databases or other online sources. These data downloads can be automated and 
recorded by using scripted code for the download in many cases, as long as the 
database or online source offers web services or another API for this type of
scripted data access. In this case, you can incorporate the script in a 
RMarkdown document to record the date the data was downloaded, as well as the
code used to download it. R is able to run system calls, and one of these
will provide the current date, so this can be included in an RMarkdown file
to record the date the file is run. Further, there may be a call that can 
be made to the online data source's API that returns the working version of 
the database or source, and if so this can also be included in the RMarkdown
code used to access the data.]

RMarkdown files can be used to combine both code and more manual document
(for example, a record of which collaborator provided each type of data file).
While traditionally this more manual documentation was recommended to be 
recorded in plain-text README files in a project's directory and subdirectories
[@buffalo2015bioinformatics], RMarkdown files provide some advantages over
this traditional approach. First, RMarkdown files are themselves in plain
text, and so they offer the advantages of simple plain text documentation 
files (e.g., ones never rendered to another format) in terms of being able
to use script-based tools to search them. Further, they can be rendered into
attractive formatted documents that may be easier to share with project
team members who do not code. 

[Example of a function: recipe for making a vinaigrette. There will be a 
"basic" way that the function can run, which uses its default parameters. 
However, you can also specify and customize certain inputs (for example, 
using walnut oil instead of olive oil, or adding mustard) to tweak the 
recipe in slight ways each time you use it, and to get customized outputs.]

[History of the mouse---enable GUIs, before everything was from the terminal.]

> For bioinformatics, "all too often the software is developed without
thought toward future interoperability with other software products. As a 
result, the bioinformatics software landscape is currently characterized
by fragmentation and silos, in which each research group develops and uses
only the tools created within their lab." [@barga2011bioinformatics]

> "The group also noted the lack of agility. Although they may be aware of
a new or better algorithm they cannot easily integrate it into their 
analysis pipelines given the lack of standards across both data formats
and tools. It typically requires a complete rewrite of the code in order
to take advntge of a new technique or algorithm, requiring time and often
funding to hire developers." [@barga2011bioinformatics]

> "*Software* is the general term for sequences of instructions that make a computer
do something useful. It's 'soft' in contrast with 'hard' hardware, because it's 
intangible, not easy to put your hands on. Hardware is quite tangible: if you drop
a computer on your foot, you'll notice. Not true for software." [@kernighan2011d]



> "Modern system increasingly use general purpose hardware---a processor, some memory, 
and connections to the environment---and create specific behaviors by software. The
conventional wisdom is that software is cheaper, more flexible, and easier to change than
hardware is (especially once some device has left the factory)." [@kernighan2011d]


> "An algorithm is a precise and unambiguous recipe. It's expressed in terms of a fixed 
set of basic operations whose meanings are completely known and specified; it spells out
a sequence of steps using those operations, with all possible situations covered; it's 
guaranteed to stop eventually. On the other hand, a *program* is the opposite of 
abstract---it's a concrete statement of the steps that a real computer must perform to 
accomplish a task. The distinction between an algorithm and a program is like the difference
between a blueprint and a building; one is an idealization and the other is the real thing."
[@kernighan2011d]

> "One way to view a program is as one or more algorithms expressed in a form that a computer
can process directly. A program has to worry about practical problems like inadequate memory, 
limited processor speed, invalid and even malicious input data, faulty hardware, broken 
network connections, and (in the background and often exacerbating the other problems)
human frailty. So if an algorithm is an idealized recipe, a program is the instructions for 
a cooking robot preparing a month of meals for an army while under enemy attack." [@kernighan2011d]




> "A function has a name and a set of input data values that it needs to do its job; it does 
a computation and returns a result to the part of the program that called it. ... Functions
make it possible to create a program by building on components that have been created separately
and can be used as necessary by all programmers. A collection of related functions is usually 
called a *library*. ... The services that a function library provides are described to programmers
in terms of an *Application Programming Interface*, or *API*, which lists the functions, what
they do, how to use them in a program, what input data they require, and what values they 
produce. The API might also describe data structures---the organization of data that is passed
back and forth---and various other bits and pieces that all together define what a programmer
has to do to request services and what will be computed as a result. This specification must
be detailed and precise, since in the end the program will be interpreted by a dumb literal 
computer, not by a friendly and accomodating human." [@kernighan2011d]

> "An *operating system* is the software underpinning that manages the hardware of a 
computer and makes it possible to run other programs, which are called *applications*.
... It's a clumsy but standard terminology for programs that are more or less self-contained
and focused on a single task." [@kernighan2011d]

> "Programming languages share certain basic ideas, since they are all notations for spelling
out a computation as a sequence of steps. Every programming language thus will provide ways
to get input data upon which to compute; do arithmetic; store and retrieve intermediate 
values as computation proceeds; display results along the way; decide how to proceed on the basis 
of previous computations; and save results when the computation is finished. Languages have
*syntax*, that is, rules that define what is grammatically legal and what is not. 
Programming languages are picky on the grammatical side: you have to say it right or there
will be a complaint. Languages also have *semantics*, that is, a defined meaning for every 
construction in the language." [@kernighan2011d]

> "In programming, a *library* is a collection of related pieces of code. A library typically 
includes the code in compiled form, along with needed source code declarations [for C++]. 
Libraries can include stand-alone functions, classes, type declarations, or anything else that
can appear in code." [@spraul2012think]

> "It's a good idea to avoid referring to specific dataframe rows in your analysis code. 
This would produce code fragile to row permutations or new rows that may be generated
by rerunning a previous analysis step. In every case in which you might need to refer
to a specific row, it's avoidable by using subsetting... Similarly, it's a good idea
to refer to columns by their column name, *not* their position. While columns may be 
less likely to change across dataset versions than rows, it still happens. Column names
are more specific than positions, and also lead to more readable code."
[@buffalo2015bioinformatics]


> "One of the fundamental contributions of the Unix system [is] the idea of a *pipe*.
A pipe is a way to connect the output of one program to the input of another program
without any temporary file; a *pipeline* is a connection of two or more programs through
pipes. ... Any program that reads from a terminal can read from a pipe instead; any program
that writes on the terminal can write to a pipe. ... The programs in a pipeline actually 
run at the same time, not one after another. This means that the programs in a pipeline
can be interactive; the kernel looks after whatever scheduling and synchronization is needed
to make it all work. As you probably suspect by now, the shell arranges things when you
ask for a pipe; the individual programs are oblivious to the redirection." [@kernighan1984unix]

> "The features of R are organized into separate bundles called *packages*. The standard R 
installation includes about 25 of those packages, but many more can be downloaded from CRAN and
installed to expand the things that R can do. ... Once a package is installed, it must be 
*loaded* within an R session to make the extra features available. ... Of the 25 packages
that are installed by default, nine packages are *loaded* by default when we start a new
R session; these provide the basic functionality of R. All other packages must be loaded
before the relevant features can be used." [@murrell2009introduction]

> "The R environment is the software used to run R code." [@murrell2009introduction]

> "Programming in R is carried out, primarily, by manipulating and modifying data structures.
These different transformations are carried out using functions and operators. In R, 
virtually every operation is a function call, and though we separate our discussion into
operators and function calls, the distinction is not strong ... The R evaluator and
many functions are written in C but most R functions are written in R itself." 
[@gentleman2008r]


> "One way to write R code is simply to enter it interactively at the command line... This 
interactivity is beneficial for experimenting with R or for exploring a data set in a casual
manner. ... However, interactively typing code at the R command line is a very bad approach from 
the perspective of recording and documenting code because the code is lost when R is shut down. 
A superior approach in general is to write R code in a file and get R to read the code from the file." [@murrell2009introduction]

> "Languages have
*syntax*, that is, rules that define what is grammatically legal and what is not. 
Programming languages are picky on the grammatical side: you have to say it right or there
will be a complaint." [@kernighan2011d]


> "That a language is easy for the computer expert does not mean it is
necessarily easy for the non-expert, and it is likely non-experts who will do
the bulk of the programming (coding, if you wish) in the near future."
[@hamming1997art]

> "What is wanted in the long run, of course, is that the man with the problem 
does the actual writing of the code with no human interface, as we all to often 
have these days, between the person who knows the problem and the person who
knows the programming language. This date is unfortunately too far off to 
do much good immediately, but I would think by the year 2020 it would be
fairly universal practice for the expert in the field of application to do 
the actual program preparation rather than have expers in computers (and 
ignorant in the field of application) do the program preparation." [@hamming1997art]

-------------------------------------------------------------------------------

**Data pre-processing**

When we take measurements of experimental samples, we do so with the goal of
using the data we collect to gain scientific knowledge. The data are direct
measurement of something, but need to be interpreted to gain knowledge.
Sometimes direct measurements line up very closely with a research
question---for example if you are conducting a study that investigates the
mortality status of each test subject then whether or not each subject to dies
is a data point that is directly related to the research question you are aiming
to answer. In this case these data may go directly into a statistical analysis
model without extensive pre-processing. However, there are often cases where we
collect data that are not as immediately linked to the scientific question.
Instead, these data may require pre-processing before they can be used to test
meaningful scientific hypotheses. This is often the case for data extracted
using complex equipment. Equipment like mass spectrometers and flow cytometers
leverage physics, chemistry, and biology in clever ways to help us derive more
information from samples, but one tradeoff is that the data from such equipment
often require a bit of work to move into a format that is useful for answering
scientific questions.

In the research process, these pre-processing steps should be done before
the data are used for further analysis. There are the first step in working
with the data after they are collected by the equipment (or by laboratory 
personal, in the case of data from simpler process, like plating samples
and counting colony-forming units). After the data are appropriately 
pre-processed, you can use them for statistical tests---for example, to 
determine if metabolite profiles are different between experimental groups---and
also combine them with other data collected from the experiment---for example, 
to see whether certain metabolite levels are correlated with the bacterial
load in a sample.

**Approaches for pre-processing data.**

There are two main approaches for pre-processing experimental data in this 
way. First,  The interface will typically 
be a graphical-user interface (GUI), where you will use pull-down menus and
point-and-click interfaces to work through the pre-processing steps. You 
often will be able to export a pre-processed version of the data in a 
common file format, like a delimited file or an Excel file, and that version
of the data can then be read into more general data analysis software, like
Excel or R.

In the simplest case, the point-and-click interface that is used for this
approach to pre-processing could be Excel or another version of spreadsheet
software. An Excel spreadsheet might be used with data recorded "by hand" in the
laboratory, with the researcher using embedded equations (or calculating and
entering new values by hand) for steps like calculating values based on the raw
data (for example, determining the time since the start of an experiment based
on the time stamp of each data collection timepoint).

> "There are currently [2017] very few, if any, 'plug-and-play' packages that 
allow researchers to quality control (QC), analyse and interpret scRNA-seq 
data, although companies that sell the wet-lab hardware and reagents for 
scRNA-seq are increasingly offering free software (for example, Loupe from 
10x Genomics, and Singular from Fluidigm). These are user-friendly but have the
drawback that they are to some extent a 'black box', with little transparency 
as to the precise algorithmic details and parameters employed." [@haque2017practical]

The second approach is to conduct the pre-processing directly within general
data analysis software like R or Python. These programs are both open-source,
and include extensions that were created and shared by users around the world.
Through these extensions, there are often powerful tools that you can use to
pre-process complex experimental data. In fact, the algorithms used in
proprietary software are sometimes extended from algorithms first shared through
R or Python. With this approach, you will read the data into the program (R, 
for example) directly from the file output from the equipment. You can 
record all the code that you use to read in and pre-process the data in a 
code script, allowing you to reproduce this pre-processing work. You can 
also go a step further, and incorporate your code into a pre-processing 
protocol, which combines nicely formatted text with executable code, and
which we'll describe in much more detail later in this module and in the 
following two modules.

There are advantages to taking the second approach---using scripted code in an
open-source program---rather than the first---using proprietary software with a
GUI interface. The use of codes scripts ensures that the steps of pre-processing
are reproducible. 
Also, when you use scripted code to pre-process biomedical data, you will find
that the same script can often be easily adapted and re-used in later projects
that use the same type of data. You may need to change small elements, like the
file names of files with data you want to use, or some details about the methods
used for certain pre-processing steps. However, often almost all of the
pre-processing steps will repeat over different experiments that you do. By 
extending to write a pre-processing protocol, you can further support the 
ease of adapting and re-using the pre-processing steps you take with one 
experiment when you run later experiments that are similar.

### Approaches to simple preprocessing tasks

There are several approaches for tackling this type of data preprocessing, to
get from the data that you initial observe (or that is measured by a piece of
laboratory equipment) to meaningful biological measurements that can be analyzed
and presented to inform explorations of a scientific hypothesis. While there are
a number of approaches that don't involve writing code scripts for this
preprocessing, there are some large advantages to scripting preprocessing any
time you are preprocessing experimental data prior to including it in figures or
further analysis. In this section, we'll describe some common non-scripted
approaches and discuss the advantages that would be brought by instead using a
code script. In the next module, we'll walk through an example of how scripts
for preprocessing can be created and applied in laboratory research.

In cases where the pre-processing is mathematically straightforward and the
dataset is relatively small, many researchers do the preprocessing by hand in a
laboratory notebook or through an equation or macro embedded in a spreadsheet.
For example, if you have plated samples at different dilutions and are trying to
calculate from these the CFUs in the original sample, this calculation is simple
enough that it could be done by hand. However, there are advantages to instead
writing a code script to do this simple preprocessing.


### Approaches to more complex preprocessing tasks

Other preprocessing tasks can be much more complex, particularly those that need
to conduct a number of steps to extract biologically meaningful measurements
from the measurements made by a complex piece of laboratory equipment, as well
as steps to make sure these measurements can be meaningfully compared across
samples.

For these more complex tasks, the equipment manufacturer will often provide
software that can be used for the preprocessing. 
It can take a while to develop a code script for preprocessing the raw data from
a piece of complex equipment like a mass spectrometer. However, the process of
developing this script requires a thoughtful consideration of the steps of
preprocessing, and so this is often time well-spent. Again, this initial time
investment will pay off later, as the script can then be efficiently applied to
future data you collect from the equipment, saving you time in pointing and
clicking through the GUI software. Further, it's easier to teach someone else
how to conduct the preprocessing that you've done, and apply it to future 
experiments, because the script serves as a recipe. 

When you conduct data preprocessing in a script, this also gives you access to
all the other tools in the scripting language. For example, as you work through
preprocessing steps for a dataset, if you are doing it through an R script, you
can use any of the many visualization tools that are available through R. By
contrast, in GUI software, you are restricted to the visualization and other
tools included in that particular set of software, and those software developers
may not have thought of something that you'd like to do. Open-source scripting
languages like R, Python, and Julia include a huge variety of tools, and once
you have loaded your data in any of these platforms, you can use any of these
tools.

If you have developed a script for preprocessing your raw data, it also becomes
much easier to see how changes in choices in preprocessing might influence your
final results. It can be tricky to guess whether your final results are sensitive, 
for example, to what choice you make for a particular tranform for part of your
data, or in how you standardize data in one sample to make different samples 
easier to compare. If the preprocessing is in a script, then you can test making 
these changes and running all preprocessing and analysis scripts, to see if it 
makes a difference in the final conclusions. If it does, then it helps you 
identify parts of preprocessing that need to be deeply thought through for the 
type of data you're collecting, and you may want to explore the documentation on 
that particular step of preprocessing to determine what choice is best for your
data, rather than relying on defaults.

> "One way to write R code is simply to enter it interactively at the command line... This 
interactivity is beneficial for experimenting with R or for exploring a data set in a casual
manner. ... However, interactively typing code at the R command line is a very bad approach from 
the perspective of recording and documenting code because the code is lost when R is shut down. 
A superior approach in general is to write R code in a file and get R to read the code from the file." [@murrell2009introduction]


> "One way to write R code is simply to enter it interactively at the command line... This 
interactivity is beneficial for experimenting with R or for exploring a data set in a casual
manner. ... However, interactively typing code at the R command line is a very bad approach from 
the perspective of recording and documenting code because the code is lost when R is shut down. 
A superior approach in general is to write R code in a file and get R to read the code from the file." [@murrell2009introduction]

> "One way to write R code is simply to enter it interactively at the command line... This 
interactivity is beneficial for experimenting with R or for exploring a data set in a casual
manner. ... However, interactively typing code at the R command line is a very bad approach from 
the perspective of recording and documenting code because the code is lost when R is shut down. 
A superior approach in general is to write R code in a file and get R to read the code from the file." [@murrell2009introduction]

Some types of preprocessing fulfill the need of translating from the equipment
measurements to measures that are directly relevant to the scientific question.
Other preprocessing tasks are more housekeeping (e.g., reading data into memory
so the software can use it) and others aim to digest complexity in the data or
reduce or handle noise and bias in the data collection. It is particularly
common that pre-processing is necessary for data extracted using complex
equipment. Equipment like mass spectrometers and flow cytometers leverage
physics, chemistry, and biology in clever ways to help us derive more
information from samples, but one tradeoff is that the data from such equipment
often require a bit of work to move into a format that is useful for answering
scientific questions.

"To reap the full benefits of the omics revolution, we need information
technology tools capable of making sense of the vast data sets generated by
omics experiments. In fact, the development of such tools has become a
discipline unto itself, called bioinformatics. And only with those tools can
researchers hope to clear another obstacle to drug development: that posed by
so-called emergent properties---behaviors of biological systems that cannot be
predicted from the basic biochemical properties of their components."
[@barry2009new]

One way you can help to identify potential quality issues in
research data is through basic exploratory analysis. Basic distribution plots or
scatterplots of the data, for example, can often help to identify unusual
outliers, which may be the result of a data recording error or an oddity in one
of the samples. 

> "Methods to quantify mRNA abundance introduce systematic sources of variation 
that can obscure signals of interest. Consequently, an essential first step in 
most mRNA-expression analyses is normalization, whereby systemic variations 
are adjusted to make expression counts comparable across genes and / or samples.
Within-sample normalization methods adjust for gene-specific features, such 
as GC content and gene length, to facilitate comparisons of a gene's expression
within an individual sample; whereas between-sample normalization methods adjust
for sample-specific features, such as sequencing depth, to allow for comparisons of
a gene's expression across samples." [@bacher2017scnorm]

> "scRNA-seq data show systematic variation in the relationship between
transcript-specific expression and sequencing depth (which we refer to as the
count-depth relationship) that is not accommodated by a single scale factor
common to all genes in a cell. Global scale factors adjust for a count-depth
relatinoship that is assumed to be common across genes. When this relationship
is not common across genes, normalization via global scale factors leads to
overcorrection for weakly and moderately expressed genes and, in some cases,
undernormalization of highly expressed genes. To address this, SCnorm uses
quantile regression to estimate the dependence of transcript expression on
sequencing depth for every gene. Genes with similar dependence are then grouped,
and a second quantile regression is used to estimate scale factors within each
group. Within-group adjustment for sequencing depth is then performed using the
estimated scale factors to provide normalized estimates of expression."
[@bacher2017scnorm]

> "The primary goal of single-cell normalization is to remove the influence
of technical effects in the underlying molecular counts, while preserving
true biological variation." [@hafemeister2019normalization]

> "In general, the normalized expression level of a gene should not be 
correlated with the total sequencing depth of a cell. Downstream analytical 
tasks (dimension reduction, differential expression) should also not 
be influenced by variation in sequencing depth." [@hafemeister2019normalization]

> "Normalization is critical to the development of analysis techniques on
scRNA-seq and to counteract technical noise or bias. Before observed data can be
used to identify differentially expressed genes or potential subpopulations, it
must undergo these corrections, for what is observed is seldom exactly what is
present within the data set." [@lytal2020normalization]

> "Each count in a count matrix represents the successful capture, reverse transcription and sequencing of a molecule of cellular mRNA (Box 1). Count depths for identical cells can differ due to the variability inherent in each of these steps. Thus, when gene expres- sion is compared between cells based on count data, any difference may have arisen solely due to sampling effects. Normalization addresses this issue by e.g. scaling count data to obtain correct rela- tive gene expression abundances between cells." [@luecken2019current]

> "In many cases... the tools used in bulk RNA-seq can be applied to scRNA-seq.
But fundamental differences in the data mean that this is not always possible.
For one thing, single-cell data are noisier... With so little RNA to work with,
small changes in amplification and capture efficiencies can produce large
differences from cell to cell and day to day and have nothing to do with
biology. Researchers must therefore be vigilant for 'batch effects', in which
seemingly identical cells prepared on different days differ for purely technical
reasons, and for 'dropouts'---genes that are expressed in the cell but not
picked up in the sequence data. Another challenge is the scale... A typical bulk
RNA-seq experiment involves a handful of samples, but scRNA-seq studies can
involve thousands. Tools that can handle a dozen samples often slow to a crawl
when confronted with ten or a hundred times as many." [@perkel2017single]

> "A number of methods are available for between-sample normalization in bulk
RNA-seq experiments. Most of these methods calculate global scale factors (one
factor is applied to each sample, and this same factor is applied to all genes
in the sample) to adjust for sequencing depth. These methods demonstrate
excellent performance in bulk RNA-seq, but they are compromised in the
single-cell setting because of an abundance of zero-expression values and
increased technical variability." [@bacher2017scnorm]

Another example of technical noise comes from flow cytometry. In this 
case, each of the fluorescent tags, rather than emitting at a single 
wavelength, has a distribution of wavelengths across which it emits. 
These distributions overlap somewhat for some of the fluorescent tags, 
especially when measuring many of these "colors" through a multicolor 
panel?, causing something called "spillover"?, where emissions from 
one tag might be recorded as part of the signal for another tag. 

One key example of technical noise is known as "batch effects"---if different
samples are run through the equipment at different times, it can introduce
differences between the samples based on which "batch" they were measured
with. ...

> "The steps in a typical flow cytometry experiment ... present several 
variables that need to be controlled for effective standardization. These variables
include the general areas of reagents, sample handling, instrument setup
and data analysis... The effects of changes in these variables are largely 
known. For example, the stabilization and control of staining reagents 
through the use of pre-configured lyophilized-reagent plates, and centralized
data analysis, have both been shown to decrease variability in a multi-site
study. However, the wide-spread adoption of standards for controlling such 
variables has not taken place. This is in contrast to other technologies, such 
as gene expression microarrays, which have achieved a reasonable degree of 
standardization in recent years. ... Of course, microarray data are less complex
than flow cytometry data, which are based on many hierarchical gates. Still, 
a reasonable degree of standardization of flow cytometry assays should be 
possible to achieve." [@maecker2012standardizing]

Some of these procedures can be separated into two groups, normalization
processes and data correction processes, which might include batch correction
and noise correction [@luecken2019current].

Normalization in scRNA-seq can also include gene normalization:

> "In the same way that cellular count data can be normalized to make them
comparable between cells, gene counts can be scaled to improve comparisons
between genes. Gene normalization constitutes scaling gene counts to have zero
mean and unit vari- ance (z scores). ... There is currently no consensus on
whether or not to perform normalization over genes." [@luecken2019current]


> "After scaling normalization, further correction is typically required to 
ameliorate or remove batch effects. For example, in the case study dataset, 
cells from two patients were each processed on two C1 machines. Although C1 
machine is not one of the most important explanatory variables on a per-gene
level, this factor is correlated with the first principal component of the 
log-expression data. This effect cannot be removed by scaling normalization
methods, which target cell-specific biases and are not sufficient for removing 
large-scale batch effects that vary on a gene-by-gene basis. ... For the 
dataset here, we fit a linear model to the scran normalized log-expression 
values with the C1 machine as an explanatory factor. (We also use the log-total
counts from the endogenous genes, percentage of counts from the top 100 most
highly-expressed genes and percentage of counts from control genes as 
additional covariates to control for these other unwanted technical effects.) 
We then use the residuals from the fitted model for further analysis. This approach
successfully removes the C1 machine effect as a major source of variation between
cells." [@mccarthy2017scater]


> "We emphasize that it is generally preferable to incorporate batch effects or 
latent variables into statistical models used for inference. When this is not 
possible (e.g., for visualization), directly regressing out these uninteresting 
factors is required to obtain 'corrected' expression values for further analysis.
Furthermore, a general risk of removing latent factors is that interesting 
biological variation may be removed along with the presumed unwanted variation. 
Users should therefore apply such methods with appropriate caution, particularly 
when an analysis aims to discover biological conditions, such as new cell types."
[@mccarthy2017scater]

> "Once identified, important covariates and latent variables can be flagged for
inclusion in downstream statistical models or their effects regressed out of 
normalized expression values." [@mccarthy2017scater]

**Transforming data**

Scaling, normalization, log transformations, calculating time since start
of experiment, etc. Some transformations can help change data that have 
a skewed distribution into a more normally-distributed dataset, which can 
be helpful in meeting the assumptions that underlie some statistical 
tests and models. Some transformations are also helpful in visualizing 
the data. For example, if data are extremely right-skewed (that is, have 
a few very large outliers), it can be hard to see overall patterns 
when plotting the untransformed data, as those outliers force the scale
to expand to fit them, squeezing the bulk of the data into a small 
portion of the total scale of the plot. A log transformation can help to 
spread the data more evenly across the plot area, so that you can 
see patterns in the bulk of the data more easily.

> "There are various reasons for making a transformation, which may also apply to 
deriving a new variable: 1. to get a more meaningful variable (the best reason!); 
2. to stabilize variance; 3. to achieve normality (or at least symmetry); 4. to 
create additive effects (i.e. remove interaction effects); 5. to enable a linear
model to be fitted." [@chatfield1995problem]

> [For transformations] "Logarithms are often meaningful, particularly with
economic data when proportional, rather than absolute, changes are of interest.
Another application of the logarithmic transformation is ... to transform a
severely skewed distribution to normality." [@chatfield1995problem]

One critical process in this category is the process of *normalization*. ...

> "After normalization, data matrices are typically log(x+1)-trans- formed. This
transformation has three important effects. Firstly, distances between
log-transformed expression values represent log fold changes, which are the
canonical way to measure changes in expression. Secondly, log transformation
mitigates (but does not remove) the mean–variance relationship in single-cell
data (Bren- necke et al, 2013). Finally, log transformation reduces the skew-
ness of the data to approximate the assumption of many downstream analysis tools
that the data are normally distributed. While scRNA-seq data are not in fact
log-normally distributed (Vieth et al, 2017), these three effects make the log
transformation a crude, but useful tool. This usefulness is highlighted by down-
stream applications for differential expression testing (Finak et al, 2015;
Ritchie et al, 2015) or batch correction (Johnson et al, 2006; Buttner et al,
2019) that use log transformation for these purposes. It should however be noted
that log transformation of normalized data can introduce spurious differential
expression effects into the data (preprint: Lun, 2018). This effect is
particularly pronounced when normalization size factor distributions differ
strongly between tested groups." [@luecken2019current]

> "Most approaches seek to reduce these 'multi-dimensional data', with each 
dimension being the expression of one gene, into a very small number of dimensions
that can be more easily visualised and interpreted. Principal component analysis
(PCA) is a mathematical algorithm that reduces the dimensionality of data, and
is a basic and very useful tool for examining heterogeneity in scRNA-seq data."
[@haque2017practical]

> "Dimensionality reduction and visualization are, in many cases, followed by 
clustering of cells into subpopulations that represent biologically meaningful 
trends in the data, such as functional similarity or developmental 
relationship. Owing to the high dimensionality of scRNA-seq data, clustering
often requires special consideration." [@haque2017practical]

> "Cluster analysis aims to partition a group of individuals into groups or clusters
which are in some sense 'close together'. There is a wide variety of possible 
procedures. In my experience the clusters obtained depend to a large extent on the methods
used (except where the clusters are really clear-cut) and users are now aware of
the drawbacks and the precautions which need to be taken to avoid irrelevant or misleading
results." [@chatfield1995problem]


-----------------------------------------------------------------------------

> "The three main types of problem data are errors, outliers, and missing
observations. ... An error is an observation which is incorrect, perhaps
because it has been copied or typed incorrectly at some stage. An outlier is a 'wild'
or extreme observation which does not appear to be consistent with the rest of
the data. Outliers arise for a variety of reasons and can create severe problems.
... Errors and outliers are often confused. An error may or may not be an outlier, 
while an outlier may or may not be an error. ... An outlier may be caused by an error, 
but it is important to consider the alternative possibility that the observation 
is a genuine extreme result from the 'tail' of the distribution. This usually happens
when the distribution is skewed and the outlier comes from the long 'tail'."
[@chatfield1995problem]


> "There are several types of error..., including the following: 1. A recording error
arises, for example, when an instrument is misread. 2. A typing error arises when an 
observation is typed incorrectly. 3. A transcription error arises when an observation
is copied incorrectly, and so it is advisable to keep the amount of copying to a 
minimum. ..." [@chatfield1995problem

Some data collected through laboratory experiments is very straightforward and
requires little or no pre-processing before it's used in analysis. For example,
if you are regularly weighing the animals in an experiment, then the data may
require no pre-processing (in other words, you'll directly use the weight
recorded from the scale) or very minimal pre-processing (for example, if you are
keeping all animals for a treatment group in the same cage, you may weigh the
cage as a whole, in which case you could divide that weight by the number of
animals in the cage as a pre-processing step to estimate the average weight per
animal). 

Other data collected in the laboratory may require some pre-processing that
takes a few more steps, but is still fairly straightforward. For example, if you
plate bacteria from a sample at a variety of dilutions, you might count each
plate and determine a measure of Colony Forming Units from the set of plates
with different dilutions by deciding which dilution provides the clearest count
and then back-calculating based on its dilution to get the total number of
colony-forming units in the original sample. Pre-processing these data typically
will also involve transforming data, to get them in a format that is easier to
visualize or more appropriate for statistical analysis, for example, a log
transformation.


This step of pre-processing data can become much more complex with data that was
collected using complex equipment, like a flow cytometer, mass spectrometer, or
sequencer. In these cases, there are often steps required to extract from the
machine's readings a biologically-relevant measurement.

> "For beginners, caution is warrented. Bioinformatics tools can almost always yield
an answer; the question is, does that answer mean anything? Dudoit's advice is do 
some exploratory analysis, and verify that the assumptions underlying your chosen
algorithms make sense." [@perkel2017single]

> "We designed three features based on the assumption that broken cells contain a 
lower and multiple cells a higher number of transcripts compared to a typical 
high quality single cell. For the first feature we calculated the number of 
highly expressed and highly variable genes. For the second feature we calculated the
variance across genes. Lastly, we hypothesized that the number of genes expressed
at a particular level would differ between cells. Thus we binned normalized 
read counts into intervals (very low to very high) and counted the number of genes
in each interval. ... Overall, our results show that technical features [like the 
number of detected genes and the percent of mapped reads] can help distinguish
low and high quality cells." [@ilicic2016classification]

> "Before further analyses, scRNA-seq data typically require a number of bio-informatic
QC checks, where poor-quality data from single cells (arising as a result of many 
possible reasons, including poor cell viability at the time of lysis, poor mRNA
recovery and low efficiency of cDNA production) can be justifiably excluded from 
subsequent analysis. Currently, there is no consensus on exact filtering 
strategies, but most widely used criteria include relative library size, number
of detected genes and fraction of reads mapped to mitochondria-encoded genes or
synthetic spike-in RNAs. ... Other considerations are whether single cells have
actually been isolated or whether indeed two or more cells have been mistakenly 
assessed in a particular sample." [@haque2017practical]

> "For each gene, QC metrics such as the average expression level and the proportion of
cells in which the gene is expressed are computed. This can be used to identify 
low-abundance genes or genes with high dropout rates that should be filtered out 
prior to downstream analyses." [@mccarthy2017scater]

> "Methods to quantify mRNA abundance introduce systematic sources of variation 
that can obscure signals of interest. Consequently, an essential first step in 
most mRNA-expression analyses is normalization, whereby systemic variations 
are adjusted to make expression counts comparable across genes and / or samples.
Within-sample normalization methods adjust for gene-specific features, such 
as GC content and gene length, to facility comparisons of a gene's expression
within an individual sample; whereas between-sample normalization methods adjust
for sample-specific features, such as sequencing depth, to allow for comparisons of
a gene's expression across samples." [@bacher2017scnorm]


> "A number of methods are available for between-sample normalization in bulk RNA-seq
experiments. Most of these methods calculate global scale factors (one factor is
applied to each sample, and this same factor is applied to all genes in the sample) 
to adjust for sequencing depth. These methods demonstrate excellent performance in 
bulk RNA-seq, but they are compromised in the single-cell setting because of an 
abundance of zero-expression values and increased technical variability." [@bacher2017scnorm]

> "Normalization is critical to the development of analysis techniques on scRNA-seq and 
to counteract technical noise or bias. Before observed data can be used to identify 
differentially expressed genes or potential subpopulations, it must undergo these 
corrections, for what is observed is seldom exactly what is present within the data set."
[@lytal2020normalization]

> "Scaling normalization is typically required in RNA-seq data analysis to 
remove biases caused by differences in sequencing depth, capture efficiency
or composition effects between samples." [@mccarthy2017scater]


> "The primary goal of single-cell normalization is to remove the influence
of technical effects in the underlying molecular counts, while preserving
true biological variation." [@hafemeister2019normalization]

There are also cases where pre-processing steps could be used to 
remove patterns from technical noise or even from biological patterns that 
are unrelated to the scientific question of interest. In terms of technical 
noise, for example, there are cases where pre-processing steps could be used 
to help remove variation that's introduced by running the experimental samples
in different batches. In terms of biological patterns, one pattern that may 
be desirable to remove through pre-processing is gene expression related to 
a cell's phase in the cell cycle.

> "We emphasize that it is generally preferable to incorporate batch effects or 
latent variables into statistical models used for inference. When this is not 
possible (e.g., for visualization), directly regressing out these uninteresting 
factors is required to obtain 'corrected' expression values for further analysis.
Furthermore, a general risk of removing latent factors is that interesting 
biological variation may be removed along with the presumed unwanted variation. 
Users should therefore apply such methods with appropriate caution, particularly 
when an analysis aims to discover biological conditions, such as new cell types."
[@mccarthy2017scater]

> "Once identified, important covariates and latent variables can be flagged for
inclusion in downstream statistical models or their effects regressed out of 
normalized expression values." [@mccarthy2017scater]

> "Most approaches seek to reduce these 'multi-dimensional data', with each 
dimension being the expression of one gene, into a very small number of dimensions
that can be more easily visualised and interpreted. Principal component analysis
(PCA) is a mathematical algorithm that reduces the dimensionality of data, and
is a basic and very useful tool for examining heterogeneity in scRNA-seq data."
[@haque2017practical]


> "One common type of single-cell analysis, for instance, is dimension reduction. 
This process simplifies data sets to facilitate the identification of similar 
cells. ... scRNA-seq data represent each cell as 'a list of 20,000 gene-expression
values.' Dimensionality-reduction algorithms such as principal components analysis
(PCA) and t-distributed stochastic neighbour embedding (t-SNE) effectively project
those shapes into two or three dimensions, making clusters of similar cells apparent." 
[@perkel2017single]


> "Principal component analysis rotates the p observed values to p new orthogonal 
variables, called principal components, which are linear combinations of the original
variables and are chosen in turn to explain as much of the variation as possible.
It is sometimes possible to confine attention to the first two or three components, 
which reduces the effective dimensionality of the problem. In particular, a scatter
diagram of the first two components is often helpful in detecting clusters of 
individuals or outliers." [@chatfield1995problem]

> "In multicellular organisms, cells carry out a diverse array of complex, specialized
functions. This specialization occurs mostly through the expression of cell type--specific
genes and proteins that generate the appropriate structures and molecular networks. 
A central challenge in the biomedical sciences, however, has been to identify the 
distinct lineages and phenotypes of the specialized cells in organ systems, and track 
their molecular evolution during differentiation." [@benoist2011flow]

> "Since the 1970s, fluoresence-based flow cytometry has been the leading technique
for studying and sorting cell populations. It involves passing cells through 
flow chambers at high rates (> 20,000 cells/s) and using lasers to excite fluorescent
tags ('fluorochromes') that are usually attached to antibodies; different antibodies
are tagged with different colors, enabling researchers to quantify molecules that 
define cell subtypes or reflect activation of specific pathways. Progess in instrument
design, multi-laser combinations, and novel fluorochromes has led to experimental
configurations that simultaneously measure up to 15 markers. This has enabled very 
detailed description of cell subtypes, perhaps most extensively in the immune
system, where the Immunological Genome Project is profiling >200 distinct cell typles.
Fluorescence cytometry seems to have reached a technical plateau, however: In practice, 
researchers typically measure only 6 to 10 cell markers because they are limited by 
the specral overlap between fluorochromes." [@benoist2011flow]

> "In 1954, Wallace Coulter developed an instrument that could measure cell size and
count the absolute number of cells, and thus the discipline of flow cytometry was
born. Further developments enabled the production of instruments that could measure
cell size and nucleic acid content using a two-dimensional approach that compared
light scatter and light absorption. These instruments were cumbersome and required
specialist operators, but immunologists began to use them to investigate the 
functions of immune cells. ... By the mid-1980s, bench-top flow cytometers were 
available and as the technology advanced the instruments became progressively 
smaller. Coupled with the availability of monoclonal antibodies, the increasing 
number of available fluorochromes (compounds that emit light at a greater wavelength
than the light source they are excited with) and computer improvements, flow
cytometers are now accessible for almost every clinical laboratory." [@barnett2008cd4]


> "Flow cytometry enables the examination of microscopic particles (such as cells) that
are suspended in a stream of fluid which is termed sheath fluid. This fluid is 
focused hydrodynamically such that the cells flow in single file through a flow cell
in which a beam of light (usually a laser) is focused. As the cells pass through the 
laser beam they scatter the light so that forward scatter (FSC) and side scatter (SSC)
light is captured; this enables the size and granularity of the cells to be determined.
In addition, if a cell is labelled with antibodies that carry a fluorochrome, as the 
cell passed in front of the laser beam the fluorochrome emits light at a wavelength
that is higher than the single wavelength light source and which can be detected by 
fluorescence detectors. The flow cytometers that are in clinical use can analyse at
least four fluorochromes simultaneously, in addition to the FSC and SSC. This is known 
as multiparametric analysis. The information that is generated is computer analysed
so that specific analysis regions (known as gates) can be created, which allows
the user to build up a profile of the size, granularity and antigen profile of
the target cell population." [@barnett2008cd4]

> "T cells have an essential role in protection against a variety of infections. Indeed, 
the development of successful vaccines against HIV, malaria or tuberculosis will 
require the generation of potent and durable T-cell responses. ... As T cells are
functionally heterogeneous and mediate their effects through a variety of mechanisms, 
a major hurdle in quantifying protective T-cell responses has been the technical 
limitations in assessing the complexity of such responses. Methods to define the 
full characteristics of T cells are crucial for developing preventative and 
therapeutic vaccines for infections and cancer." [@seder2008t]

> "Flow cytometry has increasingly become a tool of choice for the analysis of 
cellular phenotype and function in the immune system. It is arguably the most 
powerful technology available for probing human immune phenotypes, because it 
measures multiple parameters on many individual cells. Flow cytometry thus 
allows for the characterization of many subsets of cells, including rare subsets, 
in a complex mixture such as blood. And because of the wide array of antibody 
reagents and protocols available, flow cytometry can be used to assess not only 
the expression of cell-surface proteins, but also that of intracellular 
phosphoproteins and cytokines, as well as other functional readouts."
[@maecker2012standardizing]

> "Immune phenotypes: Measurable aspects of the immune system, such as the 
proportions of various cell subsets or measures of cellular immune function."
[@maecker2012standardizing]

> "Gates: Sequential filters that are applied to a set of flow cytometry data to 
focus the analysis on particular cell subsets of interest." [@maecker2012standardizing]

> "One of the main roles that statistics play in science is explaining variation---variation
of observed data. That variation can actually be true signal that you're interested in, 
but there can also be variations due to noise or confounding signal. So I think of
statistical modeling as the process of explaining variation in the data according 
to concrete variables that have been measured." [@mak2011john]




> "This process of accounting for, and possibly removing, sources of variation that
are not of biological interest is called normalization. There are two distinct 
approaches to normalization. One of them I would call 'unsupervised' in that it 
does not taken into account the study design. These are the most popular methods
because they require the least amount of statistical modeling and knowledge of 
statistics. The other approach, which is one I strongly favor, is what I would
call 'supervised normalization'. This approach directly takes into account the 
study design. I find this appealing because is one is trying to parse sources of 
variation, then it seems all sources of variation should be considered. If I 
perform an experiment with 20 microarrays, say 10 treated and 10 control, then 
I want to utilize this information when separating and removing technical 
sources of variation. Another component of normalization, which is gaining 
popularity, is normalizing by principal components. Again, I think this should
be done in the context of the study design, which was the goal of a recent 
method I worked on called 'surrogate variable analysis.'" [@mak2011john]


> "The biggest, the easiest way [for a biologist doing RNA-Seq to tell that 
better normalization of the data is needed]---the way that I discovered the 
importance of normalization in the microarray context---is the lack of 
reproducibility across different studies. You can have three studies that are
all designed to study the same thing, and you just see basically no reproducibility, 
in terms of differentially expressed genes. And every time I encountered that, it
could always be traced back to normalization. So, I'd say that the biggest sign and 
the biggest reason why you want to use normalization is to have a clear signal
that's reproducible." [@mak2011john]

> "Data-analysis pipelines are replete with configuration decisions, assumptions, 
dependencies and contingencies that move quickly beyond documentation, making 
troubleshooting incrediably difficult. ... Teams had to visit each others' labs more
than once to understand and fully implement computational-analysis pipelines for
large microscopy datasets." [@raphael2020controlled]

> "Improved reproducibility comes from pinning down methods." [@lithgow2017long]

> "Our first task, to develop a protocol, seemed straightforward. But subtle disparities
were endless. In one particularly painful teleconference, we spent an hour debating
the proper procedure for picking up worms and placing them on new agar plates. Some
batches of worms lived a full day longer with gentler techniques. Because a worm's 
lifespan is only about 20 days, this is a big deal. Hundreds of e-mails and many 
teleconferences later, we converged on a technique but still had a stupendous three-day
difference in lifespan between labs. The problem, it turned out, was notation---one
lab determined age on the basis of when an egg hatched, others on when it was laid." 
[@lithgow2017long]



> "The distinction between a preproducible scientific report and current common
practice is like the difference between a partial list of ingredients and a
recipe. To bake a good loaf of bread, it isn’t enough to know that it contains
flour. It isn't even enough to know that it contains flour, water, salt and
yeast. The brand of flour might be omitted from the recipe with advantage, as
might the day of the week on which the loaf was baked. But the ratio of
ingredients, the operations, their timing and the temperature of the oven
cannot. Given preproducibility---a 'scientific recipe'---we can attempt to make
a similar loaf of scientific bread. If we follow the recipe but do not get the
same result, either the result is sensitive to small details that cannot be
controlled, the result is incorrect or the recipe was not precise enough (things
were omitted to disadvantage). Depending on the discipline, preproducibility
might require information about materials (including organisms and their care),
instruments and procedures; experimental design; raw data at the instrument
level; algorithms used to process the raw data; computational tools used in
analyses, including any parameter settings or ad hoc choices; code, processed
data and software build environments; or analyses that were tried and
abandoned." [@stark2018before]

> "If I publish an advertisement for my work (that is, a paper long on results
but short on methods) and it's wrong, that makes me untrustworthy. If I say:
'here’s my work' and it's wrong, I might have erred, but at least I am honest.
If you and I get different results, preproducibility can help us to identify
why---and the answer might be fascinating." [@stark2018before]

> "As chemists, we have to be able to go to the literature, take a procedure, 
and carry out a similar or identical transformation with our own hands. Frustratingly,
this doesn't always happen, and the next-to-worst-case scenario possible is 
when it's one of your own reactions that can't be reproduced by a lab 
elsewhere. Unsurprisingly, one step worse than this is when you can't even 
reproduce one of your own reactions in your own lab!" [@gibb2014reproducibility]

> "If there is nothing wrong with the reagents and reproducibility is still an 
issue, then as I like to tell students, there are two options: (1) the physical 
constants of the universe and hence the laws of physics are in a state of flux 
in their round-bottomed flask, or (2) the researcher is doing something wrong 
and either doesn't know it or doesn't want to know it. Then I ask them which 
explanation they think I'm leaning towards." [@gibb2014reproducibility]



> "Consider a set of measurements that reflect some underlying true values
(say, species represented by DNA sequences from their genomes) but
have been degraded by technical noise. Clustering can be used to remove 
such noise." [@holmes2018modern]


> With data that give the number of reads for each gene in a sample, "The 
data have a large dynamic range, starting from zero up to millions. The 
variance and, more generally, the distribution shape of the data in different
parts of the dynamic range are very different. We need to take this 
phenomenon, called heteroscedascity, into account. The data are non-negative
integers, and their distribution is not symmetric---thus normal or log-normal
distribution models may be a poor fit. We need to understand the systematic
sampling biases and adjust for them. Confusingly, such adjustment is often 
called normalization. Examples are the total sequencing depth of an experiment
(even if the true abundance of a gene in two libraries is the same, we expect
different numbers of reads for it depending on the total number of reads
sequenced) and differing sampling probabilities (even if the true abundance of 
two genes within a biological sample is the same, we expect different numbers
of reads for them if they have differing biophysical properties, such as length, 
GC content, secondary structure, binding partners)." [@holmes2018modern]

> "Often, systematic biases affect the data generation and are worth taking
into account. Unfortunately, the term normalization is commonly used for that
aspect of the analysis, even though it is misleading; it has nothing to do 
with the normal distribution, nor does it involve a data transformation. 
Rather, what we aim to do is identify the nature and magnitude of 
systematic biases and take them into account in our model-based analysis of the
data. The most important systematic bias [for count data from high-throughput
sequencing applications like RNA-Seq] stems from variations in the total number
of reads in each sample. If we have more reads for one library than for another, 
then we might assume that, everything else being equal, the counts are 
proportional to each other with some proportionality factor *s*. Naively, 
we could propose that a decent estimate of *s* for each sample is simply 
given by the sum of the counts of all genes. However, it turns out that we 
can do better..." [@holmes2018modern]

> "When testing for differential expression, we operate on raw counts and
use discrete distributions. For other downstream analyses---e.g., for 
visualization or clustering---it can be useful to work with transformed versions
of the count data. Maybe the most obvious choice of transformation is the 
logarithm. However, since count values for a gene can be zero, some analysts
advocate the use of pseudocounts, i.e., transformations of the form
y = log2(n + 1) or more generally y = log2(n + n0)." [@holmes2018modern]

> "The data sometimes contain isolated instances of very large counts that
are apparently unrelated to the experimental or study design and may be
considered outliers. Outliers can arise for many reasons, including rare 
technical or experimental artifacts, read mapping problems in the case of
genetically differing samples, and genuine but rare biological events. In 
many cases, users appear primarily interested in genes that show consistent 
behavior, and this is the reason why, by default, genes that are affected by such 
outliers are set aside by `DESeq`. The function calculates, for every gene
and for every sample, a diagnostic test for outliers called Cook's distance. 
Cook's distance is a measure of how much a single sample is influencing the
fitted coefficients for a gene, and a large value of Cook's distance is 
intended to indicate an outlier count. `DESeq2` automatically flags genes
with Cook's distance above a cutoff and sets their p-values and adjusted
p-values to NA. ... With many degrees of freedom---i.e., many more samples
than number of parameters to be estimated---it might be undesirable to remove
entire genes from the analysis just becuase their data include a single 
count outlier. An alternative strategy is to replace the outlier counts with 
the trimmed mean over all sample, adjusted by the size factor for that
sample. This approach is conservative: it will not lead to false positives, 
as it replaces the outlier value with the value predicted by the null hypothesis." [@holmes2018modern]

> "Since the sampling depthy is typically different for different sequencing
runs (replicates), we need to estimate the effect of this variable parameter
and take it into account in our model. ... Often this part of the analysis 
is called normalization (the term is not particularly descriptive, but 
unfortunately it is now well established in the literature)." [@holmes2018modern]

> "Sometimes we explicitly know about factors that cause bias, for instance, when
different reagent batches were used in different phases of the experiment. We
call these batch effects (Leek et al., 2010). At other times, we may expect that
such factors are at work but have no explicit record of them. We call these
latent factors. We can treat them as adding to the noise, and in Chapter 
4 we saw how to use mixture models to do so. But this may not be enough; with 
high-dimensional data, noise caused by latent factors tends to be correlated, 
and this can lead to faulty inference (Leek et al., 2010). The good news is that
these same correlations can be exploited to estimate latent factors from 
the data, model them as bias, and thus reduce the noise (Leek and Storey 2007; 
Stegle et al. 2010)." [@holmes2018modern]

> "Regular noise can be modeled by simple probability models such as independent
normal distributions or Poissons, or by mixtures such as the gamma-Poisson or
Laplace. We can use relatively straightforward methods to take such noise into
account in our data analyses and to compute the probability of extraordinarily 
large or small values. In the real world, this is only part of the story: 
measurements can be completely off-scale (a sample swap, a contamination, or 
a software bug), and they can all go awry at the same time (a whole microtiter
plate went bad, affecting all data measured from it). Such events are hard to model
or even correct for---our best chance of dealing with them is data quality 
assessment, outlier detection, and documented removal." [@holmes2018modern]

> "We distinguis between data quality assessment (QA)---steps taken to measure
and monitor data quality---and quality control---the removal of bad data. 
These activities pervade all phases of an analysis, from assembling the raw 
data over transformation, summarization, model fitting, hypothesis testing or
screening for 'hits' to interpretation. QA-related questions include: 
1. How do the marginal distributions of the variables look (histograms, 
ECDF plots)? 2. How do their joint distributions look (scatterplots, pair plots)?
3. How well do replicated agree (as compared to different biological conditions)?
Are the magnitudes of different between several conditions plausible?
4. Is there evidence of batch effects? These could be of a categorical (stepwise)
or continuous (gradual) nature, e.g., due to changes in experimental reagents, 
protocols or environmental factors. Factors associated with such effects may 
be explicitly known, or unknown and latent, and often they are somewhere in 
between (e.g., when a measurement apparatus slowly degrades over time, and 
we have recorded the times, but don't really know exactly when the degradation
becomes bad). For the last two sets of questions, heatmaps, principal components
plots, and other ordination plots (as we have seen in Chapters 7 and 9) are 
useful." [@holmes2018modern]

> "It's not easy to define quality, and the word is used with many meanings. The
most pertinent for us is fitness for purpose, and this contrasts with other 
definitions that are based on normative specifications. For instance, in 
differential expression analysis with RNA-Seq data, our purpose may be the 
detection of differentially expressed genes between two biological conditions. 
We can check specificiations such as the number of reads, read length, base
calling quality and fraction of aligned reads, but ultimately these measures in 
isolation have little bearing on our purpose. More to the point will be the 
identification of samples that are not behaving as expected, e.g., because of 
a sample swap or degradation, or genes that were not measured properly. 
... Useful plots include ordination plots ... and heatmaps ...
A quality metric is any value that we use to measure quality, and having 
explicit quality metrics helps in automating QA/QC." [@holmes2018modern]


The use of version control tools and platforms, like git and GitHub, not only 
helps in transparent and trackable recording of data, but also brings some
additional advantages. First, this combination of tools
aids in collaboration across a research group, as we discuss in depth in the next
chapter. 

> **On the root causes for irreproducibility in biomedical research:** "First, 
a lack of standards for data generation leads to problems with the comparability 
and integration of data sets." [@waltemath2016modeling]




Finally, there are huge benefits further down the data analysis pipeline that
come with always recording data in the same format. If your group is working
with a statistician or data analyst, it becomes much easier for that person to
quickly understand a new file if it follows the same format as previous files.
Further, if you work with a statistician or data analyst, he or she probably
creates code scripts to read in, re-format, analyze, and visualize the data
you've shared. If you always record data using the same format, these scripts
can be reused with very little modification. This saves valuable time, and it
helps make more time for more interesting statistical analysis if your
collaborator can trim time off reading in and reformatting the data in their
statistical programming language.

For most statistical programs, data can be easily read in from a spreadsheet if
the computer can parse it in the following way: first, read in the first row,
and assign each cell in that row as the *name* of a column. Then, read in the
second row, and put each cell in the column the corresponds with the name of the
cell in the same position in the first row. Also, set the data type for that
column (e.g., number, character) based on the data type in this cell. Then, keep
reading in rows until getting to a row that's completely blank, and that will be
the end of the data. If any of the rows has more cell than the first row, then
that means that something went wrong, and should result in stopping or giving a
warning. If any of the rows have fewer cells than the first row, then that means
that there are missing data in that row, and should probably be recorded as
missing values for any cells the row is "short" compared to the first row.

"Tabular plain-text data formats are used extensively in computing. The basic
format is incrediably simple: each row (also known as a record) is kept on its
own line, and each column (also known as a field) is separate by some delimiter."
[@buffalo2015bioinformatics]

"Cleaning data is a short-term solution, and preventing errors is promoted as
a permanent solution. The drawback to cleaning data is that the process never
ends, is costly, and may allow many errors to avoid detection."
[@keller2017evolution]

"Solutions to integrating the new generation of large-scale data sets require
approaches akin to those used in physics, climatology and other quantitative
disciplines that have mastered the collection of large data sets."
[@schadt2010computational]

This
specific spreadsheet allowed the researcher who was conducting the experiment to
(1) calculate the amount of initial inoculum (cell culture) to add to each tube
to begin the study, (2) record the raw data absorbance measurements, (3) graph
the data on both a log and linear scale, and (4) calculate doubling time in two
phases of growth using the equation listed above.

Finally, when you are designing the data collection template, you should try to
avoid using formats that may be "auto-converted" by the spreadsheet program. For
example, if you enter a value like "7-9-19" into a cell, the spreadsheet may try
to automatically convert it to a date. Perhaps it is a date, but even if it is,
the spreadsheet algorithm might make problematic assumptions in the conversion.
For example, it might assume that "7-9-19" means July 9, 2019, when you meant
for it to represent September 7, 1919. Further, there are cases where you might
enter a value that is not a date, but that the spreadsheet thinks is based on
its formatting. This was found to be a problem, for example, for some gene
names. To avoid potential autoconversion by the spreadsheet, consider putting
any character stings, including entries with dates and identifiers, inside 
quotation marks when you enter them in the spreadsheet program. The spreadsheet 
program will respect this as a sign to leave the entry as-is, rather than 
attempting automatic formatting into a date or other special class of data.

For example, take a look at the graph in the top left corner on the second page
of the report shown in Figure \@ref(fig:growthreport1). This figure shows the
growth curve from the collected data, and it adds a shaded area to show the time
range that was used to estimate doubling times for each sample. This provides a
helpful quality check for this experiment. Bacterial growth goes through several
phases, including an initial lag phase, an exponential growth phase (when the
bacteria are regularly doubling), a stationary phase (when growth starts to slow
down, because of exhaustion of nutrients or buildup of waste), and a dying
phase. The doubling time should be calculated only during the exponential phase 
of growth, as the equation used to calculate it relies on describing growth during
a period of regular doubling. When the growth curve is plotted with a log scale on 
the y-axis, the growth curve will look approximately linear in this exponential 
growth region. By including the plot on the top left of the second page in 
Figure \@ref(fig:growthreport1), the researcher can quickly see that, in this 
experiment, the selected time range for calculating doubling time might not be 
appropriate---for the low oxygen condition, in particular, this time range looks
like it included some measurements made during the transition into the stationary 
phase of growth. By quickly being able to assess this, the researcher 
can reassess whether a different time range should be use to calculate the 
doubling time for this experiment.

The report shown in Figure \@ref(fig:growthreport1) provides results that are 
very similar to those calculated in the original spreadsheet, to show that 
you don't need to give up fast and clear summaries and visuals if you simplify 
the template for collecting data. However, this report template could easily be 
made more sophisticated. For example, you could add code into the report that
would perform quality control checks. In the example case, the cell growth is 
measured using optical density, and while this measure is proportional to cell
density in many cases, the measurement can be prone to error once the optical 
density is very high. Therefore, you could, for example, add a check into the report to 
highlight any measures of optical density that are higher than a certain value.

> "Get a new repository in the directory you are working in via: `git init`. Ok, you 
now have a revision control system in place. You might not see it, because Git stores
all its files in a directory names `.git`, where the dot means that all the usual
utilities like `ls` will take it to be hidden. You can look for it via, e.g., 
`ls -a` or via a show hidden files option in your favorite file manager. ...
Given that all the data about a repository is in the `.git\ subdirectory of your
project directory, the analog to freeing a repository is simple: `rm -rf .git`." 
[@klemens201421st]

> "Calling `git commit -a` writes a new commit object to the repostiory based on 
all the changes the index was able to track, and clears the index. Having saved your
work, you can now continue to add more. Further---and this is the real, major 
benefit of revision control so far---you can delete whatever you want, confident
that it can be recovered if you need it back. Don't clutter up the code with large
blocks of commented-out obsolete routines---delete!" [@klemens201421st]

> "Having generated a commit object, your interactions with it will mostly consist of 
looking at its contents... The key metadata is the name of the object, which is
assigned via an unpleasant but sensible naming convention: the SHA1 has, a 
40-digit hexadecimal number that can be assigned to an object, in a manner that
lets us assume that no two objects will have the same hash, and that the same object
will have the same name in every copy of the repository. When you commit your files, 
you'll see the first few digits of the hash on the screen... Fortunately, you 
need only as much of the hash as will uniquely identify your commit."
[@klemens201421st]

> "Lists aren't external to the creative process, they are intrinsic to it. They are
a natural part of any project of scale, whether we like it or not." [@savage2020every]

> "The maker in me knows that this is where lists really shine, that it is their capacity 
for simplifying the complex that sets them apart from all other planning tools. 
Not just at the beginning of a project, either, but at every step along the 
creative process, because no matter how exacting the list you make at the outset, 
there will always be things that you missed or, more frequently, that change. It's 
like trying to measure a coastline: it's fractal." [@savage2020every]

> "The value of a list is that it frees you up to think more creatively, by 
defining a project's scope and scale for you on the page, so your brain doesn't
have to hold on to so much information. The beauty of the checkbox is that it
does the same thing with regard to progress, allowing you to monitor the status
of your project, without having to mentally keep track of everything." [@savage2020every]

> "The best part of making a list is, you guessed it, crossing things off. But when you 
physically cross them out, like with a pen, you can make them harder to read, which 
destroys their informational value beyond that single project and, to me at least, makes
the whole thing feel incomplete. The checkbox allowed me to cross something off my list, 
to see clearly *that* I'd crossed it off, and at the same time retain all its 
information while not also adding to the cognitive load of interpreting the list." 
[@savage2020every]

> "**Use version control.** An example is git. This takes time to learn, but the
time is well invested. In the long run, it will be infinitely better than all your
self-grown attempts at managing evolving code with version numbers, switches,
and the like. Moreover, this is the sanest option for collaborative work on code,
and it provides an extra backup of your codebase, especially if the server is
distinct from your personal computer." [@holmes2018modern]

In addition to the data that you record in the laboratory by hand, the type of
study may also typically have data that's generated and recorded by laboratory
equipment. For example, the type of study may often include data collected from
flow cytometry, to measure certain cell populations in samples, or from mass
spectometry, to measure levels of certain molecules. For these data, the
recording format will typically be determined by the equipment, and so you won't
need to create data collection templates for the data. However, you should store
these data files in your project directory as well, where they are easy to
access and integrate with other data as you analyze the data for the study.

Let's look at a more basic way to create a project template. This
involves no fancy tools---in fact, it's so straightforward that at first it
might seem to simple to be useful. For this basic approach, you will create an
example file directory that includes template files and that captures you
desired project directory structure, and then members of your group will copy
and rename that template every time they start a new project of that type.

### Step 4: Testing the project template and refining it

- Edge cases: For example, sometimes only one treatment condition, so not 
possible to run a statistical test comparing conditions
- Didn't think of something: Maybe they add an assay?
- Iterative process to refine what goes in the report. What do we need 
to know from each experiment?



The report template is included in the project directory template, so it will be
copied and available for you to use anytime you start a new project using that 
template. However, you are not obligated to keep the report identical to the template. 
Instead, the template report serves as a starting point, and you can add to it or
adapt it as you work on a study. 

For example, in our example template report, we've included the results of
applying a statistical test that compares each treatment group to the control
group. Instead, for a specific study of this type, you may want to control
treatments against each other. For example, some of the studies in this example
set of studies include a positive control group, where the mice are treated with
a drug that is already in common use for the disease. In some cases, the
researcher may want to control the bacterial load in groups treated with a novel
drug to groups treated with the positive control. You could easily add to the
report template in that case, adding statistical tests where you compare
different treatment groups to each other.

In the example, there was one clear edge case that came up a few times. 
For some experiments, only only treatment group was used. In these cases, 
we can create a report with most of the usual elements, but we can't 
run a statistical analysis that compares groups because there is only 
one group. 

We discovered this edge case as we tried applying our project template to 
multiple experiments. Once we identified it, there were a few approaches
we could take. First, we could adapt the reports by hand in cases where
this happens. To do this, we just need to open the Rmd report for the
report file for those projects and remove the part that compares across 
groups. If this edge case happens rarely, this can be a reasonable 
approach. 

Another approach is to write code in the report template that addresses 
these cases. Because the full report is in code, we can write in code 
that checks how many treatment groups were included and only runs the 
statistical analysis if it was more than one group. This is the 
approach we took in this example. 

### Step 5: Using the project template

- Value comes from using it consistently
- This example is for a set of very similar studies---the same idea can 
apply for developing project templates for a wider range of studies, but 
it will require more thought to have something that works across more
variety

Figure \@ref(fig:basicprojecttemplateuse) gives a basic walk-through of the
simple steps you'll use to start a new project directory once you've created
this type of template. First, you will find the project directory template in
your computer's file system, copy it to where you'd like to save the files for
the new project, and rename the directory to your new project's name. At this
point, you can use RStudio to make this directory an RStudio Project. Next,
you'll open the data collection template files and replace the placeholder
example data in the template (shown in red font) with the real data from your
study. The placeholder data can help you remember the format you should use to
record the real data. Finally, once you've recorded the data for the study or 
experiment, you can open the example report template file. If you've designed
this report template well, it should run with the new data you've recorded to 
create a report for the experiment. At this stage, you can add to the report 
or customize it for the new project by changing the Rmarkdown file and re-rendering
it to update the report. 

```{r basicprojecttemplateuse2, fig.cap = "Steps in using a basic project directory template that you have created for a type of study or experiment.", fig.fullwidth = TRUE, out.width = "\\textwidth"}
knitr::include_graphics("figures/project_template_basic_use.png")
```

All in all, pre-processing aims to extract useful information from the
measurments that you've collected from your experiment, so that this information
can then be used within statistical analysis and visualization to test important
hypotheses that provide insights on your scientific question. Data
*preprocessing*, then, is a term that covers steps must be done after data are
collected but before they are used for further analysis. After the data are
appropriately pre-processed, you can use them for statistical tests and also
combine them with other data collected from the experiment.

In a commentary for *Nature*, Lithgow and coauthors
describe how critical it is to share these details: 

> "Our first task, to develop a protocol, seemed straightforward. But subtle disparities
were endless. In one particularly painful teleconference, we spent an hour debating
the proper procedure for picking up worms and placing them on new agar plates. Some
batches of worms lived a full day longer with gentler techniques. Because a worm's 
lifespan is only about 20 days, this is a big deal. Hundreds of e-mails and many 
teleconferences later, we converged on a technique but still had a stupendous three-day
difference in lifespan between labs. The problem, it turned out, was notation---one
lab determined age on the basis of when an egg hatched, others on when it was laid." 
[@lithgow2017long]

-------------------------------------------------------------------------------

> "The benefit of working with a programming language is that you have the code in
a file. This means that you can easily reuse that code. If the code has
parameters it can even be applied to problems that follow a similar pattern."
[@janssens2014data]

> "Data exploration in spreadsheet software is typically conducted via menus and
dialog boxes, which leaves no record of the steps taken." [@murrell2009introduction]

> "One reason Unix developers have been cool toward GUI interfaces is that, in their
designers' haste to make them 'user-friendly' each one often becomes frustratingly 
opaque to anyone who has to solve user problems---or, indeed, interact with it anywhere
outside the narrow range predicted by the user-interface designer." [@raymond2003art]

> "Many operating systems touted as more 'modern' or 'user friendly' than Unix achieve their
surface glossiness by locking users and developers into one interface policy, and offer an
application-programming interface that for all its elaborateness is rather narrow and rigid. 
On such systems, tasks the designers have anticipated are very easy---but tasks they have
not anticipated are often impossible or at best extremely painful. Unix, on the other hand, has
flexibility in depth. The many ways Unix provides to glue together programs means that components
of its basic toolkit can be combined to produce useful effects that the designers of the individual
toolkit parts never anticipated." [@raymond2003art]

> "During the late 1950s and early 1960s, another step was taken towards getting the 
computer to do more for programmers, arguably the most important step in the history of 
programming. This was the development of 'high-level' programming languages that were
independent of any particular CPU architecture. High-level languages make it possible to 
express computations in terms that are closer to the way a person might express them." 
[@kernighan2011d]

> "In early times, most software was developed by companies and most source code was 
unavailable, a trade secret of whoever developed it." [@kernighan2011d]

> "I think that it's important for a well-informed person to know something about 
programming, perhaps only that it can be surprisingly difficult to get very simple
programs working properly. There is nothing like doing battle with a computer to teach 
this lesson, but also to give people a taste of the wonderful feeling of accomplishment
when a program does work for the first time. It may also be valuable to have enough 
programming experience that you are cautious when someone says that programming is easy, 
or that there are no errors in a program. If you have trouble making 10 lines of code 
work after a day of struggle, you might be legitimately skeptical of someone who claims
that a million-line program will be delivered on time and bug-free." [@kernighan2011d]

> "Computer code is the preferred approach to communicating our instructions to the 
computer. The approach allows us to be precise and expressive, it provides a complete
record of our actions, and it allows others to replicate our work." [@murrell2009introduction]

> "Many biologists are first exposed to the R language by following a cookbook-type
approach to conduct a statistical analysis like a t-test or an analysis of 
variance (ANOVA). ALthough R excels at these and more complicated statistical 
tests, R's real power is as a data programming lanugage you can use to explore and
understand data in an open-ended, highly interactive, iterative way. Learning R as a 
data programming language will give you the freedom to experiment and problem solve
during data analysis---exactly what we need as bioinformaticians." [@buffalo2015bioinformatics]

> "Quite often, users don’t appreciate the opportunities. Noncomputational
biologists don’t know when to complain about the status quo. With modest amounts
of computational consulting, long or impossible jobs can become much shorter or
richer." --- Barry Demchak in [@altschul2013anatomy]

> "Now there are a lot of strong, young, faculty members who label themselves
as computational analysts, yet very often want wet-lab space. They're not
content just working off data sets that come from other people. They want to be
involved in data generation and experimental design and mainstreaming
computation as a valid research tool. Just as the boundaries of biochemistry and
cell biology have kind of blurred, I think the same will be true of
computational biology. It’s going to be alongside biochemistry, or molecular
biology or microscopy as a core component." --- Richard Durbin in
[@altschul2013anatomy]

> "I would say that computation is now as important to biology as chemistry is.
Both are useful background knowledge. Data manipulation and use of information
are part of the technology of biology research now. Knowing how to program also
gives people some idea about what's going on inside data analysis. It helps them
appreciate what they can and can't expect from data analysis software." ---
Richard Durbin in [@altschul2013anatomy]

> "**Does every new biology PhD student need to learn how to program?** To some,
the answer might be “no” because that’s left to the experts, to the people
downstairs who sit in front of a computer. But a similar question would be: does
every graduate student in biology need to learn grammar? Clearly, yes. Do they
all need to learn to speak? Clearly, yes. We just don't leave it to the
literature experts. That’s because we need to communicate. Do students need to
tie their shoes? Yes. It has now come to the point where using a computer is as
essential as brushing your teeth. If you want some kind of a competitive edge,
you’re going to want to make as much use of that computer as you can. The
complexity of the task at hand will mean that canned solutions don’t exist. It
means that if you’re using a canned solution, you’re not at the edge of
research." --- Martin Krzywinski in [@altschul2013anatomy]

> "Although we are tackling many different types of data, questions, and 
statistical methods hands-on, we maintain a consistent computational approach 
by keeping all the computation under one roof: the R programming language and
statistical environment, enhanced by the biological data infrastructure and 
specialized method packages from the Bioconductor project." [@holmes2018modern]

> "The availablility of over 10,000 packages [in R] ensures that almost all 
statistical methods are available, including the most recent developments. 
Moreover, there are implementations of or interfaces to many methods from 
computer science, mathematics, machine learning, data management, visualization
and internet technologies. This puts thousands of person-years of work by 
experts at your fingertips." [@holmes2018modern]


> "After they've been built in, mechanical fasteners make everything after that 
easier. They allow for disassembly, reconfiguration, as well as replacement."
[@savage2020every]

> "That's the reason I prefer mechanical solutions. They can be undone. Whatever
I'm putting together can be pulled apart again without damaging the construction. 
... it takes more engineering, more fiddling, and definitely more time. But the 
trade-off is more options. And I want options. That's the space I like to exist in 
as a maker." [@savage2020every]

> "Standards are for products what grammar is for language. People sometimes
criticize standards for making life a matter of routine rather than inspiration.
Some argue that standards hinder creativity and keep us slaves to the past. 
But try imagining a world without standards. From tenderloin beef cuts to 
the geometric design of highways, standards may diminish variety and 
authenticity, but they improve efficiency. From street signs to nutrition 
labels, standards provide a common language of reason. From Internet 
protocols to MP3 audio formats, standards enable systems to work together. 
From paper sizes ... to George Laurer's Universal Product Code, standards
offer the convenience of comparability." [@madhavan2015applied]

> "The lawmakers of the growing nation [Canada] were eager to establish
a coast-to-coast railway system within the decade. ... Throughout his 
surveys, Fleming relied to crude geometric calculations based on 
longitude, as there was no uniform time across the regions. 'There was
no system. Like the rail lines, the different times touched or overlapped
at 300 points in the country.' ... Even regionally, timekeeping was in 
disarray. If it was 12:13 in Boston, it was 12:27 in Philadelphia and 
12:32 in Buffalo. in 1832 the United States had about 229 miles of 
railroads. By 1880, the country had increased its rail infrastructure 
to close to 95,000 miles. To perserve the sanity of the train driver, 
each railroad company began to maintain its own time. Clocks had up 
to six dials, and train stations displayed the time in various 
citis. A train going from Baltimore, Maryland, to Scranton, Pennsylvania, 
in those days might follow Baltimore time, creating the danger of collisions
when trains operated on a single track." [@madhavan2015applied]

> "Simplicity is not about stripping features down to a bare minimum. 
It's about achieving elegance while maintaining performance." [@madhavan2015applied]

> "The essence of good technology is that it's intuitive, and it evolves. 
Ideally, you don't even want to know it's there." [@madhavan2015applied]



**Uses the same data structure throughout**

The centralizing principal of the tidyverse approach is the format in which data
is stored throughout "tidyverse" coding---the tidy dataframe. Briefly, you can
think of this format in two parts. First, there's the R object type that the
data should be stored in---a basic "dataframe" object. The dataframe object type
is a very basic two-dimensional format for storing data in R. When you print it
out, it will remind you of looking at data in a spreadsheet. The two
dimensions---rows and columns---allow you to include data for one or more
observations, with different values that were measured for each.

The tidyverse approach hinges on using the same data structure throughout your
coding pipeline---specifically, the tidy dataframe structure (see module 2.3 for
more details on this structure). By insisting on the same data structure
throughout, this approach is able to offer small functions that can be chained
together to solve complex problems.

This idea rests on the idea of the power of modularity. You can think of this 
in terms of children's toys---building bricks like Legos are powerful because
they are modular, while a toy like a stuffed animal is not. Each individual 
Lego is small and simple, and would be pretty boring by itself. However, 
because the blocks can be combined in different ways, they can be used to 
create very complex and interesting structures. By contrast, something that is 
not modular, like a stuffed animal, always retains the same structure. While it
might be more interesting and complex to start with than a single Lego block, 
it will not evolve or contribute to something more interesting. 

This modularity works in the same way that it does for Lego bricks. Lego 
bricks can be combined in interesting ways because they all take the same input
and give the same output---the shape of the tubes on the bottom of each brick
accept the shape of the studs at the top of each brick, so they can be
put together in essentially infinite combinations. The tidyverse approach 
in R works in a similar way---the functions in this approach almost all 
input data that are in a dataframe structure (or in a vector structure, for 
functions that operate on columns in the dataframe) and they almost all output
data in the same structure that they input it. As a result, the functions can 
be chained together in interesting ways, where the output of one function 
can feed directly into the input of another. 

> "A common observation is that more of the data scientist’s time is occupied
with data cleaning, manipulation, and 'munging' than it is with actual
statistical modeling (Rahm and Do, 2000; Dasu and Johnson, 2003). Thus, the
development of tools for manipulating and transforming data is necessary for
efficient and effective data analysis. One important choice for a data scientist
working in R is how data should be structured, particularly the choice of
dividing observations across rows, columns, and multiple tables. The concept of
'tidy data,' introduced by Wickham (2014a), offers a set of guidelines for
organizing data in order to facilitate statistical analysis and visualization. ... This framework makes it easy for analysts to reshape, combine, group and otherwise manipulate data. Packages such as ggplot2, dplyr, and many built-in R modeling and plotting functions require the input to be in a tidy form, so keeping the data in this form allows multiple tools
to be used in sequence in a seamless analysis pipeline (Wickham, 2009; Wickham and Francois,
2014)."
[@robinson2014broom]

**Small, simple tools**

 

 > "Even though the Unix system introduces a number of innovative programs and techniques, 
no single program or idea makes it work well. Instead, what makes it effective is an approach
to programming, a philosophy of using the computer. Although that philosophy can't be written
down in a single sentence, at its heart is the idea that the power of a system comes more from
the relationships among programs than from the programs themselves. Many Unix programs do 
quite trivial things in isolation, but, combined with other programs, become general and 
useful tools." [@kernighan1984unix]

Finally, when your research files are kept in one place and well-organized, it
makes it easier for you to share those as a supplement to articles you write
about the research, and are also more likely to do so. One article notes:

> Without clear instructions, many researchers struggle to avoid chaos in their
file structures, and so are understandably reluctant to expose their workflow
for others to see. This may be one of the reasons that so many requests for
details about method, including requests for data and code, are turned down or
go unanswered". [@marwick2018packaging]

"Regardless of the particular project you're working on, your project
directory should be laid out in a consistent and understandable fashion. Clear
project organization makes it easier for both you and collaborators to figure
out exactly where and what everything is." [@buffalo2015bioinformatics]

> "The goal of a research compendium is to provide a standard and easily
recognizable way for organizing the digital materials of a project to enable
others to inspect, reproduce, and extend the research. There are three generic
principles that define research compendia, independent of particular software
tools, and disciplinary contexts. 1. A research compendium should organize its
files according to the prevailing conventions of the scholarly community,
whether that be an academic discipline or a lab group. Following these
conventions will help other people recognize the structure of the project, and
also support tool building which takes advantage of the shared structure. 2. A
research compendium should maintain a clear separation of data, method, and
output, while unambiguously expressing the relationship between those three. In
practice, this means data files must be separate from code files. This is
important to let others easily identify how the original researcher operated on
the data to generate the results. Keeping data and method separate treats the
data as 'read-only,' so that the original data are untouched and all
modifications are transparently documented in the code. The output files should
be considered as disposable, with a mindset that one can always easily
regenerate the output using the code and data. The relationship between which
code operates on which data in which order to produce which outputs must be
specified as well. In his advice to industry data scientists, Ben Baumer’s
article in this collection similarly highlights the importance of keeping data
separate from the presentation of data, or research outputs. 3. A research
compendium should specify the computational environment that was used for the
original analysis. At its most basic, this could be a plain text file that
includes a short list of the names and version numbers of the software and other
critical tools used for the analysis. In more complex approaches, described
below, the computational environment can be automatically preserved or
reproduced as well." [@marwick2018packaging]

With a data collection template for collecting a certain type of data, each
researcher in your lab could copy and rename this file each time they collect a
new set of data---by ensuring a common structure when collecting the data,
including file format, column names, and so on, you can build code scripts that
will work on data collected for all your experiments. You may also have some
standard reports that you want to create with types of data you commonly
collect, and so you could include templates for those reports in your R Project
template. Again, these can be copied and adapted within the project---the
template serves as a starting point so you don't have to start with a blank
slate with every project, but it is not restrictive and can be adapted to each
project as you work on that project.

In addition to the data that you record in the laboratory by hand, you may also
typically have data that's generated and recorded by laboratory equipment. For
example, the type of study may often include data collected from flow cytometry,
to measure certain cell populations in samples, or from mass spectometry, to
measure levels of certain molecules. For these data, the recording format will
typically be determined by the equipment, and so you won't need to create data
collection templates for the data. However, you should store these data files in
your project directory as well, where they are easy to access and integrate with
other data as you analyze the data for the study.

One paper suggests structuring research project
files as R packages, with the following subdirectories in a project
directory [@vuorre2021sharing]: 

- data: Processed R datasets
- data-raw: Raw starting data, in any machine-readable format
- docs
- experiments: I think this is to write surveys, etc., that would run online?
- man: Help files for the R functions
- manuscript: Paper for the project, starting in a mark-up language and rendering to pdf
- model: Code for running models, can be rendered to pdf or HTML
- posters: Code for creating posters with RMarkdown (posterdown), renders to pdf or HTML
- R: Code for project-specific functions
- slides: Code for creating slides with RMarkdown, render to HTML
- vignettes

Here is the suggestion from another paper [@marwick2018packaging]: 

> "An ideal package-based file organization for a more complex project would look like this: 

> . A README.md file that describes the overall project and where to get started. It can be helpful to include graphical summary of the interlocking pieces of the project. 

> . Script files with reusable functions go in the R/ directory. If these functions are documented using Roxygen, then the documentation will be automatically generated in a man/ directory.

> .Raw data files are kept in the data/ directory. If your data are very large, or streaming, an alternative is to include a small-sample dataset so that people can try out the techniques without having to run very expensive computations.

> . Analysis scripts and reports files go in the analysis/ directory. In many cases it can be useful to give the analysis scripts ascending names, for example 001-load.R, 002-clean.R etc. This kind of file-naming helps with organisation, but it does not capture the full tree of dependencies in the way a Makefile or an R Markdown file does. To manage more complex workflows, the analysis/ directory could include either an R markdown file, a Makefile or a Makefile.R file. These files are important because they control the order of the code execution. In more complex projects, careful use of caching or a Makefile can save time by only running code that has not changed since it was last run.

> . A DESCRIPTION file in the project root provides formally structured, machine- and human-readable information about the authors, the project license, the software dependencies, and other metadata of the compendium. When a DESCRIPTION file is included along with the other items above, then the compendium is also a formal, installable R package. When your compendium is an R package, you can take advantage of many time-saving tools for package development, testing, and sharing (e.g., the devtools package that we noted above). R’s built-in citation() function can use that metadata, along with references to any publications that result from the project, to provide users with the necessary information to cite your work.

They provide some more details on some subdirectories: 

> "The R/ directory contains custom functions that are used repeatedly throughout the project. The man/ directory contains the manual (i.e., documentation) for the use of the functions." [@marwick2018packaging]

Another project (workflowr) suggests the following subdirectories [@blischak2019creating]: 

- analysis: Rmd files with code to visualize, model, etc.
- code: "intended for longer-running scripts, compiled code (e.g., C++)
and other source code supporting the data analysis" [@blischak2019creating]
- data: "for storing raw data files" [@blischak2019creating]
- docs: Generated results from Rmd files (HTMLs that can be posted online)
- output: "for saving processed data files and other outputs generated by 
the scripts and analyses" [@blischak2019creating]

Another paper notes some of the key characteristics that can help make 
a project reproducible: 

> "Using this literature as a guideline, we identify several key features of reproducible work. These recommendations are a matter of opinion—due to the lack of agreement on which components of reproducibility are most important, we select those that are mentioned most often, as well as some that are mentioned less but that we view as important.
1. A well-designed file structure:
1. a. Separate folders for different file types.
1. b. No extraneous files.
1. c. Minimal clutter.
2. Good documentation:
2. a. Files are clearly named, preferably in a way where the order in which they should be run is clear.
2. b. A README is present.
2. c. Dependencies are noted.
2. d. Code files contain descriptive comments.
3. Reproducible file paths:
3. a. No absolute paths, or paths leading to locations outside of a project's directory, are used in code—only portable (relative) paths.
[Others not related to directory structure]." [@bertin2021creating]

One R package, designed to set-up a project directory structure, suggests
including a README file not only in the top-level directory, but also in 
each subdirectory [@prodigenr]. That project incorporates the following
subdirectories: 

- R, "Should contain the R scripts and functions used for the analysis"
- data, "If relevant, is where the processed (or simulated) data that is 
used for the project as well as the results of the project's analysis"
- data-raw "If relevant, is where the scripts that process the raw data
into the usable data are kept and, optionally where the raw data is 
also kept"
- doc, "Should contain the files related to presenting the project's 
scientific output. Already has the report / manuscript inside". Can 
include the report, but also slides and other reporting output. 

> "Within a given project, I use a top-level organization that is logical, with chronological organization at the next level, and logical organization below that. A sample project, called msms, is shown in Figure 1. At the root of most of my projects, I have a data directory for storing fixed data sets, a results directory for tracking computational experiments peformed on that data, a doc directory with one subdirectory per manuscript, and directories such as src for source code and bin for compiled binaries or scripts. Within the data and results directories, it is often tempting to apply a similar, logical organization. For example, you may have two or three data sets against which you plan to benchmark your algorithms, so you could create one directory for each of them under data. In my experience, this approach is risky, because the logical structure of your final set of experiments may look drastically different from the form you initially designed. This is particularly true under the results directory, where you may not even know in advance what kinds of experiments you will need to perform. If you try to give your directories logical names, you may end up with a very long list of directories with names that, six months from now, you no longer know how to interpret.
Instead, I have found that organizing my data and results directories chronologically makes the most sense. Indeed, with this approach, the distinction between data and results may not be useful. Instead, one could imagine a top-level directory called something like experiments, with subdirectories with names like 2008-12-19. Optionally, the directory name might also include a word or two indicating the topic of the experiment therein. In practice, a single experiment will often require more than one day of work, and so you may end up working a few days or more before creating a new subdirectory. Later, when you or someone else wants to know what you did, the chronological structure of your work will be self-evident.
Below a single experiment directory, the organization of files and directories is logical, and depends upon the structure of your experiment. In many simple experiments, you can keep all of your files in the current directory. If you start creating lots of files, then you should introduce some directory structure to store files of different types. This directory structure will typically be generated automatically from a driver script, as discussed below." [@noble2009quick]

This project also incorporates "TODO.md" in the top level of the project
directory, as well as a "DESCRIPTION" file that "includes metadata about
your project, in a machine readable format, and that also stores a list
of the R packages your project depends on" [@prodigenr].


> "**Centralize the location of the raw data files and automate the derivation of 
intermediate data.** Store the input data on a centralized file server that is 
profesionally backed up. Mark the files as read-only. Have a clear and linear 
workflow for computing the derived data (e.g., normalized, summarized, transformed, 
etc.) from the raw files, and store these in a separate directory. Anticipate that
this workflow will need to be run several times, and version it. Use the 
`BiocFileCache` package to mirror these files on your personal computer. 
[footnote: A more basic alternative is the rsync utility. A popular solution offered
by some organizations is based on ownCloud. Commercial options are Dropbox, 
Google Drive and the like]." [@holmes2018modern]

Here are some specific steps to create the files for the project directory
template:

1. Review the list of data you typically collect or files you create for that
type of study or experiment, a list you created when designing the blueprint for
the directory template
2. Create template files for any data collection that is typical for that type
of study or experiment. Use example or placeholder data to create examples of
those files.
3. Create a directory structure that divides the types of files into
subdirectories of similar types.
4. Create one or more templates of report files that access and report on the
data in the project template

There are a few factors that could keep data
from being suited for a tidyverse approach. We will discuss these 
characteristics in detail in this module, to help you understand when 
you might need another approach in part of your pipeline. 

Next, we will describe how you can tackle data preprocessing and analysis 
even if your data is not suited to using the tidyverse. In many cases, 
this can be done using a collection of packages available for R through 
a platform called Bioconductor. In many cases, you'll be able to combine
the two approaches, using Bioconductor when needed and the tidyverse
approach for steps in your pipeline when the data is more suited to this
approach. This is a powerful combination, as it leverages the efficiency 
and power of the tidyverse approach with more customized tools that are 
necessary to tackle more complex processes. 

> "Big data is encountered in genomics for two reasons: the size of the genome
and the heterogeneity of populations. Complex organisms, such as plants and
animals, have genomes on the order of billions of base pairs (the human genome
consists of over three billion base pairs). The diversity of populations,
whether of organisms, tissues or cells, means we need to sample deeply to detect
low frequency events. To interrogate long and/or numerous genomic sequences,
many measurements are necessary. For example, a typical whole genome sequencing
experiment will consist of over one billion reads of 75–100 bp each. The reads
are aligned across billions of positions, most of which have been annotated in
some way. This experiment may be repeated for thousands of samples. Such a data
set does not fit within the memory of a current commodity computer, and is not
processed in a timely and interactive manner. To successfully wrangle a large
data set, we need to intimately understand its structure and carefully consider
the questions posed of it." [@lawrence2014scalable]

> "A major challenge in the analysis of scRNA-seq data is the scalability of analysis methods as datasets increase in size over time. This is particularly problematic as experiments now frequently produce millions of cells [50–53], possibly across multiple batches, making it challenging to even load the data into memory and perform downstream analyses including quality control, batch correction and dimensionality reduction. Providing analysis methods, such as unsupervised clustering, that do not require data to be loaded into memory is an imperative step for scalable analyses. While large-scale scRNA-seq data are now routinely stored in on-disk data formats (e.g. HDF5 files), the methods to process and analyze these data are lagging." [@hicks2021mbkmeans]

> "If you use too much memory, R will complain. The key issue is that R holds
all the data in RAM. This is a limitation if you have huge datasets. The up-side
is flexibility—in particular, R imposes no rules on what data are like."
[@burns2011r] 

> "Random access memory (RAM) is a type of computer memory that can be accessed
randomly: any byte of memory can be accessed without touching the preceding
bytes. RAM is found in computers, phones, tablets and even printers. The amount
of RAM R has access to is incredibly important. Since R loads objects into RAM,
the amount of RAM you have available can limit the size of data set you can
analyse." 
[@gillespie2016efficient]

> "A rough rule of thumb is that your RAM should be three times the size of your
data set." 
[@gillespie2016efficient]

> "RAM is cheap and thinking hurts." 
Uwe Ligges (about memory requirements in R) R-help (June 2007)

> "The strengths of R are also its weaknesses: the R API encourages users to
store entire data sets in memory as vectors. These vectors are implicitly and
silently copied to achieve copy-on-write semantics, contribuing to high memory
usage and poor performance." 
[@lawrence2014scalable]

> "Our ultimate goal is to process and summarize a large data set in its
entirety, and iteration enables this by limiting the resource commitment at a
given point in time. Limiting resource consumption generalizes beyond iteration
and is a fundamental technique for computing with big data. In many cases, it
may render iteration unnecessary. Two effective approaches for being frugal with
data are restriction and compression. Restriction means controlling which data
are loaded and lets us avoid wasting resources on irrelevant or excessive data.
Compression helps by representing the same data with fewer resources."
[@lawrence2014scalable]

In the previous modules and sections of this module, we have talked about two
topics. First we have talked about the convenience and power of tidyverse tools.
These tools can be used at points in your workflow when the data can be stored
in a simple standard format: the tidy dataframe format. We have also talked
about reasons why there are advantages to using more complex data storage
formats earlier in the process.

> "**Out-of memory data and chunking.** Some datasets are too big to load 
into random access memory (RAM) and manipulate all at once. Chunking means
splitting the data into manageable portions ('chunks') and then sequentially 
loading each portion, computing on it, storing the results and removing it
from memory before loading the next portion. R also offers infrastructure
for working with large datasets that are stored on disk in a relational 
database management systems (the DBI package) or in HDF5 (the rhdf5 package).
The Bioconductor project provides the class `SummarizedExperiment`, which 
can store big data matrices either in RAM or in an HDF5 backend in a manner 
that is transparent to the user of objects of this class." [@holmes2018modern]

> "Big data is encountered in genomics for two reasons: the size of the genome
and the heterogeneity of populations. Complex organisms, such as plants and
animals, have genomes on the order of billions of base pairs (the human genome
consists of over three billion base pairs). The diversity of populations,
whether of organisms, tissues or cells, means we need to sample deeply to detect
low frequency events. To interrogate long and/or numerous genomic sequences,
many measurements are necessary. For example, a typical whole genome sequencing
experiment will consist of over one billion reads of 75–100 bp each. The reads
are aligned across billions of positions, most of which have been annotated in
some way. This experiment may be repeated for thousands of samples. Such a data
set does not fit within the memory of a current commodity computer, and is not
processed in a timely and interactive manner. To successfully wrangle a large
data set, we need to intimately understand its structure and carefully consider
the questions posed of it." [@lawrence2014scalable]


> "A major challenge in the analysis of scRNA-seq data is the scalability of analysis methods as datasets increase in size over time. This is particularly problematic as experiments now frequently produce millions of cells [50–53], possibly across multiple batches, making it challenging to even load the data into memory and perform downstream analyses including quality control, batch correction and dimensionality reduction. Providing analysis methods, such as unsupervised clustering, that do not require data to be loaded into memory is an imperative step for scalable analyses. While large-scale scRNA-seq data are now routinely stored in on-disk data formats (e.g. HDF5 files), the methods to process and analyze these data are lagging." [@hicks2021mbkmeans]


> "Reading in a large dataset for which you do not have enough RAM is one easy way to freeze up your computer (or at least your R session). This is usually an unpleasant experience that usually requires you to kill the R process, in the best case scenario, or reboot your computer, in the worst case." [@peng2016r]

> "If you use too much memory, R will complain. The key issue is that R holds
all the data in RAM. This is a limitation if you have huge datasets. The up-side
is flexibility—in particular, R imposes no rules on what data are like." [@burns2011r] 


> "Random access memory (RAM) is a type of computer memory that can be accessed
randomly: any byte of memory can be accessed without touching the preceding
bytes. RAM is found in computers, phones, tablets and even printers. The amount
of RAM R has access to is incredibly important. Since R loads objects into RAM,
the amount of RAM you have available can limit the size of data set you can
analyse." [@gillespie2016efficient]

> "A rough rule of thumb is that your RAM should be three times the size of your
data set."  [@gillespie2016efficient]

> "RAM is cheap and thinking hurts." Uwe Ligges (about memory requirements in R) R-help (June 2007)

> "The strengths of R are also its weaknesses: the R API encourages users to
store entire data sets in memory as vectors. These vectors are implicitly and
silently copied to achieve copy-on-write semantics, contribuing to high memory
usage and poor performance." [@lawrence2014scalable]

> "Our ultimate goal is to process and summarize a large data set in its
entirety, and iteration enables this by limiting the resource commitment at a
given point in time. Limiting resource consumption generalizes beyond iteration
and is a fundamental technique for computing with big data. In many cases, it
may render iteration unnecessary. Two effective approaches for being frugal with
data are restriction and compression. Restriction means controlling which data
are loaded and lets us avoid wasting resources on irrelevant or excessive data.
Compression helps by representing the same data with fewer resources." [@lawrence2014scalable]

> "There is a cost to the free lunch. That `print` is generic means that what
you see is not what you get (sometimes). In the printing of an object you may
see a number that you want---an R-squared for example---but don’t know how
to grab that number." [@burns2011r]

> "Bioconductor packages support the reading of many of the data types and formats
produced by measurement instruments used in modern biology, as well as the 
needed technology-specific 'preprocessing' routines. This community is 
actively keeping these up-to-date with the rapid developments in the 
instrument market." [@holmes2018modern]

> "The core of the engineering mind-set is what I call modular systems thinking.
... A specific technique in modular systems thinking, for example, includes
a functional blend of deconstruction (breaking down a larger system into 
its modules) and reconstructionism (putting those modules back together). The
focus is on identifying the strong and weak links---how the modules work, 
don't work, or could potentially work---and applying this knowledge to 
engineer useful outcomes. ... there's a top-down design strategy---'divide
and conquer'---in which each subtask is separately attacked in a progression
toward achieving the final objective." [@madhavan2015applied]

At its most basic, a template for a project directory is a computer file
directory that includes the subdirectories (with standardized names for each
subdirectory) that you want to include---for example, you may know that you will
always want the project directory to include subdirectories for "raw_data" (with
its own subdirectories for different types of data, for example for "cfus" and
"flow"), "data" (with clean versions of the data, after conducting any needed
preprocessing, like calculating colony-forming units in a sample based on data
from plating at different dilutions, or the output from gating flow cytometry
data), "reports" (for writing, posters, and presentation slides), and "R" (for
common scripts that you use for preprocessing, visualization, and data
analysis). 

One critical step in the design process is to iterate: make a first version of
something (a prototype) and then try it out to see how it works, then revise and
improve based on what you find out in practice [@osann2020design]. In this case,
that means creating a project directory template, but considering it a first
draft until you try it in practice. You can refine this template once you've
tried it and identified where it works and where it doesn't. A prototype doesn't
have to be fully refined, just built out enough that the users can test it out
[@osann2020design].

This lets you get a
firmer idea of what problem you're trying to solve and what is needed to solve
this. Another early step is to synthesize based on what you've observed
[@osann2020design]. This allows you to think about a variety of needs and
prioritize and refine them so that you have a very clear criteria to determine
if what you design is successful in addressing the problem you were trying to
solve.

[Data] The recorded data files and the files that come directly from equipment can all
be considered raw data files. In addition, you may typically create some files
with pre-processed data. For example, if you have flow cytometry data, you may
initially get large files in a format specific to flow cytometry ("fcs" files)
from the equipment. You may use a program to pre-process these files, for
example, to manually gate the data to identify specific cell populations. In
addition to saving the raw data files directly from the flow cytometer, you'll
also want to save the processed data files in your project directory, since
these are the files that you'll analyze and integrate with other data from the
project.

You could create
subdirectories both for the raw data and for the processed data that result from
pre-processing steps. For example, you might want to store the raw fcs files
within a subdirectory called "data_raw" and the processed (gated) csv files
within a subdirectory called "data". 

[Is Figure \@ref(fig:projecttemplatecomplex) an orphan?]

```{r projecttemplatecomplex, fig.cap = "Example of a more complex project directory structure that could be created, with directories added to store data collected through flow cytometry and single cell RNA sequencing.", fig.fullwidth = FALSE, out.width = "\\textwidth"}
knitr::include_graphics("figures/project_template_morecomplex.png")
```

When you create a project directory template, we recommend that you create a
subdirectory named something like "reports" to use to store any Rmarkdown report
files for the project. This organization will make it clear where you've stored
your reports in the project directory. You'll be able to use file and directory
pathnames to access all the data in the project, so it will be easy to use the
study's data in the report even if they're in separate subdirectories.

All files for this project
can be stored within a well-designed directory, and this directory can be
enhanced into something called an R Project very easily. In this module, we'll
explore how to use an R Project and what advantages it offers compared to other
ways of organizing the files associated with a study. In particular, we'll build
on ideas from earlier modules about creating reproducible data collection
templates, as in this example, the use of a common template across many studies
in a set makes it very easy to create and apply a common reporting template to
the data, easily creating a reproducible report for each of the nineteen studies
in the example set of studies. Further, we'll look at how this organization
allows not only for reporting on specific studies in a reproducible way, but
also makes it easier to create an overall report that combines results and
details from all studies in the set.

When you create a project directory template, we recommend that you create a
subdirectory named something like "reports" to use to store any Rmarkdown report
files for the project. This organization will make it clear where you've stored your
reports in the project directory. You'll be able to use file and directory pathnames
to access all the data in the project, so it will be easy to use the study's data
in the report even if they're in separate subdirectories. There's only one tool 
you'll need to do this---you'll need to learn how to use relative pathnames 
within R code to access files in a different part of your project directory. 

What are **tools** in programming? They can include: 

- algorithms / approaches / numerical methods (regular expressions)
- packages (`stringr` for regular expressions)
- functions
- data structures (e.g., tidy dataframe, Bioconductor data structures)
- simple, multi-use tools (functions): "small, sharp tools

Later, we will talk about an approach called the "tidyverse" approach, which
focuses on sets of these general purpose tools. This approach allows you to 
use the same set of tools for many problems, and it does so by focusing on 
keeping your data in the same type of data structure (a tidy dataframe) for
as much of the pipeline as you can. This approach therefore fosters the 
approach of starting by learning a small set of tools very well. 

> "The functional orientations of Shepherd-Barron and the Wright brothers
(or Edison) have one thing in common. It's what the Santa Fe Institute 
scholar Brian Arthur calls *deep craft*---the ability to know intimately 
the various functionalities and how to effectively combine them. 'It 
consists in [knowing] what is likely not to work, what methods to use, 
whom to talk to, what theories to look to, and above all of how to 
manipulate phenomena that may be freshly discovered and poorly understood',
Arthur writes. Systems-engineering approaches that underline the 
efficiency and reliability of failure-tolerance products like ATMs and 
airplanes have a strong connection to deep craft." [@madhavan2015applied]

**Modular, not monolithic**

[Combine this section with "iterate"?]

[Modular is: easier to understand, easier to maintain, easier to develop
(big steps, then refine)]

Another key thing to remember as you develop R scripts is that they are all,
even those that do very complex things, made up of lots of smaller, simpler
pieces. R scripts are, in other words, modular, not monolithic.

As you develop a script, plan to create it by chaining together simpler steps. You can 
start by trying to map out the key things that you'll need to do in the pipeline, and
then you can dive into each of those sections and start writing the code to 
achieve that step. Think of this process as like outlining a paper before you 
write it. For example, if you are preprocessing data that measured bacterial 
growth rates, some of the broad sections you may have in your pipeline are: 
(1) reading in the data, (2) converting it into a tidy data format so it's easier to work 
with, (3) determining the time range in the experiment when each sample was in its
exponential growth phase, (4) using data in that time range to estimate the growth
rate for each sample, and so on. Start by dividing your script into these sections
(you can mark each off with a code comment---how to use these are described 
later in this section), and then you can start to develop the code, starting with 
the code to input the data. 

This method---which breaks a big problem into smaller ones---is a useful
problem-solving approach across many fields. Madhavan, in a 2015 book called
*Applied Minds: How Engineers Think*, labels this approach "modular systems
thinking". He notes that this approach, "includes a functional blend of
deconstruction (breaking down a larger system into its modules) and
reconstructionism (putting those modules back together)" [@madhavan2015applied].
He even calls this type of modular systems thinking, which he notes is a species
of "divide and conquer", "the core of the engineering mind-set."
[@madhavan2015applied]

He describes, in fact, how the Wright brothers used this approach to invent 
the airplane: 

> "Wilbur and Orville Wright took about four years to implement their first
prototype of a flying machine. While their competitors focused on the design of
winds, fuselage, and propulsion, the Wright brothers were devoted to getting the
fundamental functions of lift, thrust, drag, and yaw correct. Consistent with
the notion of modular thinking, they solved each puzzle at a subsystems level
before they moved to the next layer of the assembly, and along the way they
invented new instruments and measurement techniques." [@madhavan2015applied]

Thinking of a code script as something that is modular is critical as you start 
learning how to code in a language like R. By dividing the full pipeline into smaller
steps, you can tackle it one piece at a time. You can look for resources that are 
linked to a specific part of the problem, rather than feeling like you need to 
find other examples of full, start-to-finish pipelines that have done the same thing
that you're trying to do. For example, the step of reading in data that you recorded
in an Excel spreadsheet comes down to a simple task---reading data into R from an 
Excel file. It doesn't matter whether that file has data on bacterial growth rates
or on mice weights or on anything else---the process of reading it into R is the 
same. Therefore, when you're working on this step, you can google help on reading 
Excel files into R if you're stuck, without worrying about whether the resources
that you find to help used examples of the type of data you have or other types of 
data. 

> "Very little of what I've built over the years is monolithic---just a single chunk.
Most of the time, I build things in components, then attach those pieces together 
as I go. So yes, the component parts are pieces that have been made small in 
precise ways from larger chunks of material, but eventually they will be assembled
to create much larger and more complex objects than any of the raw source materials."
[@savage2020every]

**Iterate!**

Code is something that you develop over multiple drafts---it's the kind of thing
that you build by trying out one thing, seeing how it works, then revising, reworking,
and extending to get to the pipeline you ultimately want. You will quickly feel
frustrated if you try to build your pipelines in a single go, without this process
of iteration. Just as many writers suffer writer's block more often when they 
try to get everything perfect in a first draft, you will feel frustrated if you 
think that you should build your code to its final form, from start to 
finish, in just one try. As Madhavan notes, 

> "Tweaking and prototyping are basic human habits." [@madhavan2015applied]

[Examples of prototyping include: thumbnails, storyboards, outlines]

[Prototype and then refine]

When you build a code pipeline, try to break your process up into some key chunks. 
For example, you might know that you'll first need to input the data into R
(first chunk), then preprocess the data by log-transforming one of the measurements
(second chunk), and then run a statistical hypothesis test to compare two groups
that you measured (third chunk). Once you've broken the code into these chunks, 
you can focus on a single chunk at a time and iterate the code for that task until 
you're happy with it. 

For example, if the first chunk is to read in the data, you might first try to 
use a standard tool that you've used before, like the `readr` function `read_csv`, 
which reads in data from a CSV file. However, maybe this tool doesn't work for 
the file format your data are in (perhaps they're in an Excel file, for example), 
and so you get an error when you try to do that. Keep calm when you get that error, 
and just iterate back and look up some tools for reading Excel files into R. 
When you've found some, try adapting the examples from their help files or vignettes
to work in your code pipeline. If that works, then spend some time exploring the
input that you get, so you can start to think about how you'll work with the data
for the next chunk in your pipeline. 

Once you're done with this process of iteration, make sure that you go back and 
clean up the code pipeline, so that the code script is streamlined to walk 
efficiently through the tasks. Also, rerun the whole script in a new R session, 
so you can make sure that it is self-contained. One of the most common 
reasons for a script not working the same later is that, when it was originally 
developed, the coder was relying on some objects that were created in their R
environment at the console, not through code in the script, and so were available
when running the code originally, but not when it's opened in a new R session. 
You can often identify and fix these issues much more quickly by testing your 
code pipeline in a new R session right after you create it, rather than 
waiting and running into that problem days or weeks later, when you try to rerun 
the script.

> "Prototypes create new capabilities. Prototypes foster adaptation to new 
forms, new expections, and new offshoots of technologies. Prototypes are
the starting points toward our ultimate creative destinations." [@madhavan2015applied]



> "Programming in the real world tends to happen on a large scale. The strategy is similar
to what one might use to write a book or undertake any other big project: figure out what
to do, starting with a broad specification that is broken into smaller and smaller pieces, 
then work on the pieces separately, while making sure that they hang together. In programming, 
pieces tend to be of a size such that one person can write the precise computational steps
in some programming language. Ensuring that the pieces written by different programmers
work together is challenging, and failing to get this right is a major source of errors. 
For instance, NASA's Mars Climate Orbiter failed in 1999 because the flight system software
used metric units for thrust, but course correction data was entered in English units, 
causing an erroneous trajectory that brought the Orbiter too close to the planet's 
surface." [@kernighan2011d]

> "Developing code in R is a back-and-forth between writing code in a rerunnable script
and exploring data interactively in the R interpreter. To be reproducible, all steps 
that lead to results you'll use later must be recorded in the R script that accompanies
your analysis and interactive work. While R can save a history of the commands you've
entered in the interpreter during a session (with the command `savehistory()`), 
storing your steps in a well-commented R script makes your life much easier when you 
need to backtrack to understand what you did or change your analysis." [@buffalo2015bioinformatics]

> "People not doing the computational work tend to think that you can write a
program very fast. That, I think, is frankly not true. It takes a lot of time to
implement a prototype. Then it actually takes a lot of time to really make it
better." --- Heng Li in [@altschul2013anatomy]


> "Software, like many other things in computing, is organized into layers, analogous to 
geological strata, that separate one concern from another. Layering is one of the important
ideas that help programmers to manage complexity." [@kernighan2011d]

> "At the simplest level, programming languages provide a mechanism called functions that make
it possible for one programmer to write code that performs a useful a useful task, then package
it in a form that other programmers can use in their programs without having to know how it 
works." [@kernighan2011d]

> "**Use functions.** It's better than copy-pasting (or repeatedly source-ing)
stretches of code." [@holmes2018modern]

> **Use the R package system.** Soon you'll note recurring function or variable
definitions that you want to share between your different scripts. It is fine to 
use the R function `source` to manage them initially, but it is never too early 
to move them into your own package---at the latest when you find yourself staring
to write emails or code comments explaining to others (or to yourself) how to use
some functionality. Assembling existing code into an R package is not hard, and it 
offers you many goodies, including standardized ways of composing documentation, 
showing code usage examples, code testing, versioning and provision to others. 
And quite likely you'll soon appreciate the benefits of using namespaces." 
[@holmes2018modern]

Some tools that can help you avoid repetition include: 

- Faceting in `ggplot2`
- The `map` family of functions in the `purrr` package
- The `group_by` and `summarize` / `mutate` workflow in the `dplyr` package


**Scripts are for humans to read, not just computers**

A code script, by definition, is a set of instructions that you send to the 
computer to evaluate. However, many starting programmers make things hard
on themselves by treating the script as a document that is only for the computer. 

While it is true that the code script needs to be written so that the 
computer can evaluate it, that is only the first level of usefulness for the
script. Keep in mind that one of the key advantages of writing a computer
script is so that the code becomes reproducible---you and others can revisit
it, rerun it, and adapt it at a later time. Otherwise, in a programming 
language like R, you could just run all your code at the console, rather than 
saving it in a script. One of the biggest barriers to reproducibility is unclear
code. This can make it harder for others to figure out what the does. It also 
makes it harder for you and others in your team to figure out what you did when 
you look at the code months or years later.

It is helpful to separate the process of developing your code into two steps: 
first creating, and then editing. [Something about writing fast]

To make sure that your code is "legible" by humans, take the time to clean it
up once you have it running. (As an added bonus, it's often the case that you'll
find a few bugs when you do so, and so you can catch and fix those.) Go back 
through the whole script once you have built your pipeline. There are several
specific things you can do during this editing process: [enumerate]. 

[Break up monolithic code; Remove dead-end code; Improve names]

Make sure you add useful comments to your code. In R, any line that starts with
a "#" will be ignored by R, so you can use those to write messages for humans
that the computer will skip. You can use comments in a variety of ways to make
your code clearer to humans. First, you can use them to divide the code script
into sections and to label those sections. This will help a human reader get an
overview of the script when they first look at it and navigate the script as
they continue to work with it. You can also use code comments to explain coding
choices that you've made along the way that might not immediately be clear. For
example, if you needed to use a function that is outside of your core set of
tools, and that you found to tackle a specific problem, you could comment on
that, and perhaps add a link to more information on that function.

It also is good practice to follow style guidelines. You can also clean up the
code substantially by editing it to follow a set of style guidelines for R code.
These are guidelines that specify things that don't matter to the computer, but
that can help human readers. For example, there are many cases where R can
evaluate code the same way whether spaces have been used to separate pieces of
the code or not, but a thoughtful use of spacing can help a human read the code.
There are several style guidelines available for R and other programming
languages, and it is less important which one you use than that you follow one
consistently (at least by the time you edit your code for clarity to humans).
One style guide you can use is the one created by Hadley Wickham available at:
https://style.tidyverse.org/index.html.

Finally, it is useful to use this editing stage to simplify the logic and 
function calls within the script. Often, when you are first trying to get some
code to work, you might come up with a pretty ungainly function call to do 
so. Once you've gotten it working and figured out a general approach, you can 
often go back and rethink how you've coded the approach, and you can often 
come up with a simpler way to perform the same task using less code or 
clearer code. The more you can simplify your code, the lower the risk for 
bugs and the easier it will be for humans to understand. 

> "Filming [of Apocalypse Now] started in March 1976 and was scheduled to
take six weeks. It took sixteen months. The director, Francis Ford Coppola, 
shot around 230 hours of footage, with multiple takes of the same scene. He
was hoping to capture a magical or unusual performance. He encouraged the 
actors to ad-lib, which produced some poetic moments but also hours of unusable
footage. Once filming was finished, the movie had to be cut and recut for 
months. It took Coppola and his film editor Walter Murch nearly three years to 
edith the footage... Why produce a lot of work and then throw away 95 percent?
The two processes seem contradictory. Why not just produce the 5 percent ultimately
used? Because you don't know *which* 5 percent that will be. Editing can 
be hard becuase you're discarding things you have put a lot of energy into
making." [@judkins2016art]

> "You might not write well every day, but you can always edit a bad page. 
You can't edit a blank page." -- Jodi Picoult, cited in [@judkins2016art]

> "That a language is easy for the computer expert does not mean it is 
necessarily easy for the non-expert, and it is likely non-experts who will do the
bulk of the programming (coding, if you wish) in the near future." [@hamming1997art]

> "The code that a programmer writes, whether in assembly language or (much more likely) in 
a high-level language, is called *source code*. ... Source code is readable by other programmers, 
though perhaps with some effort, so it can be studied and adapted, and any innovations or ideas
it contains are visible." [@kernighan2011d]

> "*Document your methods and workflows.* This should include full command lines (copied
and pasted) that are run through the shell that generate data or intermediate results. 
Even if you use the default values in software, be sure to write these values down; 
later versions of the program may use different default values. Scripts naturally 
document all steps and parameters ..., but be sure to document any command-line options
used to run this script. In general, any command that produces results in your work needs
to be documented somewhere." [@buffalo2015bioinformatics]

Later sections will give deeper dives into other topics that might also form part
of the editing process.

**Some things belong in the console, not the script**

[Dead-end code]

Finally, keep in mind that all of the code you write, as you develop a script with 
a pipeline, does *not* need to be recorded in the script. Of course you will want
to include all the code that is necessary for the script as a whole to work on its
own. However, as you develop code, you'll take some steps to explore your data or
to do things like installing packages that you don't have yet. 

For example, as you work on your code, you'll likely want to look at the contents
of your R objects as you work on them. If you have read in your data from a file into
an R object, you'll want to look and make sure it looks like it read in correctly. 
If you have created a new object that summarizes the original data by taking the 
average of each group, you'll probably want to look at that as you develop the code
to create it, again to check and explore as you build the code. To do this, you can 
call the object's name (e.g., type `my_data` and run it) or use functions like 
`head`, which prints out the first few rows or items of the object (e.g., `head(my_data)`), 
`tail`, which prints out the last few rows or items of the object, or `str`, which 
summarizes the structure and contents of the object. 

All of these are useful to run as you draft and edit your code, but calls like this that
print out pieces of the data to check aren't necessary in the final R script (and in 
fact can make it messier than it needs to be and result in a lot of extra print-out at
the console when you run the code as a batch once you've finalized it). As you develop
your code, then, try to get in the habit of not writing these types of exploratory 
function calls in the script you're developing. Instead, write them directly in the 
console and run them from there. 

[Figure---writing exploratory code in the console versus the R script. Same for installing
packages]

The other piece of code that you should run in the console rather than saving in
the R script is code to install new packages. Since you only need to install a
package once (until you get a different computer or update base R), you don't
need to run `install.package` function calls everytime you run the code in a
script. Including these functions in the script will therefore just slow it
down. Instead, go to your console directly to write the function calls to
install new packages (or, if you prefer, in RStudio you can go to Tools on the
menu bar and select Install Packages).

The final goal is to develop an R script that has everything it needs to run the
full pipeline from in a fresh R session. In other words, if it uses functions
from packages, it will include the code to load those packages (`library`
function calls). For every R object that it uses, there will be code that
creates that object in the script. It will not include, however, extra pieces of
code that were used to explore the objects as you built the code. To test that
the R script can run in a fresh R session, you can close R and reopen it (make
sure that you've set your global options in R to never save the workspace to
.Rdata on exit, to not restore the .Rdata into the workspace on exit, and to not
save the history, all of which you can set by going to the RStudio Tools menu
item and selecting Global Options). When you reopen R, there will not be any
objects in the environment, and there will not be a history of any of the
previous function calls that you ran. Try running the code in the script in this
fresh environment and make sure that you weren't relying on anything that you
did outside the script to make the code work. If the code can run in a fresh
environment like this, then any R user should be able to rerun everything in the
script themselves (although they may need to install a few new libraries first
if they don't have all the required libraries yet).

------------------------------------------------------------------------------------


### Potential quotes

> "Every maker needs to give themselves the space to screw up in the pursuit of 
perfecting a new skill or in learning something they've never tried before. 
Screwing up IS learning." [@savage2020every]

> "Mistake tolerance is particularly valuable in this aspect of the creative
process. When you know what you want to make, but you're not exactly sure what 
it should look like or how it should operate, you need to give yourself permission
to experiment, to iterate your way there. That's not just how you get to what you 
want, it's how you get good at it. You have to do it over and over and over 
again. Anticipating mistakes is how you put space around the unfamiliar and 
the unknown." [@savage2020every]

> "If you expect to nail it the first go round every time you build something 
new---or worse, you demand it of yourself and you punish yourself when you come 
up short---you will never be happy with what you make and making will never 
make you happy". [@savage2020every]

BioC Conference

> "Just as writing a book involves an outline and a rough draft (so many drafts!),
which get polished into a final manuscript, making things often benefits from a 
preliminary stage where the big details get worked out, and then a final fabrication
stage where the small details get worked out. Cardboard is a low threshold material
that can make discussion of ideas at the perliminary stage so much easier and 
more complete." [@savage2020every]

> "In my professional life, I have worked with every conceivable type of client and 
collaborator, from those who were makers with the same or greater expertise as me, 
who understood deeply what I was talking about when we discussed a build, to clients
who couldn't glue two blocks of wood together if you put the blocks in their hands, 
covered with glue, and told them to clap. Being able to communicate your ideas to 
clients and collaborators is one of the most important skills to possess as a maker, 
otherwise some of your projects may never get off the ground." [@savage2020every]

> "To make anything, it's critical to have a physical understanding of how all the
component parts of your project will fit together." [@savage2020every]

> "What material can you wrap your arms around to gain a complete sense for the skills
you want to master and the objects you want to make? Is it cardboard? Muslin fabric?
Crappy butcher cuts? Scrap wood? The backside of recycled printer paper? A word
processor? It really doesn't matter as long as it allows you to be messy and it
keeps you moving forward in your journey as a maker." [@savage2020every]


> "Once you have a new tool you thing you need, spend some time getting to know it 
physically. With certain tools, I'll go so far as to take them apart, just to 
understand them better, inside and out. ... If you are unfamiliar with a tool or
inexperienced with the techniques required to use it, getting comfortable like this
is the most important thing you can do, because you might really need this thing, but
if you are intimidated by it, you aren't going to want to use it, and then 
what's the point?" [@savage2020every]

> "If you've never used a tool before, reviews and articles about it can only get
you so far. You need to work with a tool in order to see how the tool works for YOU.
You need on-the-ground experience with it in your hands." [@savage2020every]

With open-source, you can think of it as builing a collection of different tools, 
rather than a black box. The tools can come from different companies (like real tools)---you
don't have to limit to consuming the tools created by a single producer (as with 
most proprietary software systems). 

For free and open-source software, you don't have to invest money to get more tools.
Instead, the investment for each new tool is the time that it takes to learn how 
to use it in your workflow. This includes several elements. You'll need to understand
what primary input is used by the function, both in terms of the input's conceptual 
content and the format or structure in which those data are stored. You'll need to 
understand the content and format of the output of the function in a similar way, so that
you can join it with other functions in your workflow. You'll want to make sure 
you understand the main choices that you can modify with the function, through setting
different parameters, as well as the reason behind the defaults that are used for 
those parameters. Finally, ideally you'll want to understand a bit about how the 
function operates to move from the input you give it to the output it gives back to 
you.

> "'Freemon Dyson, a famous physicist, suggested that science moves forward by 
inventing new tools,' Kevin [Kelly, founding editor of Wired magazine] began as
we talked on the phone one morning about tools. 'When we invented the telescope, 
suddenly we had astrophysicists, and astronomy, and we moved forward. The invention
of the microscope opened up the small world of biology to us. In a broad sense, 
science moves forward by inventing tools, because when you have those tools 
they give you a new way of thinking." [@savage2020every]

> "My initial reaction was shock---at both the ingenuity of the solution and the fact
that I'd forgotten all about it---but that quickly resolved into gratitude. I was
grateful to mayself for taking the time a year before to save me this time now.
'Thank you, me-from-the-past!' I literally said to myself." [@savage2020every]

> "Complex equipment breaks, and when it breaks you need the technical expertise to 
fix it, and you need replacement parts. ... Some studies suggest that as much as 95 
percent of medical technology donated to developing countries breaks within the 
first five years of use." [@johnson2011good]

> "Good ideas ... are, inevitably, constrained by the parts and skills that surround 
them. We have a natural tendance to romanticize breathrough innovations, 
imagining momentous ideas transcending their surroundings, a gifted mind somehow
seeing over the detritus of old ideas and ossified tradition. But ideas are works
of bricolage; they're built out of that detritus. We take the ideas we've inherited
or that we've stumbled across, and we jigger them together into some new shape."
[@johnson2011good]

> "Good ideas are not conjured out of thin air; they are built out of a collection 
of existing parts, the composition of which expands (and, occasionally, contracts) 
over time. Some of these parts are conceptual: ways of solving problems, or new 
definitions of what constitutes a problem in the first place. Some of them are, 
literally, mechanical parts." [@johnson2011good]

> "What kind of environment creates good ideas? The simplest way to answer is this: 
innovative environments are better at helping their inhabitants explore the adjacent
possible, because they expose a wide and diverse sample of spare parts---mechanical
or conceptual---and they encourage novel ways of recombining those parts. Environments
that block or limit new combinations---by punishing experimentation, by obscuring 
certain branches of possibility, by making the current state so satisfying that no 
one bothers to explore the edges---will, on average, generate and circulate 
fewer innovations that environments that encourage exploration." [@johnson2011good]

> "Part of coming up with a good idea is discovering what those spare parts are, and 
ensuring that you're not just recycling the same old ingredients. ... The trick to 
having good ideas is not to sit around in glorious isolation and try to think big 
thoughts. The trick is to get more parts on the table." [@johnson2011good]

> "*Document the version of the software that you ran.* This may seem unimportant, but
remember the example from 'Reproducible Research' on page 6 where my colleagues and I 
traced disagreeing results down to a single piece of software being updated. These 
details matter. Good bioinformatices software usually has a command-line option to 
return the current version. Software managed with a version control system such as 
Git has explicit identifiers to every version, which can be used to document the 
precise version you ran... If no version information is available, a release date, 
link to the software, and download date will suffice." [@buffalo2015bioinformatics]


> "*Document when you downloaded data.* It's important to include when the data was downloaded, 
as the external data source (such as a website or server) might change in the future. For example, 
a script that downloads data directly from a database might produce different results if 
rerun after the external database is updated. Consequently, it's important to document
when data came into your repository." [@buffalo2015bioinformatics]


> "All of this [documentation] information is best stored in plain-text README files.
Plain text can easily be read, searched, and edited directly from the command line, 
making it the perfect choice for portable and accessible README files. It's also available
on all computer systems, meaning you can document your steps when working directly on 
a server or computer cluster. Plain text also lacks complex formatting, which can create
issues when copying and pasting commands from your documentation back into the command 
line." [@buffalo2015bioinformatics]

> "The sandbox of the budding builder is not *making* as much as it is 
modification: taking something that exists and making it better, either 
functionally or aesthetically or both. Often that involves attaching and 
securing parts that were not originally intended to go together." [@savage2020every]

As you start learning to write code in R, don't force yourself to stare at an 
empty R script file and try to come up with a full script from scratch. One of the 
best ways to learn R is to find some scripts that others have written for tasks 
that are similar to the ones that you want to do, then work through those to figure
out each function call, and how those function calls add up to the full pipeline. 

This method of reverse engineering is useful in many areas when you're trying to 
figure out how things work. ...

Once you understand a few other R scripts, you can start trying to modify them and 
to pull pieces from different scripts to use as building blocks as you put together
your own script. There's no need to reinvent the wheel---if someone else has 
shared an R script that comes close to doing what you need to do, start there and
then change and evolve that idea to suit your own needs. 

> "All creative work builds on what has gone before. When someone declares
that something is original, it's because they are unaware of the influences. 
The creative make the most of things they admire and aren't ashamed to 
be inspired by something they respect. The bad news: everything has already
been done. The good news: it can be done again." [@judkins2016art]

> "Johannes Gutenberg invented his printing press by repurposing a wine 
press for use with olive oil--based ink and block printing." [@madhavan2015applied]


However, it is critical that you work through and understand any example code
that you bring in and modify in your own workflow.

> "Appropriate methods are 'very data-set dependent'... The methods and tuning
parameters may need to be adjusted to account for variable such as sequencing
length. But John Marioni at Cancer Research UK in Cambridge says it's important not 
to put complete faith in the pipeline. 'Just because the satellite navigation
tells you to drive into the river, you don't drive into the river,' he says." 
[@perkel2017single]

> "Do not reinvent the wheel. It pays to reuse existing software. Integrative
frameworks and associated application stores already house hundreds of tools
(for example, as of May 2012, Galaxy ToolShed contains ~ 1,700 tools). It is
likely that a script for a particular problem has been already written. Ask
around through existing resources such as SEQanswers43 and BioStar44."
[@nekrutenko2012next]


> "If you're going to build a house today, you don't start by cutting down trees to make 
lumber and digging clay to make your own bricks. Instead, you buy prefabricated pieces like
doors, windows, plumbing fixtures, a furnace, and a water heater. House construction is still
a big job, but it's manageable because you can build on the work of many others and rely 
on an infrastructure, indeed an entire industry, that will help. The same is true of 
programming. Hardly any significant program is created from nothing. Many components written
by others can be taken off the shelf and used. For instance, if you're writing a program for
Windows or a Mac, there are libraries of prefabricated menus, buttons, text editors, graphics, 
network connections, database access, and so on. Much of the job is understanding the components
and gluing them together in your own way. Of course, many of these components in turn rest on
other simpler and more basic ones, often for several layers. Below that, everything runs on 
the operating system, a program that manages the hardware and controls everything that happens."
[@kernighan2011d]

> "There is also a problem with discovering software that exists; often people
reinvent the wheel just because they don’t know any better. Good repositories
for software and best practice workflows, especially if citable, would be a
start." --- James Taylor in [@altschul2013anatomy]

In the tidyverse approach, you can often
chain the functions in any order, since each function inputs and outputs the
same data structure. With a Bioconductor pipeline, however, there
will be functions that input one data structure and output a different one.
As a result, Bioconductor functions, instead of being "small"
functions that do one simple thing, often carry out a number of complex 
steps within each function call.

> "These advances have helped to ensure that R and Bioconductor remain relevant
in the age of high-throughput sequencing. We plan to continue in this direction
by designing and implementing abstractions that enable user code to be agnostic
to the mode of data storage, whether it be memory, files or databases. This will
bring much needed agility to resource allocation and will enable the user to be
more resourceful, without the burden of increased complexity."
[@lawrence2014scalable]


> "Bioconductor packages support the reading of many of the data types and formats
produced by measurement instruments used in modern biology, as well as the 
needed technology-specific 'preprocessing' routines. This community is 
actively keeping these up-to-date with the rapid developments in the 
instrument market." 


-----------------------------------------------------------------------

Many excellent
resources exist for learning the tidyverse approach, and so we won't recover that
information here. Instead, we will focus on how to interface between this
approach and the object-based approach that's more common with Bioconductor
packages. Bioconductor packages often take an object-based approach, and with
good reason because of the complexity and size of many early versions of
biomedical data in the pre-processing process. There are also resources for
learning to use specific Bioconductor packages, as well as some general
resources on Bioconductor, like *R Programming for Bioinformatics* [ref].
However, there are fewer resources available online that teach how to coordinate
between these two approaches in a pipeline of code, so that you can leverage the
needed power of Bioconductor approaches early in your pipeline, as you
pre-process large and complex data, and then shift to use a tidyverse approach
once your data is amenible to this more straightforward approach to analysis and
visualization.

The heart of making this shift is learning how to convert data, when possible, 
from a more complex, class-type data structure (built on the flexible list 
data structure) to the simpler, more standardized two-dimensional dataframe 
structure that is required for the tidyverse approach. In this subsection, we'll 
cover approaches for converting your data from Bioconductor data structures to 
dataframes. 

If you are lucky, this might be very straightforward. A pair of packages called
`broom` and `biobroom` have been created specifically to facilitate the conversion
of data from more complex structures to dataframes. The `broom` package was 
created first, by David Robinson, to convert the data stored in the objects that 
are created by fitting statistical models into tidy dataframes. Many of the functions
in R that run statistical tests or fit statistical models output results in a 
more complex, list-based data structure. These structures have nice "print" methods, 
so if fitting the model or running the test is the very last step of your pipeline, 
you can just read the printed output from R. However, often you want to include 
these results in further code---for example, creating plots or tables that show
results from several statistical tests or models. The `broom` package includes 
several functions for pulling out different bits of data that are stored in the 
complex data structure created by fitting the model or running the test and convert
those pieces of data into a tidy dataframe. This tidy dataframe can then be 
easily used in further code using a tidyverse approach. 

The `biobroom` package was created to meet a similar need with data stored in some
of the complex structures commonly used in Bioconductor packages.



-----------------------------------------------------------------------












> "The Bioconductor project distributes the software as a number of different R
packages, including Rsamtools, IRanges, GenomicRanges, GenomicAlignments,
Biostrings, rtracklayer, biovizBase and BiocParallel. The software enables the
analyst to conserve computational resources, iteratively generate summaries and
visualize data at arbitrary levels of detail. These advances have helped to
ensure that R and Bioconductor remain relevant in the age of high-throughput
sequencing. We plan to continue in this direction by designing and implementing
abstractions that enable user code to be agnostic to the mode of data storage,
whether it be memory, files or databases. This will bring much needed agility to
resource allocation and will enable the user to be more resourceful, without the
burden of increased complexity."  [@lawrence2014scalable]



> "The biobroom package contains methods for converting standard objects in Bioconductor into a 'tidy format'. It serves as a complement to the popular broom package, and follows the same division (tidy/augment/glance) of tidying methods."
[@biobroom]

> "Tidying data makes it easy to recombine, reshape and visualize bioinformatics analyses. Objects that can be tidied include: ExpressionSet object,
GRanges and GRangesList objects, RangedSummarizedExperiment object, MSnSet object,
per-gene differential expression tests from limma, edgeR, and DESeq2, qvalue object for multiple hypothesis testing." [@biobroom]

> "We are currently working on adding more methods to existing Bioconductor objects." [@biobroom]

> "All biobroom tidy and augment methods return a tbl_df by default (this prevents them from printing many rows at once, while still acting like a traditional data.frame)." [@biobroom]

> "The concept of 'tidy data' offers a powerful framework for structuring data
to ease manipulation, modeling and visualization. However, most R functions,
both those builtin and those found in third-party packages, produce output that
is not tidy, and that is therefore difficult to reshape, recombine, and
otherwise manipulate. Here I introduce the broom package, which turns the output
of model objects into tidy data frames that are suited to further analysis,
manipulation, and visualization with input-tidy tools." [@robinson2014broom]

> "Tools are classified as 'messy-output' if their output does not fit into this
[tidy] framework. Unfortunately, the majority of R modeling tools, both from the
built-in stats package and those in common third party packages, are
messy-output. This means the data analyst must tidy not only the original data,
but the results at each intermediate stage of an analysis." [@robinson2014broom]

> "The broom package is an attempt to solve this issue, by bridging the gap from
untidy outputs of predictions and estimations to create tidy data that is easy
to manipulate with standard tools. It centers around three S3 methods, tidy,
augment, and glance, that each take an object produced by R statistical
functions (such as lm, t.test, and nls) or by popular third-party packages (such
as glmnet, survival, lme4, and multcomp) and convert it into a tidy data frame
without rownames (Friedman et al., 2010; Therneau, 2014; Bates et al., 2014;
Hothorn et al., 2008). These outputs can then be used with input-tidy tools such
as dplyr or ggplot2, or downstream statistical tests. broom should be
distinguished from packages such as reshape2 and tidyr, which rearrange and
reshape data frames into different forms (Wickham, 2007b, 2014b). Those packages
perform essential tasks in tidy data analysis but focus on manipulating data
frames in one specific format into another. In contrast, broom is designed to
take data that is not in a data frame (sometimes not anywhere close) and convert
it to a tidy data frame." [@robinson2014broom]

> "`tidy` constructs a data frame that summarizes the model’s statistical
components, which we refer to as the component level. In a regression such as
the above it may refer to coefficient estimates, p-values, and standard errors
for each term in a regression. The tidy generic is flexible- in other models it
could represent per-cluster information in clustering applications, or per-test
information for multiple comparison functions. ... `augment` add columns to the
original data that was modeled, thus working at the observation level. This
includes predictions, residuals and prediction standard errors in a regression,
and can represent cluster assignments or classifications in other applications.
By convention, each new column starts with . to ensure it does not conflict with
existing columns. To ensure that the output is tidy and can be recombined,
rownames in the original data, if present, are added as a column called
.rownames. ... Finally, `glance` constructs a concise one-row summary of the
model level values. In a regression this typically contains values such as R2 ,
adjusted R2 , residual standard error, Akaike Information Criterion (AIC), or
deviance. In other applications it can include calculations such as cross
validation accuracy or prediction error that are computed once for the entire
model. ... These three methods appear across many analyses; indeed, the fact
that these three levels must be combined into a single S3 object is a common
reason that model outputs are not tidy. Importantly, some model objects may have
only one or two of these methods defined. (For example, there is no sense in
which a Student’s T test or correlation test generates information about each
observation, and therefore no augment method exists). " [@robinson2014broom]

> "While model inputs usually require tidy inputs, such attention to detail
doesn’t carry over to model outputs. Outputs such as predictions and estimated
coefficients aren’t always tidy. For example, in R, the default representation
of model coefficients is not tidy because it does not have an explicit variable
that records the variable name for each estimate, they are instead recorded as
row names. In R, row names must be unique, so combining coefficients from many
models (e.g., from bootstrap resamples, or subgroups) requires workarounds to
avoid losing important information. This knocks you out of the flow of analysis
and makes it harder to combine the results from multiple models."
[@wickham2014tidy]

> "Right now, in labs across the world, machines are sequencing the genomes of the life 
on earth. Even with rapidly decreasing costs and huge technological advancements in 
genome sequencing, we're only seeing a glimpse of the biological information contained
in every cell, tissue, organism, and ecosystem. However, the smidgen of total biological 
information we're gathering amounts to mountains of data biologists need to work with. At 
no other point in human history has our ability to understand life's complexities been so 
dependent on our skills to work with and analyze data." [@buffalo2015bioinformatics]

> "Bioinformaticians are concerned with deriving biological understanding from large
amounts of data with specialized skills and tools. Early in biology's history, the 
datasets were small and manageable. Most biologists could analyze their own data after
taking a statistics course, using Microsoft Excel on a personal desktop computer.
However, this is all rapidly changing. Large sequencing datasets are widespread, and will 
only become more common in the future. Analyzing this data takes different tools, new skills,
and many computers with large amounts of memory, processing power, and disk space."
[@buffalo2015bioinformatics]

> "Unfortunately, many of the biologist's common computational tools can't scale to the
size and complexity of modern biological data. Complex data formats, interfacing 
numerous programs, and assessing software and data make large bioinformatics datasets 
difficult to work with." [@buffalo2015bioinformatics]

> "Bioconductor's pakcage system is a bit different than those on the Comprehensive R 
Archive Network (CRAN). Bioconductor packages are released on a set schedule, twice 
a year. Each release is coordinated with a version of R, making Bioconductor's versions
tied to specific R versions. The motivation behind this strict coordination is that it 
allows for packages to be thoroughly tested before being released for public use. 
Additionally, because there's considerable code re-use within the Bioconductor project, 
this ensures that all package versions within a Bioconductor release are compatible
with one another. For users, the end result is that packages work as expected and 
have been rigorously tested before you use it (this is good when your scientific
results depend on software reliability!). If you need the cutting-edge version of a 
package for some reason, it's always possible to work with their development branch."
[@buffalo2015bioinformatics]

> "When installing Bioconductor packages, we use the `biocLite()` function. `biocLite()`
installs the correct version of a package for your R version (and its corresponding
Bioconductor version)." [@buffalo2015bioinformatics]

Wickham highlights how this
standardization makes an approach focused on tidy data so powerful:

> "A standard makes initial data cleaning easier because you do not need to
start from scratch and reinvent the wheel every time. The tidy data standard has
been designed to facilitate initial exploration and analysis of the data, and to
simplify the development of data analysis tools that work well together."
[@wickham2014tidy]

As a result, the tidyverse collection of tools is pretty easy to learn, compared
to other sets of functions in scripting languages, and pretty easy to expand
your knowledge of once you know some of its functions. Wickham notes: 

> "The goal of [the tidy tools] principles is to provide a uniform interface so
that tidyverse packages work together naturally, and once you’ve mastered one,
you have a head start on mastering the others." [@wickhem2017tidy]

> "Lists aren't external to the creative process, they are intrinsic to it. They are
a natural part of any project of scale, whether we like it or not." [@savage2020every]

> "The maker in me knows that this is where lists really shine, that it is their capacity 
for simplifying the complex that sets them apart from all other planning tools. 
Not just at the beginning of a project, either, but at every step along the 
creative process, because no matter how exacting the list you make at the outset, 
there will always be things that you missed or, more frequently, that change. It's 
like trying to measure a coastline: it's fractal." [@savage2020every]

> "The best part of making a list is, you guessed it, crossing things off. But when you 
physically cross them out, like with a pen, you can make them harder to read, which 
destroys their informational value beyond that single project and, to me at least, makes
the whole thing feel incomplete. The checkbox allowed me to cross something off my list, 
to see clearly *that* I'd crossed it off, and at the same time retain all its 
information while not also adding to the cognitive load of interpreting the list." 
[@savage2020every]
