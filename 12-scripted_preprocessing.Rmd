#  Experimental Data Preprocessing

This section includes modules on: 

- [Module 3.1: Principles of pre-processing experimental data](#module12)
- [Module 3.2: Selecting software options for pre-processing](#module12a)
- [Module 3.3: Introduction to scripted data pre-processing in R](#module13)
- [Module 3.4: Tips for improving reproducibility when writing R scripts](#module13a)
- [Module 3.5: Simplify scripted pre-processing through R's 'tidyverse' tools](#module14)
- [Module 3.6: Complex data types in experimental data pre-processing](#module15)
- [Module 3.7: Introduction to reproducible data pre-processing protocols](#module18)
- [Module 3.8: RMarkdown for creating reproducible data pre-processing protocols](#module19)
- [Module 3.9: Example: Creating a reproducible data pre-processing protocol](#module20)

## Principles of pre-processing experimental data {#module12}

The experimental data collected for biomedical research often requires
pre-processing before it can be analyzed. This stage of working with
experimental data has critical implications for the reproducibility and rigor of
later data analysis. Use of point-and-click software and/or propritary software
can limit the transparency and reproducibility of this analysis stage and is
time-consuming for repeated tasks. In this module, we will explain how
preprocessing can be broken into common themes and processes. In the next
module, we will explain how scripted pre-processing, especially using open
source software, can improve transparency and reproducibility fo this 
stage of working with biomedical data.

**Objectives.** After this module, the trainee will be able to:

- Define "pre-processing" of experimental data 
- Understand key themes and processes in pre-processing and identify these
processes in their own pipelines

### What is data preprocessing?

When you are conducting an experiment that involves work in the wet lab, you
will do a lot of work before you have any data ready for the computer. You may,
for example, have conducted an extensive period of work that involved laboratory
animals or cell cultures. In many cases, you will have run some samples through
very advanced equipment, like cytometers or sequencers. Once you have completed
this long and hard process, you may ask yourself, "I ran the experiment, I ran
the equipment... Aren't I done with the hard work?".

For certain types of data, you may be able to proceed directly to statistical
analysis. For example, if you collected the weights of lab animals, you are
probably directly using those data to answer questions like whether they
differed between treatment groups. However, with a lot of
biomedical data, you will not be able to move directly to analyzing the data.
Instead, you will need to start with a stage of *pre-processing* the data: that
is, taking computational steps to prepare the data before it's in an appropriate
format to be used in statistical analysis.

There are several reasons that pre-processing is often necessary. The first is 
that many biomedical data are collected using extremely complex equipment and 
scientific principles. The pre-processing in this case is used to extract scientific
meaning from data that might have been collected using measurements that are 
more closely linked to the complex process than to the final scientific question. 
Next, there will be some cases where practical concerns made it easier to 
collect data in one way and pre-process it later to get it to a format that 
aligns with the scientific question. For example, if you want the average weight
of mice in different treatment groups, it may be more practical to weigh the 
cage that contains all the mice in each treatment group rather than weigh
each mouse individually. This makes life in the lab easier, but means you'll need
to do some more computational pre-processing of the data to make sense of it 
appropriately. Third, there are now frequent cases where an assay generates
a very large set of measures---for example, expression levels of thousands of
genes for each sample---and some pre-processing might help in digesting the 
complexity inherent in this type of high-dimensional data. Finally, preprocessing
is often necessary to check for and resolve quality control issues within the
data.

In this module, we'll talk more about what preprocessing is. As part of this
discussion, we'll describe several common themes when preprocessing
scientific data. 

All in all, pre-processing aims to extract useful information from the
measurments that you've collected from your experiment, so that this information
can then be used within statistical analysis and visualization to test important
hypotheses that provide insights on your scientific question. Data
*preprocessing*, then, is a term that covers steps must be done after data are
collected but before they are used for further analysis. After the data are
appropriately pre-processed, you can use them for statistical tests and also
combine them with other data collected from the experiment.

In any scientific field, when you work with data, it will often take much more
time to prepare the data for analysis than it takes to set up and run the
statistical analysis itself [@robinson2014broom]. This is certainly true with
complex biomedical data, including data for flow cytometry, transcriptomics,
proteomics, and metabolomics. It is a worthwhile investment of time to learn
strategies to make preprocessing of this data more efficient and reproducible,
and it is critical---for the rigor of the entire experiment---to ensure that the
preprocessing is done correctly and can be repeated by others.

These preprocessing steps, in fact, should be as clear and practical to follow
as the types of protocols you would follow for a wet lab procedure. Key to 
reproducibility is that a procedure is described in enough detail that others
can follow it exactly. In a commentary for *Nature*, Lithgow and coauthors
describe how critical it is to share these details: 

> "Our first task, to develop a protocol, seemed straightforward. But subtle disparities
were endless. In one particularly painful teleconference, we spent an hour debating
the proper procedure for picking up worms and placing them on new agar plates. Some
batches of worms lived a full day longer with gentler techniques. Because a worm's 
lifespan is only about 20 days, this is a big deal. Hundreds of e-mails and many 
teleconferences later, we converged on a technique but still had a stupendous three-day
difference in lifespan between labs. The problem, it turned out, was notation---one
lab determined age on the basis of when an egg hatched, others on when it was laid." 
[@lithgow2017long]

### Common themes and processes in data preprocessing

This preprocessing will vary depending on the way the data were collected and
the scientific questions you hope to answer, and often it will take a lot of
work to develop a solid pipeline for preprocessing data from a specific assay.
However, there are some common themes that drive the need for such preprocessing
of data across types of data collection and research questions. These common
themes provide a framework that can help as you design data preprocessing
pipelines, or as you interpret and apply pipelines that were developed by other
researchers. The rest of this module will describe several of the most common
themes in data preprocessing.

**Extracting scientifically-relevant measurement**

One common purpose of preprocessing is to translate the measurements that
you directly collect into measurements that are meaningful for your scientific
research question. Scientific research uses a variety of complex techniques and
equipment to initially collect data. As a result of these inventions and
processes, the data that are directly collected in the laboratory by a person or
piece of equipment might require quite a bit of preprocessing to be translated
into a measure that meaningfully describes a process of interest. A key element
of preprocessing data is to translate the acquired data into a format that can
more directly answer scientific questions.

This type of preprocessing will vary substantially from assay to assay, with
algorithms that are tied to the methodology of the assay itself. We'll describe
some examples of this idea, moving from very simple translation to processes
that are much more complex (and more typical of the data collected at present in
many types of biomedical research assays).

As a basic example, some assays will use equipment that can measure the
intensity of color of a sample or the sample's opacity. Some of these measures
might be directly (or at least proportionally) interpretable. For example,
opacity might provide information about how high the concentration of bacteria
are in a sample. Others might need more interpretation, based on the scientific
underpinnings of the assay. For example, in an enzyme-linked immunosorbent assay
(ELISA), antibody levels are detected as a measure of the intensity of color of
a sample at various dilutions, but to interpret this correctly, you need to know
the exact process that was used for that assay, as well as the dilutions that
were measured.

The complexity of this "translation" scales up as you move to data that are
collected using more complex processes. Biomedical research today leverages
extraordinarily complex equipment and measurement processes to learn more about
health and disease. These invented processes of measuring can
provide extraordinarily detailed and informative data, allowing us to "see"
elements of biological processes that could not be seen at that level before.
However, they all require steps to translate the data that are directly
recorded by equipment into data that are more scientifically meaningful.

One example is flow cytometry, which can characterize immune cell populations. 
In flow cytometry, immune cells are characterized based on proteins that are
present both within and on the surface of each cell, as well as properties like
cell size and granularity [@maecker2012standardizing, @barnett2008cd4]. Flow
cytometry identifies these proteins through a complicated process that involves
lasers and fluorescent tags and that leverages a key biological process---that
an antibody can have a very specific affinity for one specific protein
[@barnett2008cd4]. 

The process starts by identifying proteins that can help to
identify specific immune cell populations (e.g., CD3 and CD4 proteins in
combination can help identify helper T cells). This collection of proteins is
the basis of a panel that's developed for that flow cytometry experiment. For
each of the proteins on the panel, you will incorporate an antibody with a
specific affinity for that protein. If that antibody sticks to the cell in a
substantial number, it indicates the presence of its associated protein on the
cell. 

To be able to measure which of the antibodies stick to which cells, each type of
antibody is attached to a specific fluorescent tag (each of these is often
referred to as a "color" in descriptions of flow cytometry) [@benoist2011flow].
Each fluorescent tag included in the panel will emit wavelength in a certain
well-defined range after it is exposed to light at wavelengths of a certain
range. As each cell passes through the flow cytometer, lasers activate these
fluorescent tags, and you can measure the intensity of light emitted at specific
wavelengths to identify which proteins in the panel are present on or in each
cell [@barnett2008cd4].

This is an extraordinarily clever way to identify cells, but the complexity of
the process means that a lot of preprocessing work must be done on the resulting
measurements. To interpret the data that are recorded by a flow cytometer
(intensity of light at different wavelengths)---and to generate a
characterization of immune cell populations from these data---you need to
incorporate a number of steps of translation. These include steps that
incorporate information about which fluorescent tags were attached to which
antibodies, which proteins in the cell each of those antibodies attach to, which
immune cells those proteins help characterize, what wavelength each fluorescent
tag emits at, and so on. In some cases, the measuring equipment will provide
software that performs some of this preprocessing before you get the first
version of the data, but some may need to be performed by hand, especially if
you need to customize based on your research question. Further, it's critical to
understand the process, to decide if it's appropriate for your specific
scientific question.

Similarly complex processes are used to collect data for many single-cell and
high throughput assays, including transcriptomics, metabolomics, proteomics, 
and single cell RNA-sequencing. It can require complex and sometimes lengthy
algorithms and pipelines to extract direct scientifically-relevant measures 
from the measures that the laboratory equipment captures in these cases.
Depending on the assay, this preprocessing can include sequence alignment 
and assembly (if sequencing data were collected) or peak identification and 
alignment (if data was collected using mass spectrometry, for example).

As Anton Nekrutenko and Taylor James note in an article on the reproducibility 
of next-generation sequencing: 

> "Meaningful interpretation of sequencing data has become particularly
important. Yet such interpretation relies heavily on complex computation---a new
and unfamiliar domain to many of our biomedical colleagues---which, unlike
data generation, is not universally accessible to everyone."
[@nekrutenko2012next]

In another article, Paul Flicek and Ewan Birney also capture this idea: 

> "One thing that has not changed in the last 10 years is that the individual
outputs of the sequence machines are essentially worthless by themselves. ...
Fundamental to creating biological understanding from
the increasing piles of sequence data is the development of analysis algorithms
able to assess the success of the experiments and synthesize the data into
manageable and understandable pieces."
[@flicek2009sense]

The discipline of bioinformatics works to develop these types of algorithms
[@barry2009new]. Many of them are available through open-source, scripted
software like R and Python. These types of preprocessing algorithms are often
also available as proprietary software, sometimes sold by equipment
manufacturers and sometimes separately.

**Quality assessment and control**

Another common step in preprocessing is to identify and resolve quality control
issues. These are cases where some error or problem occurred in the data
recording and measurement, or some of the samples are poor quality and need to
be discarded. 

There are a variety of reasons why biomedical data might have quality control
issues. First, when data are recorded "by hand" (including into a spreadsheet),
the person who is recording the data can miss a number or mis-type a number. For
example, if you are recording the weights of mice for an experiment, you may
forget to include a decimal in one recorded value, or invert two numbers. These
types of errors include recording errors (reading the value from the instrument
incorrectly), typing errors (making a mistake when entering the value into a
spreadsheet or other electronic record), and copying errors (introduced when
copying from one record to another) [@chatfield1995problem]. 

While some of these can be hard to identify later, in many cases you can
identify and fix recording errors through exploratory analysis of the data. For
example, if most recorded mouse weights are around 25 grams, but one is recorded
as 252 grams, you may be able to identify that the recorder missed a decimal point 
when recorded one weight. In this case, you could identify the error as 
an extreme outlier---in fact, beyond a value that would make physical sense.

Other quality control issues may come in the form of missing data (e.g., you
forget to measure one mouse at one time point), or larger issues, like a quality
problem with a whole sample. In these cases, it is important to
identify missingness in the data, so that as a next step you can try to
determine why certain data points are missing (e.g., are they missing at random,
or is there some process that makes certain data points more likely to be
missing, in which case this missingness may bias later analysis), to help you
decide how to handle those missing values [@chatfield1995problem].

Some quality control issues will be very specific to a type of data or assay. 
For example, one common theme in quality control repeats across methods
that measure data at the level of the single cell. Some examples of this type of
single-cell resolution measurement include flow cytometry and single-cell
RNA-seq. In these cases, some of the measurements might be made on cells that
are in some way problematic. This can include cells that are dead or damaged
[@ilicic2016classification], and it can also include cases where a measurement
that was meant to be taken on a single cell was instead taken on two or more
cells that were stuck together, or on a piece of debris or, in the case of
droplet-based single cell RNA-seq, an empty droplet. 

Quality control steps can help to identify and remove these problematic
observations. For example, flow cytometry panels will often include a marker for
dead cells, which can then be used when the data are gated to identify and
exclude these cells, while the size measure made of the cells (forward scatter)
can identify cases where two or more cells were stuck together and passed
through the equipment at the same time. In scRNA-seq, low quality cells may be
identified based on a relatively high mitochondrial DNA expression compared to
expression of other genes, potentially because if a cell ruptured before it was
lysed for the assay, much of the cytoplasm and its messenger RNA would have
escaped, but not RNA from the mitochondria [@ilicic2016classification]. Cells
can be removed in the preprocessing of scRNA-seq data based on this and related
criteria (low number of detected genes, small relative library size)
[@ilicic2016classification].

**Addressing practical concerns and limitations in data collection**

Another common reason for preprocessing is related to addressing things you 
did while collecting the data---specifically, things you did for 
practical purposes or under practical limitations. These will they need to 
be handled, when possible, in computational preprocessing. 

More generally, this type of preprocessing addresses something called 
*noise* in the data. When we collect biomedical
research data, we are often collecting data in the hope that it will measure
some meaningful biological variation between two or more conditions. For
example, we may measure it in the hope that there is a meaningful difference in
gene expression between a sample taken from an animal that is diseased versus
one that is healthy, with the aim of finding a biomarker of the disease.

There are, however, several sources of variation in data we collect. The first
of these is variation that comes from *meaningful* biological variation between
samples---the type of variation that we are trying to measure and 
use to answer scientific questions. We often call this the signal in the 
data [@chatfield1995problem]. 

There are other sources of variation, however. These sources are irrelevant to our
scientific question, and so we often call them "noise"---in other words, they
cause our data to change from one sample to the next in a way that might blur
the signal that we care about (variation from meaningful biological
differences). We therefore often take steps in preprocessing to try to limit or
remove this varation to help us see the meaningful biological variation more
clearly.

There are two main sources of this other variation or noise: biological and
technical. Biological noise in data comes from biological processes, but from
ones that are irrelevant to the process that we care about in our particular
experiment. For example, cells express different genes depending on where they
are in the cell cycle. However, if you are trying to use single cell
RNA-sequencing to explore variation in gene expression by cell type, you might
consider this growth-related variation as noise, even though it represents a
biological process.

The second source of noise is technical. Technical noise comes from variation
that is introduced in the process of collecting data, rather than from
biological processes. In the introduction to the module, we brought up the 
example of weighing mice by cage rather than individually; one example of 
technical noise in this case would be the differences across the samples that's 
based on the number of mice in each cage. 

As another example, part of the process of single cell RNA-seq
involves amplifying complementary DNA that are developed from the messenger RNA
in each cell in the sample. How much the complementary DNA are amplified in this
process, however, varies across cells [@perkel2017single]. If this isn't
addressed in preprocessing, then this "amplification bias" prevents any
meaningful comparison across cells.

> "In many cases... the tools used in bulk RNA-seq can be applied to scRNA-seq.
But fundamental differences in the data mean that this is not always possible.
For one thing, single-cell data are noisier... With so little RNA to work with,
small changes in amplification and capture efficiencies can produce large
differences from cell to cell and day to day and have nothing to do with
biology. Researchers must therefore be vigilant for 'batch effects', in which
seemingly identical cells prepared on different days differ for purely technical
reasons, and for 'dropouts'---genes that are expressed in the cell but not
picked up in the sequence data. Another challenge is the scale... A typical bulk
RNA-seq experiment involves a handful of samples, but scRNA-seq studies can
involve thousands. Tools that can handle a dozen samples often slow to a crawl
when confronted with ten or a hundred times as many." [@perkel2017single]

> "Methods to quantify mRNA abundance introduce systematic sources of variation 
that can obscure signals of interest. Consequently, an essential first step in 
most mRNA-expression analyses is normalization, whereby systemic variations 
are adjusted to make expression counts comparable across genes and / or samples.
Within-sample normalization methods adjust for gene-specific features, such 
as GC content and gene length, to facilitate comparisons of a gene's expression
within an individual sample; whereas between-sample normalization methods adjust
for sample-specific features, such as sequencing depth, to allow for comparisons of
a gene's expression across samples." [@bacher2017scnorm]

> "A number of methods are available for between-sample normalization in bulk
RNA-seq experiments. Most of these methods calculate global scale factors (one
factor is applied to each sample, and this same factor is applied to all genes
in the sample) to adjust for sequencing depth. These methods demonstrate
excellent performance in bulk RNA-seq, but they are compromised in the
single-cell setting because of an abundance of zero-expression values and
increased technical variability." [@bacher2017scnorm]

> "scRNA-seq data show systematic variation in the relationship between
transcript-specific expression and sequencing depth (which we refer to as the
count-depth relationship) that is not accommodated by a single scale factor
common to all genes in a cell. Global scale factors adjust for a count-depth
relatinoship that is assumed to be common across genes. When this relationship
is not common across genes, normalization via global scale factors leads to
overcorrection for weakly and moderately expressed genes and, in some cases,
undernormalization of highly expressed genes. To address this, SCnorm uses
quantile regression to estimate the dependence of transcript expression on
sequencing depth for every gene. Genes with similar dependence are then grouped,
and a second quantile regression is used to estimate scale factors within each
group. Within-group adjustment for sequencing depth is then performed using the
estimated scale factors to provide normalized estimates of expression."
[@bacher2017scnorm]

In some cases, there are ways to reduce some of the variation that comes from
processes that aren't of interest for your scientific question, either from
biological or technical sources. Some of this variation might just lower the
statistical power of the analysis, but some can go further and bias the results.

One example of a process that can help adjust for unwanted variation is 
normalization. Let's start with a very simple example to explain what 
normalization does. Say that you wanted to measure the height of three people, 
so you can compare to determine who is tallest and who is shortest. 
However, rather than standing on an even surface, they are all standing on 
ladders that are different heights. If you measure the height of the top of
each person's head from the ground, you will not be able to compare their
heights correctly, because each has the height of their ladder incorporated
into the measure. If you knew the height of each person's ladder, though,
you could normalize your measure by subtracting each ladder's height from 
the total measurement, and then you could meaningfully compare the heights
to determine which person is tallest. 

Normalization plays a similar role in preprocessing many forms of biomedical
data. One simple example is when comparing the weights of two groups of 
mice. Often, a group of mice might be measured collectively in their cage, 
rather than taken out and weighed individually. Say that you have three 
treated mice in one cage and four control mice in another cage. You can 
weigh both cages of mice, but to compare these weights, you will need to 
normalize the measurement by dividing by the total number of mice that are
in each cage (in other words, taking the average weight per mouse). This
type of averaging is a very simple example of normalizing data.

As another example, scRNA-seq data can be prone to something called
amplification bias. This occurs because, while the different fragments are all
amplified before their sequences are read, some fragments are amplified more
times than others. If two fragments had the exact same abundence in the original
cell, but one was amplified more than the other, that one would be measured as
having a higher level in the sample if this amplification bias were not
accounted for. One way to adjust for this is ... [Sequencing depth / read depth
in other sequencing data analysis?]

> "Normalization is critical to the development of analysis techniques on
scRNA-seq and to counteract technical noise or bias. Before observed data can be
used to identify differentially expressed genes or potential subpopulations, it
must undergo these corrections, for what is observed is seldom exactly what is
present within the data set." [@lytal2020normalization]

> "The primary goal of single-cell normalization is to remove the influence
of technical effects in the underlying molecular counts, while preserving
true biological variation." [@hafemeister2019normalization]

> "In general, the normalized expression level of a gene should not be 
correlated with the total sequencing depth of a cell. Downstream analytical 
tasks (dimension reduction, differential expression) should also not 
be influenced by variation in sequencing depth." [@hafemeister2019normalization]

> "Scaling normalization is typically required in RNA-seq data analysis to 
remove biases caused by differences in sequencing depth, capture efficiency
or composition effects between samples." [@mccarthy2017scater]

> "Each count in a count matrix represents the successful capture, reverse transcription and sequencing of a molecule of cellular mRNA (Box 1). Count depths for identical cells can differ due to the variability inherent in each of these steps. Thus, when gene expres- sion is compared between cells based on count data, any difference may have arisen solely due to sampling effects. Normalization addresses this issue by e.g. scaling count data to obtain correct rela- tive gene expression abundances between cells." [@luecken2019current]

Other normalization preprocessing might be used to adjust for sequencing 
depth for gene expression data, so that you can meaningfully compare the 
measures of a gene's expression in different samples or treatment 
groups, which can be done by calculating and adjusting for a global 
scale factor [@bacher2017scnorm].

In scRNA-seq, processes like the use of UMIs can allow you to later account for
amplification bias, in which some of the initial mRNA molecules are amplified
more than others [@haque2017practical].

Another example of technical noise comes from flow cytometry. In this 
case, each of the fluorescent tags, rather than emitting at a single 
wavelength, has a distribution of wavelengths across which it emits. 
These distributions overlap somewhat for some of the fluorescent tags, 
especially when measuring many of these "colors" through a multicolor 
panel?, causing something called "spillover"?, where emissions from 
one tag might be recorded as part of the signal for another tag. 

One key example of technical noise is known as "batch effects"---if different
samples are run through the equipment at different times, it can introduce
differences between the samples based on which "batch" they were measured
with. ...

> "The steps in a typical flow cytometry experiment ... present several 
variables that need to be controlled for effective standardization. These variables
include the general areas of reagents, sample handling, instrument setup
and data analysis... The effects of changes in these variables are largely 
known. For example, the stabilization and control of staining reagents 
through the use of pre-configured lyophilized-reagent plates, and centralized
data analysis, have both been shown to decrease variability in a multi-site
study. However, the wide-spread adoption of standards for controlling such 
variables has not taken place. This is in contrast to other technologies, such 
as gene expression microarrays, which have achieved a reasonable degree of 
standardization in recent years. ... Of course, microarray data are less complex
than flow cytometry data, which are based on many hierarchical gates. Still, 
a reasonable degree of standardization of flow cytometry assays should be 
possible to achieve." [@maecker2012standardizing]

> "The biggest, the easiest way [for a biologist doing RNA-Seq to tell that
better normalization of the data is needed]---the way that I discovered the
importance of normalization in the microarray context---is the lack of
reproducibility across different studies. You can have three studies that are
all designed to study the same thing, and you just see basically no
reproducibility, in terms of differentially expressed genes. And every time I
encountered that, it could always be traced back to normalization. So, I'd say
that the biggest sign and the biggest reason why you want to use normalization
is to have a clear signal that's reproducible." [@mak2011john]

Some of these procedures can be separated into two groups, normalization
processes and data correction processes, which might include batch correction
and noise correction [@luecken2019current].

Normalization in scRNA-seq can also include gene normalization:

> "In the same way that cellular count data can be normalized to make them
comparable between cells, gene counts can be scaled to improve comparisons
between genes. Gene normalization constitutes scaling gene counts to have zero
mean and unit vari- ance (z scores). ... There is currently no consensus on
whether or not to perform normalization over genes." [@luecken2019current]

There can be patterns in the data that result from the way that the
data are collected. For example, the data can have something called batch 
effects if they have consistent differences based on who was doing the 
measuring or which batch the sample was run with. For example, if two 
researchers are working to weigh the mice for an experiment, the 
weights recorded by one of the researchers might tend to be, on average, lower
than those recorded by the other researcher, perhaps because the two scales
they are using are calibrated a bit differently. Similarly, settings or 
conditions can change in subtle ways between different runs on a piece of 
equipment, and so the samples run in different batches might have some 
differences in output based on the batch. 

These batch effects can often be addressed through statistical modeling, 
as long as they are identified and are not aligned with a difference you 
are trying to measure (in other words, if all the samples for the control 
animals are run in one batch and all those for the treated animals in 
another batch, you would not be able to separate the batch effect from 
the effect of treatment). 

There are some methods that adjust for batch effects by fitting a regression
model that includes the batch as a factor, and then using the residuals from
that model for the next steps of analysis ("regressing out" those batch effects)
[@mccarthy2017scater]. You can also incorporate this directly into a statistical
model that is being used for the main statistical hypothesis testing of interest
[@mccarthy2017scater].

> "After scaling normalization, further correction is typically required to 
ameliorate or remove batch effects. For example, in the case study dataset, 
cells from two patients were each processed on two C1 machines. Although C1 
machine is not one of the most important explanatory variables on a per-gene
level, this factor is correlated with the first principal component of the 
log-expression data. This effect cannot be removed by scaling normalization
methods, which target cell-specific biases and are not sufficient for removing 
large-scale batch effects that vary on a gene-by-gene basis. ... For the 
dataset here, we fit a linear model to the scran normalized log-expression 
values with the C1 machine as an explanatory factor. (We also use the log-total
counts from the endogenous genes, percentage of counts from the top 100 most
highly-expressed genes and percentage of counts from control genes as 
additional covariates to control for these other unwanted technical effects.) 
We then use the residuals from the fitted model for further analysis. This approach
successfully removes the C1 machine effect as a major source of variation between
cells." [@mccarthy2017scater]

> "We emphasize that it is generally preferable to incorporate batch effects or 
latent variables into statistical models used for inference. When this is not 
possible (e.g., for visualization), directly regressing out these uninteresting 
factors is required to obtain 'corrected' expression values for further analysis.
Furthermore, a general risk of removing latent factors is that interesting 
biological variation may be removed along with the presumed unwanted variation. 
Users should therefore apply such methods with appropriate caution, particularly 
when an analysis aims to discover biological conditions, such as new cell types."
[@mccarthy2017scater]

> "Once identified, important covariates and latent variables can be flagged for
inclusion in downstream statistical models or their effects regressed out of 
normalized expression values." [@mccarthy2017scater]

**Transforming data**

Scaling, normalization, log transformations, calculating time since start
of experiment, etc. Some transformations can help change data that have 
a skewed distribution into a more normally-distributed dataset, which can 
be helpful in meeting the assumptions that underlie some statistical 
tests and models. Some transformations are also helpful in visualizing 
the data. For example, if data are extremely right-skewed (that is, have 
a few very large outliers), it can be hard to see overall patterns 
when plotting the untransformed data, as those outliers force the scale
to expand to fit them, squeezing the bulk of the data into a small 
portion of the total scale of the plot. A log transformation can help to 
spread the data more evenly across the plot area, so that you can 
see patterns in the bulk of the data more easily.

> "There are various reasons for making a transformation, which may also apply to 
deriving a new variable: 1. to get a more meaningful variable (the best reason!); 
2. to stabilize variance; 3. to achieve normality (or at least symmetry); 4. to 
create additive effects (i.e. remove interaction effects); 5. to enable a linear
model to be fitted." [@chatfield1995problem]

> [For transformations] "Logarithms are often meaningful, particularly with
economic data when proportional, rather than absolute, changes are of interest.
Another application of the logarithmic transformation is ... to transform a
severely skewed distribution to normality." [@chatfield1995problem]

One critical process in this category is the process of *normalization*. ...

> "After normalization, data matrices are typically log(x+1)-trans- formed. This
transformation has three important effects. Firstly, distances between
log-transformed expression values represent log fold changes, which are the
canonical way to measure changes in expression. Secondly, log transformation
mitigates (but does not remove) the mean–variance relationship in single-cell
data (Bren- necke et al, 2013). Finally, log transformation reduces the skew-
ness of the data to approximate the assumption of many downstream analysis tools
that the data are normally distributed. While scRNA-seq data are not in fact
log-normally distributed (Vieth et al, 2017), these three effects make the log
transformation a crude, but useful tool. This usefulness is highlighted by down-
stream applications for differential expression testing (Finak et al, 2015;
Ritchie et al, 2015) or batch correction (Johnson et al, 2006; Buttner et al,
2019) that use log transformation for these purposes. It should however be noted
that log transformation of normalized data can introduce spurious differential
expression effects into the data (preprint: Lun, 2018). This effect is
particularly pronounced when normalization size factor distributions differ
strongly between tested groups." [@luecken2019current]

**Digesting complexity in datasets**

Biomedical research has dramatically changed in the past couple of decades to
include much more high-dimensional data. These data often include measurements
for each sample on hundreds of thousands of different parameters. For example,
transcriptomics data can include measurements for each sample on the expression
level of tens of thousands of different genes [@perkel2017single]. Data 
from metabolics, proteomics, and other "omics" similarly create data at 
high-dimensional scales.

There are also some cases where data are large because of the number of
observations, rather than (or in addition to) the number of measurements taken
for each observation. One example of this is flow cytometry data, where the
observations are individual cells. Current experiments often capture in the
range of a million cells for each sample in flow cytometry, measuring for each
cell some characteristics that can be used to determine its size, granularity,
and surface proteins, all with the aim of characterizing its cell type. A more
recent example is with single cell RNA-sequencing. Again, with this technique,
observations are taken at the level of the cell, with on the order of at least
10,000 cells processed per sample. [Check with Marcela / Taru on
back-of-envelope estimates in this paragraph]

Whether data is large because it measures many features (e.g., transcriptomics)
or includes many observations (e.g., single-cell data), the sheer size of the
data can require you to digest it somehow before you can use it to answer
scientific questions. There are several preprocessing techniques that can be
used to do this. The way that you digest this size and complexity depends on
whether the data are large because they have many features or because they have
many observations.

For data with many measurements for each observation, the different measurements often 
have strong correlation structures across samples. For example, a large 
collection of genes may work in concert, and so gene expression across those
genes may be highly correlated, or a metabolite might break down into 
multiple measured metabolite features, making the measurements for those 
features highly correlated. Therefore, even though there may be thousands
of measurements that are made on each sample in high throughput data, it 
is often not the case that they are all providing separate information and
insights. 

In some cases, your data may have more measurements than samples. For example,
if you run an assay that measures the level of thousands of metabolite features,
with twenty samples for which you collect data, then you will end up with many
more measurements (columns in your dataset, if it has a tidy structure) than
observations (rows in a tidy data structure). In this case, you may have no
choice but to resolve this before later steps of analysis. This is because a
number of statistical techniques fail or provide meaningless results for
datasets with more columns than rows [@chatfield1995problem].

Another concern with data that have many measurements per observation is that
the amount of information across the measurements is lower than the number of
measurement---in other words, some of the measures are partially or fully
redundant. To get a basic idea of dimension reduction, consider this example.
Say you have conducted an experiment that includes two species of research mice,
C57 black 6 and BALB/C. You record information about each mouse, including
columns that record both which species the mouse is and what color its coat is.
Since C57 black 6 mice are always black, and BALB/C mice are always white, these
two columns of data will be perfectly correlated. Therefore, one of the two
columns adds no information---once you have one of the measurements for a mouse,
you can perfectly deduce what the other measurement will be. You could
therefore, without any lose of information, reduce the dimensions (number of
columns) of the data you've collected in this case by choosing only one of these
two columns to keep and getting rid of the other column.

This same idea scales up to much more complex data---in many high dimensional
datasets, many of the measurements (e.g., levels of metabolite features in
metabolomics data or levels of gene expression in gene expression data) will be
highly correlated with each other, essentially providing the same information
across different measurements. In this case, the complexity of the dataset can
often be substantially reduced by using a dimension reduction technique, and
then conducting analysis to compare values in some of the main principle
components of the data.

Dimension reduction techniques are therefore often a key element of data
preprocessing when working with high-dimensional biological data. Dimension
reduction helps to collect the information that is captured by the dataset into
fewer columns, or "dimensions"---to go, for instance, from columns that measure
the expression of thousands of different genes down to a few "principal
component" columns that capture the key sources of variation across these genes.
One long-standing approach to dimension reduction is principal components
analysis (PCA). Other newer techniques have been developed, as well, such as
t-distributed stochastic neighbor embedding (t-SNE) [@perkel2017single]. Newer
techniques often aim to improve on limitations of classic techniques like PCA
under the conditions of current biomedical data---for example, some may help
address problems that arise when applying dimension reduction techniques to very
large datasets [?].

> "Most approaches seek to reduce these 'multi-dimensional data', with each 
dimension being the expression of one gene, into a very small number of dimensions
that can be more easily visualised and interpreted. Principal component analysis
(PCA) is a mathematical algorithm that reduces the dimensionality of data, and
is a basic and very useful tool for examining heterogeneity in scRNA-seq data."
[@haque2017practical]

Another approach to digest the complexity of high dimensional data is to remove
some of the features that were measured entirely, an approach that is more
generally called "feature selection" in data science. One example is in
preprocessing single-cell RNA-seq data. In this case, it is common to filter
down to only some of the genes whose expression was measured. One filtering
criterion is to filter out "low quality" genes. These might be genes with low
abundance on average across samples or high dropout rates (which happens if
a transcript is present in the cell but either isn't captured or isn't amplified
and so is not present in the sequencing reads) [@haque2017practical,
@mccarthy2017scater]. Another criterion for filtering genes for single cell 
RNA-sequencing is to focus on the genes that vary substantially across different
cell types, removing the "housekeeping" genes with similar expression regardless
of the cell type. 

For data with lots of observations, like single-cell data, again the sheer size
of the data can make it difficult to explore and generate knowledge from it. 
In this case, you can often reduce complexity by finding a way to group the 
observations and then summarizing the size and other characteristics of each 
group. 

One way of doing this is with clustering techniques, which can be helpful
to explore large-scale patterns across the many observations. 
As one example, single cell RNA-seq measures messenger RNA expression for each
cell in a sample of what can be 10,000 or more cells [double-check 
with Taru]. One goal of scRNA-seq is
to use gene expression patterns in each cell to identify distinct cell types in
the sample, potentially including cell types that were not known prior to the
experiment [@perkel2017single]. To do this, it needs to used measures of the
expression of [hundreds or thousands] of genes in each cell to group the
[hundreds or thousands] of cells into groups with similar patterns of gene
expression. One common approach is to use clustering algorithms to group the
cells based on patterns in their gene expression.

One use of clustering techniques is to group cells into cell types, based
on their gene expression profiles, through scRNA-seq [@haque2017practical].

> "Dimensionality reduction and visualization are, in many cases, followed by 
clustering of cells into subpopulations that represent biologically meaningful 
trends in the data, such as functional similarity or developmental 
relationship. Owing to the high dimensionality of scRNA-seq data, clustering
often requires special consideration." [@haque2017practical]

> "Cluster analysis aims to partition a group of individuals into groups or clusters
which are in some sense 'close together'. There is a wide variety of possible 
procedures. In my experience the clusters obtained depend to a large extent on the methods
used (except where the clusters are really clear-cut) and users are now aware of
the drawbacks and the precautions which need to be taken to avoid irrelevant or misleading
results." [@chatfield1995problem]

Flow cytometry often uses a different process to leverage the different measures
taken on each cell to make sense of them. This process is referred to 
as "gating". In gating, each measure taken on the cells is considered one or 
two at a time. The cells with with characteristics on that measure or set of 
measures that are consistent with a cell type of interest are retained. 
The gating process steps through many of these "gates", filtering out cells and 
each step and only retaining the cells with markers or characteristics 
that align with a certain cell type, until the researcher is satisfied that they 
have identified all the cells of a certain type in the sample (e.g., all 
helper T cells in the sample). 

-----------------------------------------------------------------------------

> "The three main types of problem data are errors, outliers, and missing
observations. ... An error is an observation which is incorrect, perhaps
because it has been copied or typed incorrectly at some stage. An outlier is a 'wild'
or extreme observation which does not appear to be consistent with the rest of
the data. Outliers arise for a variety of reasons and can create severe problems.
... Errors and outliers are often confused. An error may or may not be an outlier, 
while an outlier may or may not be an error. ... An outlier may be caused by an error, 
but it is important to consider the alternative possibility that the observation 
is a genuine extreme result from the 'tail' of the distribution. This usually happens
when the distribution is skewed and the outlier comes from the long 'tail'."
[@chatfield1995problem]


> "There are several types of error..., including the following: 1. A recording error
arises, for example, when an instrument is misread. 2. A typing error arises when an 
observation is typed incorrectly. 3. A transcription error arises when an observation
is copied incorrectly, and so it is advisable to keep the amount of copying to a 
minimum. ..." [@chatfield1995problem

Some data collected through laboratory experiments is very straightforward and
requires little or no pre-processing before it's used in analysis. For example,
if you are regularly weighing the animals in an experiment, then the data may
require no pre-processing (in other words, you'll directly use the weight
recorded from the scale) or very minimal pre-processing (for example, if you are
keeping all animals for a treatment group in the same cage, you may weigh the
cage as a whole, in which case you could divide that weight by the number of
animals in the cage as a pre-processing step to estimate the average weight per
animal). 

Other data collected in the laboratory may require some pre-processing that
takes a few more steps, but is still fairly straightforward. For example, if you
plate bacteria from a sample at a variety of dilutions, you might count each
plate and determine a measure of Colony Forming Units from the set of plates
with different dilutions by deciding which dilution provides the clearest count
and then back-calculating based on its dilution to get the total number of
colony-forming units in the original sample. Pre-processing these data typically
will also involve transforming data, to get them in a format that is easier to
visualize or more appropriate for statistical analysis, for example, a log
transformation.

This step of pre-processing data can become much more complex with data that was
collected using complex equipment, like a flow cytometer, mass spectrometer, or
sequencer. In these cases, there are often steps required to extract from the
machine's readings a biologically-relevant measurement.

> "For beginners, caution is warrented. Bioinformatics tools can almost always yield
an answer; the question is, does that answer mean anything? Dudoit's advice is do 
some exploratory analysis, and verify that the assumptions underlying your chosen
algorithms make sense." [@perkel2017single]

> "We designed three features based on the assumption that broken cells contain a 
lower and multiple cells a higher number of transcripts compared to a typical 
high quality single cell. For the first feature we calculated the number of 
highly expressed and highly variable genes. For the second feature we calculated the
variance across genes. Lastly, we hypothesized that the number of genes expressed
at a particular level would differ between cells. Thus we binned normalized 
read counts into intervals (very low to very high) and counted the number of genes
in each interval. ... Overall, our results show that technical features [like the 
number of detected genes and the percent of mapped reads] can help distinguish
low and high quality cells." [@ilicic2016classification]

> "Before further analyses, scRNA-seq data typically require a number of bio-informatic
QC checks, where poor-quality data from single cells (arising as a result of many 
possible reasons, including poor cell viability at the time of lysis, poor mRNA
recovery and low efficiency of cDNA production) can be justifiably excluded from 
subsequent analysis. Currently, there is no consensus on exact filtering 
strategies, but most widely used criteria include relative library size, number
of detected genes and fraction of reads mapped to mitochondria-encoded genes or
synthetic spike-in RNAs. ... Other considerations are whether single cells have
actually been isolated or whether indeed two or more cells have been mistakenly 
assessed in a particular sample." [@haque2017practical]

> "For each gene, QC metrics such as the average expression level and the proportion of
cells in which the gene is expressed are computed. This can be used to identify 
low-abundance genes or genes with high dropout rates that should be filtered out 
prior to downstream analyses." [@mccarthy2017scater]

> "Methods to quantify mRNA abundance introduce systematic sources of variation 
that can obscure signals of interest. Consequently, an essential first step in 
most mRNA-expression analyses is normalization, whereby systemic variations 
are adjusted to make expression counts comparable across genes and / or samples.
Within-sample normalization methods adjust for gene-specific features, such 
as GC content and gene length, to facility comparisons of a gene's expression
within an individual sample; whereas between-sample normalization methods adjust
for sample-specific features, such as sequencing depth, to allow for comparisons of
a gene's expression across samples." [@bacher2017scnorm]

> "A number of methods are available for between-sample normalization in bulk RNA-seq
experiments. Most of these methods calculate global scale factors (one factor is
applied to each sample, and this same factor is applied to all genes in the sample) 
to adjust for sequencing depth. These methods demonstrate excellent performance in 
bulk RNA-seq, but they are compromised in the single-cell setting because of an 
abundance of zero-expression values and increased technical variability." [@bacher2017scnorm]

> "Normalization is critical to the development of analysis techniques on scRNA-seq and 
to counteract technical noise or bias. Before observed data can be used to identify 
differentially expressed genes or potential subpopulations, it must undergo these 
corrections, for what is observed is seldom exactly what is present within the data set."
[@lytal2020normalization]

> "Scaling normalization is typically required in RNA-seq data analysis to 
remove biases caused by differences in sequencing depth, capture efficiency
or composition effects between samples." [@mccarthy2017scater]


> "The primary goal of single-cell normalization is to remove the influence
of technical effects in the underlying molecular counts, while preserving
true biological variation." [@hafemeister2019normalization]

There are also cases where pre-processing steps could be used to 
remove patterns from technical noise or even from biological patterns that 
are unrelated to the scientific question of interest. In terms of technical 
noise, for example, there are cases where pre-processing steps could be used 
to help remove variation that's introduced by running the experimental samples
in different batches. In terms of biological patterns, one pattern that may 
be desirable to remove through pre-processing is gene expression related to 
a cell's phase in the cell cycle.

> "We emphasize that it is generally preferable to incorporate batch effects or 
latent variables into statistical models used for inference. When this is not 
possible (e.g., for visualization), directly regressing out these uninteresting 
factors is required to obtain 'corrected' expression values for further analysis.
Furthermore, a general risk of removing latent factors is that interesting 
biological variation may be removed along with the presumed unwanted variation. 
Users should therefore apply such methods with appropriate caution, particularly 
when an analysis aims to discover biological conditions, such as new cell types."
[@mccarthy2017scater]

> "Once identified, important covariates and latent variables can be flagged for
inclusion in downstream statistical models or their effects regressed out of 
normalized expression values." [@mccarthy2017scater]

> "Most approaches seek to reduce these 'multi-dimensional data', with each 
dimension being the expression of one gene, into a very small number of dimensions
that can be more easily visualised and interpreted. Principal component analysis
(PCA) is a mathematical algorithm that reduces the dimensionality of data, and
is a basic and very useful tool for examining heterogeneity in scRNA-seq data."
[@haque2017practical]

> "One common type of single-cell analysis, for instance, is dimension reduction. 
This process simplifies data sets to facilitate the identification of similar 
cells. ... scRNA-seq data represent each cell as 'a list of 20,000 gene-expression
values.' Dimensionality-reduction algorithms such as principal components analysis
(PCA) and t-distributed stochastic neighbour embedding (t-SNE) effectively project
those shapes into two or three dimensions, making clusters of similar cells apparent." 
[@perkel2017single]

> "Multivariate techniques may be used to reduce the dimensionality... It is potentially
dangerous to allow the number of variables to exceed the number of observations
because of non-uniqueness and singularity problems. Put simply, the unwary analyst 
may try to estimate more parameters than there are observations." [@chatfield1995problem]

> "Principal component analysis rotates the p observed values to p new orthogonal 
variables, called principal components, which are linear combinations of the original
variables and are chosen in turn to explain as much of the variation as possible.
It is sometimes possible to confine attention to the first two or three components, 
which reduces the effective dimensionality of the problem. In particular, a scatter
diagram of the first two components is often helpful in detecting clusters of 
individuals or outliers." [@chatfield1995problem]

> "In multicellular organisms, cells carry out a diverse array of complex, specialized
functions. This specialization occurs mostly through the expression of cell type--specific
genes and proteins that generate the appropriate structures and molecular networks. 
A central challenge in the biomedical sciences, however, has been to identify the 
distinct lineages and phenotypes of the specialized cells in organ systems, and track 
their molecular evolution during differentiation." [@benoist2011flow]

> "Since the 1970s, fluoresence-based flow cytometry has been the leading technique
for studying and sorting cell populations. It involves passing cells through 
flow chambers at high rates (> 20,000 cells/s) and using lasers to excite fluorescent
tags ('fluorochromes') that are usually attached to antibodies; different antibodies
are tagged with different colors, enabling researchers to quantify molecules that 
define cell subtypes or reflect activation of specific pathways. Progess in instrument
design, multi-laser combinations, and novel fluorochromes has led to experimental
configurations that simultaneously measure up to 15 markers. This has enabled very 
detailed description of cell subtypes, perhaps most extensively in the immune
system, where the Immunological Genome Project is profiling >200 distinct cell typles.
Fluorescence cytometry seems to have reached a technical plateau, however: In practice, 
researchers typically measure only 6 to 10 cell markers because they are limited by 
the specral overlap between fluorochromes." [@benoist2011flow]

> "In 1954, Wallace Coulter developed an instrument that could measure cell size and
count the absolute number of cells, and thus the discipline of flow cytometry was
born. Further developments enabled the production of instruments that could measure
cell size and nucleic acid content using a two-dimensional approach that compared
light scatter and light absorption. These instruments were cumbersome and required
specialist operators, but immunologists began to use them to investigate the 
functions of immune cells. ... By the mid-1980s, bench-top flow cytometers were 
available and as the technology advanced the instruments became progressively 
smaller. Coupled with the availability of monoclonal antibodies, the increasing 
number of available fluorochromes (compounds that emit light at a greater wavelength
than the light source they are excited with) and computer improvements, flow
cytometers are now accessible for almost every clinical laboratory." [@barnett2008cd4]

> "Flow cytometry enables the examination of microscopic particles (such as cells) that
are suspended in a stream of fluid which is termed sheath fluid. This fluid is 
focused hydrodynamically such that the cells flow in single file through a flow cell
in which a beam of light (usually a laser) is focused. As the cells pass through the 
laser beam they scatter the light so that forward scatter (FSC) and side scatter (SSC)
light is captured; this enables the size and granularity of the cells to be determined.
In addition, if a cell is labelled with antibodies that carry a fluorochrome, as the 
cell passed in front of the laser beam the fluorochrome emits light at a wavelength
that is higher than the single wavelength light source and which can be detected by 
fluorescence detectors. The flow cytometers that are in clinical use can analyse at
least four fluorochromes simultaneously, in addition to the FSC and SSC. This is known 
as multiparametric analysis. The information that is generated is computer analysed
so that specific analysis regions (known as gates) can be created, which allows
the user to build up a profile of the size, granularity and antigen profile of
the target cell population." [@barnett2008cd4]

> "T cells have an essential role in protection against a variety of infections. Indeed, 
the development of successful vaccines against HIV, malaria or tuberculosis will 
require the generation of potent and durable T-cell responses. ... As T cells are
functionally heterogeneous and mediate their effects through a variety of mechanisms, 
a major hurdle in quantifying protective T-cell responses has been the technical 
limitations in assessing the complexity of such responses. Methods to define the 
full characteristics of T cells are crucial for developing preventative and 
therapeutic vaccines for infections and cancer." [@seder2008t]

> "Flow cytometry has increasingly become a tool of choice for the analysis of 
cellular phenotype and function in the immune system. It is arguably the most 
powerful technology available for probing human immune phenotypes, because it 
measures multiple parameters on many individual cells. Flow cytometry thus 
allows for the characterization of many subsets of cells, including rare subsets, 
in a complex mixture such as blood. And because of the wide array of antibody 
reagents and protocols available, flow cytometry can be used to assess not only 
the expression of cell-surface proteins, but also that of intracellular 
phosphoproteins and cytokines, as well as other functional readouts."
[@maecker2012standardizing]

> "Immune phenotypes: Measurable aspects of the immune system, such as the 
proportions of various cell subsets or measures of cellular immune function."
[@maecker2012standardizing]

> "Gates: Sequential filters that are applied to a set of flow cytometry data to 
focus the analysis on particular cell subsets of interest." [@maecker2012standardizing]

> "One of the main roles that statistics play in science is explaining variation---variation
of observed data. That variation can actually be true signal that you're interested in, 
but there can also be variations due to noise or confounding signal. So I think of
statistical modeling as the process of explaining variation in the data according 
to concrete variables that have been measured." [@mak2011john]

> "This process of accounting for, and possibly removing, sources of variation that
are not of biological interest is called normalization. There are two distinct 
approaches to normalization. One of them I would call 'unsupervised' in that it 
does not taken into account the study design. These are the most popular methods
because they require the least amount of statistical modeling and knowledge of 
statistics. The other approach, which is one I strongly favor, is what I would
call 'supervised normalization'. This approach directly takes into account the 
study design. I find this appealing because is one is trying to parse sources of 
variation, then it seems all sources of variation should be considered. If I 
perform an experiment with 20 microarrays, say 10 treated and 10 control, then 
I want to utilize this information when separating and removing technical 
sources of variation. Another component of normalization, which is gaining 
popularity, is normalizing by principal components. Again, I think this should
be done in the context of the study design, which was the goal of a recent 
method I worked on called 'surrogate variable analysis.'" [@mak2011john]

> "Let's say you are doing an RNA-Seq experiment. The sequencer may produce a 
different number of total reads from lane to lane, and that is more likely 
driven by technology, not biology. And so, normalization would be about 
accounting for those differences. Unsupervised normalization might involve
simply just dividing by the number of lanes and taking each gene as a percentage
of the reads in the lane. Why is that less than ideal? Suppose you have two batches
of data, one flow cell that was done in November and another flow cell that was
done in December. If you're actually accounting for this variation in the
total number of reads per lane, my inclination would be to take into account the 
fact that these two flow cells were processed in different months. And it can 
be more complicated than that, too. Maybe you've taken clinical samples and there
were some differences in the clinical conditions under which they were taken. 
In supervised normalization, you would take that information into account. For 
example, the adjustment made to the raw reads may be based on a model that includes
the total number of reads per lane as well as the information about the study 
design, such as batch and biological variables." [@mak2011john]

> "The biggest, the easiest way [for a biologist doing RNA-Seq to tell that 
better normalization of the data is needed]---the way that I discovered the 
importance of normalization in the microarray context---is the lack of 
reproducibility across different studies. You can have three studies that are
all designed to study the same thing, and you just see basically no reproducibility, 
in terms of differentially expressed genes. And every time I encountered that, it
could always be traced back to normalization. So, I'd say that the biggest sign and 
the biggest reason why you want to use normalization is to have a clear signal
that's reproducible." [@mak2011john]

> "Data-analysis pipelines are replete with configuration decisions, assumptions, 
dependencies and contingencies that move quickly beyond documentation, making 
troubleshooting incrediably difficult. ... Teams had to visit each others' labs more
than once to understand and fully implement computational-analysis pipelines for
large microscopy datasets." [@raphael2020controlled]

> "Improved reproducibility comes from pinning down methods." [@lithgow2017long]

> "Our first task, to develop a protocol, seemed straightforward. But subtle disparities
were endless. In one particularly painful teleconference, we spent an hour debating
the proper procedure for picking up worms and placing them on new agar plates. Some
batches of worms lived a full day longer with gentler techniques. Because a worm's 
lifespan is only about 20 days, this is a big deal. Hundreds of e-mails and many 
teleconferences later, we converged on a technique but still had a stupendous three-day
difference in lifespan between labs. The problem, it turned out, was notation---one
lab determined age on the basis of when an egg hatched, others on when it was laid." 
[@lithgow2017long]

> "Ushering in the Enlightenment era in the late seventeenth century, chemist
Robert Boyle put forth his controversial idea of a vacuum and tasked himself
with providing descriptions of his work sufficient 'that the person I addressed
them to might, without mistake, and with as little trouble as possible, be able
to repeat such unusual experiments'. Much modern scientific communication falls
short of this standard. Most papers fail to report many aspects of the
experiment and analysis that we may not with advantage omit---things that are
crucial to understanding the result and its limitations, and to repeating the
work. We have no common language to describe this shortcoming. I’ve been in
conferences where scientists argued about whether work was reproducible,
replicable, repeatable, generalizable and other '-bles', and clearly meant quite
different things by identical terms. Contradictory meanings across disciplines
are deeply entrenched." [@stark2018before]

> "The distinction between a preproducible scientific report and current common
practice is like the difference between a partial list of ingredients and a
recipe. To bake a good loaf of bread, it isn’t enough to know that it contains
flour. It isn't even enough to know that it contains flour, water, salt and
yeast. The brand of flour might be omitted from the recipe with advantage, as
might the day of the week on which the loaf was baked. But the ratio of
ingredients, the operations, their timing and the temperature of the oven
cannot. Given preproducibility---a 'scientific recipe'---we can attempt to make
a similar loaf of scientific bread. If we follow the recipe but do not get the
same result, either the result is sensitive to small details that cannot be
controlled, the result is incorrect or the recipe was not precise enough (things
were omitted to disadvantage). Depending on the discipline, preproducibility
might require information about materials (including organisms and their care),
instruments and procedures; experimental design; raw data at the instrument
level; algorithms used to process the raw data; computational tools used in
analyses, including any parameter settings or ad hoc choices; code, processed
data and software build environments; or analyses that were tried and
abandoned." [@stark2018before]

> "If I publish an advertisement for my work (that is, a paper long on results
but short on methods) and it's wrong, that makes me untrustworthy. If I say:
'here’s my work' and it's wrong, I might have erred, but at least I am honest.
If you and I get different results, preproducibility can help us to identify
why---and the answer might be fascinating." [@stark2018before]

> "As chemists, we have to be able to go to the literature, take a procedure, 
and carry out a similar or identical transformation with our own hands. Frustratingly,
this doesn't always happen, and the next-to-worst-case scenario possible is 
when it's one of your own reactions that can't be reproduced by a lab 
elsewhere. Unsurprisingly, one step worse than this is when you can't even 
reproduce one of your own reactions in your own lab!" [@gibb2014reproducibility]

> "If there is nothing wrong with the reagents and reproducibility is still an 
issue, then as I like to tell students, there are two options: (1) the physical 
constants of the universe and hence the laws of physics are in a state of flux 
in their round-bottomed flask, or (2) the researcher is doing something wrong 
and either doesn't know it or doesn't want to know it. Then I ask them which 
explanation they think I'm leaning towards." [@gibb2014reproducibility]



> "Consider a set of measurements that reflect some underlying true values
(say, species represented by DNA sequences from their genomes) but
have been degraded by technical noise. Clustering can be used to remove 
such noise." [@holmes2018modern]

> "In many cases, different variables are measured in different units, so they
have different baselines and different scales. [Footnote: 'Common measures of
scale are the range and the standard deviation...'] For PCA and many other
methods, we therefore need to transform the numeric values to some common scale
in order to make comparisons meaningful. Centering means subtracting the mean,
so that the mean of the centered data is at the origin. Scaling or standardizing
then means dividing by the standard deviation, so that the new standard
deviation is 1. ... To perform these operations, there is the R function
`scale`, whose default behavior when given a matrix or a dataframe is to make
each column have a mean of 0 and a standard deviation of 1. ... We have already
encountered other data transformation choices in Chapters 4 and 5, where we used
the log and asinh functions. The aim of these transformations is (usually)
variance stabilization, i.e., to make the variances of the replicate
measurements of one and the same variable in different parts of the dynamic
range more similar. In contrast, the standardizing transformation described
above aims to make the scale (as measured by mean and standard deviation) of
*different* variables the same. Sometimes it is preferable to leave variables at
different scales because they are truely of different importance. If their
original scale is relevant, then we can (and should) leave the data alone. In
other cases, the variables have different precisions known a priori. We will see
in Chapter 9 that there are several ways of weighting such variables. After
preprocessing the data, we are ready to undertake data simplification through
dimension reduction." [@holmes2018modern]

> With data that give the number of reads for each gene in a sample, "The 
data have a large dynamic range, starting from zero up to millions. The 
variance and, more generally, the distribution shape of the data in different
parts of the dynamic range are very different. We need to take this 
phenomenon, called heteroscedascity, into account. The data are non-negative
integers, and their distribution is not symmetric---thus normal or log-normal
distribution models may be a poor fit. We need to understand the systematic
sampling biases and adjust for them. Confusingly, such adjustment is often 
called normalization. Examples are the total sequencing depth of an experiment
(even if the true abundance of a gene in two libraries is the same, we expect
different numbers of reads for it depending on the total number of reads
sequenced) and differing sampling probabilities (even if the true abundance of 
two genes within a biological sample is the same, we expect different numbers
of reads for them if they have differing biophysical properties, such as length, 
GC content, secondary structure, binding partners)." [@holmes2018modern]

> "Often, systematic biases affect the data generation and are worth taking
into account. Unfortunately, the term normalization is commonly used for that
aspect of the analysis, even though it is misleading; it has nothing to do 
with the normal distribution, nor does it involve a data transformation. 
Rather, what we aim to do is identify the nature and magnitude of 
systematic biases and take them into account in our model-based analysis of the
data. The most important systematic bias [for count data from high-throughput
sequencing applications like RNA-Seq] stems from variations in the total number
of reads in each sample. If we have more reads for one library than for another, 
then we might assume that, everything else being equal, the counts are 
proportional to each other with some proportionality factor *s*. Naively, 
we could propose that a decent estimate of *s* for each sample is simply 
given by the sum of the counts of all genes. However, it turns out that we 
can do better..." [@holmes2018modern]

> "When testing for differential expression, we operate on raw counts and
use discrete distributions. For other downstream analyses---e.g., for 
visualization or clustering---it can be useful to work with transformed versions
of the count data. Maybe the most obvious choice of transformation is the 
logarithm. However, since count values for a gene can be zero, some analysts
advocate the use of pseudocounts, i.e., transformations of the form
y = log2(n + 1) or more generally y = log2(n + n0)." [@holmes2018modern]

> "The data sometimes contain isolated instances of very large counts that
are apparently unrelated to the experimental or study design and may be
considered outliers. Outliers can arise for many reasons, including rare 
technical or experimental artifacts, read mapping problems in the case of
genetically differing samples, and genuine but rare biological events. In 
many cases, users appear primarily interested in genes that show consistent 
behavior, and this is the reason why, by default, genes that are affected by such 
outliers are set aside by `DESeq`. The function calculates, for every gene
and for every sample, a diagnostic test for outliers called Cook's distance. 
Cook's distance is a measure of how much a single sample is influencing the
fitted coefficients for a gene, and a large value of Cook's distance is 
intended to indicate an outlier count. `DESeq2` automatically flags genes
with Cook's distance above a cutoff and sets their p-values and adjusted
p-values to NA. ... With many degrees of freedom---i.e., many more samples
than number of parameters to be estimated---it might be undesirable to remove
entire genes from the analysis just becuase their data include a single 
count outlier. An alternative strategy is to replace the outlier counts with 
the trimmed mean over all sample, adjusted by the size factor for that
sample. This approach is conservative: it will not lead to false positives, 
as it replaces the outlier value with the value predicted by the null hypothesis." [@holmes2018modern]

> "Since the sampling depthy is typically different for different sequencing
runs (replicates), we need to estimate the effect of this variable parameter
and take it into account in our model. ... Often this part of the analysis 
is called normalization (the term is not particularly descriptive, but 
unfortunately it is now well established in the literature)." [@holmes2018modern]

> "Sometimes we explicitly know about factors that cause bias, for instance, when
different reagent batches were used in different phases of the experiment. We
call these batch effects (Leek et al., 2010). At other times, we may expect that
such factors are at work but have no explicit record of them. We call these
latent factors. We can treat them as adding to the noise, and in Chapter 
4 we saw how to use mixture models to do so. But this may not be enough; with 
high-dimensional data, noise caused by latent factors tends to be correlated, 
and this can lead to faulty inference (Leek et al., 2010). The good news is that
these same correlations can be exploited to estimate latent factors from 
the data, model them as bias, and thus reduce the noise (Leek and Storey 2007; 
Stegle et al. 2010)." [@holmes2018modern]

> "Regular noise can be modeled by simple probability models such as independent
normal distributions or Poissons, or by mixtures such as the gamma-Poisson or
Laplace. We can use relatively straightforward methods to take such noise into
account in our data analyses and to compute the probability of extraordinarily 
large or small values. In the real world, this is only part of the story: 
measurements can be completely off-scale (a sample swap, a contamination, or 
a software bug), and they can all go awry at the same time (a whole microtiter
plate went bad, affecting all data measured from it). Such events are hard to model
or even correct for---our best chance of dealing with them is data quality 
assessment, outlier detection, and documented removal." [@holmes2018modern]

> "We distinguis between data quality assessment (QA)---steps taken to measure
and monitor data quality---and quality control---the removal of bad data. 
These activities pervade all phases of an analysis, from assembling the raw 
data over transformation, summarization, model fitting, hypothesis testing or
screening for 'hits' to interpretation. QA-related questions include: 
1. How do the marginal distributions of the variables look (histograms, 
ECDF plots)? 2. How do their joint distributions look (scatterplots, pair plots)?
3. How well do replicated agree (as compared to different biological conditions)?
Are the magnitudes of different between several conditions plausible?
4. Is there evidence of batch effects? These could be of a categorical (stepwise)
or continuous (gradual) nature, e.g., due to changes in experimental reagents, 
protocols or environmental factors. Factors associated with such effects may 
be explicitly known, or unknown and latent, and often they are somewhere in 
between (e.g., when a measurement apparatus slowly degrades over time, and 
we have recorded the times, but don't really know exactly when the degradation
becomes bad). For the last two sets of questions, heatmaps, principal components
plots, and other ordination plots (as we have seen in Chapters 7 and 9) are 
useful." [@holmes2018modern]

> "It's not easy to define quality, and the word is used with many meanings. The
most pertinent for us is fitness for purpose, and this contrasts with other 
definitions that are based on normative specifications. For instance, in 
differential expression analysis with RNA-Seq data, our purpose may be the 
detection of differentially expressed genes between two biological conditions. 
We can check specificiations such as the number of reads, read length, base
calling quality and fraction of aligned reads, but ultimately these measures in 
isolation have little bearing on our purpose. More to the point will be the 
identification of samples that are not behaving as expected, e.g., because of 
a sample swap or degradation, or genes that were not measured properly. 
... Useful plots include ordination plots ... and heatmaps ...
A quality metric is any value that we use to measure quality, and having 
explicit quality metrics helps in automating QA/QC." [@holmes2018modern]

> "Getting data ready for analysis or visualization often involves a lot of 
shuffling until they are in the right shape and format for an analytical 
algorithm or a graphics routine." [@holmes2018modern]

