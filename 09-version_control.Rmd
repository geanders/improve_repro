## Harnessing version control for transparent data recording

As a research project progresses, a typical practice in many experimental
research groups is to save new versions of files (e.g., 'draft1.doc',
'draft2.doc'), so that changes can be reverted. However, this practice leads to
an explosion of files, and it becomes hard to track which files represent the
'current' state of a project. Version control allows researchers to edit and
change research project files more cleanly, while maintaining the power to
'backtrack' to previous versions, messages included to explain changes. We will
explain what version control is and how it can be used in research projects to
improve the transparency and reproducibility of research, particularly for data
recording.

**Objectives.** After this module, the trainee will be able to:

- Describe version control  
- Explain how version control can be used to improve reproducibility 
for data recording

### What is version control?

Version control developed as a way to coordinate collaborative work on 
software programming projects. The term "version" here refers to the current
state of a document or set of documents, for example the source code for a
computer program. The idea of "control" is to allow for safe changes and updates
to this version while more than one person is working on it. 
The general term "version control" can refer to any method of
syncing contributions from several people to a file or set of files, and
very early on it was done by people rather than through a computer program. 
Some software developers have shared their memories of taking their
code to the "Source Control" team, who then integrated it into the main
source code file by hand [@irving2011astonishments]. 

> "Tracking all that detail is just the sort of thing computers
are good at and humans are not." [@raymond2003art]

As the proverb about too many cooks in the kitchen captures, any time you have
multiple people working on a project, it introduces the chance for conflicts.
While higher-level conflicts, like about what you want the final product to look
like or who should do which jobs, can't be easily managed by a computer program,
now the complications of integrating everyone's contributions---and letting
people work in their own space and then bring together their individual work
into one final joint project---can be. While these programs for version control
were originally created to help with programmers developing code, they can be
used now to coordinate group work on numerous types of file-based projects,
including scientific manuscripts, books, and websites [@raymondunderstanding].
And although they can work with projects that include binary code, they thrive
in projects with a heavier concentration of text-based files, and so they fit in
nicely in a scientific research / data analysis workflow that is based on data
stored in plain text formats and data analysis scripts written in plain text
files, tools we discuss in other parts of this book.

In earlier types of version control programs, there was one central "main"
repository the file or set of files the team was working on
[@raymondunderstanding; @target2018version]. Very early on, this was kept on one
computer [@irving2011astonishments]. A team member who wanted to make a change
would "check out" the file he or she wanted to work on, make changes, and then
check it back in as the newest main version [@raymond2003art. While one team
member had this file checkout out, other members would often be "locked" out of
making any changes to that file---they could look at it, but couldn't make any
edits [@raymondunderstanding; @target2018version]. This meant that there was no
chance of two people trying to change the same part of a file at the same time.
In spirit, this early system is pretty similar to the idea of sending a file
around the team by email, with the understanding that only one person works on
it at a time. While the "main" version is in different people's hands at
different times, to do work, you all agree that only one person will work on it
at a time. A slightly more modern analogy is the idea of having a single version
of a file in Dropbox or Google Docs, and avoiding working on the file when you
see that another team member is working on it. (However, both of these examples
other key features of version control, which we'll discuss a bit later.)

> "The most primitive (but still very common) method [of version control] is all 
hand-hacking. You snapshot the project periodically by manually copying everything
in it to a backup. You include history comments in source files. You make verbal
or email arrangements with other developers to keep their hands off certain files
while you hack them." [@raymond2003art]

> "The hidden costs of this hand-hacking method are high, especially when (as 
frequently happens) it breaks down. The procedures take time and concentration; 
they're prone to error, and tend to get slipped under pressure or when the project
is in trouble---that is exactly when they are needed. ... To avoid these problems, 
you can use a *version-control system* (VCS), a suite of programs that 
automates away most of the drudgery involved in keeping an annotated history of
your project and avoiding modification conflicts." [@raymond2003art]

> "In a medium-sized project, it often happens that a (relatively small) number
of people work simultaneously on a single set of files, the 'program' or the
'project'. Often these people have additional tasks, causing their working
speeds to differ greatly. One person may be working a steady ten hours a day on
the project, a second may have barely time to dabble in the project enough to
keep current, while a third participant may be sent off on an urgent temporary
assignment just before finishing a modification. It would be nice if each
participant could be abstracted from the vicissitudes of the lives of the
others." [@grune1986concurrent]

This system is pretty clunky, though. In particular, it probably usually
increases the amount of time that it takes the team to finish the project,
because only one person can work on a file at a time. Later types of version
control programs moved toward a different style, allowing for *distributed*
rather than *centralized* collaborative work on a file or a set of files
[@raymondunderstanding; @irving2011astonishments]. Under the distributed model,
all team members can have their own version of all the files, work on them and
make records of changes they make to the files, and then occassionally sync with
everyone else to share your changes with them and bring their changes into your
copy of the files. 

This functionality is called *concurrency*, since it allows team members to
concurrently work on the same set of files [@raymondunderstanding]. This idea
allowed for the development of other useful features and styles of working,
including *branching* to try out new ideas that you're not sure you'll
ultimately want to go with and *forking*, a key tool used in open-source
software development, which among other things facilitates someone who isn't
part of the original team getting a copy of the files they can work with and
suggesting some changes that might be helpful.

So, this is the basic idea of modern version control---for a project that involves a set
of computer files, everyone on has their own copy of a directory with those files
on their own computer, makes changes at the time and in the spots in the files that
they want, and then regularly re-syncs their local directory with everyone else's
to share changes and updates. 

> "VCSs are a huge boon to productivity and code quality in many ways, even for small 
single-developer projects. They automate away many procedures that are just tedious
work. They help a lot in recovering from mistakes. Perhaps most importantly, they 
free programmers to experiment by guarnateeing that reversion to a known-good
state will always be easy." [@raymond2003art]

There is one key feature of modern version control that's
critical to making this work---merging files that started the same but were edited in
different ways and now need to be put back together, bringing any changes made from the
original version. This step is called *merging* the files. While this is typically 
described using the plural, "files", at a higher-level, you can thing of this as 
just merging the *changes* that two people have made as they edited a single file, a
file where they both started out with identical copies. 

Think of the file broken up into each of its separate lines. There will be some lines
that neither person changed. Those are easy to handle in the "merge"---they stay the
same as in the original copy of the file. Next, there will be some lines that one
person changed, but that the other person didn't. It turns out that these are pretty
easy to handle, too. If only one person changed the line, then you use their version---it's
the most up-to-date, since if both people started out with the same version, it means
that the other person didn't make any changes to that part of the file. 
Finally, there may be a few lines that both people changed. These are called *merge conflicts*. 
They're places in the file where there's not a clear, easy-to-automate way that
the computer can know which version to put into the integrated, latest version of the
file. Different version control programs handle these merge conflicts in different ways. 
For the most common version control program used today, *git*, these spots in the 
file are flagged with a special set of symbols when you try to integrate the two updated
versions of the file. Along with the special symbols to denote a conflict, there will 
also be *both* versions of the conflicting lines of the file. Whoever is integrating the 
files must go in and pick the version of those lines to use in the integrated version
of the file, or write in some compromise version of those lines that brings in elements
from both people's changes, and then delete all the symbols denoting that was a conflict
and save this latest version of the file. 

There are a number of other features of version control that make it useful for
collaborating on file-based projects with teams. First, these systems allow you
to explain your changes as you make them---in other words, it allows for
*annotation* of the developing and editing process [@raymondunderstanding]. This
provides the team with a full history of why the files evolved in the way they
did across the team. It also provides a way to communicate across the team
members. 

> "You will likely share your code with multiple lab mates or collaborators, 
and they may have suggestions on how to improve it. If you email the code
to multiple people, you will have to manually incorporate all the changes 
each of them sends." [@blischak2016quick]

[*change tracking*]
For example, if one person is the key person working on a certain file,
but has run into a problem with one spot and asks another team member to take a
go, then the second team member isn't limited to just looking at the file and
then emailing some suggestions. Instead, the second person can make sure he or
she has the latest version of that file, make the changes they think will help,
*commit* those changes with a message (a *commit message*) about why they think
this change will fix the problem, and then push that latest version of the file
back to the first person. If there are several places where it would help to
change the file, then these can be fixed through several separate commits, each
with their own message. The first person, who originally asked for help, can
read through the updates in the file (most platforms for using version control
will now highlight where all these changes are in the file) and read the second
person's message or messages about why each change might help. Even better, days
or months later, when team members are trying to figure out why a certain change
was made in that part of the file, can go back and read these messages to get an
explanation. 

> "You know your code has changed; do you know why? It's easy to forget the 
reasons for changes, and step on them later. If you have collaborators on a 
project, how do you know what they have changed while you weren't looking, and
who was responsible for each change?" [@raymond2003art]

There's another feature of version control that can be demonstrated through this
example. Because the changes made to the files in the project are regularly
*committed*, with a message for each commit and a record of what was changed in
that commit, the team members can, at any point, go back to the *exact* version
of the project at any earlier point in time. Each of the commits is given its
own ID tag ([name for this? hash?]), and *git* has a number of commands that let
you "roll back" to earlier versions, by going back to the version as it was when
a certain commit was made, provided *reversability* within the project files
[@raymondunderstanding].

> "If you make a change, and discover it's not viable, how can you revert
to a code version that is known good? If reversion is difficult or unreliable, 
it's hard to risk making changes at all (you could trash the whole project, or
make many hours of painful work for yourself)." [@raymond2003art]

It turns out that this functionality---of being able to "roll back" to earlier
versions---has a wonderful side benefit when it comes to working on a large
project. It means that you **don't** need to save earlier versions of each file.
You can maintain one and only one version of each project file in the project's
directory, with the confidence that you never "lose" old versions of the file
[@perkel2018git; @blischak2016quick]. If you're working on a manuscript, for
example, when it's time to edit, you can cut whole paragraphs, and if you ever
need to get them back, they'll be right there in the commit history for your
project, with their own commit message about why they were cut (hopefully a nice
clear one that will make it easy to find that commit if you ever need those
paragraphs again).

[xkcd / cartoon of this?]

> "Early in his graduate career, John Blischak found himself creating figures
for his advisor's grant application. Blischak was using the programming language
R to generate the figures, and as he iterated and optimized his code, he ran
into a familiar problem: Determined not to lose his work, he gave each new
version a different filename---analysis_1, analysis_2, and so on, for
instance---but failed to document how they had evolved. 'I had no idea what had
changed between them,' says Blischak... Using Git, Blischak says, he no longer
needed to maintain multiple copies of his files. 'I just keep overwriting it and
changing it and saving the snapshots. And if the professor comes back and says,
'oh, you sent me an email back in March with this figure', I can say, 'okay,
well, I'll just bo back to the March version of my code and I can recreate
it'.'" [@perkel2018git]

> "Have a quick look back up at those decades of progress. Yes, some of the
advances were also enabled by increasing computer power. But, mainly, they were
simply made by people thinking of cleverer ways of collaborating."
[@irving2011astonishments]

> "Using GitHub or any similar versioning / tracking system is not a replacement
for good project management; it is an extension, an improvement for good
project and file management." [@perez2016ten]

The most common version control program currently used for scienctific projects is
*git*. This program was created by Linus Torvalds, who also created the Linux 
operating system, in 2005 as a way to facilitate the team working on Linux 
development. This program for version control thrives in large collaborative
projects, for example open-source software development projects that include
numerous contributors, both regular and occasional [@brown2018git].

> "If your software engineering career, like mine, is no older than GitHub, then 
git may be the only version control software you have ever used. While people 
sometimes grouse about its steep learning curve or unintuitive interface, git has
become everyone's go-to for version control." [@target2018version]

In recent years, some complementary tools have been developed that make the process of 
collaborating together using version control software easier. 
Other tools can helps in collaborating on file-based projects, including *bug trackers*
or *issue trackers*, which allow the team to keep a running "to-do" list of what needs
to be done to complete the project. Other helpful tools, particularly for compiled
software projects, include *build systems*. Some of these features come within 
Software Configuration Management (SCM) programs [@raymondunderstanding]

> "GitHub issues are a great way to keep track of bugs, tasks, feature requests,
and enhancements. While classical issue trackers are primarily intended to be 
used as bug trackers, in contrast, GitHub issue trackers follow a different 
philosophy: each tracker has its own section in every repository and can be used
to trace bugs, new ideas, and enhancements by using a powerful tagging system.
The main objective of issues in GitHub is promoting collaboration and providing 
context using cross-references. Raising an issue does not require lengthy forms
to be completed. It only requires a title and, preferably, at least a short description.
Issues have very clear formatting and provide space for anyone with a GitHub account
to provide feedback. ... Additional elements of issues are (i) color-coded labels
that help to categorize and filter issues, (ii) milestones, and (iii) one assignee 
responsible for working on the issue." [@perez2016ten]

> "As another illustration of issues and their generic and wide application, we
and others used GitHub issues to discuss and comment on changes in manuscripts
and address reviewers' comments." [@perez2016ten]

GitHub was created in 2008 as a web-based platform to facilitate collaborating
on projects running under git version control. It can provide an easier entry
to using git for version control than trying to learn to use git from the
command line [@perez2016ten]. It also plays well with RStudio, making it easy
to integrate a collaborative workflow through GitHub from the same RStudio
window on your computer where you are otherwise doing your analysis [@perez2016ten].

> "The astonishment was that you might want to make even your tiny hacks to 
other people's code public. Before GitHub, we tended to keep those on our own
computer. Nowadays, it is so each to make a fork, or even edit the code directly 
in your browser, that potentially anyone can find even your least polished
bug fixes immediately." [@irving2011astonishments]

> Resources like GitHub are "essential for collaborative software projects
because they enable the organization and sharing of programming tasks between
different remote contributors." [@perez2016ten]

On GitHub, you can set the access to a project to be either public or private,
and can be converted easily from one form to the other over the course of the
project [@metz2015github]. A private project can be viewed only by fellow team
members, while a public project can be viewed by anyone. What's more, while only
collaborators on a public project can directly change the code, anyone else can
*suggest* changes through a process of copying a version of the project
(*forking* it), making the changes they would like to suggest, and the asking
the project's owners to consider integrating the changes back into the main
version of the project through a *pull request*. GitHub therefore creates a
platform where people can explore, adapt, and add to other people's coding
projects, enabling a community of coders [@perez2016ten].

> "Much more than a code warehouse, GitHub is effectively a social network for
software development." [@perkel2018git]

> GitHub has been described as "a kind of bazaar that offers just about any 
piece of code you might want---and so much of it free." [@metz2015github]

GitHub can also be used to collaborate on, host, and publish websites and other
online content [@perez2016ten]. It's GitHub Pages functionality, for example, is
now being used to host a number of books created in R using the `bookdown`
package, including the online version of this book.

> "The traditional way to promote scientific software is by publishing an associated
paper in the peer-reviewed scientific literature, though, as pointed out by Buckheir and
Donoho, this is just advertising. Additional steps can boost the visibility of 
an organization. For example, GitHub Pages are simple websites freely hosted by 
GitHub. Users can create and host blog websites, help pages, manuals, tutorials, 
and websites related to specific projects." [@perez2016ten]

> "VCSs, by the way, are not merely good for program code; the manuscript of this 
book was maintained as a collection of files under RCS while it was being written."
[@raymond2003art]

### Improving scientific research with version control

> "The purpose of a lab notebook is to provide a lasting record of events in a
laboratory. In the same way that a chemistry experiment would be nearly
impossible without a lab notebook, scientific computing would be a nightmare of
inefficiency and uncertainty without version-control systems."
[@tippmannmy2014digital]

Traditionally, experimental data collected in a laboratory was recorded in a 
paper laboratory notebook. A laboratory notebook plays the role not only as the 
initial recording of data, but also can serve as, for example, a legal record
of the data recorded in the lab [@mascarelli2014research].

> Laboratory notebooks "can be used as evidence for securing patents, to settle
legal issues or to pass a project from one researcher to another."
[@mascarelli2014research]


As computers and other electronic tools continue to develop, there have been 
efforts to move toward recording data directly into a computer, skipping a paper 
step. Even simple data analysis is almost always done using a computer, so all
data will be eventually entered into a computer, meaning that paper entry serves
as an extra intermediate step. Any stage of copying data from one format to
another, especially when done by a human rather than a machine, introduces the 
chance to copying errors. Further, direct entry into a digital form makes data
recording more efficient, since it skips one step of copying (from paper into 
the computer). Additionally, paper notebooks 



There have been a number of efforts, both formal and informal, to move from paper
laboratory notebooks to electronic alternatives. In some fields that rely heavily on
computational analysis, there's very little use (if any) of paper laboratory 
notebooks [@butler2005electronic].

> "Whereas chemists have 400 years' practice using a lab book for their
experiments, bioinformaticians, who do all their work on a computer, have none."
[@butler2005electronic]

> "Lab notebooks are stuck in a time warp, still handwritten, on paper. Many
scientists can barely understand their own scribblings from last week, let alone
five years from now---as anyone who has had to decipher the hieroglyphics of a 
co-worker can testify."  [@butler2005electronic]

In other fields, where researchers have traditionally used paper lab notebooks,
companies have been working for a while to develop electronic laboratory
notebooks specifically tailored to scientific research needs
[@giles2012digital]. These were adopted more early in pharmaceutical industrial
labs, where companies had the budgets to get customized versions and the
authority to require their use, but have taken longer to be adapted in academic
laboratories [@giles2012digital; @butler2005electronic].

> "Since at least the 1990s, articles on technology have predicted the imminent, 
widespread adoption of electronic laboratory notebooks (ELNs) by researchers. It has 
yet to happen---but more and more scientists are taking the plunge." [@kwok2018lab]

Some academics are using more generalized electronic alternatives, like Dropbox, 
Google applications, OneNote, and Evernote [@perkel2011coding; @kwok2018lab; 
@giles2012digital; @powell2012lab].

> "Some groups have ditched notebooks in favour of software from Google, 
such as free-to-use tools for sharing documents, spreadsheets and calendars."
[@giles2012digital]


> "One reason for GitHub's success is that it offers more than a simple source
code hosting service. It provides developers and researchers with a dynamic
and collaborative environment, often referred to as a social coding platform, 
that supports peer review, commenting, and discussion. A diverse range of efforts, 
ranging from individual to large bioinformatics projects, laboratory repositories, 
as well as global collaborations, have found GitHub to be a productive place
to share code and ideas and collaborate." [@perez2016ten]

There are a number of advantages to moving to an online replacement for paper
lab notebooks. In addition to an increased efficiency in recording data, electronic
alternatives can also be easier to search, allowing for deeper and more 
comprehensive investigations of the data collected across multiple experiments
[@giles2012digital; @butler2005electronic].

> "Handwritten lab notebooks are usually chaotic and always unsearchable." 
[@perkel2011coding]

> "But the biggest advantage of going electronic should appeal to both industry and
academia alike. With paper records, vast amounts of data lie unused and 
effectively hidden, whereas e-notebooks allow results to be instantly shared with
collaborators." [@butler2005electronic]

> "Digital notebooks may also help researchers to probe correlations that are too
time-consuming to pursue using paper-based methods." [@giles2012digital]

Large funding agencies often now require a plan for managing and sharing data. ...
For software development projects, they may ask for plans on how to maintain
access to products if the main package is "orphaned". 

Some scientists have started using git and GitHub as a way to collaborate on 
writing scientific manuscripts [@perez2016ten] and grant proposals [examples].

The combination of git and GitHub can also help as a way to backup study data
and code [@blischak2016quick; @perez2016ten; @perkel2018git]. Together, git and
GitHub provide a structure where the project directory (repository) is copied on
multiple computers, both the users' laptop or desktop computers and on a remote
server hosted by GitHub or a similar organization. This set-up makes it easy to
bring all the project files onto a new computer---all you have to do is clone
the project repository. It also ensures that there are copies of the full
project directory, including all its files, in multiple places.

> "Pushing to GitHub also has the added benefit of backing up your code in 
case anythign were to happen to your computer. Also, it can be used to manually
transfer your code across multiple machines, similar to a service like Dropbox
but with the added capabilities and control of Git." [@blischak2016quick]

> "Due to its distributed design, each up-to-date local Git repository is an
entire exact historical copy of everything that was committed--file changes,
commit message logs, etc. These copies act as independent backups as well,
present on each user's storage device." [@perez2016ten]



### Tracking experimental data with version control tools

Some version control tools, including git, can be used to track changes in 
binary files (see Chapter [x] for a discussion of binary versus plain text
files). However, git does not take to these types of files naturally. In particular,
git typically will not be able to show users a useful comparison of the differences
between two versions of a binary file. By comparison, git excels in tracking
changes made to plain text files. For these files, whether they record code, 
data, or text, git can show line-by-line differences between two versions of the
file. This makes it very easy to go through the history of "commits" to a plain
text file in a git-tracked repository and see what change was made at each 
time point, and then read through the commit messages associated with those commits
to see why a change was made. For example, if a value was entered in the wrong
row of a csv, and the researcher then made a commit to correct that data entry 
mistake, the researcher could explain the problem and its resolution in the 
commit message for that change. 

> "You can version control any file that you put in a Git repository, whether it is
text-based, an image, or a giant data file. However, just because you *can* version
control something, does not mean that you *should*. Git works best for plain, 
text-based documents such as your scripts or your manuscript if written in LaTeX
or Markdown. This is because for text files, Git saves the entire file only the
first time you commit it and then saves just your changes with each commit.
This takes up very little space, and Git has the capability to compare between
versions (using `git diff`). You can commit a non-text file, but a full copy
of the file will be saved in each commit that modifies it. ... Things *not* to 
version control are large data files that never change, binary files (including
Word and Excel documents), and the output of your code." [@blischak2016quick]

> "One practical consideration when using GitHub, for example, is dealing with
large binary files. Binary files such as images, videos, executable files, or 
many raw data used in bioinformatics, are stored as a single large entity in Git.
As a result, every change, even if minimal, leads to a complete new copy of the 
file in the repository, producing large size increments and the inability to search
and compare file content across revisions." [@perez2016ten]

> "Many biological applications require handling large data files. While Git is 
best suited for collaboratively writing small text files, nonetheless, 
collaboratively working on projects in the biological sciences necessitates
managing this data. The example analysis pipeline in this tutorial starts by 
downloading data files in BAM format that contain the alignments of short reads
from a ChIP-seq experiment to the human genome. Since these large, binary files are 
not going to change, there is no reason to version them with Git. Thus, hosting them
on a remote http (as ENCODE has done in this case) or ftp site allows each collaborator
to download it to her machine as needed, e.g., using `wget`, `curl`, or `rsync`."
[@blischak2016quick]

> If you need to use git for really large files that *do* change, you may want to 
consider "solutions for versioning large files within a Git repository without 
actually saving the file with Git, e.g., git-annex or git-fat. Recently, GitHub has
created their own solution for managing large files called Git Large File Storage (LFS).
Instead of committing the entire large file to Git, which quickly becomes unmanageable, it
commits a text pointer. This text pointer refers to a specific file saved on a remote
GitHub server. Thus, when you clone a repository, it only downloads the latest version
of the large file. If you check out an older version of the repository, it automatically
downloads the old version of the large file from the remote server." [@blischak2016quick]

The ability in git to compare differences between two plain text files derives from
a longstanding Unix command line tool called `diff`. 
[diff Unix tool]

> "But something else happened in the year of the AT&T divestiture that would have
more long-term importance for Unix. A programmer/linguist named Larry Wall quietly
invented the *patch*(1) utility. The *patch* program, a simple tool that applies 
changebars generated by *diff*(1) to a base file, meant that Unix developers could
cooperate by passing around patch sets---incremental changes to code---rather than
entire code files. This was important not only because patches are less bulky than 
full files, but because patches would often apply cleanly even if much of the
base file had changed since the patch-sender fetched his copy. With this tool, 
streams of development on a sommon source-code base could diverge, run in parallel, 
and re-converge. The *patch* program did more than any other single tool to 
enable collaborative devvelopment over the Internet---a method that would revitalize
Unix after 1990." [@raymond2003art]

> "This is an often-overlooked strength of the Unix tradition. Many of its most
effective tools are thin wrappers around a direct translation of some single
powerful algorithm. Perhaps the clearest example of this is *diff*(1), the Unix
tool for reporting differences between related files. This tool and its dual, 
*patch*(1), have become central to the network-distributed development style of 
modern Unix. A valuable property of diff is that it seldom surprises anyone. 
It doesn't have special cases or painful edge conditions, because it uses a 
simple, mathematically sound method of sequence comparison. This has consequences: 
'By virtue of a mathematical model and a solid algorithm, Unix diff contrasts
markedly with it imitators. First, the central engine is solid, small, and has 
never needed one line of maintenance. Second, the results are clear and consistent, 
unmarred by surprises where heuristics fail.' -Doug McIlroy. Thus, people who
use diff can develop an intuitive feel for what it will do in any given situation 
without necessarily understanding the central algorithm perfectly." [@raymond2003art]

> The diff program is "so bug-free that [its] correct functioning is taken utterly 
for granted, and compact enough to fit easily in a programmer's hand. Only a part of 
these good qualities are due to the polishing that comes with a long service life and
frequent use; most of it is that, having been constructed around a strong and 
provably correct algorithmic core, they never needed much polishing in the first place."
[@raymond2003art]

> Advice on best practices when working with open-source developers: "If your change includes
a new file that doesn't exist in the code, then of course you have to send the whole file.
But if you're modifying an already-existing file, don't send the whole file. Send a 
diff instead; specifically, send the output of the *diff*(1) command run to compare the 
baseline distributed version against your modified version." [@raymond2003art]

> "The *diff*(1) command and its dual, *patch*(1), are the most basic tools of open-source
development. Diffs are better than whole files because the developer you're sending
a patch to may have changed the baseline version since you got your copy. By sending him
a diff you save him the effort of separating your changes from him; you show 
respect for his time." [@raymond2003art]

Platforms for using git often include nice tools for visualizing differences between 
two files. For example, GitHub automatically shows these using colors to highlight
addititions and substractions of plain text for one file compared to another version
of it when you look through a repository's commit history. Similarly, RStudio provides
a new "Commit" window that can be used to compare differences between the original
and revised version of plain text files at a particular stage in the commit history. 

In a project that uses git and GitHub version control tools, it is easy to share
the final data and code online once an associated manuscript is published, an
increasingly common request or requirement from journals and funding agencies
[@blischak2016quick]. Sharing code and data allows a more complete assessment of
the research by reviewers and readers and makes it easier for other researchers
to build off the published results in their own work, extending and adapting the
code to explore their own datasets or ask their own research questions
[@perez2016ten]. In GitHub, a repository's setting can be changed from "Private"
to "Public", allowing anyone to explore the files in the repository. Because git
tracks the full history of changes to these documents, it includes functionality
that let's you tag the code and data at a specific point (for example, the date
when a paper was submitted) so that viewers can look at that specific "version"
of the repository files, even while the project team continues to move forward
in improving files in the directory. At the more advanced end of functionality,
there are even ways to assign a DOI to a specific version of a GitHub repository
[@perez2016ten].

> "To enable and ensure the replicabilty and tracability of scientific claims, it
is essential that the scientific publication, the corresponding datasets, and the
data analysis are made publicly available." [@perez2016ten]

> "[For many journals] Sharing data, code, and other materials is quickly moving
from 'desired' to 'required'. ... Granting agencies, philanthropic foundations,
and other major sponsors of scientific research are all moving in the same
direction, and, to our knowledge, none has relaxed or reduced sharing
requirements in the last decade." [@blischak2016quick]



As with other tools and techniques described in this book, there is an
investment required to learn how to use git and GitHub. However, both can bring
dramatic gains to efficiency, transparency, and organization of research
projects, even if you only use a small subset of its basic functionality
[@perez2016ten]. In Chapter [x] we provide guidance on getting started with
using git and Github to track a scientific research project.

> "Although Git has a complex set of commands and can be used for rather complex 
operations, learning to apply the basics requires only a handful of new concepts
and commands and will provide a solid ground to efficiently track code and related
content for research projects." [@perez2016ten]

> "VCSs have their problems. The biggest one is that using a VCS involves extra
steps every time you want to edit a file, steps that developers in a hurry tend 
to want to skip if they have to be done by hand." [@raymond2003art]


### Subsection 1


"Or maybe your goal is that your data is *usable* in a wide range of
applications? If so, consider adopting standard formats and metadata 
standards early on. At the very least, keep track of versions of data
and code, with associated dates." [@goodman2014ten]


**Email attachments in lieu of common access files.**

...

For example, one group of researchers investigated a large collection of emails
from Enron [@hermans2015enron]. They found that passing Excel files through
email attachements was a common practice, and that messages within emails
suggested that spreadsheets were stored locally, rather than in a location that
was accessible to all team members [@hermans2015enron], which meant that team
members might often be working on different versions of the same spreadsheet
file. They note that "the practice of emailing spreadsheets is known to result in 
serious problems in terms of accountability and errors, as people do not have
access to the latest version of a spreadsheet, but need to be updated of changes 
via email." [@hermans2015enron]

"Team members regularly pass data files back and forth by hand, by email, and by
using shared lab or project servers, websites, and databases."
[@edwards2011science]

**Version control for spreadsheets**

"Recent versions of spreadsheets now incorporate a 'Traack Changes' functionality
which enables highlighting of changes made by different users along with a comment
and review system. Such tools are a start toward this but more robust version control
systems are required particularly in the context of increasingly online and 
collaborative method of working where large teams interact with a single document
concurrently." [@birch2018future]

### Subsection 2

### Discussion questions

