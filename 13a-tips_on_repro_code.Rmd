## Tips for improving reproducibility when writing R scripts {#module13a}

This module is meant for researchers who are using R already as part of their
research. It is meant as a complement and alternative to the previous script, 
which focused on readers who are new to creating code scripts. 

**Objectives.** After this module, the trainee will be able to:

- Improve the reproducibility of their scripts by leveraging tips meant for
researchers who are already incorporating scripts in their research

### Tips and guidelines for writing code scripts

[Possibly move this paragraph to previous module] 
Learning to code can seem daunting, but it's not any more difficult than
learning any new language. Many people from a variety of disciplines have
learned to code to help with their research. Doing so can pay big dividends in
terms of reproducibility and efficiency.

[Possibly move this paragraph to previous module]
In this section, we'll provide some tips to make it easier as you get started.
If you are new to coding, these can give you a framework for how to tackle what
can seem the daunting task of learning to code, as well as help you see that
there are approachable techniques. If you already know how to code, these tips
and guidelines can help in improving that code and give you some new directions
in how to code efficiently and reproducibly.

**Write code for computers, rewrite it for humans**

Even when code for a project is available, it can be hard to understand and 
reproduce the analysis. One common culprit is that the code isn't written for
humans to read. One way to improve the reproducibility of your code, therefore, 
is to make sure you edit it so that it's clear for humans, not just computers. 

During World War I and World War II, the British and US used a special type of
camouflage on some of their ships called "dazzle camouflage". This type of 
camouflage uses large geometric shapes, often in black and white, and it makes
the ships look a bit like zebras. Unlike other types of camouflage, this type
doesn't conceal the ship---it's still very clear that it's there. However, to 
be able to hit a ship at sea, people needed to know not only where it was, but
also where it was going. This is because the ship will move some between the 
time that a ballistic is fired and when it lands, and so the people needed to 
calibrate for that movement and aim at where the ship would be by the time 
the ballistic got to it. Dazzle camouflage makes it much harder to make out 
where the ship is headed. 

Often, people will write code for research projects that looks like it's using
dazzle camouflage. It's easy to see that there's something there when you look
at the code script, but it's very hard to figure out what it's doing or where
it's trying to go. This type of code will be hard for others
to figure out, and also it will be hard for you to figure out when you come 
back to the code in the future. 

The best way to avoid this type of code is to get into the practice of editing
your code. When you first write code, you don't want to write it slowly and
carefully---rather, you'll usually be best at figuring out how to get something
to work if you get in the flow and get down some code without worrying about 
how clear it is. This is fine, but get in the habit of thinking of this as 
just the first step: in your initial coding, you're getting the code to work 
for the computer, but later you will need to go back and clean up the code so
it's clear for humans, too. Editing the code will make it easier to understand
(both by others and by you) and will also make the code easier to maintain and
extend in the future. 

This idea is similar to writing. Many writing experts recommend that you consider
your writing process as having several stages. First, you want to write in a 
drafting process, where you get your ideas on paper but without editing yourself
much. This is a stage of getting the ideas out. In a separate stage, you can 
edit, and at this stage you want to have your audience clearly in mind, editing 
to make the writing clear for them. By separating this, you can use your mind
in a more creative, less constrained way as you create ideas, and then in a 
more critical way as you refine those ideas for your audience. 

While this practice is familiar to many writers, it's less well known as a
process that benefits coders, especially those who are not professional
programmers but rather who code as a small part of their larger work, like
researchers. If you don't already, try adding in editing stages as you develop
your code. It is most helpful to take time to edit code if you're still within a
day or two of writing it, so it's helpful to work in this editing stage fairly
frequently. Since it often requires less energy and brain power than the initial 
stage (getting the code to work with the computer), it can be helpful to 
incorporate editing time at times in your day when your energy is otherwise low. 
For example, taking ten or fifteen minutes to edit existing code can be a good
way to start coding for the day, before you get to the heavier lifting of 
writing new code. 

As you edit your code, there are a few specific things that you can do to make 
it clearer for humans to read. Some editing steps that we will cover in this 
module are: 

- Improve the names you're using within the code
- Break up monolithic code
- Add useful comments
- Remove dead-end code

Let's take a closer look at how you can do each of these steps. 

One important step when editing your code is to use better names for things like
data objects and columns within dataframes. When you're coding quickly, you 
might often use "placeholder" types of names for objects. For example, a coder 
might tend to name objects "df" (for "dataframe") or "ex" as they're first 
getting the code to work. 

There's no problem in using these types of generic names as you initially 
develop your code. In fact, there's a rich history of these placeholder 
object names, and they even have a fancy name, *metasyntactic variables*. 
Different coding languages have developed different ones that are popular, 
as have coders in different countries. For example, many C programmers will 
name things "foo" and "bar" as they initially work on their code, while 
Italians often use the Italian words for different Disney characters
("pippo", "pluto", "paperino"). 

The problem isn't in using these placeholder names; the problem is when you 
don't later edit your code to use better names. These generic types of names
will tell you nothing about what's stored in each object when you go back 
and read the code later. With better names for each object, you can read through
the code and in some ways it will document itself, without even needing to 
read the code comments to figure out what's going on. 

There is a style guide that is focused on the tidyverse approach available
at https://style.tidyverse.org/syntax.html. It includes guidance on how to 
select good names for objects in R within its section on "Syntax". Generally, 
some good principles include that the name of the object should give you an
idea of what's contained in the object. For example, if you have a dataframe
that has the weights of mice from your experiment, it's much better to name
it "mouse_weights" rather than something generic like "foo". Some of the 
other guidance will help make your life easier as a coder, including things
like using only lowercase letters. 

You can also edit the names of columns within dataframes, which can help improve
the clarity of your code, especially since code will often reference specific
columns by name. Similar principles apply to column names: ideally, you want
their names to describe what they contain. There are also some rules that will
make it easier to work with the column names. For example, column names can
include spaces, but if they do, it makes using them within R harder. Each time
you refer to that column name, you have to surround the name in backticks so R
will process its full name as a single name, rather than thinking its name ends
at the first space. This becomes a pain as you write a lot of code that refers
to that column. It's also helpful to keep column names fairly short, so you can
see the full name as you work with the dataset and resulting output.

When it comes to column names, some of your editing might be to improve on names
that you quickly wrote yourself as you coded. However, a common reason for 
ungainly column names is that you've read in data from a file format like Excel, 
where it was easy for the person who entered the data to include spaces and 
special characters in the column names. In this case, there are some tools in 
R that can help you quickly improve the column names. In particular, the 
`janitor` package has a function called `clean_names` that will do a lot of the
work for you, including converting the name to lowercase, removing special 
characters (like "*" and "&"), and replacing spaces with underscores. If you 
need to make more targeted changes to column names, you can do so using the 
`rename` function from the `dplyr` package. 

The next thing you can do to edit your code is to break up "monolithic" code---
that is, code that isn't clearly divided to show sections or steps in the 
process. When you're first creating your code, you won't want to take the time
to nicely organize it into logical sections. However, once you are ready to 
edit your code, you will find that breaking into clear sections and labeling 
them will help you and others navigate the code first at a higher level 
(understanding the big picture of how it works by looking at the major
steps it takes) and only diving into the details of each section once the 
big picture is clear. 

Again, this process mimics the process used by many writers. It's common to 
create drafts and notes that lack clear organization, but instead are just 
collecting the raw material that will be shaped into a final article or book. 
However, this raw material then needs to be organized and edited to make it 
into something that others can navigate and make sense of. The writer Robert
Caro famously wrote massive books about political figures like Robert Moses and
Lyndon Johnson. To wrangle all his research into books, he noted that, 
"I can't start writing until I've thought it through and can see it whole 
in my mind."

In a similar way, once you've gotten code to work, you should make sure you have
a clear picture of the whole process and how it tackles the problem at a 
"big picture" level. One of these steps might be something like reading in 
and cleaning the data, while another step might be identifying and addressing
outliers in the data. Once you've identified the big steps, try as much as
possible to group the code into these big steps, then you can use code comments
and blank lines in your code to separate these sections and label them to 
describe what they're doing. 

As you do this, you might find that you move some of your code around in the 
script, which is fine as long as it doesn't affect the computer being able to 
process the script. For example, one of your big steps might be loading in 
packages you'll need. Rather than having a lot of `library` calls sprinkled
throughout your code, you can group these all together at the start of your
script in a section called something like "Loading packages". This will 
clean up these calls from other parts of your script; also, by having all 
your library calls at the start of the code script, another person could 
immediately see which packages they'll need to have installed to run your code.

Another way that you can break up monolithic code is to split it into more lines. 
R will process the code whether it's all on one line or split into separate 
lines: R just keeps reading until it gets to the end of the function call
either way. This means that you can use the "Return" key to break up your code
lines so you're always able to see the full line of code without scrolling. 

One common standard is to keep all of your lines of code to 80 characters or 
fewer. RStudio has the functionality to reformat your code to meet this 
standard. In the RStudio menus, if you go to the "Code" menu, you can select
to "Reformat Code". This can help clean up long lines of code in your editing 
process. 

As you are breaking up monolithic code, it can also be helpful to use 
code comments to add notes about why you are doing certain things. In R, you 
can add a code comment anytime after a `#` on a line; R won't read anything 
that comes after that symbol on a line. You can use this to add small messages
for humans that describes your code. 

As you add these comments, keep in mind that it's often more useful to 
describe why you're doing something rather than what you're doing. With a lot 
of R code---especially in the tidyverse approach---the functions often have
names that clearly describe what they do. For example, the function that 
you use to rename a column is called `rename`, while the function that you 
use to select certain columns is called `select`. Therefore, your code should 
do a fairly good job of self-documenting in terms of describing what it's 
doing. 

Instead, you can use code comments to remind yourself or others of 
why you're implementing certain steps. For example, rather than having a code
comment that says "Rename columns", you could say, "The columns that come
from the Excel file generated by the cytometer include a lot of special 
characters, which we need to remove to make it easier to work with the data in 
R." By explaining why you're doing something, you'll also help yourself when it's
time to maintain or extend your code. You'll be able to tell, for example, 
whether changing or deleting a certain line of code will cause a big problem 
in other areas of code. 

Another useful step when you edit your code is to edit out pieces we'll call
"dead-end code". These are pieces of code that aren't contributing to the 
process of your script. 

There are two main types of this dead-end code that we often see. First, there's
code that you use during your interactive coding process to check on things. For
example, you might use the `View` function to take a look at a data frame at a
certain step in your process, or use functions like `summary` and `str` to
explore what's in different objects.

It's great to do this kind of exploration as you code; in fact, one of the 
advantages of interactive software like R is that you can explore as you 
develop your scripts. However, these are tools that help you develop a script, 
but not ones that are necessary for the final script to run. Instead, they just
gunk up the code that's doing the real work. 

There are two things you can do regarding this type of dead-end code. The first
is that you can get in the habit of running it in your console, rather than 
having it in your script, even when you're developing the code. However, this 
does require switching between the console and the script as you write code, 
which can interrupt your flow. An alternative is to run these in a script as 
you write the code, but then delete any of these exploratory calls as you 
edit your script. 

There's also a second type of dead-end code. This is code that you were trying 
out to solve a particular problem, but that ultimately didn't work (or that 
was superseded by a better approach). Often, you may have worked a long time 
on that piece of code, or it might contain some really clever approach that 
you're proud of. However, leaving it your script, if it isn't contributing to 
the ultimate process you ended up with, will only get it the way of understanding
your primary code. It will lead a reader down a rabbit hole, rather than allowing
them to move step by step through your logic. 

In writing, there are similarly areas that aren't contributing to the forward
movement of a piece but that authors are reluctant to remove because they love
them for one reason or another. This has resulted in the famous advice to 
authors (from Stephen King, among others) to "murder your darlings". In other 
words, be brave enough to edit out anything that isn't contributing to the 
necessary progress of your piece. Coders should take this advice in a similar 
way when it comes to pieces of code in their scripts that don't ultimately 
contribute to the pipeline they've developed. 

**Modify rather than start from scratch**

[Anecdote: reverse engineering? Hardware Hacker?]

[*Everyone* does this, it's part of the open-source aesthetic. Also, long 
history in science. See anecdotes / quotes about adapting.]

[There are many places that you can find example code to start from: 
vignettes, helpfiles, StackOverflow, code included with papers (although 
be mindful of attribution / permissions), cheatsheets, even ChatGPT.]

As you code, keep in mind that you shouldn't reinvent the wheel. There are 
excellent resources available that can help provide you with a starting 
point for many of the coding tasks you'll need to do. For example, most
packages have tutorials called vignettes that provide a starting point
in how to use the package, and helpfiles often include short example code
that you can adapt to your own purposes. Online resources like StackOverflow
also provide advice and example code for many challenges you might come up 
against as you're coding. Google can also be used to help you solve coding 
problems, especially if you become familiar with some of its special 
operators, which can help you refine your search (see https://support.google.com/websearch/answer/2466433?hl=en for more on this). 

There's no problem with using any of these as starting points as you develop
your own code. However, it's often tempting for a coder to leave some lines
of code "as-is" if they've found an example solution that works. Instead, it's 
critical that you make sure you fully understand why each line of code in your
script works the way it does. Further, if you're adapting example code to 
your own problem, you should edit it if possible to use the set of tools you're
most familar with. 

When you find a piece of example code that you think will help with something
you need to do in your own code, you'll first want to make sure that you can 
get it to work with any example data it came with, before you try it out with 
your own data. If it won't run with its own data, there are a few trouble-shooting
steps you can take. First, make sure you have all the required packages 
installed and loaded. Second, make sure that you've saved the example data
to the right place on your computer if the example code reads in data from 
a file. Finally, make sure that you have the same versions of 
packages and of R. If the code still doesn't work after you've resolved these
issues, you may want to move on to finding other example code. 

Once you've gotten the code to run on the example data, walk through it line
by line to understand what it does. For each step, make sure you understand 
what the input looks like and what the output looks like. If code is nested
(function calls are placed within function calls), be sure that you understand
the code at each level of nesting. If the code uses piping to move the output
of one call to the input of the next, make sure you've worked through each 
of the lines in the pipe individually. 

There are two tools that can help as you dissect the code in this way. First,
when you work in R study, you can highlight code in a script and then use the
"Run" button to run only that code. This functionality allows you to run a
nested function call without running the whole line of code, or to run only part
of a series of piped calls (by highlighting everything up to the piping symbol
on a line and then running it). The other tool that's useful is a function in
the `dplyr` package called `pull`. This function allows you to extract a column
from a dataframe as a vector. This is helpful when you're dissecting nested
calls in piped code, as often a function will operate on a single column of the
dataframe. This function allows you to pull out that column and then test the
function call to see what it's doing with that column.

Once you figure out what the example code does on the data it comes with, you 
can adapt it to work with your own data. As you do, pay close attention to 
how your data are similar or different to the example data. At this stage, your
goal will be to get the example code to work with your data. 

Many researchers stop at this step---they've gotten example code to work with 
their own data (and hopefully worked through it to understand why). However, 
example code often follows a very different style than code you write yourself. 
For example, you may use the tidyverse approach, while the example might use
code written in a base R style. Further, different coders think about and 
tackle problems in different ways, which can lead to the case where any 
example code that you've adapted in your script feels different from your usual
code. 

This can result in spots of your code that you will later be very worried to 
change, because while it works, you don't understand why well enough to feel 
comfortable making any change. This can make your code fragile and hard to 
maintain. Instead, take a moment to adapt the logic that you've learned from 
the example to use your own set of tools. For example, if the example code is 
written in base R but you prefer tidyverse tools, rewrite the code's logic 
to use tidyverse tools. 

You gain several advantages when you adapt code to use the tools you're familiar
with. For example, bugs will be less likely, and if there are bugs, you'll 
catch them more quickly, since you're familiar with the tools that the code is
using. The code will also be much easier for you to understand and maintain in 
the future. 

> "The sandbox of the budding builder is not *making* as much as it is 
modification: taking something that exists and making it better, either 
functionally or aesthetically or both. Often that involves attaching and 
securing parts that were not originally intended to go together." [@savage2020every]

As you start learning to write code in R, don't force yourself to stare at an 
empty R script file and try to come up with a full script from scratch. One of the 
best ways to learn R is to find some scripts that others have written for tasks 
that are similar to the ones that you want to do, then work through those to figure
out each function call, and how those function calls add up to the full pipeline. 

This method of reverse engineering is useful in many areas when you're trying to 
figure out how things work. ...

Once you understand a few other R scripts, you can start trying to modify them and 
to pull pieces from different scripts to use as building blocks as you put together
your own script. There's no need to reinvent the wheel---if someone else has 
shared an R script that comes close to doing what you need to do, start there and
then change and evolve that idea to suit your own needs. 

To find some starting scripts to learn from, there are a few tactics you can try. 
First, check around with colleagues to see if they have R code for data preprocessing
tasks that they do in their lab. If they work with similar types of data, and use R, 
they're likely to have come up with some scripts that achieve tasks you also need to 
do. 

Another excellent source of example R code are the vignettes and examples that come 
with many R packages. If you are using functions from an R package, then there is likely 
a vignette that comes with that package, and there may also be examples within the 
helpfiles for each of the package's functions. A package vignette is a tutorial that
walks you through the major functionality of the package, showing how to use the 
key functions in the package in an extended example. Some packages will have multiple
vignettes, showing a range of things that you can do with the package. 

To find out if there is a vignette for a package that you're using, you can google the 
package name and "vignette". You can also find out from the console in R using the 
function `vignette`. For example, to find out if the package `readxl`, which helps read in 
data from Excel files, has any vignettes, you can run `vignette(package = "readxl")`.
This will tell you that the package has two, one called "cell-and-column-types" and 
one called "sheet-geometry". To open one of these, you can again use the `vignette` 
function. For example, `vignette("cell-and-column-types", package = "readxl")` would 
open the first of the two vignettes within your R session.

To open the helpfile for any function in R, at the console type a question mark and then 
the function name. For example, `?read_excel` will open the helpfile for the `read_excel`
function (you will need to make sure you've run `library("readxl")` to load the package
with this function). The helpfile provides useful information for running the function, 
and one of the most useful parts is the "Examples" section. Scroll down to the bottom 
of the helpfile to find this section. It includes several examples that you can copy into 
your R script or console and try yourself, to figure out the types of inputs that the
function needs and how different options for the function modify how it works. 

[How to dissect code: reverse engineering. Steps: (1) run with example 
data; (2) understand required input and expected output; (3) for nested code, 
word inside out; (4) for piped code, one line at a time]

[How to adapt code for tools you don't know: adopt *idea* to tools you do know; 
learn any tools as new tools for cases that can't be adapted.]

> "All creative work builds on what has gone before. When someone declares
that something is original, it's because they are unaware of the influences. 
The creative make the most of things they admire and aren't ashamed to 
be inspired by something they respect. The bad news: everything has already
been done. The good news: it can be done again." [@judkins2016art]

[Anecdote: adapting]

> "Extracting penicillin from the mold was no child's play... Instead of 
designing and building a reactor for the chemical reactions from scratch---which
meant more time, money, and uncertainty---[Margaret] Hutchinson opted for something
that was already functional. Some researchers had found that mold from 
cantaloupe could be an effective source for penicillin, so she started 
there. Her team then revised a fermentation process that Pfizer was using to 
produce food additives like citric acid and gluconic acid from sugars, with 
the help of microbes. Hutchinson swiftly helped convert a run-down 
Brooklyn ice factory into a production facility. The deep-tank fermentation
process produced great quantities of mold by mixing sugar, salt, milk, minerals, 
and fodder through a chemical separation process that Hutchinson knew very 
well from the refinery business." [@madhavan2015applied]

[Anecdote: adapting]

> "Johannes Gutenberg invented his printing press by repurposing a wine 
press for use with olive oil--based ink and block printing." [@madhavan2015applied]

However, it is critical that you work through and understand any example code
that you bring in and modify in your own workflow.

> "Appropriate methods are 'very data-set dependent'... The methods and tuning
parameters may need to be adjusted to account for variable such as sequencing
length. But John Marioni at Cancer Research UK in Cambridge says it's important not 
to put complete faith in the pipeline. 'Just because the satellite navigation
tells you to drive into the river, you don't drive into the river,' he says." 
[@perkel2017single]

> "Do not reinvent the wheel. It pays to reuse existing software. Integrative
frameworks and associated application stores already house hundreds of tools
(for example, as of May 2012, Galaxy ToolShed contains ~ 1,700 tools). It is
likely that a script for a particular problem has been already written. Ask
around through existing resources such as SEQanswers43 and BioStar44."
[@nekrutenko2012next]

> "If you're going to build a house today, you don't start by cutting down trees to make 
lumber and digging clay to make your own bricks. Instead, you buy prefabricated pieces like
doors, windows, plumbing fixtures, a furnace, and a water heater. House construction is still
a big job, but it's manageable because you can build on the work of many others and rely 
on an infrastructure, indeed an entire industry, that will help. The same is true of 
programming. Hardly any significant program is created from nothing. Many components written
by others can be taken off the shelf and used. For instance, if you're writing a program for
Windows or a Mac, there are libraries of prefabricated menus, buttons, text editors, graphics, 
network connections, database access, and so on. Much of the job is understanding the components
and gluing them together in your own way. Of course, many of these components in turn rest on
other simpler and more basic ones, often for several layers. Below that, everything runs on 
the operating system, a program that manages the hardware and controls everything that happens."
[@kernighan2011d]

> "There is also a problem with discovering software that exists; often people
reinvent the wheel just because they don’t know any better. Good repositories
for software and best practice workflows, especially if citable, would be a
start." --- James Taylor in [@altschul2013anatomy]

**Modular, not monolithic**

[Combine this section with "iterate"?]

[Modular is: easier to understand, easier to maintain, easier to develop
(big steps, then refine)]

Another key thing to remember as you develop R scripts is that they are all,
even those that do very complex things, made up of lots of smaller, simpler
pieces. R scripts are, in other words, modular, not monolithic.

As you develop a script, plan to create it by chaining together simpler steps. You can 
start by trying to map out the key things that you'll need to do in the pipeline, and
then you can dive into each of those sections and start writing the code to 
achieve that step. Think of this process as like outlining a paper before you 
write it. For example, if you are preprocessing data that measured bacterial 
growth rates, some of the broad sections you may have in your pipeline are: 
(1) reading in the data, (2) converting it into a tidy data format so it's easier to work 
with, (3) determining the time range in the experiment when each sample was in its
exponential growth phase, (4) using data in that time range to estimate the growth
rate for each sample, and so on. Start by dividing your script into these sections
(you can mark each off with a code comment---how to use these are described 
later in this section), and then you can start to develop the code, starting with 
the code to input the data. 

This method---which breaks a big problem into smaller ones---is a useful
problem-solving approach across many fields. Madhavan, in a 2015 book called
*Applied Minds: How Engineers Think*, labels this approach "modular systems
thinking". He notes that this approach, "includes a functional blend of
deconstruction (breaking down a larger system into its modules) and
reconstructionism (putting those modules back together)" [@madhavan2015applied].
He even calls this type of modular systems thinking, which he notes is a species
of "divide and conquer", "the core of the engineering mind-set."
[@madhavan2015applied]

He describes, in fact, how the Wright brothers used this approach to invent 
the airplane: 

> "Wilbur and Orville Wright took about four years to implement their first
prototype of a flying machine. While their competitors focused on the design of
winds, fuselage, and propulsion, the Wright brothers were devoted to getting the
fundamental functions of lift, thrust, drag, and yaw correct. Consistent with
the notion of modular thinking, they solved each puzzle at a subsystems level
before they moved to the next layer of the assembly, and along the way they
invented new instruments and measurement techniques." [@madhavan2015applied]

Thinking of a code script as something that is modular is critical as you start 
learning how to code in a language like R. By dividing the full pipeline into smaller
steps, you can tackle it one piece at a time. You can look for resources that are 
linked to a specific part of the problem, rather than feeling like you need to 
find other examples of full, start-to-finish pipelines that have done the same thing
that you're trying to do. For example, the step of reading in data that you recorded
in an Excel spreadsheet comes down to a simple task---reading data into R from an 
Excel file. It doesn't matter whether that file has data on bacterial growth rates
or on mice weights or on anything else---the process of reading it into R is the 
same. Therefore, when you're working on this step, you can google help on reading 
Excel files into R if you're stuck, without worrying about whether the resources
that you find to help used examples of the type of data you have or other types of 
data. 

> "Very little of what I've built over the years is monolithic---just a single chunk.
Most of the time, I build things in components, then attach those pieces together 
as I go. So yes, the component parts are pieces that have been made small in 
precise ways from larger chunks of material, but eventually they will be assembled
to create much larger and more complex objects than any of the raw source materials."
[@savage2020every]

**Iterate!**

Code is something that you develop over multiple drafts---it's the kind of thing
that you build by trying out one thing, seeing how it works, then revising, reworking,
and extending to get to the pipeline you ultimately want. You will quickly feel
frustrated if you try to build your pipelines in a single go, without this process
of iteration. Just as many writers suffer writer's block more often when they 
try to get everything perfect in a first draft, you will feel frustrated if you 
think that you should build your code to its final form, from start to 
finish, in just one try. As Madhavan notes, 

> "Tweaking and prototyping are basic human habits." [@madhavan2015applied]

[Examples of prototyping include: thumbnails, storyboards, outlines]

[Prototype and then refine]

When you build a code pipeline, try to break your process up into some key chunks. 
For example, you might know that you'll first need to input the data into R
(first chunk), then preprocess the data by log-transforming one of the measurements
(second chunk), and then run a statistical hypothesis test to compare two groups
that you measured (third chunk). Once you've broken the code into these chunks, 
you can focus on a single chunk at a time and iterate the code for that task until 
you're happy with it. 

For example, if the first chunk is to read in the data, you might first try to 
use a standard tool that you've used before, like the `readr` function `read_csv`, 
which reads in data from a CSV file. However, maybe this tool doesn't work for 
the file format your data are in (perhaps they're in an Excel file, for example), 
and so you get an error when you try to do that. Keep calm when you get that error, 
and just iterate back and look up some tools for reading Excel files into R. 
When you've found some, try adapting the examples from their help files or vignettes
to work in your code pipeline. If that works, then spend some time exploring the
input that you get, so you can start to think about how you'll work with the data
for the next chunk in your pipeline. 

Once you're done with this process of iteration, make sure that you go back and 
clean up the code pipeline, so that the code script is streamlined to walk 
efficiently through the tasks. Also, rerun the whole script in a new R session, 
so you can make sure that it is self-contained. One of the most common 
reasons for a script not working the same later is that, when it was originally 
developed, the coder was relying on some objects that were created in their R
environment at the console, not through code in the script, and so were available
when running the code originally, but not when it's opened in a new R session. 
You can often identify and fix these issues much more quickly by testing your 
code pipeline in a new R session right after you create it, rather than 
waiting and running into that problem days or weeks later, when you try to rerun 
the script.

> "Prototypes create new capabilities. Prototypes foster adaptation to new 
forms, new expections, and new offshoots of technologies. Prototypes are
the starting points toward our ultimate creative destinations." [@madhavan2015applied]



> "Programming in the real world tends to happen on a large scale. The strategy is similar
to what one might use to write a book or undertake any other big project: figure out what
to do, starting with a broad specification that is broken into smaller and smaller pieces, 
then work on the pieces separately, while making sure that they hang together. In programming, 
pieces tend to be of a size such that one person can write the precise computational steps
in some programming language. Ensuring that the pieces written by different programmers
work together is challenging, and failing to get this right is a major source of errors. 
For instance, NASA's Mars Climate Orbiter failed in 1999 because the flight system software
used metric units for thrust, but course correction data was entered in English units, 
causing an erroneous trajectory that brought the Orbiter too close to the planet's 
surface." [@kernighan2011d]

> "Developing code in R is a back-and-forth between writing code in a rerunnable script
and exploring data interactively in the R interpreter. To be reproducible, all steps 
that lead to results you'll use later must be recorded in the R script that accompanies
your analysis and interactive work. While R can save a history of the commands you've
entered in the interpreter during a session (with the command `savehistory()`), 
storing your steps in a well-commented R script makes your life much easier when you 
need to backtrack to understand what you did or change your analysis." [@buffalo2015bioinformatics]

> "People not doing the computational work tend to think that you can write a
program very fast. That, I think, is frankly not true. It takes a lot of time to
implement a prototype. Then it actually takes a lot of time to really make it
better." --- Heng Li in [@altschul2013anatomy]

**Do not repeat yourself**

As you become more familiar with programming with R, you can start to evolve your 
style of writing scripts in more advanced ways. A key one is to learn how to limit
how often you repeat the same code. As you write data preprocessing pipelines, you'll
find that you often need to do the same thing, or variations on the same thing, over
and over. For example, you may need to read in and clean several files of the same 
type and structure. You will likely, at first at least, find yourself copying and 
pasting the same code to several parts of your script, with only minor changes to 
that code (e.g., changing the R object that you input each time). 

Don't worry too much about this as you start to learn how to write R scripts. This
is a normal part of the drafting process. However, as you get better at using R, you'll
want to learn techniques that can help you avoid this repetition. 

There are a few reasons that you'll want to avoid repetition in your code when
possible. First, these repeated copies of the same or similar code will make your
code script much longer and harder to read through later to figure out what you did. 
Second, it is hard to keep these copies of code in sync with each other. For example, if 
you have several copies of the code you use to check for outliers in your data, and you 
decide you want to change how you are doing that, you'll need to find every copy of
the code in your script and make sure you make the same change in each place. Instead, 
if you have less repetition in your code, then you can make the change in a single place
and ensure that the change will be in place everywhere you are doing that process. 

There are a few tools that are useful to develop to help avoid repetition. The first
is to learn how to write your own R functions. Any R user can write a new function, and
you can write these for your personal use, in addition to writing them in packages you
plan to share with others. When you wrap a function, it encapsulates the code for something
that you need to do, and it allows you to do that thing anywhere else in your code 
just by calling that new function, rather than copying all the lines of the original code.
This is an excellent way to write the code you need to use often in one place, rather than 
copying and pasting the same code throughout your R script. 

Since you need to run the code that defines the function before you use it, it
often makes sense to write any code that creates these functions near the top of
your code script. If you find that you've written a lot of functions, or that
you've written functions that you'd like to use in more than one of your data
preprocessing scripts, you can even save the code that creates the functions in
a separate R script and just source that separate script at the top of each
script that uses the function, using the `source` call (and eventually you could
even think of creating your own package with those functions).

There is one other excellent set of tool for avoiding repetition that we want to mention. 
Again, it is likely more complex that what you'll want to start off with as you 
learn to write R scripts, but once you are comfortable with the basics, it's a 
powerful tool for creating code scripts that are as short and simple as possible 
while doing very powerful things. This set of tools all focus on iteration. They 
include `for` loops, which allow you to step through elements in a data structure 
and apply the same code to each. They also include a set of tools in the `purrr` 
library that allow you to apply the same code, through a function, to each element
in a larger data structure. These are excellent tools when you are doing something 
like reading in a lot of similar files and combining them into a single R object for
preprocessing. 

We will not go into details about how to write R functions or these iteration
tools in these modules, as our aim here is to get you started and give you an
overview of where you might want to go next. If you do want to learn to write
your own R functions, there's a chapter describing the process in the free
online book "R for Data Science" with guidance on this topic
(https://r4ds.had.co.nz/functions.html). If you'd like to learn more about tools
for iteration, the same book also has a chapter on that
(https://r4ds.had.co.nz/iteration.html).

> "Software, like many other things in computing, is organized into layers, analogous to 
geological strata, that separate one concern from another. Layering is one of the important
ideas that help programmers to manage complexity." [@kernighan2011d]

> "At the simplest level, programming languages provide a mechanism called functions that make
it possible for one programmer to write code that performs a useful a useful task, then package
it in a form that other programmers can use in their programs without having to know how it 
works." [@kernighan2011d]

> "**Use functions.** It's better than copy-pasting (or repeatedly source-ing)
stretches of code." [@holmes2018modern]

> **Use the R package system.** Soon you'll note recurring function or variable
definitions that you want to share between your different scripts. It is fine to 
use the R function `source` to manage them initially, but it is never too early 
to move them into your own package---at the latest when you find yourself staring
to write emails or code comments explaining to others (or to yourself) how to use
some functionality. Assembling existing code into an R package is not hard, and it 
offers you many goodies, including standardized ways of composing documentation, 
showing code usage examples, code testing, versioning and provision to others. 
And quite likely you'll soon appreciate the benefits of using namespaces." 
[@holmes2018modern]

Some tools that can help you avoid repetition include: 

- Faceting in `ggplot2`
- The `map` family of functions in the `purrr` package
- The `group_by` and `summarize` / `mutate` workflow in the `dplyr` package

**Scripts are for humans to read, not just computers**

A code script, by definition, is a set of instructions that you send to the 
computer to evaluate. However, many starting programmers make things hard
on themselves by treating the script as a document that is only for the computer. 

While it is true that the code script needs to be written so that the 
computer can evaluate it, that is only the first level of usefulness for the
script. Keep in mind that one of the key advantages of writing a computer
script is so that the code becomes reproducible---you and others can revisit
it, rerun it, and adapt it at a later time. Otherwise, in a programming 
language like R, you could just run all your code at the console, rather than 
saving it in a script. One of the biggest barriers to reproducibility is unclear
code. This can make it harder for others to figure out what the does. It also 
makes it harder for you and others in your team to figure out what you did when 
you look at the code months or years later.

It is helpful to separate the process of developing your code into two steps: 
first creating, and then editing. [Something about writing fast]

To make sure that your code is "legible" by humans, take the time to clean it
up once you have it running. (As an added bonus, it's often the case that you'll
find a few bugs when you do so, and so you can catch and fix those.) Go back 
through the whole script once you have built your pipeline. There are several
specific things you can do during this editing process: [enumerate]. 

[Break up monolithic code; Remove dead-end code; Improve names]

Make sure you add useful comments to your code. In R, any line that starts with
a "#" will be ignored by R, so you can use those to write messages for humans
that the computer will skip. You can use comments in a variety of ways to make
your code clearer to humans. First, you can use them to divide the code script
into sections and to label those sections. This will help a human reader get an
overview of the script when they first look at it and navigate the script as
they continue to work with it. You can also use code comments to explain coding
choices that you've made along the way that might not immediately be clear. For
example, if you needed to use a function that is outside of your core set of
tools, and that you found to tackle a specific problem, you could comment on
that, and perhaps add a link to more information on that function.

It also is good practice to follow style guidelines. You can also clean up the
code substantially by editing it to follow a set of style guidelines for R code.
These are guidelines that specify things that don't matter to the computer, but
that can help human readers. For example, there are many cases where R can
evaluate code the same way whether spaces have been used to separate pieces of
the code or not, but a thoughtful use of spacing can help a human read the code.
There are several style guidelines available for R and other programming
languages, and it is less important which one you use than that you follow one
consistently (at least by the time you edit your code for clarity to humans).
One style guide you can use is the one created by Hadley Wickham available at:
https://style.tidyverse.org/index.html.

Finally, it is useful to use this editing stage to simplify the logic and 
function calls within the script. Often, when you are first trying to get some
code to work, you might come up with a pretty ungainly function call to do 
so. Once you've gotten it working and figured out a general approach, you can 
often go back and rethink how you've coded the approach, and you can often 
come up with a simpler way to perform the same task using less code or 
clearer code. The more you can simplify your code, the lower the risk for 
bugs and the easier it will be for humans to understand. 

> "Filming [of Apocalypse Now] started in March 1976 and was scheduled to
take six weeks. It took sixteen months. The director, Francis Ford Coppola, 
shot around 230 hours of footage, with multiple takes of the same scene. He
was hoping to capture a magical or unusual performance. He encouraged the 
actors to ad-lib, which produced some poetic moments but also hours of unusable
footage. Once filming was finished, the movie had to be cut and recut for 
months. It took Coppola and his film editor Walter Murch nearly three years to 
edith the footage... Why produce a lot of work and then throw away 95 percent?
The two processes seem contradictory. Why not just produce the 5 percent ultimately
used? Because you don't know *which* 5 percent that will be. Editing can 
be hard becuase you're discarding things you have put a lot of energy into
making." [@judkins2016art]

> "You might not write well every day, but you can always edit a bad page. 
You can't edit a blank page." -- Jodi Picoult, cited in [@judkins2016art]

> "That a language is easy for the computer expert does not mean it is 
necessarily easy for the non-expert, and it is likely non-experts who will do the
bulk of the programming (coding, if you wish) in the near future." [@hamming1997art]

> "The code that a programmer writes, whether in assembly language or (much more likely) in 
a high-level language, is called *source code*. ... Source code is readable by other programmers, 
though perhaps with some effort, so it can be studied and adapted, and any innovations or ideas
it contains are visible." [@kernighan2011d]

> "*Document your methods and workflows.* This should include full command lines (copied
and pasted) that are run through the shell that generate data or intermediate results. 
Even if you use the default values in software, be sure to write these values down; 
later versions of the program may use different default values. Scripts naturally 
document all steps and parameters ..., but be sure to document any command-line options
used to run this script. In general, any command that produces results in your work needs
to be documented somewhere." [@buffalo2015bioinformatics]

Later sections will give deeper dives into other topics that might also form part
of the editing process.

**Some things belong in the console, not the script**

[Dead-end code]

Finally, keep in mind that all of the code you write, as you develop a script with 
a pipeline, does *not* need to be recorded in the script. Of course you will want
to include all the code that is necessary for the script as a whole to work on its
own. However, as you develop code, you'll take some steps to explore your data or
to do things like installing packages that you don't have yet. 

For example, as you work on your code, you'll likely want to look at the contents
of your R objects as you work on them. If you have read in your data from a file into
an R object, you'll want to look and make sure it looks like it read in correctly. 
If you have created a new object that summarizes the original data by taking the 
average of each group, you'll probably want to look at that as you develop the code
to create it, again to check and explore as you build the code. To do this, you can 
call the object's name (e.g., type `my_data` and run it) or use functions like 
`head`, which prints out the first few rows or items of the object (e.g., `head(my_data)`), 
`tail`, which prints out the last few rows or items of the object, or `str`, which 
summarizes the structure and contents of the object. 

All of these are useful to run as you draft and edit your code, but calls like this that
print out pieces of the data to check aren't necessary in the final R script (and in 
fact can make it messier than it needs to be and result in a lot of extra print-out at
the console when you run the code as a batch once you've finalized it). As you develop
your code, then, try to get in the habit of not writing these types of exploratory 
function calls in the script you're developing. Instead, write them directly in the 
console and run them from there. 

[Figure---writing exploratory code in the console versus the R script. Same for installing
packages]

The other piece of code that you should run in the console rather than saving in
the R script is code to install new packages. Since you only need to install a
package once (until you get a different computer or update base R), you don't
need to run `install.package` function calls everytime you run the code in a
script. Including these functions in the script will therefore just slow it
down. Instead, go to your console directly to write the function calls to
install new packages (or, if you prefer, in RStudio you can go to Tools on the
menu bar and select Install Packages).

The final goal is to develop an R script that has everything it needs to run the
full pipeline from in a fresh R session. In other words, if it uses functions
from packages, it will include the code to load those packages (`library`
function calls). For every R object that it uses, there will be code that
creates that object in the script. It will not include, however, extra pieces of
code that were used to explore the objects as you built the code. To test that
the R script can run in a fresh R session, you can close R and reopen it (make
sure that you've set your global options in R to never save the workspace to
.Rdata on exit, to not restore the .Rdata into the workspace on exit, and to not
save the history, all of which you can set by going to the RStudio Tools menu
item and selecting Global Options). When you reopen R, there will not be any
objects in the environment, and there will not be a history of any of the
previous function calls that you ran. Try running the code in the script in this
fresh environment and make sure that you weren't relying on anything that you
did outside the script to make the code work. If the code can run in a fresh
environment like this, then any R user should be able to rerun everything in the
script themselves (although they may need to install a few new libraries first
if they don't have all the required libraries yet).

### Discussion questions




------------------------------------------------------------------------------------


### Potential quotes

> "Every maker needs to give themselves the space to screw up in the pursuit of 
perfecting a new skill or in learning something they've never tried before. 
Screwing up IS learning." [@savage2020every]

> "Mistake tolerance is particularly valuable in this aspect of the creative
process. When you know what you want to make, but you're not exactly sure what 
it should look like or how it should operate, you need to give yourself permission
to experiment, to iterate your way there. That's not just how you get to what you 
want, it's how you get good at it. You have to do it over and over and over 
again. Anticipating mistakes is how you put space around the unfamiliar and 
the unknown." [@savage2020every]

> "If you expect to nail it the first go round every time you build something 
new---or worse, you demand it of yourself and you punish yourself when you come 
up short---you will never be happy with what you make and making will never 
make you happy". [@savage2020every]

BioC Conference

> "Just as writing a book involves an outline and a rough draft (so many drafts!),
which get polished into a final manuscript, making things often benefits from a 
preliminary stage where the big details get worked out, and then a final fabrication
stage where the small details get worked out. Cardboard is a low threshold material
that can make discussion of ideas at the perliminary stage so much easier and 
more complete." [@savage2020every]

> "In my professional life, I have worked with every conceivable type of client and 
collaborator, from those who were makers with the same or greater expertise as me, 
who understood deeply what I was talking about when we discussed a build, to clients
who couldn't glue two blocks of wood together if you put the blocks in their hands, 
covered with glue, and told them to clap. Being able to communicate your ideas to 
clients and collaborators is one of the most important skills to possess as a maker, 
otherwise some of your projects may never get off the ground." [@savage2020every]

> "To make anything, it's critical to have a physical understanding of how all the
component parts of your project will fit together." [@savage2020every]

> "What material can you wrap your arms around to gain a complete sense for the skills
you want to master and the objects you want to make? Is it cardboard? Muslin fabric?
Crappy butcher cuts? Scrap wood? The backside of recycled printer paper? A word
processor? It really doesn't matter as long as it allows you to be messy and it
keeps you moving forward in your journey as a maker." [@savage2020every]


> "Once you have a new tool you thing you need, spend some time getting to know it 
physically. With certain tools, I'll go so far as to take them apart, just to 
understand them better, inside and out. ... If you are unfamiliar with a tool or
inexperienced with the techniques required to use it, getting comfortable like this
is the most important thing you can do, because you might really need this thing, but
if you are intimidated by it, you aren't going to want to use it, and then 
what's the point?" [@savage2020every]

> "If you've never used a tool before, reviews and articles about it can only get
you so far. You need to work with a tool in order to see how the tool works for YOU.
You need on-the-ground experience with it in your hands." [@savage2020every]

With open-source, you can think of it as builing a collection of different tools, 
rather than a black box. The tools can come from different companies (like real tools)---you
don't have to limit to consuming the tools created by a single producer (as with 
most proprietary software systems). 

For free and open-source software, you don't have to invest money to get more tools.
Instead, the investment for each new tool is the time that it takes to learn how 
to use it in your workflow. This includes several elements. You'll need to understand
what primary input is used by the function, both in terms of the input's conceptual 
content and the format or structure in which those data are stored. You'll need to 
understand the content and format of the output of the function in a similar way, so that
you can join it with other functions in your workflow. You'll want to make sure 
you understand the main choices that you can modify with the function, through setting
different parameters, as well as the reason behind the defaults that are used for 
those parameters. Finally, ideally you'll want to understand a bit about how the 
function operates to move from the input you give it to the output it gives back to 
you.

> "'Freemon Dyson, a famous physicist, suggested that science moves forward by 
inventing new tools,' Kevin [Kelly, founding editor of Wired magazine] began as
we talked on the phone one morning about tools. 'When we invented the telescope, 
suddenly we had astrophysicists, and astronomy, and we moved forward. The invention
of the microscope opened up the small world of biology to us. In a broad sense, 
science moves forward by inventing tools, because when you have those tools 
they give you a new way of thinking." [@savage2020every]

> "My initial reaction was shock---at both the ingenuity of the solution and the fact
that I'd forgotten all about it---but that quickly resolved into gratitude. I was
grateful to mayself for taking the time a year before to save me this time now.
'Thank you, me-from-the-past!' I literally said to myself." [@savage2020every]

> "Complex equipment breaks, and when it breaks you need the technical expertise to 
fix it, and you need replacement parts. ... Some studies suggest that as much as 95 
percent of medical technology donated to developing countries breaks within the 
first five years of use." [@johnson2011good]

> "Good ideas ... are, inevitably, constrained by the parts and skills that surround 
them. We have a natural tendance to romanticize breathrough innovations, 
imagining momentous ideas transcending their surroundings, a gifted mind somehow
seeing over the detritus of old ideas and ossified tradition. But ideas are works
of bricolage; they're built out of that detritus. We take the ideas we've inherited
or that we've stumbled across, and we jigger them together into some new shape."
[@johnson2011good]

> "Good ideas are not conjured out of thin air; they are built out of a collection 
of existing parts, the composition of which expands (and, occasionally, contracts) 
over time. Some of these parts are conceptual: ways of solving problems, or new 
definitions of what constitutes a problem in the first place. Some of them are, 
literally, mechanical parts." [@johnson2011good]

> "What kind of environment creates good ideas? The simplest way to answer is this: 
innovative environments are better at helping their inhabitants explore the adjacent
possible, because they expose a wide and diverse sample of spare parts---mechanical
or conceptual---and they encourage novel ways of recombining those parts. Environments
that block or limit new combinations---by punishing experimentation, by obscuring 
certain branches of possibility, by making the current state so satisfying that no 
one bothers to explore the edges---will, on average, generate and circulate 
fewer innovations that environments that encourage exploration." [@johnson2011good]

> "Part of coming up with a good idea is discovering what those spare parts are, and 
ensuring that you're not just recycling the same old ingredients. ... The trick to 
having good ideas is not to sit around in glorious isolation and try to think big 
thoughts. The trick is to get more parts on the table." [@johnson2011good]

> "*Document the version of the software that you ran.* This may seem unimportant, but
remember the example from 'Reproducible Research' on page 6 where my colleagues and I 
traced disagreeing results down to a single piece of software being updated. These 
details matter. Good bioinformatices software usually has a command-line option to 
return the current version. Software managed with a version control system such as 
Git has explicit identifiers to every version, which can be used to document the 
precise version you ran... If no version information is available, a release date, 
link to the software, and download date will suffice." [@buffalo2015bioinformatics]


> "*Document when you downloaded data.* It's important to include when the data was downloaded, 
as the external data source (such as a website or server) might change in the future. For example, 
a script that downloads data directly from a database might produce different results if 
rerun after the external database is updated. Consequently, it's important to document
when data came into your repository." [@buffalo2015bioinformatics]

> "All of this [documentation] information is best stored in plain-text README files.
Plain text can easily be read, searched, and edited directly from the command line, 
making it the perfect choice for portable and accessible README files. It's also available
on all computer systems, meaning you can document your steps when working directly on 
a server or computer cluster. Plain text also lacks complex formatting, which can create
issues when copying and pasting commands from your documentation back into the command 
line." [@buffalo2015bioinformatics]

