## Complex data types in experimental data pre-processing

Raw data from many biomedical experiments, especially those that use
high-throughput techniques, can be very large and complex. Because of the scale
and complexity of these data, software for pre-processing the data in R often
uses complex, 'untidy' data formats. While these formats are necessary for
computational efficiency, they add a critical barrier for researchers wishing to
implement reproducibility tools. In this module, we will explain why use of
complex data formats is often necessary within open source pre-processing
software and outline the hurdles created in reproducibility tool use among
laboratory-based scientists.

**Objectives.** After this module, the trainee will be able to:

- Explain why R software for pre-processing biomedical data often stores 
data in complex, 'untidy' formats 
- Describe how these complex data formats can create barriers to 
laboratory-based researchers seeking to use reproducibility tools for 
data pre-processing

### Subsection 1

In previous modules, we have gone into a lot of detail about all of the advantages of the tidyverse approach. However as you work with biomedical data, particularly complex data from complex research equipment like mass spectrometers and Flow cytometers, you may find that it is unreasonable to start with a tiny burst approach from the first steps of pre processing the data. In this module we will explain why the tidyverse approach is currently not appropriate throughout all steps of pre-processing, analysis, and visualization of the types of data that you may collect through a biomedical research experiment. We will present some of the approaches and data storage methods used in passages in the bioconductor project, as well as explain some more about the purpose an approach of bioconductor. This will include an explanation of the more complex structures that are used to store data through many of the packages in by a conductor. These largely leverage a system in our call BF4 object-oriented system. We will cover several of the most popular object classes that are used to store data for bioconductor packages. Any leader module we will explain how early steps in data preprocessing, which use the bioconductor approach and bioconductor data storage objects, can be combined and transferred during it works slow to convert to using a tidyverse approach leader in the workflow when it is appropriate to store data in simpler structures like dataframes.

When you process data using a programming language, there will be different
structures that you can use to store data as you work with it. In other modules, 
we've discussed the "tidyverse" approach to processing data in R---this approach 
emphasizes the dataframe as a way to store data while you're working with it. 
In fact, its use of this data structure for data storage is one of the defining 
features of the "tidyverse" approach. 

Data in R can be stored in a variety of other formats, too. When you are working 
with biological data---in particular, complex or large data output from laboratory
equipment---there can be advantages to using data structures besides dataframes. 
In this section, we'll discuss some of the complex characteristics of biomedical 
data that recommend the use of data structures in R beyond the dataframe. We'll 
also discuss how the use of these other data structures can complicate the use of
"tidyverse" functions and principles that you might learn in beginning R programming
courses and books. In later modules, we'll discuss how to connect your work in R 
to clean and analyze data by performing earlier pre-processing steps using more 
complex data structures and then transferring when possible to dataframes for 
storing data, to allow you to take advantage of the power and ease of the 
"tidyverse" approach as early as possible in your pipeline. 

### Subsection 2

There are two main features of biomedical data---in particular, data collected from 
laboratory equipment like flow cytometers and mass spectrometers---that make it 
useful to use more complex data structures in R in the earlier stages of preprocessing
the data. First, the data are often very large, in some cases so large that it is
difficult [or impossible?] to read them into R. Second, the data might combine 
various elements, each with their own natural structures, that you'd like to keep 
together as you move through the steps of preprocessing the data. 

[Data size]

[Different elements in the data] 
Most laboratory equipment can output a raw data file that you can then read into R. 
For many types of laboratory equipment, these raw data files follow a strict format. 
For example [flow cytometery format...]...

The file formats will often have different pieces of data stored in specific spots. 
For example, the equipment might record not only the measurements taken for the 
sample, but also information about the setting that were applied to the equipment
while the measurements were taken, the date of the measurements, and other metadata
that may be useful to access when preprocessing the data. Each piece of data may 
have different "dimensions". For example, the measurements might provide one 
measurement per metabolite feature or per marker. Some metadata might also be
provided with these dimensions (e.g., metadata about the markers for flow 
cytometry data), but other metadata might be provided a single time per sample
or even per experiment---for example, the settings on the equipment when the 
sample or samples were run. 

When it comes to data structures, dataframes and other two-dimensional data storage
structures (you can visualize these as similar to the format of data in a spreadsheet, 
with rows and columns) work well to store data where all data conform to a common 
dimension. For example, a dataframe would work well to store the measurements 
for each marker in each sample in a flow cytometry experiment. In this case, 
each column could store the values for a specific marker and each row could 
provide measurements for a sample. In this way, you could read the measurements
for one marker across all samples by reading down a column, or read the measurements
across all markers for one sample by reading across a row. 

When you have data that doesn't conform to these common dimensions [unit of
measurement?] however, a dataframe may work poorly to store the data. For
example, if you have measurements taken at the level of the equipment settings
for the whole experiment, these don't naturally fit into the dataframe format.
In the "tidyverse" approach, one approach to handling data with different units
of measurement is to store data for each unit of measurement in a different
dataframe and to include identifiers that can be used to link data across the
dataframes. More common, however, in R extensions for preprocessing biomedical
data is to use more complex data structures that can store data with different
units of measurement in different slots within the data structure, and use these
in conjunction with specific functions that are built to work with that specific
data structure, and so know where to find each element within the data
structure.

### Subsection 3

### Practice quiz

